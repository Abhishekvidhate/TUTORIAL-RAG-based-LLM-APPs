{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "9422852399274b0ab3f88afc09191ab0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_045096e350284c388be71245942d41ca",
              "IPY_MODEL_33f10b1ac46e44a4b8378cf978c2236d",
              "IPY_MODEL_5c9ae65d92694bf29180c083d31e5a86"
            ],
            "layout": "IPY_MODEL_8cf7aa130e8b4c4c80e354e948916cca"
          }
        },
        "045096e350284c388be71245942d41ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5f2b30a5d8f04f9faa47393089573690",
            "placeholder": "​",
            "style": "IPY_MODEL_1e2f12b3e87b4c5e9d313b7243dc1ed6",
            "value": "modules.json: 100%"
          }
        },
        "33f10b1ac46e44a4b8378cf978c2236d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a5661a1dd2214d47b666796534571305",
            "max": 349,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_11b3603c6d4a49908e4eca9ee195db76",
            "value": 349
          }
        },
        "5c9ae65d92694bf29180c083d31e5a86": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_df130ab37c9d4a44b82f299010aba689",
            "placeholder": "​",
            "style": "IPY_MODEL_7e69d70ef12742358dd9dd913c12759c",
            "value": " 349/349 [00:00&lt;00:00, 20.4kB/s]"
          }
        },
        "8cf7aa130e8b4c4c80e354e948916cca": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5f2b30a5d8f04f9faa47393089573690": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1e2f12b3e87b4c5e9d313b7243dc1ed6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a5661a1dd2214d47b666796534571305": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "11b3603c6d4a49908e4eca9ee195db76": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "df130ab37c9d4a44b82f299010aba689": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7e69d70ef12742358dd9dd913c12759c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4759a41091b3482e8f658290d879d2b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_70a9548f2b504dc0bf865c117576a420",
              "IPY_MODEL_01815fd0763c471a943e99703fc179ff",
              "IPY_MODEL_ba045ec626134d69a7b1d0f55c66032e"
            ],
            "layout": "IPY_MODEL_557abb5bb1394efe8556b9baaa468564"
          }
        },
        "70a9548f2b504dc0bf865c117576a420": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fc47e61786714ef2bcdad497f4d8c804",
            "placeholder": "​",
            "style": "IPY_MODEL_49036fc5d39e45e59aa9d89cca64244d",
            "value": "config_sentence_transformers.json: 100%"
          }
        },
        "01815fd0763c471a943e99703fc179ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ea3bd93d49b24523b9aa6504d2e17e65",
            "max": 116,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_32704d828ed54d7ba12f51275d64146f",
            "value": 116
          }
        },
        "ba045ec626134d69a7b1d0f55c66032e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d65131df864a4a318b829541a88e0234",
            "placeholder": "​",
            "style": "IPY_MODEL_e8621e6ef355424ca23e78e8bbc7078c",
            "value": " 116/116 [00:00&lt;00:00, 6.94kB/s]"
          }
        },
        "557abb5bb1394efe8556b9baaa468564": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fc47e61786714ef2bcdad497f4d8c804": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "49036fc5d39e45e59aa9d89cca64244d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ea3bd93d49b24523b9aa6504d2e17e65": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "32704d828ed54d7ba12f51275d64146f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d65131df864a4a318b829541a88e0234": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e8621e6ef355424ca23e78e8bbc7078c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7a4abc76695c411d9f608d1ce57c878f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a598cd188aff4e1da09238269ab9af3a",
              "IPY_MODEL_8416306021fd4116b5348afffa1e42a5",
              "IPY_MODEL_14a73de9dfcd4c19a21889df03142e29"
            ],
            "layout": "IPY_MODEL_2b6ae5e793d44a539e641321319e963e"
          }
        },
        "a598cd188aff4e1da09238269ab9af3a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_830513963e294d43880736f89f035c55",
            "placeholder": "​",
            "style": "IPY_MODEL_787caffddbf043efa19e28e20b5583e8",
            "value": "README.md: 100%"
          }
        },
        "8416306021fd4116b5348afffa1e42a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_59c1658884594a4ea34e73d1930dbfd0",
            "max": 10659,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a8aef2cf48fe4867a6fc976d6291a244",
            "value": 10659
          }
        },
        "14a73de9dfcd4c19a21889df03142e29": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e0869e6d8a67450f9fbb508f0e3fe853",
            "placeholder": "​",
            "style": "IPY_MODEL_c91a86233e1f410d8b38da1f9af8d866",
            "value": " 10.7k/10.7k [00:00&lt;00:00, 599kB/s]"
          }
        },
        "2b6ae5e793d44a539e641321319e963e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "830513963e294d43880736f89f035c55": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "787caffddbf043efa19e28e20b5583e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "59c1658884594a4ea34e73d1930dbfd0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a8aef2cf48fe4867a6fc976d6291a244": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e0869e6d8a67450f9fbb508f0e3fe853": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c91a86233e1f410d8b38da1f9af8d866": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ff3def560e034184850d354ae0799feb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_71d5bf03568146b7ac9d4eae820e6df6",
              "IPY_MODEL_cf7e80d4cab94249bd10958d7ae0399b",
              "IPY_MODEL_026ae44220a643d0aa2cde1264dee832"
            ],
            "layout": "IPY_MODEL_37f00bfb116a4c7e9d4ae45b1d1b36dc"
          }
        },
        "71d5bf03568146b7ac9d4eae820e6df6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a0a8b60b43bd4d82828e0606644b34a1",
            "placeholder": "​",
            "style": "IPY_MODEL_2df34c9b387a47529d940540222036a7",
            "value": "sentence_bert_config.json: 100%"
          }
        },
        "cf7e80d4cab94249bd10958d7ae0399b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_153a35e905174de887c869b394065649",
            "max": 53,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3a678dfdb2e0438ba10148f0a244493e",
            "value": 53
          }
        },
        "026ae44220a643d0aa2cde1264dee832": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_143cd328d10a4662a8cf31be11c0bbc8",
            "placeholder": "​",
            "style": "IPY_MODEL_66c449a1f3c34852847f32167e46285b",
            "value": " 53.0/53.0 [00:00&lt;00:00, 2.39kB/s]"
          }
        },
        "37f00bfb116a4c7e9d4ae45b1d1b36dc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a0a8b60b43bd4d82828e0606644b34a1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2df34c9b387a47529d940540222036a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "153a35e905174de887c869b394065649": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3a678dfdb2e0438ba10148f0a244493e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "143cd328d10a4662a8cf31be11c0bbc8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "66c449a1f3c34852847f32167e46285b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5ff989c9349b4ad3a2e3db9e6483c8d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_734a2e96ab6f4883974bd7ca7a7d17f6",
              "IPY_MODEL_94cf1170c0f64755947be9b99fb87bb8",
              "IPY_MODEL_7ff7bc1486aa486284fce7b20a1e0b8d"
            ],
            "layout": "IPY_MODEL_153b05f1621445b7904bd0de6156f886"
          }
        },
        "734a2e96ab6f4883974bd7ca7a7d17f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fbbeef11473b4306a5751326a15ae3e1",
            "placeholder": "​",
            "style": "IPY_MODEL_31c7116aaf58461f8381dd6d59d7a7fc",
            "value": "config.json: 100%"
          }
        },
        "94cf1170c0f64755947be9b99fb87bb8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_79e6f6690d3f480084d70a90f9ed13a9",
            "max": 612,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_378c23a271b240e1a0e115eb9a2f8865",
            "value": 612
          }
        },
        "7ff7bc1486aa486284fce7b20a1e0b8d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4a6f0c76eef744b3844993b6599d780a",
            "placeholder": "​",
            "style": "IPY_MODEL_98c018357dd74ed0b8046da1bde1c206",
            "value": " 612/612 [00:00&lt;00:00, 32.7kB/s]"
          }
        },
        "153b05f1621445b7904bd0de6156f886": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fbbeef11473b4306a5751326a15ae3e1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "31c7116aaf58461f8381dd6d59d7a7fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "79e6f6690d3f480084d70a90f9ed13a9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "378c23a271b240e1a0e115eb9a2f8865": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4a6f0c76eef744b3844993b6599d780a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "98c018357dd74ed0b8046da1bde1c206": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d928c5dbf1c64b838602c5aff0b43f3e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a51e4c50b0b4491984ea5b25fbc06a2e",
              "IPY_MODEL_5e435c2cab2849c097d9d22dc7933330",
              "IPY_MODEL_8a1d173c73914106b737fda2cbf00c7c"
            ],
            "layout": "IPY_MODEL_d3efb3de822c428b9c8d491b865c3a2f"
          }
        },
        "a51e4c50b0b4491984ea5b25fbc06a2e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7c649407cfdb44b496d42fb880713c44",
            "placeholder": "​",
            "style": "IPY_MODEL_81fbfc49ef244086a539e19533c5b4eb",
            "value": "model.safetensors: 100%"
          }
        },
        "5e435c2cab2849c097d9d22dc7933330": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4f69eedf70ee4c88aaef12b977b63239",
            "max": 90868376,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_18a45983efbe4281bf6eacacbc553281",
            "value": 90868376
          }
        },
        "8a1d173c73914106b737fda2cbf00c7c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8ae899bd6f8d4c8698587b4cacf1fd7c",
            "placeholder": "​",
            "style": "IPY_MODEL_50247d0985044ac2896e9e4d3030dd9b",
            "value": " 90.9M/90.9M [00:00&lt;00:00, 181MB/s]"
          }
        },
        "d3efb3de822c428b9c8d491b865c3a2f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7c649407cfdb44b496d42fb880713c44": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "81fbfc49ef244086a539e19533c5b4eb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4f69eedf70ee4c88aaef12b977b63239": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "18a45983efbe4281bf6eacacbc553281": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8ae899bd6f8d4c8698587b4cacf1fd7c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "50247d0985044ac2896e9e4d3030dd9b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7b5d38b25802462eb227ecfb71c1f4dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_db04bd7f08f142bf9da562740d078552",
              "IPY_MODEL_44aafa4eae264500ba4294cc493ad4d3",
              "IPY_MODEL_630d3cc4b759497991212f6ba32263c2"
            ],
            "layout": "IPY_MODEL_91a29a80885c4949a9fa47aa2a3a4f45"
          }
        },
        "db04bd7f08f142bf9da562740d078552": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_78409fbc6b824ef8b5dcfb46a61dbf5c",
            "placeholder": "​",
            "style": "IPY_MODEL_2d4372f0d2ea475eb3798e5f6a6575cd",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "44aafa4eae264500ba4294cc493ad4d3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c97174c06e41447da48ad90e9cfc353a",
            "max": 350,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_cba8d805f22f47839f24f4b7e41cfeef",
            "value": 350
          }
        },
        "630d3cc4b759497991212f6ba32263c2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8e1e4a90ca754a22ac438e5dba5be0b5",
            "placeholder": "​",
            "style": "IPY_MODEL_5cfaef55e82f47d3a1dc4b2503fdd7b5",
            "value": " 350/350 [00:00&lt;00:00, 21.0kB/s]"
          }
        },
        "91a29a80885c4949a9fa47aa2a3a4f45": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "78409fbc6b824ef8b5dcfb46a61dbf5c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2d4372f0d2ea475eb3798e5f6a6575cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c97174c06e41447da48ad90e9cfc353a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cba8d805f22f47839f24f4b7e41cfeef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8e1e4a90ca754a22ac438e5dba5be0b5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5cfaef55e82f47d3a1dc4b2503fdd7b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2c4cee6d1e0648d4923eae9d20eaa6b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8e51bc0701d24e8f9cb0a49d9ffc7609",
              "IPY_MODEL_32c071f25d7d4857b1e057e31deedcf5",
              "IPY_MODEL_c572d50efeb54dbd9baad56b3d3f8805"
            ],
            "layout": "IPY_MODEL_e708ec7e8a9743c8b024af1e73dc9c7e"
          }
        },
        "8e51bc0701d24e8f9cb0a49d9ffc7609": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_332aa522df834523addd91bcb53d95b0",
            "placeholder": "​",
            "style": "IPY_MODEL_b7aff9c2fc5e4e3a971f350f28e8836b",
            "value": "vocab.txt: 100%"
          }
        },
        "32c071f25d7d4857b1e057e31deedcf5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ac1f25287db642f5910c4328fb291502",
            "max": 231508,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c5dbe2e501c34a03ad84e12943983b12",
            "value": 231508
          }
        },
        "c572d50efeb54dbd9baad56b3d3f8805": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_785a376ec50d42e7a0d074761163d45d",
            "placeholder": "​",
            "style": "IPY_MODEL_ef889e8d50be47b1807d54b8e858a4e4",
            "value": " 232k/232k [00:00&lt;00:00, 1.40MB/s]"
          }
        },
        "e708ec7e8a9743c8b024af1e73dc9c7e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "332aa522df834523addd91bcb53d95b0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b7aff9c2fc5e4e3a971f350f28e8836b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ac1f25287db642f5910c4328fb291502": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c5dbe2e501c34a03ad84e12943983b12": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "785a376ec50d42e7a0d074761163d45d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ef889e8d50be47b1807d54b8e858a4e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d1a2bc2820cb400eb73a7135d6fecc87": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_985fc03c885b4741b488933edf870d71",
              "IPY_MODEL_a3ab0162e401455790a5e637f2b32efb",
              "IPY_MODEL_29233d16a5484357869f80ae6da46971"
            ],
            "layout": "IPY_MODEL_039ae667277d404e927831cc38dac525"
          }
        },
        "985fc03c885b4741b488933edf870d71": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7b26c27bffcd49279c272fac96a3d0ec",
            "placeholder": "​",
            "style": "IPY_MODEL_9b70409d3ada4c2ba7593f4077ffdf46",
            "value": "tokenizer.json: 100%"
          }
        },
        "a3ab0162e401455790a5e637f2b32efb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_29ad6ee6914e4430807dba947ce54fac",
            "max": 466247,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3dd674deed71418b9f42ecf392b1b13c",
            "value": 466247
          }
        },
        "29233d16a5484357869f80ae6da46971": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_727b520c38d74abda6d97fe621da4169",
            "placeholder": "​",
            "style": "IPY_MODEL_7562580557db4751870ebb687f651b80",
            "value": " 466k/466k [00:00&lt;00:00, 1.90MB/s]"
          }
        },
        "039ae667277d404e927831cc38dac525": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7b26c27bffcd49279c272fac96a3d0ec": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9b70409d3ada4c2ba7593f4077ffdf46": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "29ad6ee6914e4430807dba947ce54fac": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3dd674deed71418b9f42ecf392b1b13c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "727b520c38d74abda6d97fe621da4169": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7562580557db4751870ebb687f651b80": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "55d0c55407914b53a2cb5c09463e45db": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_375cb2731f184e09a8595ce7c7d8e101",
              "IPY_MODEL_4240dc265886403997c29389b1fa9b72",
              "IPY_MODEL_72a641755ceb4726aa718cceb43e891a"
            ],
            "layout": "IPY_MODEL_85908de9d0d04570a8d66713c68e7d6a"
          }
        },
        "375cb2731f184e09a8595ce7c7d8e101": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_357603c901364107beac27a22869620c",
            "placeholder": "​",
            "style": "IPY_MODEL_e02c2dabe6cc4267963d23e9d296b215",
            "value": "special_tokens_map.json: 100%"
          }
        },
        "4240dc265886403997c29389b1fa9b72": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b2d2c1d4ca004772aabbb5e8bccf7f6d",
            "max": 112,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a4088effa87e49d78baedc17c68de025",
            "value": 112
          }
        },
        "72a641755ceb4726aa718cceb43e891a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_afabba6d15814d9d991e51c974a19c5a",
            "placeholder": "​",
            "style": "IPY_MODEL_57999e243240438987df3a2f85b7b7cf",
            "value": " 112/112 [00:00&lt;00:00, 4.21kB/s]"
          }
        },
        "85908de9d0d04570a8d66713c68e7d6a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "357603c901364107beac27a22869620c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e02c2dabe6cc4267963d23e9d296b215": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b2d2c1d4ca004772aabbb5e8bccf7f6d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a4088effa87e49d78baedc17c68de025": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "afabba6d15814d9d991e51c974a19c5a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "57999e243240438987df3a2f85b7b7cf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fa107784588f4450bd109042bd8711c2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c469621996c34b15b5b32234c704953d",
              "IPY_MODEL_bca463af7cd54287a0fa85da88aae812",
              "IPY_MODEL_b1c4b6c7692f42f39cba29e1533f0e97"
            ],
            "layout": "IPY_MODEL_9a73f8a61cbe43349de00f5ef74c97c3"
          }
        },
        "c469621996c34b15b5b32234c704953d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_32b5d143aee7400ebf767b9693f47611",
            "placeholder": "​",
            "style": "IPY_MODEL_6794b3eaa2944f23ac41afca5aaf796a",
            "value": "1_Pooling/config.json: 100%"
          }
        },
        "bca463af7cd54287a0fa85da88aae812": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_baaa8ecb7f32400087b928f28431b4a9",
            "max": 190,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_503f8e4c93a841e29a055b5a0e7a7eed",
            "value": 190
          }
        },
        "b1c4b6c7692f42f39cba29e1533f0e97": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5c52c8c938e2468a8345e7885c3a6c91",
            "placeholder": "​",
            "style": "IPY_MODEL_a6c28245801a409ab1c064e6d7f4d71a",
            "value": " 190/190 [00:00&lt;00:00, 8.16kB/s]"
          }
        },
        "9a73f8a61cbe43349de00f5ef74c97c3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "32b5d143aee7400ebf767b9693f47611": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6794b3eaa2944f23ac41afca5aaf796a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "baaa8ecb7f32400087b928f28431b4a9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "503f8e4c93a841e29a055b5a0e7a7eed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5c52c8c938e2468a8345e7885c3a6c91": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a6c28245801a409ab1c064e6d7f4d71a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **what is RAG?**\n",
        "\n",
        "RAG is a technique for augmenting LLM knowledge with additional data.\n",
        "\n",
        "\n",
        "LLMs can reason about wide-ranging topics, but their knowledge is limited to the public data up to a specific point in time that they were trained on. If you want to build AI applications that can reason about private data or data introduced after a model's cutoff date, you need to augment the knowledge of the model with the specific information it needs. The process of bringing the appropriate information and inserting it into the model prompt is known as Retrieval Augmented Generation (RAG)"
      ],
      "metadata": {
        "id": "Qh-H28xi7iCW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Installing all dependencies**"
      ],
      "metadata": {
        "id": "qNHcy-5az39b"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "5bqCct4KsTK_",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "87878bd0-b173-4761-a533-793bb065b4c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain\n",
            "  Downloading langchain-0.2.1-py3-none-any.whl (973 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/973.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m256.0/973.5 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━\u001b[0m \u001b[32m778.2/973.5 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m973.5/973.5 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.30)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.9.5)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Requirement already satisfied: langchain-core<0.3.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.2.3)\n",
            "Collecting langchain-text-splitters<0.3.0,>=0.2.0 (from langchain)\n",
            "  Downloading langchain_text_splitters-0.2.0-py3-none-any.whl (23 kB)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.1.67)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.25.2)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.7.1)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.3.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.0->langchain) (1.33)\n",
            "Requirement already satisfied: packaging<24.0,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.0->langchain) (23.2)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.3)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (2.18.2)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2024.2.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.0->langchain) (2.4)\n",
            "Installing collected packages: langchain-text-splitters, langchain\n",
            "Successfully installed langchain-0.2.1 langchain-text-splitters-0.2.0\n",
            "Collecting langchain-groq\n",
            "  Downloading langchain_groq-0.1.4-py3-none-any.whl (11 kB)\n",
            "Collecting groq<1,>=0.4.1 (from langchain-groq)\n",
            "  Downloading groq-0.8.0-py3-none-any.whl (105 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.4/105.4 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: langchain-core<0.3,>=0.1.45 in /usr/local/lib/python3.10/dist-packages (from langchain-groq) (0.2.3)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from groq<1,>=0.4.1->langchain-groq) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from groq<1,>=0.4.1->langchain-groq) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from groq<1,>=0.4.1->langchain-groq)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from groq<1,>=0.4.1->langchain-groq) (2.7.1)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from groq<1,>=0.4.1->langchain-groq) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from groq<1,>=0.4.1->langchain-groq) (4.11.0)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3,>=0.1.45->langchain-groq) (6.0.1)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3,>=0.1.45->langchain-groq) (1.33)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.65 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3,>=0.1.45->langchain-groq) (0.1.67)\n",
            "Requirement already satisfied: packaging<24.0,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3,>=0.1.45->langchain-groq) (23.2)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3,>=0.1.45->langchain-groq) (8.3.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->groq<1,>=0.4.1->langchain-groq) (3.7)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->groq<1,>=0.4.1->langchain-groq) (1.2.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain-groq) (2024.2.2)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain-groq)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain-groq)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3,>=0.1.45->langchain-groq) (2.4)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.65->langchain-core<0.3,>=0.1.45->langchain-groq) (3.10.3)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.65->langchain-core<0.3,>=0.1.45->langchain-groq) (2.31.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->groq<1,>=0.4.1->langchain-groq) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->groq<1,>=0.4.1->langchain-groq) (2.18.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.65->langchain-core<0.3,>=0.1.45->langchain-groq) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.65->langchain-core<0.3,>=0.1.45->langchain-groq) (2.0.7)\n",
            "Installing collected packages: h11, httpcore, httpx, groq, langchain-groq\n",
            "Successfully installed groq-0.8.0 h11-0.14.0 httpcore-1.0.5 httpx-0.27.0 langchain-groq-0.1.4\n",
            "Collecting langchain-community\n",
            "  Downloading langchain_community-0.2.1-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (2.0.30)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (3.9.5)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n",
            "  Downloading dataclasses_json-0.6.6-py3-none-any.whl (28 kB)\n",
            "Requirement already satisfied: langchain<0.3.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.2.1)\n",
            "Requirement already satisfied: langchain-core<0.3.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.2.3)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.1.67)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (1.25.2)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (8.3.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (4.0.3)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading marshmallow-3.21.2-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from langchain<0.3.0,>=0.2.0->langchain-community) (0.2.0)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain<0.3.0,>=0.2.0->langchain-community) (2.7.1)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.0->langchain-community) (1.33)\n",
            "Requirement already satisfied: packaging<24.0,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.0->langchain-community) (23.2)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.0->langchain-community) (3.10.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (2024.2.2)\n",
            "Requirement already satisfied: typing-extensions>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (4.11.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.0.3)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.0->langchain-community) (2.4)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.0->langchain-community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.0->langchain-community) (2.18.2)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: mypy-extensions, marshmallow, typing-inspect, dataclasses-json, langchain-community\n",
            "Successfully installed dataclasses-json-0.6.6 langchain-community-0.2.1 marshmallow-3.21.2 mypy-extensions-1.0.0 typing-inspect-0.9.0\n",
            "Requirement already satisfied: langchain-core in /usr/local/lib/python3.10/dist-packages (0.2.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-core) (6.0.1)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core) (1.33)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.65 in /usr/local/lib/python3.10/dist-packages (from langchain-core) (0.1.67)\n",
            "Requirement already satisfied: packaging<24.0,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core) (23.2)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain-core) (2.7.1)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-core) (8.3.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core) (2.4)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.65->langchain-core) (3.10.3)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.65->langchain-core) (2.31.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain-core) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain-core) (2.18.2)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain-core) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.65->langchain-core) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.65->langchain-core) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.65->langchain-core) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.65->langchain-core) (2024.2.2)\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement langchain-text-splitter (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for langchain-text-splitter\u001b[0m\u001b[31m\n",
            "\u001b[0mCollecting langchain-astradb\n",
            "  Downloading langchain_astradb-0.3.3-py3-none-any.whl (27 kB)\n",
            "Collecting astrapy<2.0,>=1.2 (from langchain-astradb)\n",
            "  Downloading astrapy-1.2.0-py3-none-any.whl (139 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.6/139.6 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: langchain-core<0.3,>=0.1.31 in /usr/local/lib/python3.10/dist-packages (from langchain-astradb) (0.2.3)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain-astradb) (1.25.2)\n",
            "Collecting bson<0.6.0,>=0.5.10 (from astrapy<2.0,>=1.2->langchain-astradb)\n",
            "  Downloading bson-0.5.10.tar.gz (10 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting cassio<0.2.0,>=0.1.4 (from astrapy<2.0,>=1.2->langchain-astradb)\n",
            "  Downloading cassio-0.1.7-py3-none-any.whl (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.9/44.9 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting deprecation<2.2.0,>=2.1.0 (from astrapy<2.0,>=1.2->langchain-astradb)\n",
            "  Downloading deprecation-2.1.0-py2.py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: httpx[http2]<1,>=0.25.2 in /usr/local/lib/python3.10/dist-packages (from astrapy<2.0,>=1.2->langchain-astradb) (0.27.0)\n",
            "Requirement already satisfied: toml<0.11.0,>=0.10.2 in /usr/local/lib/python3.10/dist-packages (from astrapy<2.0,>=1.2->langchain-astradb) (0.10.2)\n",
            "Collecting uuid6<2024.2.0,>=2024.1.12 (from astrapy<2.0,>=1.2->langchain-astradb)\n",
            "  Downloading uuid6-2024.1.12-py3-none-any.whl (6.4 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3,>=0.1.31->langchain-astradb) (6.0.1)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3,>=0.1.31->langchain-astradb) (1.33)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.65 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3,>=0.1.31->langchain-astradb) (0.1.67)\n",
            "Requirement already satisfied: packaging<24.0,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3,>=0.1.31->langchain-astradb) (23.2)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3,>=0.1.31->langchain-astradb) (2.7.1)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3,>=0.1.31->langchain-astradb) (8.3.0)\n",
            "Requirement already satisfied: python-dateutil>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from bson<0.6.0,>=0.5.10->astrapy<2.0,>=1.2->langchain-astradb) (2.8.2)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from bson<0.6.0,>=0.5.10->astrapy<2.0,>=1.2->langchain-astradb) (1.16.0)\n",
            "Collecting cassandra-driver<4.0.0,>=3.28.0 (from cassio<0.2.0,>=0.1.4->astrapy<2.0,>=1.2->langchain-astradb)\n",
            "  Downloading cassandra_driver-3.29.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.9/18.9 MB\u001b[0m \u001b[31m37.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests<3.0.0,>=2.31.0 in /usr/local/lib/python3.10/dist-packages (from cassio<0.2.0,>=0.1.4->astrapy<2.0,>=1.2->langchain-astradb) (2.31.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx[http2]<1,>=0.25.2->astrapy<2.0,>=1.2->langchain-astradb) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx[http2]<1,>=0.25.2->astrapy<2.0,>=1.2->langchain-astradb) (2024.2.2)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx[http2]<1,>=0.25.2->astrapy<2.0,>=1.2->langchain-astradb) (1.0.5)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx[http2]<1,>=0.25.2->astrapy<2.0,>=1.2->langchain-astradb) (3.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx[http2]<1,>=0.25.2->astrapy<2.0,>=1.2->langchain-astradb) (1.3.1)\n",
            "Collecting h2<5,>=3 (from httpx[http2]<1,>=0.25.2->astrapy<2.0,>=1.2->langchain-astradb)\n",
            "  Downloading h2-4.1.0-py3-none-any.whl (57 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx[http2]<1,>=0.25.2->astrapy<2.0,>=1.2->langchain-astradb) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3,>=0.1.31->langchain-astradb) (2.4)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.65->langchain-core<0.3,>=0.1.31->langchain-astradb) (3.10.3)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain-core<0.3,>=0.1.31->langchain-astradb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain-core<0.3,>=0.1.31->langchain-astradb) (2.18.2)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain-core<0.3,>=0.1.31->langchain-astradb) (4.11.0)\n",
            "Collecting geomet<0.3,>=0.1 (from cassandra-driver<4.0.0,>=3.28.0->cassio<0.2.0,>=0.1.4->astrapy<2.0,>=1.2->langchain-astradb)\n",
            "  Downloading geomet-0.2.1.post1-py3-none-any.whl (18 kB)\n",
            "Collecting hyperframe<7,>=6.0 (from h2<5,>=3->httpx[http2]<1,>=0.25.2->astrapy<2.0,>=1.2->langchain-astradb)\n",
            "  Downloading hyperframe-6.0.1-py3-none-any.whl (12 kB)\n",
            "Collecting hpack<5,>=4.0 (from h2<5,>=3->httpx[http2]<1,>=0.25.2->astrapy<2.0,>=1.2->langchain-astradb)\n",
            "  Downloading hpack-4.0.0-py3-none-any.whl (32 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.31.0->cassio<0.2.0,>=0.1.4->astrapy<2.0,>=1.2->langchain-astradb) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.31.0->cassio<0.2.0,>=0.1.4->astrapy<2.0,>=1.2->langchain-astradb) (2.0.7)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx[http2]<1,>=0.25.2->astrapy<2.0,>=1.2->langchain-astradb) (1.2.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from geomet<0.3,>=0.1->cassandra-driver<4.0.0,>=3.28.0->cassio<0.2.0,>=0.1.4->astrapy<2.0,>=1.2->langchain-astradb) (8.1.7)\n",
            "Building wheels for collected packages: bson\n",
            "  Building wheel for bson (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for bson: filename=bson-0.5.10-py3-none-any.whl size=11976 sha256=ce393b567dd2237d8a04c04de7daa82304f464b85dbc581f475f6e4a83e1d1e9\n",
            "  Stored in directory: /root/.cache/pip/wheels/36/49/3b/8b33954dfae7a176009c4d721a45af56c8a9c1cdc3ee947945\n",
            "Successfully built bson\n",
            "Installing collected packages: uuid6, hyperframe, hpack, geomet, deprecation, h2, cassandra-driver, bson, cassio, astrapy, langchain-astradb\n",
            "Successfully installed astrapy-1.2.0 bson-0.5.10 cassandra-driver-3.29.1 cassio-0.1.7 deprecation-2.1.0 geomet-0.2.1.post1 h2-4.1.0 hpack-4.0.0 hyperframe-6.0.1 langchain-astradb-0.3.3 uuid6-2024.1.12\n",
            "Collecting langchainhub\n",
            "  Downloading langchainhub-0.1.17-py3-none-any.whl (4.8 kB)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchainhub) (2.31.0)\n",
            "Collecting types-requests<3.0.0.0,>=2.31.0.2 (from langchainhub)\n",
            "  Downloading types_requests-2.32.0.20240602-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchainhub) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchainhub) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchainhub) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchainhub) (2024.2.2)\n",
            "Installing collected packages: types-requests, langchainhub\n",
            "Successfully installed langchainhub-0.1.17 types-requests-2.32.0.20240602\n",
            "Collecting sentence_transformers\n",
            "  Downloading sentence_transformers-3.0.0-py3-none-any.whl (224 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.7/224.7 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: transformers<5.0.0,>=4.34.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.41.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.66.4)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (2.3.0+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.25.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.11.4)\n",
            "Requirement already satisfied: huggingface-hub>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (0.23.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (9.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (3.14.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (2023.6.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (6.0.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (2.31.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (3.1.4)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.11.0->sentence_transformers)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.11.0->sentence_transformers)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.11.0->sentence_transformers)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.11.0->sentence_transformers)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.11.0->sentence_transformers)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.11.0->sentence_transformers)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.11.0->sentence_transformers)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.11.0->sentence_transformers)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.11.0->sentence_transformers)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch>=1.11.0->sentence_transformers)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.11.0->sentence_transformers)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (2.3.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.11.0->sentence_transformers)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.40-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m54.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence_transformers) (2024.5.15)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence_transformers) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence_transformers) (0.4.3)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers) (3.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence_transformers) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->sentence_transformers) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, sentence_transformers\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.40 nvidia-nvtx-cu12-12.1.105 sentence_transformers-3.0.0\n",
            "Collecting Unstructured\n",
            "  Downloading unstructured-0.14.3-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: chardet in /usr/local/lib/python3.10/dist-packages (from Unstructured) (5.2.0)\n",
            "Collecting filetype (from Unstructured)\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Collecting python-magic (from Unstructured)\n",
            "  Downloading python_magic-0.4.27-py2.py3-none-any.whl (13 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from Unstructured) (4.9.4)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from Unstructured) (3.8.1)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from Unstructured) (0.9.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from Unstructured) (2.31.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from Unstructured) (4.12.3)\n",
            "Collecting emoji (from Unstructured)\n",
            "  Downloading emoji-2.12.1-py3-none-any.whl (431 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m431.4/431.4 kB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: dataclasses-json in /usr/local/lib/python3.10/dist-packages (from Unstructured) (0.6.6)\n",
            "Collecting python-iso639 (from Unstructured)\n",
            "  Downloading python_iso639-2024.4.27-py3-none-any.whl (274 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m274.7/274.7 kB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langdetect (from Unstructured)\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m26.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from Unstructured) (1.25.2)\n",
            "Collecting rapidfuzz (from Unstructured)\n",
            "  Downloading rapidfuzz-3.9.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m38.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting backoff (from Unstructured)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from Unstructured) (4.11.0)\n",
            "Collecting unstructured-client (from Unstructured)\n",
            "  Downloading unstructured_client-0.22.0-py3-none-any.whl (28 kB)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from Unstructured) (1.14.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->Unstructured) (2.5)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json->Unstructured) (3.21.2)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json->Unstructured) (0.9.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from langdetect->Unstructured) (1.16.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->Unstructured) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->Unstructured) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->Unstructured) (2024.5.15)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk->Unstructured) (4.66.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->Unstructured) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->Unstructured) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->Unstructured) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->Unstructured) (2024.2.2)\n",
            "Collecting deepdiff>=6.0 (from unstructured-client->Unstructured)\n",
            "  Downloading deepdiff-7.0.1-py3-none-any.whl (80 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m80.8/80.8 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jsonpath-python>=1.0.6 (from unstructured-client->Unstructured)\n",
            "  Downloading jsonpath_python-1.0.6-py3-none-any.whl (7.6 kB)\n",
            "Requirement already satisfied: mypy-extensions>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from unstructured-client->Unstructured) (1.0.0)\n",
            "Requirement already satisfied: packaging>=23.1 in /usr/local/lib/python3.10/dist-packages (from unstructured-client->Unstructured) (23.2)\n",
            "Collecting pypdf>=4.0 (from unstructured-client->Unstructured)\n",
            "  Downloading pypdf-4.2.0-py3-none-any.whl (290 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m290.4/290.4 kB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from unstructured-client->Unstructured) (2.8.2)\n",
            "Collecting ordered-set<4.2.0,>=4.1.0 (from deepdiff>=6.0->unstructured-client->Unstructured)\n",
            "  Downloading ordered_set-4.1.0-py3-none-any.whl (7.6 kB)\n",
            "Building wheels for collected packages: langdetect\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993227 sha256=75f0aec32695d9d532e2edff702f13cfa3f97ba6c1037256a23a78b4252ce2b8\n",
            "  Stored in directory: /root/.cache/pip/wheels/95/03/7d/59ea870c70ce4e5a370638b5462a7711ab78fba2f655d05106\n",
            "Successfully built langdetect\n",
            "Installing collected packages: filetype, rapidfuzz, python-magic, python-iso639, pypdf, ordered-set, langdetect, jsonpath-python, emoji, backoff, deepdiff, unstructured-client, Unstructured\n",
            "Successfully installed Unstructured-0.14.3 backoff-2.2.1 deepdiff-7.0.1 emoji-2.12.1 filetype-1.2.0 jsonpath-python-1.0.6 langdetect-1.0.9 ordered-set-4.1.0 pypdf-4.2.0 python-iso639-2024.4.27 python-magic-0.4.27 rapidfuzz-3.9.3 unstructured-client-0.22.0\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain # to build LLM APPs easily\n",
        "!pip install langchain-groq # GROQ-Langchain integration for using LPU of GROQ\n",
        "!pip install langchain-community # 3rd party integration with langchain\n",
        "!pip install langchain-core # to parse output of LLM i.e give proper structured output to us or human readable\n",
        "!pip install langchain-text-splitter # Splitting & chunkinf\n",
        "!pip install langchain-astradb # VectorDB/store\n",
        "!pip install langchainhub\n",
        "\n",
        "!pip install sentence_transformers # huggingface's models(text embedding models)\n",
        "!pip install Unstructured # for loading data via markdown"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain-core"
      ],
      "metadata": {
        "id": "wj_rwZ1xtml6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **load environment variables**"
      ],
      "metadata": {
        "id": "aJifujlOy94G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import getpass\n",
        "import os\n",
        "\n",
        "os.environ['HUGGINGFACEHUB_API_TOKEN'] = getpass.getpass('HUGGINGFACEHUB_API_TOKEN')\n",
        "os.environ['GROQ_API_KEY'] = getpass.getpass('GROQ_API_KEY')\n",
        "os.environ['LANGCHAIN_API_KEY'] = getpass.getpass('LANGCHAIN_API_KEY')\n",
        "# etc api keys"
      ],
      "metadata": {
        "id": "jZHIqXCys5Sk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "86caf5fb-b1be-4910-cd72-12d28ddfa05a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "HUGGINGFACEHUB_API_TOKEN··········\n",
            "GROQ_API_KEY··········\n",
            "LANGCHAIN_API_KEY··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Loading Data : WebBaseLoader**"
      ],
      "metadata": {
        "id": "6f2YbygYzEqm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import bs4\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "\n",
        "loader = WebBaseLoader([\"link1\", \"link2\"])\n",
        "docs = loader.load()\n",
        "docs # will contain text\n",
        "\n",
        "# or\n",
        "\n",
        "loader = WebBaseLoader(\n",
        "    web_paths=([\"link1\",\"link2\"]),\n",
        "    bs_kwargs=dict(\n",
        "        parse_only=bs4.SoupStrainer(\n",
        "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
        "        )\n",
        "    ),\n",
        ")\n",
        "\n",
        "docs = loader.load()\n",
        "docs # will contain text\n",
        "\n",
        "# can also get other IMP info too like fetch all urls\n"
      ],
      "metadata": {
        "id": "f45SoKS2vRC9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " WebBaseLoader API Reference :\n",
        "\n",
        " [WebBaseLoader](https://api.python.langchain.com/en/latest/document_loaders/langchain_community.document_loaders.web_base.WebBaseLoader.html)"
      ],
      "metadata": {
        "id": "SufZRLUCzppT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Loading Data: Markdown**\n",
        "\n",
        "API Reference:\n",
        "\n",
        "[UnstructuredMarkdownLoader](https://api.python.langchain.com/en/latest/document_loaders/langchain_community.document_loaders.markdown.UnstructuredMarkdownLoader.html)"
      ],
      "metadata": {
        "id": "evxCE-oQ84zO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import UnstructuredMarkdownLoader\n",
        "\n",
        "markdown_path = \"/content/sklearn_user_guide.md\"\n",
        "loader = UnstructuredMarkdownLoader(markdown_path)\n",
        "\n",
        "docs = loader.load()\n",
        "docs # will contain text"
      ],
      "metadata": {
        "id": "hzT2amvV9A_X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "577f8ab8-5729-4084-e498-b7a4543e8865",
        "collapsed": true
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(page_content='scikit-learn User Guide\\n\\nUser guide: contents — scikit-learn 1.4.2 documentation\\nscikit-learn 1.4.2\\nOther versions\\nUser Guide\\n\\n1. Supervised learning\\n\\n2. Unsupervised learning\\n\\n3. Model selection and evaluation\\n\\n4. Inspection\\n\\n5. Visualizations\\n\\n6. Dataset transformations\\n\\n7. Dataset loading utilities\\n\\n8. Computing with scikit-learn\\n\\n9. Model persistence\\n\\n10. Common pitfalls and recommended practices\\n\\n11. Dispatching\\n\\nUser Guide¶\\n\\n1. Supervised learning\\n\\n1.1. Linear Models\\n\\n1.1.1. Ordinary Least Squares\\n\\n1.1.2. Ridge regression and classification\\n\\n1.1.3. Lasso\\n\\n1.1.4. Multi-task Lasso\\n\\n1.1.5. Elastic-Net\\n\\n1.1.6. Multi-task Elastic-Net\\n\\n1.1.7. Least Angle Regression\\n\\n1.1.8. LARS Lasso\\n\\n1.1.9. Orthogonal Matching Pursuit (OMP)\\n\\n1.1.10. Bayesian Regression\\n\\n1.1.11. Logistic regression\\n\\n1.1.12. Generalized Linear Models\\n\\n1.1.13. Stochastic Gradient Descent - SGD\\n\\n1.1.14. Perceptron\\n\\n1.1.15. Passive Aggressive Algorithms\\n\\n1.1.16. Robustness regression: outliers and modeling errors\\n\\n1.1.17. Quantile Regression\\n\\n1.1.18. Polynomial regression: extending linear models with basis functions\\n\\n1.2. Linear and Quadratic Discriminant Analysis\\n\\n1.2.1. Dimensionality reduction using Linear Discriminant Analysis\\n\\n1.2.2. Mathematical formulation of the LDA and QDA classifiers\\n\\n1.2.3. Mathematical formulation of LDA dimensionality reduction\\n\\n1.2.4. Shrinkage and Covariance Estimator\\n\\n1.2.5. Estimation algorithms\\n\\n1.3. Kernel ridge regression\\n\\n1.4. Support Vector Machines\\n\\n1.4.1. Classification\\n\\n1.4.2. Regression\\n\\n1.4.3. Density estimation, novelty detection\\n\\n1.4.4. Complexity\\n\\n1.4.5. Tips on Practical Use\\n\\n1.4.6. Kernel functions\\n\\n1.4.7. Mathematical formulation\\n\\n1.4.8. Implementation details\\n\\n1.5. Stochastic Gradient Descent\\n\\n1.5.1. Classification\\n\\n1.5.2. Regression\\n\\n1.5.3. Online One-Class SVM\\n\\n1.5.4. Stochastic Gradient Descent for sparse data\\n\\n1.5.5. Complexity\\n\\n1.5.6. Stopping criterion\\n\\n1.5.7. Tips on Practical Use\\n\\n1.5.8. Mathematical formulation\\n\\n1.5.9. Implementation details\\n\\n1.6. Nearest Neighbors\\n\\n1.6.1. Unsupervised Nearest Neighbors\\n\\n1.6.2. Nearest Neighbors Classification\\n\\n1.6.3. Nearest Neighbors Regression\\n\\n1.6.4. Nearest Neighbor Algorithms\\n\\n1.6.5. Nearest Centroid Classifier\\n\\n1.6.6. Nearest Neighbors Transformer\\n\\n1.6.7. Neighborhood Components Analysis\\n\\n1.7. Gaussian Processes\\n\\n1.7.1. Gaussian Process Regression (GPR)\\n\\n1.7.2. Gaussian Process Classification (GPC)\\n\\n1.7.3. GPC examples\\n\\n1.7.4. Kernels for Gaussian Processes\\n\\n1.8. Cross decomposition\\n\\n1.8.1. PLSCanonical\\n\\n1.8.2. PLSSVD\\n\\n1.8.3. PLSRegression\\n\\n1.8.4. Canonical Correlation Analysis\\n\\n1.9. Naive Bayes\\n\\n1.9.1. Gaussian Naive Bayes\\n\\n1.9.2. Multinomial Naive Bayes\\n\\n1.9.3. Complement Naive Bayes\\n\\n1.9.4. Bernoulli Naive Bayes\\n\\n1.9.5. Categorical Naive Bayes\\n\\n1.9.6. Out-of-core naive Bayes model fitting\\n\\n1.10. Decision Trees\\n\\n1.10.1. Classification\\n\\n1.10.2. Regression\\n\\n1.10.3. Multi-output problems\\n\\n1.10.4. Complexity\\n\\n1.10.5. Tips on practical use\\n\\n1.10.6. Tree algorithms: ID3, C4.5, C5.0 and CART\\n\\n1.10.7. Mathematical formulation\\n\\n1.10.8. Missing Values Support\\n\\n1.10.9. Minimal Cost-Complexity Pruning\\n\\n1.11. Ensembles: Gradient boosting, random forests, bagging, voting, stacking\\n\\n1.11.1. Gradient-boosted trees\\n\\n1.11.2. Random forests and other randomized tree ensembles\\n\\n1.11.3. Bagging meta-estimator\\n\\n1.11.4. Voting Classifier\\n\\n1.11.5. Voting Regressor\\n\\n1.11.6. Stacked generalization\\n\\n1.11.7. AdaBoost\\n\\n1.12. Multiclass and multioutput algorithms\\n\\n1.12.1. Multiclass classification\\n\\n1.12.2. Multilabel classification\\n\\n1.12.3. Multiclass-multioutput classification\\n\\n1.12.4. Multioutput regression\\n\\n1.13. Feature selection\\n\\n1.13.1. Removing features with low variance\\n\\n1.13.2. Univariate feature selection\\n\\n1.13.3. Recursive feature elimination\\n\\n1.13.4. Feature selection using SelectFromModel\\n\\n1.13.5. Sequential Feature Selection\\n\\n1.13.6. Feature selection as part of a pipeline\\n\\n1.14. Semi-supervised learning\\n\\n1.14.1. Self Training\\n\\n1.14.2. Label Propagation\\n\\n1.15. Isotonic regression\\n\\n1.16. Probability calibration\\n\\n1.16.1. Calibration curves\\n\\n1.16.2. Calibrating a classifier\\n\\n1.16.3. Usage\\n\\n1.17. Neural network models (supervised)\\n\\n1.17.1. Multi-layer Perceptron\\n\\n1.17.2. Classification\\n\\n1.17.3. Regression\\n\\n1.17.4. Regularization\\n\\n1.17.5. Algorithms\\n\\n1.17.6. Complexity\\n\\n1.17.7. Mathematical formulation\\n\\n1.17.8. Tips on Practical Use\\n\\n1.17.9. More control with warm_start\\n\\n2. Unsupervised learning\\n\\n2.1. Gaussian mixture models\\n\\n2.1.1. Gaussian Mixture\\n\\n2.1.2. Variational Bayesian Gaussian Mixture\\n\\n2.2. Manifold learning\\n\\n2.2.1. Introduction\\n\\n2.2.2. Isomap\\n\\n2.2.3. Locally Linear Embedding\\n\\n2.2.4. Modified Locally Linear Embedding\\n\\n2.2.5. Hessian Eigenmapping\\n\\n2.2.6. Spectral Embedding\\n\\n2.2.7. Local Tangent Space Alignment\\n\\n2.2.8. Multi-dimensional Scaling (MDS)\\n\\n2.2.9. t-distributed Stochastic Neighbor Embedding (t-SNE)\\n\\n2.2.10. Tips on practical use\\n\\n2.3. Clustering\\n\\n2.3.1. Overview of clustering methods\\n\\n2.3.2. K-means\\n\\n2.3.3. Affinity Propagation\\n\\n2.3.4. Mean Shift\\n\\n2.3.5. Spectral clustering\\n\\n2.3.6. Hierarchical clustering\\n\\n2.3.7. DBSCAN\\n\\n2.3.8. HDBSCAN\\n\\n2.3.9. OPTICS\\n\\n2.3.10. BIRCH\\n\\n2.3.11. Clustering performance evaluation\\n\\n2.4. Biclustering\\n\\n2.4.1. Spectral Co-Clustering\\n\\n2.4.2. Spectral Biclustering\\n\\n2.4.3. Biclustering evaluation\\n\\n2.5. Decomposing signals in components (matrix factorization problems)\\n\\n2.5.1. Principal component analysis (PCA)\\n\\n2.5.2. Kernel Principal Component Analysis (kPCA)\\n\\n2.5.3. Truncated singular value decomposition and latent semantic analysis\\n\\n2.5.4. Dictionary Learning\\n\\n2.5.5. Factor Analysis\\n\\n2.5.6. Independent component analysis (ICA)\\n\\n2.5.7. Non-negative matrix factorization (NMF or NNMF)\\n\\n2.5.8. Latent Dirichlet Allocation (LDA)\\n\\n2.6. Covariance estimation\\n\\n2.6.1. Empirical covariance\\n\\n2.6.2. Shrunk Covariance\\n\\n2.6.3. Sparse inverse covariance\\n\\n2.6.4. Robust Covariance Estimation\\n\\n2.7. Novelty and Outlier Detection\\n\\n2.7.1. Overview of outlier detection methods\\n\\n2.7.2. Novelty Detection\\n\\n2.7.3. Outlier Detection\\n\\n2.7.4. Novelty detection with Local Outlier Factor\\n\\n2.8. Density Estimation\\n\\n2.8.1. Density Estimation: Histograms\\n\\n2.8.2. Kernel Density Estimation\\n\\n2.9. Neural network models (unsupervised)\\n\\n2.9.1. Restricted Boltzmann machines\\n\\n3. Model selection and evaluation\\n\\n3.1. Cross-validation: evaluating estimator performance\\n\\n3.1.1. Computing cross-validated metrics\\n\\n3.1.2. Cross validation iterators\\n\\n3.1.3. A note on shuffling\\n\\n3.1.4. Cross validation and model selection\\n\\n3.1.5. Permutation test score\\n\\n3.2. Tuning the hyper-parameters of an estimator\\n\\n3.2.1. Exhaustive Grid Search\\n\\n3.2.2. Randomized Parameter Optimization\\n\\n3.2.3. Searching for optimal parameters with successive halving\\n\\n3.2.4. Tips for parameter search\\n\\n3.2.5. Alternatives to brute force parameter search\\n\\n3.3. Metrics and scoring: quantifying the quality of predictions\\n\\n3.3.1. The scoring parameter: defining model evaluation rules\\n\\n3.3.2. Classification metrics\\n\\n3.3.3. Multilabel ranking metrics\\n\\n3.3.4. Regression metrics\\n\\n3.3.5. Clustering metrics\\n\\n3.3.6. Dummy estimators\\n\\n3.4. Validation curves: plotting scores to evaluate models\\n\\n3.4.1. Validation curve\\n\\n3.4.2. Learning curve\\n\\n4. Inspection\\n\\n4.1. Partial Dependence and Individual Conditional Expectation plots\\n\\n4.1.1. Partial dependence plots\\n\\n4.1.2. Individual conditional expectation (ICE) plot\\n\\n4.1.3. Mathematical Definition\\n\\n4.1.4. Computation methods\\n\\n4.2. Permutation feature importance\\n\\n4.2.1. Outline of the permutation importance algorithm\\n\\n4.2.2. Relation to impurity-based importance in trees\\n\\n4.2.3. Misleading values on strongly correlated features\\n\\n5. Visualizations\\n\\n5.1. Available Plotting Utilities\\n\\n5.1.1. Display Objects\\n\\n6. Dataset transformations\\n\\n6.1. Pipelines and composite estimators\\n\\n6.1.1. Pipeline: chaining estimators\\n\\n6.1.2. Transforming target in regression\\n\\n6.1.3. FeatureUnion: composite feature spaces\\n\\n6.1.4. ColumnTransformer for heterogeneous data\\n\\n6.1.5. Visualizing Composite Estimators\\n\\n6.2. Feature extraction\\n\\n6.2.1. Loading features from dicts\\n\\n6.2.2. Feature hashing\\n\\n6.2.3. Text feature extraction\\n\\n6.2.4. Image feature extraction\\n\\n6.3. Preprocessing data\\n\\n6.3.1. Standardization, or mean removal and variance scaling\\n\\n6.3.2. Non-linear transformation\\n\\n6.3.3. Normalization\\n\\n6.3.4. Encoding categorical features\\n\\n6.3.5. Discretization\\n\\n6.3.6. Imputation of missing values\\n\\n6.3.7. Generating polynomial features\\n\\n6.3.8. Custom transformers\\n\\n6.4. Imputation of missing values\\n\\n6.4.1. Univariate vs. Multivariate Imputation\\n\\n6.4.2. Univariate feature imputation\\n\\n6.4.3. Multivariate feature imputation\\n\\n6.4.4. Nearest neighbors imputation\\n\\n6.4.5. Keeping the number of features constant\\n\\n6.4.6. Marking imputed values\\n\\n6.4.7. Estimators that handle NaN values\\n\\n6.5. Unsupervised dimensionality reduction\\n\\n6.5.1. PCA: principal component analysis\\n\\n6.5.2. Random projections\\n\\n6.5.3. Feature agglomeration\\n\\n6.6. Random Projection\\n\\n6.6.1. The Johnson-Lindenstrauss lemma\\n\\n6.6.2. Gaussian random projection\\n\\n6.6.3. Sparse random projection\\n\\n6.6.4. Inverse Transform\\n\\n6.7. Kernel Approximation\\n\\n6.7.1. Nystroem Method for Kernel Approximation\\n\\n6.7.2. Radial Basis Function Kernel\\n\\n6.7.3. Additive Chi Squared Kernel\\n\\n6.7.4. Skewed Chi Squared Kernel\\n\\n6.7.5. Polynomial Kernel Approximation via Tensor Sketch\\n\\n6.7.6. Mathematical Details\\n\\n6.8. Pairwise metrics, Affinities and Kernels\\n\\n6.8.1. Cosine similarity\\n\\n6.8.2. Linear kernel\\n\\n6.8.3. Polynomial kernel\\n\\n6.8.4. Sigmoid kernel\\n\\n6.8.5. RBF kernel\\n\\n6.8.6. Laplacian kernel\\n\\n6.8.7. Chi-squared kernel\\n\\n6.9. Transforming the prediction target (y)\\n\\n6.9.1. Label binarization\\n\\n6.9.2. Label encoding\\n\\n7. Dataset loading utilities\\n\\n7.1. Toy datasets\\n\\n7.1.1. Iris plants dataset\\n\\n7.1.2. Diabetes dataset\\n\\n7.1.3. Optical recognition of handwritten digits dataset\\n\\n7.1.4. Linnerrud dataset\\n\\n7.1.5. Wine recognition dataset\\n\\n7.1.6. Breast cancer wisconsin (diagnostic) dataset\\n\\n7.2. Real world datasets\\n\\n7.2.1. The Olivetti faces dataset\\n\\n7.2.2. The 20 newsgroups text dataset\\n\\n7.2.3. The Labeled Faces in the Wild face recognition dataset\\n\\n7.2.4. Forest covertypes\\n\\n7.2.5. RCV1 dataset\\n\\n7.2.6. Kddcup 99 dataset\\n\\n7.2.7. California Housing dataset\\n\\n7.2.8. Species distribution dataset\\n\\n7.3. Generated datasets\\n\\n7.3.1. Generators for classification and clustering\\n\\n7.3.2. Generators for regression\\n\\n7.3.3. Generators for manifold learning\\n\\n7.3.4. Generators for decomposition\\n\\n7.4. Loading other datasets\\n\\n7.4.1. Sample images\\n\\n7.4.2. Datasets in svmlight / libsvm format\\n\\n7.4.3. Downloading datasets from the openml.org repository\\n\\n7.4.4. Loading from external datasets\\n\\n8. Computing with scikit-learn\\n\\n8.1. Strategies to scale computationally: bigger data\\n\\n8.1.1. Scaling with instances using out-of-core learning\\n\\n8.2. Computational Performance\\n\\n8.2.1. Prediction Latency\\n\\n8.2.2. Prediction Throughput\\n\\n8.2.3. Tips and Tricks\\n\\n8.3. Parallelism, resource management, and configuration\\n\\n8.3.1. Parallelism\\n\\n8.3.2. Configuration switches\\n\\n9. Model persistence\\n\\n9.1. Python specific serialization\\n\\n9.1.1. Security & maintainability limitations\\n\\n9.1.2. A more secure format: skops\\n\\n9.2. Interoperable formats\\n\\n10. Common pitfalls and recommended practices\\n\\n10.1. Inconsistent preprocessing\\n\\n10.2. Data leakage\\n\\n10.2.1. How to avoid data leakage\\n\\n10.2.2. Data leakage during pre-processing\\n\\n10.3. Controlling randomness\\n\\n10.3.1. Using None or RandomState instances, and repeated calls to fit and split\\n\\n10.3.2. Common pitfalls and subtleties\\n\\n10.3.3. General recommendations\\n\\n11. Dispatching\\n\\n11.1. Array API support (experimental)\\n\\n11.1.1. Example usage\\n\\n11.1.2. Support for Array API-compatible inputs\\n\\n11.1.3. Common estimator checks\\n\\n1.1. Linear Models — scikit-learn 1.4.2 documentation\\n\\n1.1. Linear Models\\n\\n1.1.1. Ordinary Least Squares\\n1.1.1.1. Non-Negative Least Squares\\n1.1.1.2. Ordinary Least Squares Complexity\\n\\n1.1.2. Ridge regression and classification\\n1.1.2.1. Regression\\n1.1.2.2. Classification\\n1.1.2.3. Ridge Complexity\\n1.1.2.4. Setting the regularization parameter: leave-one-out Cross-Validation\\n\\n1.1.3. Lasso\\n1.1.3.1. Setting regularization parameter\\n1.1.3.1.1. Using cross-validation\\n1.1.3.1.2. Information-criteria based model selection\\n1.1.3.1.3. AIC and BIC criteria\\n1.1.3.1.4. Comparison with the regularization parameter of SVM\\n\\n1.1.4. Multi-task Lasso\\n\\n1.1.5. Elastic-Net\\n\\n1.1.6. Multi-task Elastic-Net\\n\\n1.1.7. Least Angle Regression\\n\\n1.1.8. LARS Lasso\\n\\n1.1.9. Orthogonal Matching Pursuit (OMP)\\n\\n1.1.10. Bayesian Regression\\n1.1.10.1. Bayesian Ridge Regression\\n1.1.10.2. Automatic Relevance Determination - ARD\\n\\n1.1.11. Logistic regression\\n1.1.11.1. Binary Case\\n1.1.11.2. Multinomial Case\\n1.1.11.3. Solvers\\n1.1.11.3.1. Differences between solvers\\n\\n1.1.12. Generalized Linear Models\\n1.1.12.1. Usage\\n\\n1.1.13. Stochastic Gradient Descent - SGD\\n\\n1.1.14. Perceptron\\n\\n1.1.15. Passive Aggressive Algorithms\\n\\n1.1.16. Robustness regression: outliers and modeling errors\\n1.1.16.1. Different scenario and useful concepts\\n1.1.16.2. RANSAC: RANdom SAmple Consensus\\n1.1.16.3. Theil-Sen estimator: generalized-median-based estimator\\n1.1.16.4. Huber Regression\\n\\n1.1.17. Quantile Regression\\n\\n1.1.18. Polynomial regression: extending linear models with basis functions\\n\\n1.1. Linear Models¶\\n\\nThe following are a set of methods intended for regression in which\\nthe target value is expected to be a linear combination of the features.\\nIn mathematical notation, if (\\\\hat{y}) is the predicted\\nvalue.\\n[\\\\hat{y}(w, x) = w_0 + w_1 x_1 + ... + w_p x_p]\\nAcross the module, we designate the vector (w = (w_1,\\n..., w_p)) as coef_ and (w_0) as intercept_.\\nTo perform classification with generalized linear models, see\\nLogistic regression.\\n- 1.1.1. Ordinary Least Squares¶\\nLinearRegression fits a linear model with coefficients\\n(w = (w_1, ..., w_p)) to minimize the residual sum\\nof squares between the observed targets in the dataset, and the\\ntargets predicted by the linear approximation. Mathematically it\\nsolves a problem of the form:\\n[\\\\min_{w} || X w - y||2^2]\\nLinearRegression will take in its fit method arrays X, y\\nand will store the coefficients (w) of the linear model in its\\ncoef member:\\n\\nfit to an order-3 polynomial data\\n\\nx = np.arange(5)\\ny = 3 - 2 * x + x ** 2 - x ** 3\\nmodel = model.fit(x[:, np.newaxis], y)\\nmodel.named_steps[\\'linear\\'].coef_\\narray([ 3., -2.,  1., -1.])\\nThe linear model trained on polynomial features is able to exactly recover\\nthe input polynomial coefficients.\\nIn some cases it’s not necessary to include higher powers of any single feature,\\nbut only the so-called interaction features\\nthat multiply together at most (d) distinct features.\\nThese can be gotten from PolynomialFeatures with the setting\\ninteraction_only=True.\\nFor example, when dealing with boolean features,\\n(x_i^n = x_i) for all (n) and is therefore useless;\\nbut (x_i x_j) represents the conjunction of two booleans.\\nThis way, we can solve the XOR problem with a linear classifier:\\nfrom sklearn.linear_model import Perceptron\\nfrom sklearn.preprocessing import PolynomialFeatures\\nimport numpy as np\\nX = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\\ny = X[:, 0] ^ X[:, 1]\\ny\\narray([0, 1, 1, 0])\\nX = PolynomialFeatures(interaction_only=True).fit_transform(X).astype(int)\\nX\\narray([[1, 0, 0, 0],\\n[1, 0, 1, 0],\\n[1, 1, 0, 0],\\n[1, 1, 1, 1]])\\nclf = Perceptron(fit_intercept=False, max_iter=10, tol=None,\\n...                  shuffle=False).fit(X, y)\\nAnd the classifier “predictions” are perfect:\\nclf.predict(X)\\narray([0, 1, 1, 0])\\nclf.score(X, y)\\n1.0\\n\\n1.2. Linear and Quadratic Discriminant Analysis — scikit-learn 1.4.2 documentation\\n\\nInstall\\nUser Guide\\nAPI\\nExamples\\nCommunity\\nGetting Started\\nTutorial\\nWhat\\'s new\\nGlossary\\nDevelopment\\nFAQ\\nSupport\\nRelated packages\\nRoadmap\\nGovernance\\nAbout us\\nGitHub\\nOther Versions and Download\\nMore\\nGetting Started\\nTutorial\\nWhat\\'s new\\nGlossary\\nDevelopment\\nFAQ\\nSupport\\nRelated packages\\nRoadmap\\nGovernance\\nAbout us\\nGitHub\\nOther Versions and Download\\nToggle Menu\\nPrevUp\\nNext\\nscikit-learn 1.4.2\\nOther versions\\nPlease cite us if you use the software.\\n\\n1.2. Linear and Quadratic Discriminant Analysis\\n\\n1.2.1. Dimensionality reduction using Linear Discriminant Analysis\\n\\n1.2.2. Mathematical formulation of the LDA and QDA classifiers\\n1.2.2.1. QDA\\n1.2.2.2. LDA\\n\\n1.2.3. Mathematical formulation of LDA dimensionality reduction\\n\\n1.2.4. Shrinkage and Covariance Estimator\\n\\n1.2.5. Estimation algorithms\\n\\n1.2. Linear and Quadratic Discriminant Analysis¶\\n\\nLinear Discriminant Analysis\\n(LinearDiscriminantAnalysis) and Quadratic\\nDiscriminant Analysis\\n(QuadraticDiscriminantAnalysis) are two classic\\nclassifiers, with, as their names suggest, a linear and a quadratic decision\\nsurface, respectively.\\nThese classifiers are attractive because they have closed-form solutions that\\ncan be easily computed, are inherently multiclass, have proven to work well in\\npractice, and have no hyperparameters to tune.\\nThe plot shows decision boundaries for Linear Discriminant Analysis and\\nQuadratic Discriminant Analysis. The bottom row demonstrates that Linear\\nDiscriminant Analysis can only learn linear boundaries, while Quadratic\\nDiscriminant Analysis can learn quadratic boundaries and is therefore more\\nflexible.\\nExamples:\\nLinear and Quadratic Discriminant Analysis with covariance ellipsoid: Comparison of LDA and QDA\\non synthetic data.\\n- 1.2.1. Dimensionality reduction using Linear Discriminant Analysis¶\\nLinearDiscriminantAnalysis can be used to\\nperform supervised dimensionality reduction, by projecting the input data to a\\nlinear subspace consisting of the directions which maximize the separation\\nbetween classes (in a precise sense discussed in the mathematics section\\nbelow). The dimension of the output is necessarily less than the number of\\nclasses, so this is in general a rather strong dimensionality reduction, and\\nonly makes sense in a multiclass setting.\\nThis is implemented in the transform method. The desired dimensionality can\\nbe set using the n_components parameter. This parameter has no influence\\non the fit and predict methods.\\nExamples:\\nComparison of LDA and PCA 2D projection of Iris dataset: Comparison of LDA and PCA\\nfor dimensionality reduction of the Iris dataset\\n- 1.2.2. Mathematical formulation of the LDA and QDA classifiers¶\\nBoth LDA and QDA can be derived from simple probabilistic models which model\\nthe class conditional distribution of the data (P(X|y=k)) for each class\\n(k). Predictions can then be obtained by using Bayes’ rule, for each\\ntraining sample (x \\\\in \\\\mathcal{R}^d):\\n[P(y=k | x) = \\\\frac{P(x | y=k) P(y=k)}{P(x)} = \\\\frac{P(x | y=k) P(y = k)}{ \\\\sum_{l} P(x | y=l) \\\\cdot P(y=l)}]\\nand we select the class (k) which maximizes this posterior probability.\\nMore specifically, for linear and quadratic discriminant analysis,\\n(P(x|y)) is modeled as a multivariate Gaussian distribution with\\ndensity:\\n[P(x | y=k) = \\\\frac{1}{(2\\\\pi)^{d/2} |\\\\Sigma_k|^{1/2}}\\\\exp\\\\left(-\\\\frac{1}{2} (x-\\\\mu_k)^t \\\\Sigma_k^{-1} (x-\\\\mu_k)\\\\right)]\\nwhere (d) is the number of features.\\n1.2.2.1. QDA¶\\nAccording to the model above, the log of the posterior is:\\n[\\\\begin{split}\\\\log P(y=k | x) &= \\\\log P(x | y=k) + \\\\log P(y = k) + Cst \\\\\\n&= -\\\\frac{1}{2} \\\\log |\\\\Sigma_k| -\\\\frac{1}{2} (x-\\\\mu_k)^t \\\\Sigma_k^{-1} (x-\\\\mu_k) + \\\\log P(y = k) + Cst,\\\\end{split}]\\nwhere the constant term (Cst) corresponds to the denominator\\n(P(x)), in addition to other constant terms from the Gaussian. The\\npredicted class is the one that maximises this log-posterior.\\nNote\\nRelation with Gaussian Naive Bayes\\nIf in the QDA model one assumes that the covariance matrices are diagonal,\\nthen the inputs are assumed to be conditionally independent in each class,\\nand the resulting classifier is equivalent to the Gaussian Naive Bayes\\nclassifier naive_bayes.GaussianNB.\\n1.2.2.2. LDA¶\\nLDA is a special case of QDA, where the Gaussians for each class are assumed\\nto share the same covariance matrix: (\\\\Sigma_k = \\\\Sigma) for all\\n(k). This reduces the log posterior to:\\n[\\\\log P(y=k | x) = -\\\\frac{1}{2} (x-\\\\mu_k)^t \\\\Sigma^{-1} (x-\\\\mu_k) + \\\\log P(y = k) + Cst.]\\nThe term ((x-\\\\mu_k)^t \\\\Sigma^{-1} (x-\\\\mu_k)) corresponds to the\\nMahalanobis Distance\\nbetween the sample (x) and the mean (\\\\mu_k). The Mahalanobis\\ndistance tells how close (x) is from (\\\\mu_k), while also\\naccounting for the variance of each feature. We can thus interpret LDA as\\nassigning (x) to the class whose mean is the closest in terms of\\nMahalanobis distance, while also accounting for the class prior\\nprobabilities.\\nThe log-posterior of LDA can also be written [3] as:\\n[\\\\log P(y=k | x) = \\\\omega_k^t x + \\\\omega_{k0} + Cst.]\\nwhere (\\\\omega_k = \\\\Sigma^{-1} \\\\mu_k) and (\\\\omega_{k0} =\\n-\\\\frac{1}{2} \\\\mu_k^t\\\\Sigma^{-1}\\\\mu_k + \\\\log P (y = k)). These quantities\\ncorrespond to the coef_ and intercept_ attributes, respectively.\\nFrom the above formula, it is clear that LDA has a linear decision surface.\\nIn the case of QDA, there are no assumptions on the covariance matrices\\n(\\\\Sigma_k) of the Gaussians, leading to quadratic decision surfaces.\\nSee [1] for more details.\\n- 1.2.3. Mathematical formulation of LDA dimensionality reduction¶\\nFirst note that the K means (\\\\mu_k) are vectors in\\n(\\\\mathcal{R}^d), and they lie in an affine subspace (H) of\\ndimension at most (K - 1) (2 points lie on a line, 3 points lie on a\\nplane, etc.).\\nAs mentioned above, we can interpret LDA as assigning (x) to the class\\nwhose mean (\\\\mu_k) is the closest in terms of Mahalanobis distance,\\nwhile also accounting for the class prior probabilities. Alternatively, LDA\\nis equivalent to first sphering the data so that the covariance matrix is\\nthe identity, and then assigning (x) to the closest mean in terms of\\nEuclidean distance (still accounting for the class priors).\\nComputing Euclidean distances in this d-dimensional space is equivalent to\\nfirst projecting the data points into (H), and computing the distances\\nthere (since the other dimensions will contribute equally to each class in\\nterms of distance). In other words, if (x) is closest to (\\\\mu_k)\\nin the original space, it will also be the case in (H).\\nThis shows that, implicit in the LDA\\nclassifier, there is a dimensionality reduction by linear projection onto a\\n(K-1) dimensional space.\\nWe can reduce the dimension even more, to a chosen (L), by projecting\\nonto the linear subspace (H_L) which maximizes the variance of the\\n(\\\\mu^_k) after projection (in effect, we are doing a form of PCA for the\\ntransformed class means (\\\\mu^k)). This (L) corresponds to the\\nn_components parameter used in the\\ntransform method. See\\n[1] for more details.\\n- 1.2.4. Shrinkage and Covariance Estimator¶\\nShrinkage is a form of regularization used to improve the estimation of\\ncovariance matrices in situations where the number of training samples is\\nsmall compared to the number of features.\\nIn this scenario, the empirical sample covariance is a poor\\nestimator, and shrinkage helps improving the generalization performance of\\nthe classifier.\\nShrinkage LDA can be used by setting the shrinkage parameter of\\nthe LinearDiscriminantAnalysis class to ‘auto’.\\nThis automatically determines the optimal shrinkage parameter in an analytic\\nway following the lemma introduced by Ledoit and Wolf [2]. Note that\\ncurrently shrinkage only works when setting the solver parameter to ‘lsqr’\\nor ‘eigen’.\\nThe shrinkage parameter can also be manually set between 0 and 1. In\\nparticular, a value of 0 corresponds to no shrinkage (which means the empirical\\ncovariance matrix will be used) and a value of 1 corresponds to complete\\nshrinkage (which means that the diagonal matrix of variances will be used as\\nan estimate for the covariance matrix). Setting this parameter to a value\\nbetween these two extrema will estimate a shrunk version of the covariance\\nmatrix.\\nThe shrunk Ledoit and Wolf estimator of covariance may not always be the\\nbest choice. For example if the distribution of the data\\nis normally distributed, the\\nOracle Approximating Shrinkage estimator sklearn.covariance.OAS\\nyields a smaller Mean Squared Error than the one given by Ledoit and Wolf’s\\nformula used with shrinkage=”auto”. In LDA, the data are assumed to be gaussian\\nconditionally to the class. If these assumptions hold, using LDA with\\nthe OAS estimator of covariance will yield a better classification\\naccuracy than if Ledoit and Wolf or the empirical covariance estimator is used.\\nThe covariance estimator can be chosen using with the covariance_estimator\\nparameter of the discriminant_analysis.LinearDiscriminantAnalysis\\nclass. A covariance estimator should have a fit method and a\\ncovariance attribute like all covariance estimators in the\\nsklearn.covariance module.\\nExamples:\\nNormal, Ledoit-Wolf and OAS Linear Discriminant Analysis for classification: Comparison of LDA classifiers\\nwith Empirical, Ledoit Wolf and OAS covariance estimator.\\n- 1.2.5. Estimation algorithms¶\\nUsing LDA and QDA requires computing the log-posterior which depends on the\\nclass priors (P(y=k)), the class means (\\\\mu_k), and the\\ncovariance matrices.\\nThe ‘svd’ solver is the default solver used for\\nLinearDiscriminantAnalysis, and it is\\nthe only available solver for\\nQuadraticDiscriminantAnalysis.\\nIt can perform both classification and transform (for LDA).\\nAs it does not rely on the calculation of the covariance matrix, the ‘svd’\\nsolver may be preferable in situations where the number of features is large.\\nThe ‘svd’ solver cannot be used with shrinkage.\\nFor QDA, the use of the SVD solver relies on the fact that the covariance\\nmatrix (\\\\Sigma_k) is, by definition, equal to (\\\\frac{1}{n - 1}\\nX_k^tX_k = \\\\frac{1}{n - 1} V S^2 V^t) where (V) comes from the SVD of the (centered)\\nmatrix: (X_k = U S V^t). It turns out that we can compute the\\nlog-posterior above without having to explicitly compute (\\\\Sigma):\\ncomputing (S) and (V) via the SVD of (X) is enough. For\\nLDA, two SVDs are computed: the SVD of the centered input matrix (X)\\nand the SVD of the class-wise mean vectors.\\nThe ‘lsqr’ solver is an efficient algorithm that only works for\\nclassification. It needs to explicitly compute the covariance matrix\\n(\\\\Sigma), and supports shrinkage and custom covariance estimators.\\nThis solver computes the coefficients\\n(\\\\omega_k = \\\\Sigma^{-1}\\\\mu_k) by solving for (\\\\Sigma \\\\omega =\\n\\\\mu_k), thus avoiding the explicit computation of the inverse\\n(\\\\Sigma^{-1}).\\nThe ‘eigen’ solver is based on the optimization of the between class scatter to\\nwithin class scatter ratio. It can be used for both classification and\\ntransform, and it supports shrinkage. However, the ‘eigen’ solver needs to\\ncompute the covariance matrix, so it might not be suitable for situations with\\na high number of features.\\nReferences:\\n[1]\\n(1,2)\\n“The Elements of Statistical Learning”, Hastie T., Tibshirani R.,\\nFriedman J., Section 4.3, p.106-119, 2008.\\n[2]\\nLedoit O, Wolf M. Honey, I Shrunk the Sample Covariance Matrix.\\nThe Journal of Portfolio Management 30(4), 110-119, 2004.\\n[3]\\nR. O. Duda, P. E. Hart, D. G. Stork. Pattern Classification\\n(Second Edition), section 2.6.2.\\n© 2007 - 2024, scikit-learn developers (BSD License).\\nShow this page source\\nClick to add a cell.\\n\\n1.3. Kernel ridge regression — scikit-learn 1.4.2 documentation\\n\\nInstall\\nUser Guide\\nAPI\\nExamples\\nCommunity\\nGetting Started\\nTutorial\\nWhat\\'s new\\nGlossary\\nDevelopment\\nFAQ\\nSupport\\nRelated packages\\nRoadmap\\nGovernance\\nAbout us\\nGitHub\\nOther Versions and Download\\nMore\\nGetting Started\\nTutorial\\nWhat\\'s new\\nGlossary\\nDevelopment\\nFAQ\\nSupport\\nRelated packages\\nRoadmap\\nGovernance\\nAbout us\\nGitHub\\nOther Versions and Download\\nToggle Menu\\nPrevUp\\nNext\\nscikit-learn 1.4.2\\nOther versions\\nPlease cite us if you use the software.\\n\\n1.3. Kernel ridge regression\\n\\n1.3. Kernel ridge regression¶\\n\\nKernel ridge regression (KRR) [M2012] combines Ridge regression and classification\\n(linear least squares with l2-norm regularization) with the kernel trick. It thus learns a linear\\nfunction in the space induced by the respective kernel and the data. For\\nnon-linear kernels, this corresponds to a non-linear function in the original\\nspace.\\nThe form of the model learned by KernelRidge is identical to support\\nvector regression (SVR). However, different loss\\nfunctions are used: KRR uses squared error loss while support vector\\nregression uses (\\\\epsilon)-insensitive loss, both combined with l2\\nregularization. In contrast to SVR, fitting\\nKernelRidge can be done in closed-form and is typically faster for\\nmedium-sized datasets. On the other hand, the learned model is non-sparse and\\nthus slower than SVR, which learns a sparse model for\\n(\\\\epsilon > 0), at prediction-time.\\nThe following figure compares KernelRidge and\\nSVR on an artificial dataset, which consists of a\\nsinusoidal target function and strong noise added to every fifth datapoint.\\nThe learned model of KernelRidge and SVR is\\nplotted, where both complexity/regularization and bandwidth of the RBF kernel\\nhave been optimized using grid-search. The learned functions are very\\nsimilar; however, fitting KernelRidge is approximately seven times\\nfaster than fitting SVR (both with grid-search).\\nHowever, prediction of 100000 target values is more than three times faster\\nwith SVR since it has learned a sparse model using only\\napproximately 1/3 of the 100 training datapoints as support vectors.\\nThe next figure compares the time for fitting and prediction of\\nKernelRidge and SVR for different sizes of the\\ntraining set. Fitting KernelRidge is faster than\\nSVR for medium-sized training sets (less than 1000\\nsamples); however, for larger training sets SVR scales\\nbetter. With regard to prediction time, SVR is faster\\nthan KernelRidge for all sizes of the training set because of the\\nlearned sparse solution. Note that the degree of sparsity and thus the\\nprediction time depends on the parameters (\\\\epsilon) and (C) of\\nthe SVR; (\\\\epsilon = 0) would correspond to a\\ndense model.\\nReferences:\\n[M2012]\\n“Machine Learning: A Probabilistic Perspective”\\nMurphy, K. P. - chapter 14.4.3, pp. 492-493, The MIT Press, 2012\\n© 2007 - 2024, scikit-learn developers (BSD License).\\nShow this page source\\n\\n1.4. Support Vector Machines — scikit-learn 1.4.2 documentation\\n\\nInstall\\nUser Guide\\nAPI\\nExamples\\nCommunity\\nGetting Started\\nTutorial\\nWhat\\'s new\\nGlossary\\nDevelopment\\nFAQ\\nSupport\\nRelated packages\\nRoadmap\\nGovernance\\nAbout us\\nGitHub\\nOther Versions and Download\\nMore\\nGetting Started\\nTutorial\\nWhat\\'s new\\nGlossary\\nDevelopment\\nFAQ\\nSupport\\nRelated packages\\nRoadmap\\nGovernance\\nAbout us\\nGitHub\\nOther Versions and Download\\nToggle Menu\\nPrevUp\\nNext\\nscikit-learn 1.4.2\\nOther versions\\nPlease cite us if you use the software.\\n\\n1.4. Support Vector Machines\\n\\n1.4.1. Classification\\n1.4.1.1. Multi-class classification\\n1.4.1.2. Scores and probabilities\\n1.4.1.3. Unbalanced problems\\n\\n1.4.2. Regression\\n\\n1.4.3. Density estimation, novelty detection\\n\\n1.4.4. Complexity\\n\\n1.4.5. Tips on Practical Use\\n\\n1.4.6. Kernel functions\\n1.4.6.1. Parameters of the RBF Kernel\\n1.4.6.2. Custom Kernels\\n\\n1.4.7. Mathematical formulation\\n1.4.7.1. SVC\\n1.4.7.2. SVR\\n\\n1.4.8. Implementation details\\n\\n1.4. Support Vector Machines¶\\n\\nSupport vector machines (SVMs) are a set of supervised learning\\nmethods used for classification,\\nregression and outliers detection.\\nThe advantages of support vector machines are:\\nEffective in high dimensional spaces.\\nStill effective in cases where number of dimensions is greater\\nthan the number of samples.\\nUses a subset of training points in the decision function (called\\nsupport vectors), so it is also memory efficient.\\nVersatile: different Kernel functions can be\\nspecified for the decision function. Common kernels are\\nprovided, but it is also possible to specify custom kernels.\\nThe disadvantages of support vector machines include:\\nIf the number of features is much greater than the number of\\nsamples, avoid over-fitting in choosing Kernel functions and regularization\\nterm is crucial.\\nSVMs do not directly provide probability estimates, these are\\ncalculated using an expensive five-fold cross-validation\\n(see Scores and probabilities, below).\\nThe support vector machines in scikit-learn support both dense\\n(numpy.ndarray and convertible to that by numpy.asarray) and\\nsparse (any scipy.sparse) sample vectors as input. However, to use\\nan SVM to make predictions for sparse data, it must have been fit on such\\ndata. For optimal performance, use C-ordered numpy.ndarray (dense) or\\nscipy.sparse.csr_matrix (sparse) with dtype=float64.\\n- 1.4.1. Classification¶\\nSVC, NuSVC and LinearSVC are classes\\ncapable of performing binary and multi-class classification on a dataset.\\nSVC and NuSVC are similar methods, but accept slightly\\ndifferent sets of parameters and have different mathematical formulations (see\\nsection Mathematical formulation). On the other hand,\\nLinearSVC is another (faster) implementation of Support Vector\\nClassification for the case of a linear kernel. It also\\nlacks some of the attributes of SVC and NuSVC, like\\nsupport_. LinearSVC uses squared_hinge loss and due to its\\nimplementation in liblinear it also regularizes the intercept, if considered.\\nThis effect can however be reduced by carefully fine tuning its\\nintercept_scaling parameter, which allows the intercept term to have a\\ndifferent regularization behavior compared to the other features. The\\nclassification results and score can therefore differ from the other two\\nclassifiers.\\nAs other classifiers, SVC, NuSVC and\\nLinearSVC take as input two arrays: an array X of shape\\n(n_samples, n_features) holding the training samples, and an array y of\\nclass labels (strings or integers), of shape (n_samples):\\n\\nfrom sklearn import svm\\nX = [[0, 0], [1, 1]]\\ny = [0, 1]\\nclf = svm.SVC()\\nclf.fit(X, y)\\nSVC()\\nAfter being fitted, the model can then be used to predict new values:\\nclf.predict([[2., 2.]])\\narray([1])\\nSVMs decision function (detailed in the Mathematical formulation)\\ndepends on some subset of the training data, called the support vectors. Some\\nproperties of these support vectors can be found in attributes\\nsupport_vectors_, support_ and n_support_:\\n\\nget support vectors\\n\\nclf.support_vectors_\\narray([[0., 0.],\\n[1., 1.]])\\n\\nget indices of support vectors\\n\\nclf.support_\\narray([0, 1]...)\\n\\nget number of support vectors for each class\\n\\nclf.n_support_\\narray([1, 1]...)\\nExamples:\\nSVM: Maximum margin separating hyperplane,\\nNon-linear SVM\\nSVM-Anova: SVM with univariate feature selection,\\n1.4.1.1. Multi-class classification¶\\nSVC and NuSVC implement the “one-versus-one”\\napproach for multi-class classification. In total,\\nn_classes * (n_classes - 1) / 2\\nclassifiers are constructed and each one trains data from two classes.\\nTo provide a consistent interface with other classifiers, the\\ndecision_function_shape option allows to monotonically transform the\\nresults of the “one-versus-one” classifiers to a “one-vs-rest” decision\\nfunction of shape (n_samples, n_classes).\\nX = [[0], [1], [2], [3]]\\nY = [0, 1, 2, 3]\\nclf = svm.SVC(decision_function_shape=\\'ovo\\')\\nclf.fit(X, Y)\\nSVC(decision_function_shape=\\'ovo\\')\\ndec = clf.decision_function([[1]])\\ndec.shape[1] # 4 classes: 4*3/2 = 6\\n6\\nclf.decision_function_shape = \"ovr\"\\ndec = clf.decision_function([[1]])\\ndec.shape[1] # 4 classes\\n4\\nOn the other hand, LinearSVC implements “one-vs-the-rest”\\nmulti-class strategy, thus training n_classes models.\\nlin_clf = svm.LinearSVC(dual=\"auto\")\\nlin_clf.fit(X, Y)\\nLinearSVC(dual=\\'auto\\')\\ndec = lin_clf.decision_function([[1]])\\ndec.shape[1]\\n4\\nSee Mathematical formulation for a complete description of\\nthe decision function.\\nDetails on multi-class strategies\\nClick for more details\\n¶\\nNote that the LinearSVC also implements an alternative multi-class\\nstrategy, the so-called multi-class SVM formulated by Crammer and Singer\\n[16], by using the option multi_class=\\'crammer_singer\\'. In practice,\\none-vs-rest classification is usually preferred, since the results are mostly\\nsimilar, but the runtime is significantly less.\\nFor “one-vs-rest” LinearSVC the attributes coef_ and intercept_\\nhave the shape (n_classes, n_features) and (n_classes,) respectively.\\nEach row of the coefficients corresponds to one of the n_classes\\n“one-vs-rest” classifiers and similar for the intercepts, in the\\norder of the “one” class.\\nIn the case of “one-vs-one” SVC and NuSVC, the layout of\\nthe attributes is a little more involved. In the case of a linear\\nkernel, the attributes coef_ and intercept_ have the shape\\n(n_classes * (n_classes - 1) / 2, n_features) and (n_classes *\\n(n_classes - 1) / 2) respectively. This is similar to the layout for\\nLinearSVC described above, with each row now corresponding\\nto a binary classifier. The order for classes\\n0 to n is “0 vs 1”, “0 vs 2” , … “0 vs n”, “1 vs 2”, “1 vs 3”, “1 vs n”, . .\\n. “n-1 vs n”.\\nThe shape of dual_coef_ is (n_classes-1, n_SV) with\\na somewhat hard to grasp layout.\\nThe columns correspond to the support vectors involved in any\\nof the n_classes * (n_classes - 1) / 2 “one-vs-one” classifiers.\\nEach support vector v has a dual coefficient in each of the\\nn_classes - 1 classifiers comparing the class of v against another class.\\nNote that some, but not all, of these dual coefficients, may be zero.\\nThe n_classes - 1 entries in each column are these dual coefficients,\\nordered by the opposing class.\\nThis might be clearer with an example: consider a three class problem with\\nclass 0 having three support vectors\\n(v^{0}0, v^{1}_0, v^{2}_0) and class 1 and 2 having two support vectors\\n(v^{0}_1, v^{1}_1) and (v^{0}_2, v^{1}_2) respectively.  For each\\nsupport vector (v^{j}_i), there are two dual coefficients.  Let’s call\\nthe coefficient of support vector (v^{j}_i) in the classifier between\\nclasses (i) and (k) (\\\\alpha^{j}{i,k}).\\nThen dual_coef_ looks like this:\\n(\\\\alpha^{0}{0,1})\\n(\\\\alpha^{1}{0,1})\\n(\\\\alpha^{2}{0,1})\\n(\\\\alpha^{0}{1,0})\\n(\\\\alpha^{1}{1,0})\\n(\\\\alpha^{0}{2,0})\\n(\\\\alpha^{1}{2,0})\\n(\\\\alpha^{0}{0,2})\\n(\\\\alpha^{1}{0,2})\\n(\\\\alpha^{2}{0,2})\\n(\\\\alpha^{0}{1,2})\\n(\\\\alpha^{1}{1,2})\\n(\\\\alpha^{0}{2,1})\\n(\\\\alpha^{1}{2,1})\\nCoefficients\\nfor SVs of class 0\\nCoefficients\\nfor SVs of class 1\\nCoefficients\\nfor SVs of class 2\\nExamples:\\nPlot different SVM classifiers in the iris dataset,\\n1.4.1.2. Scores and probabilities¶\\nThe decision_function method of SVC and NuSVC gives\\nper-class scores for each sample (or a single score per sample in the binary\\ncase). When the constructor option probability is set to True,\\nclass membership probability estimates (from the methods predict_proba and\\npredict_log_proba) are enabled. In the binary case, the probabilities are\\ncalibrated using Platt scaling [9]: logistic regression on the SVM’s scores,\\nfit by an additional cross-validation on the training data.\\nIn the multiclass case, this is extended as per [10].\\nNote\\nThe same probability calibration procedure is available for all estimators\\nvia the CalibratedClassifierCV (see\\nProbability calibration). In the case of SVC and NuSVC, this\\nprocedure is builtin in libsvm which is used under the hood, so it does\\nnot rely on scikit-learn’s\\nCalibratedClassifierCV.\\nThe cross-validation involved in Platt scaling\\nis an expensive operation for large datasets.\\nIn addition, the probability estimates may be inconsistent with the scores:\\nthe “argmax” of the scores may not be the argmax of the probabilities\\nin binary classification, a sample may be labeled by predict as\\nbelonging to the positive class even if the output of predict_proba is\\nless than 0.5; and similarly, it could be labeled as negative even if the\\noutput of predict_proba is more than 0.5.\\nPlatt’s method is also known to have theoretical issues.\\nIf confidence scores are required, but these do not have to be probabilities,\\nthen it is advisable to set probability=False\\nand use decision_function instead of predict_proba.\\nPlease note that when decision_function_shape=\\'ovr\\' and n_classes > 2,\\nunlike decision_function, the predict method does not try to break ties\\nby default. You can set break_ties=True for the output of predict to be\\nthe same as np.argmax(clf.decision_function(...), axis=1), otherwise the\\nfirst class among the tied classes will always be returned; but have in mind\\nthat it comes with a computational cost. See\\nSVM Tie Breaking Example for an example on\\ntie breaking.\\n1.4.1.3. Unbalanced problems¶\\nIn problems where it is desired to give more importance to certain\\nclasses or certain individual samples, the parameters class_weight and\\nsample_weight can be used.\\nSVC (but not NuSVC) implements the parameter\\nclass_weight in the fit method. It’s a dictionary of the form\\n{class_label : value}, where value is a floating point number > 0\\nthat sets the parameter C of class class_label to C * value.\\nThe figure below illustrates the decision boundary of an unbalanced problem,\\nwith and without weight correction.\\nSVC, NuSVC, SVR, NuSVR, LinearSVC,\\nLinearSVR and OneClassSVM implement also weights for\\nindividual samples in the fit method through the sample_weight parameter.\\nSimilar to class_weight, this sets the parameter C for the i-th\\nexample to C * sample_weight[i], which will encourage the classifier to\\nget these samples right. The figure below illustrates the effect of sample\\nweighting on the decision boundary. The size of the circles is proportional\\nto the sample weights:\\nExamples:\\nSVM: Separating hyperplane for unbalanced classes\\nSVM: Weighted samples,\\n- 1.4.2. Regression¶\\nThe method of Support Vector Classification can be extended to solve\\nregression problems. This method is called Support Vector Regression.\\nThe model produced by support vector classification (as described\\nabove) depends only on a subset of the training data, because the cost\\nfunction for building the model does not care about training points\\nthat lie beyond the margin. Analogously, the model produced by Support\\nVector Regression depends only on a subset of the training data,\\nbecause the cost function ignores samples whose prediction is close to their\\ntarget.\\nThere are three different implementations of Support Vector Regression:\\nSVR, NuSVR and LinearSVR. LinearSVR\\nprovides a faster implementation than SVR but only considers the\\nlinear kernel, while NuSVR implements a slightly different formulation\\nthan SVR and LinearSVR. Due to its implementation in\\nliblinear LinearSVR also regularizes the intercept, if considered.\\nThis effect can however be reduced by carefully fine tuning its\\nintercept_scaling parameter, which allows the intercept term to have a\\ndifferent regularization behavior compared to the other features. The\\nclassification results and score can therefore differ from the other two\\nclassifiers. See Implementation details for further details.\\nAs with classification classes, the fit method will take as\\nargument vectors X, y, only that in this case y is expected to have\\nfloating point values instead of integer values:\\nfrom sklearn import svm\\nX = [[0, 0], [2, 2]]\\ny = [0.5, 2.5]\\nregr = svm.SVR()\\nregr.fit(X, y)\\nSVR()\\nregr.predict([[1, 1]])\\narray([1.5])\\nExamples:\\nSupport Vector Regression (SVR) using linear and non-linear kernels\\n- 1.4.3. Density estimation, novelty detection¶\\nThe class OneClassSVM implements a One-Class SVM which is used in\\noutlier detection.\\nSee Novelty and Outlier Detection for the description and usage of OneClassSVM.\\n- 1.4.4. Complexity¶\\nSupport Vector Machines are powerful tools, but their compute and\\nstorage requirements increase rapidly with the number of training\\nvectors. The core of an SVM is a quadratic programming problem (QP),\\nseparating support vectors from the rest of the training data. The QP\\nsolver used by the libsvm-based implementation scales between\\n(O(n_{features} \\\\times n_{samples}^2)) and\\n(O(n_{features} \\\\times n_{samples}^3)) depending on how efficiently\\nthe libsvm cache is used in practice (dataset dependent). If the data\\nis very sparse (n_{features}) should be replaced by the average number\\nof non-zero features in a sample vector.\\nFor the linear case, the algorithm used in\\nLinearSVC by the liblinear implementation is much more\\nefficient than its libsvm-based SVC counterpart and can\\nscale almost linearly to millions of samples and/or features.\\n- 1.4.5. Tips on Practical Use¶\\nAvoiding data copy: For SVC, SVR, NuSVC and\\nNuSVR, if the data passed to certain methods is not C-ordered\\ncontiguous and double precision, it will be copied before calling the\\nunderlying C implementation. You can check whether a given numpy array is\\nC-contiguous by inspecting its flags attribute.\\nFor LinearSVC (and LogisticRegression) any input passed as a numpy\\narray will be copied and converted to the liblinear internal sparse data\\nrepresentation (double precision floats and int32 indices of non-zero\\ncomponents). If you want to fit a large-scale linear classifier without\\ncopying a dense numpy C-contiguous double precision array as input, we\\nsuggest to use the SGDClassifier class instead.  The objective\\nfunction can be configured to be almost the same as the LinearSVC\\nmodel.\\nKernel cache size: For SVC, SVR, NuSVC and\\nNuSVR, the size of the kernel cache has a strong impact on run\\ntimes for larger problems.  If you have enough RAM available, it is\\nrecommended to set cache_size to a higher value than the default of\\n200(MB), such as 500(MB) or 1000(MB).\\nSetting C: C is 1 by default and it’s a reasonable default\\nchoice.  If you have a lot of noisy observations you should decrease it:\\ndecreasing C corresponds to more regularization.\\nLinearSVC and LinearSVR are less sensitive to C when\\nit becomes large, and prediction results stop improving after a certain\\nthreshold. Meanwhile, larger C values will take more time to train,\\nsometimes up to 10 times longer, as shown in [11].\\nSupport Vector Machine algorithms are not scale invariant, so it\\nis highly recommended to scale your data. For example, scale each\\nattribute on the input vector X to [0,1] or [-1,+1], or standardize it\\nto have mean 0 and variance 1. Note that the same scaling must be\\napplied to the test vector to obtain meaningful results. This can be done\\neasily by using a Pipeline:\\nfrom sklearn.pipeline import make_pipeline\\nfrom sklearn.preprocessing import StandardScaler\\nfrom sklearn.svm import SVC\\nclf = make_pipeline(StandardScaler(), SVC())\\nSee section Preprocessing data for more details on scaling and\\nnormalization.\\nRegarding the shrinking parameter, quoting [12]: We found that if the\\nnumber of iterations is large, then shrinking can shorten the training\\ntime. However, if we loosely solve the optimization problem (e.g., by\\nusing a large stopping tolerance), the code without using shrinking may\\nbe much faster\\nParameter nu in NuSVC/OneClassSVM/NuSVR\\napproximates the fraction of training errors and support vectors.\\nIn SVC, if the data is unbalanced (e.g. many\\npositive and few negative), set class_weight=\\'balanced\\' and/or try\\ndifferent penalty parameters C.\\nRandomness of the underlying implementations: The underlying\\nimplementations of SVC and NuSVC use a random number\\ngenerator only to shuffle the data for probability estimation (when\\nprobability is set to True). This randomness can be controlled\\nwith the random_state parameter. If probability is set to False\\nthese estimators are not random and random_state has no effect on the\\nresults. The underlying OneClassSVM implementation is similar to\\nthe ones of SVC and NuSVC. As no probability estimation\\nis provided for OneClassSVM, it is not random.\\nThe underlying LinearSVC implementation uses a random number\\ngenerator to select features when fitting the model with a dual coordinate\\ndescent (i.e. when dual is set to True). It is thus not uncommon\\nto have slightly different results for the same input data. If that\\nhappens, try with a smaller tol parameter. This randomness can also be\\ncontrolled with the random_state parameter. When dual is\\nset to False the underlying implementation of LinearSVC is\\nnot random and random_state has no effect on the results.\\nUsing L1 penalization as provided by LinearSVC(penalty=\\'l1\\',\\ndual=False) yields a sparse solution, i.e. only a subset of feature\\nweights is different from zero and contribute to the decision function.\\nIncreasing C yields a more complex model (more features are selected).\\nThe C value that yields a “null” model (all weights equal to zero) can\\nbe calculated using l1_min_c.\\n- 1.4.6. Kernel functions¶\\nThe kernel function can be any of the following:\\nlinear: (\\\\langle x, x\\'\\\\rangle).\\npolynomial: ((\\\\gamma \\\\langle x, x\\'\\\\rangle + r)^d), where\\n(d) is specified by parameter degree, (r) by coef0.\\nrbf: (\\\\exp(-\\\\gamma |x-x\\'|^2)), where (\\\\gamma) is\\nspecified by parameter gamma, must be greater than 0.\\nsigmoid (\\\\tanh(\\\\gamma \\\\langle x,x\\'\\\\rangle + r)),\\nwhere (r) is specified by coef0.\\nDifferent kernels are specified by the kernel parameter:\\nlinear_svc = svm.SVC(kernel=\\'linear\\')\\nlinear_svc.kernel\\n\\'linear\\'\\nrbf_svc = svm.SVC(kernel=\\'rbf\\')\\nrbf_svc.kernel\\n\\'rbf\\'\\nSee also Kernel Approximation for a solution to use RBF kernels that is much faster and more scalable.\\n1.4.6.1. Parameters of the RBF Kernel¶\\nWhen training an SVM with the Radial Basis Function (RBF) kernel, two\\nparameters must be considered: C and gamma.  The parameter C,\\ncommon to all SVM kernels, trades off misclassification of training examples\\nagainst simplicity of the decision surface. A low C makes the decision\\nsurface smooth, while a high C aims at classifying all training examples\\ncorrectly.  gamma defines how much influence a single training example has.\\nThe larger gamma is, the closer other examples must be to be affected.\\nProper choice of C and gamma is critical to the SVM’s performance.  One\\nis advised to use GridSearchCV with\\nC and gamma spaced exponentially far apart to choose good values.\\nExamples:\\nRBF SVM parameters\\nNon-linear SVM\\n1.4.6.2. Custom Kernels¶\\nYou can define your own kernels by either giving the kernel as a\\npython function or by precomputing the Gram matrix.\\nClassifiers with custom kernels behave the same way as any other\\nclassifiers, except that:\\nField support_vectors_ is now empty, only indices of support\\nvectors are stored in support_\\nA reference (and not a copy) of the first argument in the fit()\\nmethod is stored for future reference. If that array changes between the\\nuse of fit() and predict() you will have unexpected results.\\nUsing Python functions as kernels\\nClick for more details\\n¶\\nYou can use your own defined kernels by passing a function to the\\nkernel parameter.\\nYour kernel must take as arguments two matrices of shape\\n(n_samples_1, n_features), (n_samples_2, n_features)\\nand return a kernel matrix of shape (n_samples_1, n_samples_2).\\nThe following code defines a linear kernel and creates a classifier\\ninstance that will use that kernel:\\nimport numpy as np\\nfrom sklearn import svm\\ndef my_kernel(X, Y):\\n...     return np.dot(X, Y.T)\\n...\\nclf = svm.SVC(kernel=my_kernel)\\nUsing the Gram matrix\\nClick for more details\\n¶\\nYou can pass pre-computed kernels by using the kernel=\\'precomputed\\'\\noption. You should then pass Gram matrix instead of X to the fit and\\npredict methods. The kernel values between all training vectors and the\\ntest vectors must be provided:\\nimport numpy as np\\nfrom sklearn.datasets import make_classification\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn import svm\\nX, y = make_classification(n_samples=10, random_state=0)\\nX_train , X_test , y_train, y_test = train_test_split(X, y, random_state=0)\\nclf = svm.SVC(kernel=\\'precomputed\\')\\n\\nlinear kernel computation\\n\\ngram_train = np.dot(X_train, X_train.T)\\nclf.fit(gram_train, y_train)\\nSVC(kernel=\\'precomputed\\')\\n\\npredict on training examples\\n\\ngram_test = np.dot(X_test, X_train.T)\\nclf.predict(gram_test)\\narray([0, 1, 0])\\nExamples:\\nSVM with custom kernel.\\n- 1.4.7. Mathematical formulation¶\\nA support vector machine constructs a hyper-plane or set of hyper-planes in a\\nhigh or infinite dimensional space, which can be used for\\nclassification, regression or other tasks. Intuitively, a good\\nseparation is achieved by the hyper-plane that has the largest distance\\nto the nearest training data points of any class (so-called functional\\nmargin), since in general the larger the margin the lower the\\ngeneralization error of the classifier. The figure below shows the decision\\nfunction for a linearly separable problem, with three samples on the\\nmargin boundaries, called “support vectors”:\\nIn general, when the problem isn’t linearly separable, the support vectors\\nare the samples within the margin boundaries.\\nWe recommend [13] and [14] as good references for the theory and\\npracticalities of SVMs.\\n1.4.7.1. SVC¶\\nGiven training vectors (x_i \\\\in \\\\mathbb{R}^p), i=1,…, n, in two classes, and a\\nvector (y \\\\in {1, -1}^n), our goal is to find (w \\\\in\\n\\\\mathbb{R}^p) and (b \\\\in \\\\mathbb{R}) such that the prediction given by\\n(\\\\text{sign} (w^T\\\\phi(x) + b)) is correct for most samples.\\nSVC solves the following primal problem:\\n[ \\\\begin{align}\\\\begin{aligned}\\\\min_ {w, b, \\\\zeta} \\\\frac{1}{2} w^T w + C \\\\sum_{i=1}^{n} \\\\zeta_i\\\\\\\\begin{split}\\\\textrm {subject to } & y_i (w^T \\\\phi (x_i) + b) \\\\geq 1 - \\\\zeta_i,\\\\\\n& \\\\zeta_i \\\\geq 0, i=1, ..., n\\\\end{split}\\\\end{aligned}\\\\end{align} ]\\nIntuitively, we’re trying to maximize the margin (by minimizing\\n(||w||^2 = w^Tw)), while incurring a penalty when a sample is\\nmisclassified or within the margin boundary. Ideally, the value (y_i\\n(w^T \\\\phi (x_i) + b)) would be (\\\\geq 1) for all samples, which\\nindicates a perfect prediction. But problems are usually not always perfectly\\nseparable with a hyperplane, so we allow some samples to be at a distance (\\\\zeta_i) from\\ntheir correct margin boundary. The penalty term C controls the strength of\\nthis penalty, and as a result, acts as an inverse regularization parameter\\n(see note below).\\nThe dual problem to the primal is\\n[ \\\\begin{align}\\\\begin{aligned}\\\\min_{\\\\alpha} \\\\frac{1}{2} \\\\alpha^T Q \\\\alpha - e^T \\\\alpha\\\\\\\\begin{split}\\n\\\\textrm {subject to } & y^T \\\\alpha = 0\\\\\\n& 0 \\\\leq \\\\alpha_i \\\\leq C, i=1, ..., n\\\\end{split}\\\\end{aligned}\\\\end{align} ]\\nwhere (e) is the vector of all ones,\\nand (Q) is an (n) by (n) positive semidefinite matrix,\\n(Q_{ij} \\\\equiv y_i y_j K(x_i, x_j)), where (K(x_i, x_j) = \\\\phi (x_i)^T \\\\phi (x_j))\\nis the kernel. The terms (\\\\alpha_i) are called the dual coefficients,\\nand they are upper-bounded by (C).\\nThis dual representation highlights the fact that training vectors are\\nimplicitly mapped into a higher (maybe infinite)\\ndimensional space by the function (\\\\phi): see kernel trick.\\nOnce the optimization problem is solved, the output of\\ndecision_function for a given sample (x) becomes:\\n[\\\\sum_{i\\\\in SV} y_i \\\\alpha_i K(x_i, x) + b,]\\nand the predicted class correspond to its sign. We only need to sum over the\\nsupport vectors (i.e. the samples that lie within the margin) because the\\ndual coefficients (\\\\alpha_i) are zero for the other samples.\\nThese parameters can be accessed through the attributes dual_coef_\\nwhich holds the product (y_i \\\\alpha_i), support_vectors_ which\\nholds the support vectors, and intercept_ which holds the independent\\nterm (b)\\nNote\\nWhile SVM models derived from libsvm and liblinear use C as\\nregularization parameter, most other estimators use alpha. The exact\\nequivalence between the amount of regularization of two models depends on\\nthe exact objective function optimized by the model. For example, when the\\nestimator used is Ridge regression,\\nthe relation between them is given as (C = \\\\frac{1}{alpha}).\\nLinearSVC\\nClick for more details\\n¶\\nThe primal problem can be equivalently formulated as\\n[\\\\min_ {w, b} \\\\frac{1}{2} w^T w + C \\\\sum_{i=1}^{n}\\\\max(0, 1 - y_i (w^T \\\\phi(x_i) + b)),]\\nwhere we make use of the hinge loss. This is the form that is\\ndirectly optimized by LinearSVC, but unlike the dual form, this one\\ndoes not involve inner products between samples, so the famous kernel trick\\ncannot be applied. This is why only the linear kernel is supported by\\nLinearSVC ((\\\\phi) is the identity function).\\nNuSVC\\nClick for more details\\n¶\\nThe (\\\\nu)-SVC formulation [15] is a reparameterization of the\\n(C)-SVC and therefore mathematically equivalent.\\nWe introduce a new parameter (\\\\nu) (instead of (C)) which\\ncontrols the number of support vectors and margin errors:\\n(\\\\nu \\\\in (0, 1]) is an upper bound on the fraction of margin errors and\\na lower bound of the fraction of support vectors. A margin error corresponds\\nto a sample that lies on the wrong side of its margin boundary: it is either\\nmisclassified, or it is correctly classified but does not lie beyond the\\nmargin.\\n1.4.7.2. SVR¶\\nGiven training vectors (x_i \\\\in \\\\mathbb{R}^p), i=1,…, n, and a\\nvector (y \\\\in \\\\mathbb{R}^n) (\\\\varepsilon)-SVR solves the following primal problem:\\n[ \\\\begin{align}\\\\begin{aligned}\\\\min_ {w, b, \\\\zeta, \\\\zeta^} \\\\frac{1}{2} w^T w + C \\\\sum_{i=1}^{n} (\\\\zeta_i + \\\\zeta_i^)\\\\\\\\begin{split}\\\\textrm {subject to } & y_i - w^T \\\\phi (x_i) - b \\\\leq \\\\varepsilon + \\\\zeta_i,\\\\\\n& w^T \\\\phi (x_i) + b - y_i \\\\leq \\\\varepsilon + \\\\zeta_i^,\\\\\\n& \\\\zeta_i, \\\\zeta_i^ \\\\geq 0, i=1, ..., n\\\\end{split}\\\\end{aligned}\\\\end{align} ]\\nHere, we are penalizing samples whose prediction is at least (\\\\varepsilon)\\naway from their true target. These samples penalize the objective by\\n(\\\\zeta_i) or (\\\\zeta_i^), depending on whether their predictions\\nlie above or below the (\\\\varepsilon) tube.\\nThe dual problem is\\n[ \\\\begin{align}\\\\begin{aligned}\\\\min_{\\\\alpha, \\\\alpha^} \\\\frac{1}{2} (\\\\alpha - \\\\alpha^)^T Q (\\\\alpha - \\\\alpha^) + \\\\varepsilon e^T (\\\\alpha + \\\\alpha^) - y^T (\\\\alpha - \\\\alpha^)\\\\\\\\begin{split}\\n\\\\textrm {subject to } & e^T (\\\\alpha - \\\\alpha^) = 0\\\\\\n& 0 \\\\leq \\\\alpha_i, \\\\alpha_i^ \\\\leq C, i=1, ..., n\\\\end{split}\\\\end{aligned}\\\\end{align} ]\\nwhere (e) is the vector of all ones,\\n(Q) is an (n) by (n) positive semidefinite matrix,\\n(Q_{ij} \\\\equiv K(x_i, x_j) = \\\\phi (x_i)^T \\\\phi (x_j))\\nis the kernel. Here training vectors are implicitly mapped into a higher\\n(maybe infinite) dimensional space by the function (\\\\phi).\\nThe prediction is:\\n[\\\\sum_{i \\\\in SV}(\\\\alpha_i - \\\\alpha_i^) K(x_i, x) + b]\\nThese parameters can be accessed through the attributes dual_coef_\\nwhich holds the difference (\\\\alpha_i - \\\\alpha_i^), support_vectors_ which\\nholds the support vectors, and intercept_ which holds the independent\\nterm (b)\\nLinearSVR\\nClick for more details\\n¶\\nThe primal problem can be equivalently formulated as\\n[\\\\min_ {w, b} \\\\frac{1}{2} w^T w + C \\\\sum_{i=1}^{n}\\\\max(0, |y_i - (w^T \\\\phi(x_i) + b)| - \\\\varepsilon),]\\nwhere we make use of the epsilon-insensitive loss, i.e. errors of less than\\n(\\\\varepsilon) are ignored. This is the form that is directly optimized\\nby LinearSVR.\\n- 1.4.8. Implementation details¶\\nInternally, we use libsvm [12] and liblinear [11] to handle all\\ncomputations. These libraries are wrapped using C and Cython.\\nFor a description of the implementation and details of the algorithms\\nused, please refer to their respective papers.\\nReferences:\\n[9]\\nPlatt “Probabilistic outputs for SVMs and comparisons to\\nregularized likelihood methods”.\\n[10]\\nWu, Lin and Weng, “Probability estimates for multi-class\\nclassification by pairwise coupling”, JMLR\\n5:975-1005, 2004.\\n[11]\\n(1,2)\\nFan, Rong-En, et al.,\\n“LIBLINEAR: A library for large linear classification.”,\\nJournal of machine learning research 9.Aug (2008): 1871-1874.\\n[12]\\n(1,2)\\nChang and Lin, LIBSVM: A Library for Support Vector Machines.\\n[13]\\nBishop, Pattern recognition and machine learning,\\nchapter 7 Sparse Kernel Machines\\n[14]\\n“A Tutorial on Support Vector Regression”\\nAlex J. Smola, Bernhard Schölkopf - Statistics and Computing archive\\nVolume 14 Issue 3, August 2004, p. 199-222.\\n[15]\\nSchölkopf et. al New Support Vector Algorithms\\n[16]\\nCrammer and Singer On the Algorithmic Implementation ofMulticlass\\nKernel-based Vector Machines,\\nJMLR 2001.\\n© 2007 - 2024, scikit-learn developers (BSD License).\\nShow this page source\\nClick to add a cell.\\n\\n1.5. Stochastic Gradient Descent — scikit-learn 1.4.2 documentation\\n\\nInstall\\nUser Guide\\nAPI\\nExamples\\nCommunity\\nGetting Started\\nTutorial\\nWhat\\'s new\\nGlossary\\nDevelopment\\nFAQ\\nSupport\\nRelated packages\\nRoadmap\\nGovernance\\nAbout us\\nGitHub\\nOther Versions and Download\\nMore\\nGetting Started\\nTutorial\\nWhat\\'s new\\nGlossary\\nDevelopment\\nFAQ\\nSupport\\nRelated packages\\nRoadmap\\nGovernance\\nAbout us\\nGitHub\\nOther Versions and Download\\nToggle Menu\\nPrevUp\\nNext\\nscikit-learn 1.4.2\\nOther versions\\nPlease cite us if you use the software.\\n\\n1.5. Stochastic Gradient Descent\\n\\n1.5.1. Classification\\n\\n1.5.2. Regression\\n\\n1.5.3. Online One-Class SVM\\n\\n1.5.4. Stochastic Gradient Descent for sparse data\\n\\n1.5.5. Complexity\\n\\n1.5.6. Stopping criterion\\n\\n1.5.7. Tips on Practical Use\\n\\n1.5.8. Mathematical formulation\\n1.5.8.1. SGD\\n\\n1.5.9. Implementation details\\n\\n1.5. Stochastic Gradient Descent¶\\n\\nStochastic Gradient Descent (SGD) is a simple yet very efficient\\napproach to fitting linear classifiers and regressors under\\nconvex loss functions such as (linear) Support Vector Machines and Logistic\\nRegression.\\nEven though SGD has been around in the machine learning community for\\na long time, it has received a considerable amount of attention just\\nrecently in the context of large-scale learning.\\nSGD has been successfully applied to large-scale and sparse machine\\nlearning problems often encountered in text classification and natural\\nlanguage processing.  Given that the data is sparse, the classifiers\\nin this module easily scale to problems with more than 10^5 training\\nexamples and more than 10^5 features.\\nStrictly speaking, SGD is merely an optimization technique and does not\\ncorrespond to a specific family of machine learning models. It is only a\\nway to train a model. Often, an instance of SGDClassifier or\\nSGDRegressor will have an equivalent estimator in\\nthe scikit-learn API, potentially using a different optimization technique.\\nFor example, using SGDClassifier(loss=\\'log_loss\\') results in logistic regression,\\ni.e. a model equivalent to LogisticRegression\\nwhich is fitted via SGD instead of being fitted by one of the other solvers\\nin LogisticRegression. Similarly,\\nSGDRegressor(loss=\\'squared_error\\', penalty=\\'l2\\') and\\nRidge solve the same optimization problem, via\\ndifferent means.\\nThe advantages of Stochastic Gradient Descent are:\\nEfficiency.\\nEase of implementation (lots of opportunities for code tuning).\\nThe disadvantages of Stochastic Gradient Descent include:\\nSGD requires a number of hyperparameters such as the regularization\\nparameter and the number of iterations.\\nSGD is sensitive to feature scaling.\\nWarning\\nMake sure you permute (shuffle) your training data before fitting the model\\nor use shuffle=True to shuffle after each iteration (used by default).\\nAlso, ideally, features should be standardized using e.g.\\nmake_pipeline(StandardScaler(), SGDClassifier()) (see Pipelines).\\n- 1.5.1. Classification¶\\nThe class SGDClassifier implements a plain stochastic gradient\\ndescent learning routine which supports different loss functions and\\npenalties for classification. Below is the decision boundary of a\\nSGDClassifier trained with the hinge loss, equivalent to a linear SVM.\\nAs other classifiers, SGD has to be fitted with two arrays: an array X\\nof shape (n_samples, n_features) holding the training samples, and an\\narray y of shape (n_samples,) holding the target values (class labels)\\nfor the training samples:\\n\\nfrom sklearn.linear_model import SGDClassifier\\nX = [[0., 0.], [1., 1.]]\\ny = [0, 1]\\nclf = SGDClassifier(loss=\"hinge\", penalty=\"l2\", max_iter=5)\\nclf.fit(X, y)\\nSGDClassifier(max_iter=5)\\nAfter being fitted, the model can then be used to predict new values:\\nclf.predict([[2., 2.]])\\narray([1])\\nSGD fits a linear model to the training data. The coef_ attribute holds\\nthe model parameters:\\nclf.coef_\\narray([[9.9..., 9.9...]])\\nThe intercept_ attribute holds the intercept (aka offset or bias):\\nclf.intercept_\\narray([-9.9...])\\nWhether or not the model should use an intercept, i.e. a biased\\nhyperplane, is controlled by the parameter fit_intercept.\\nThe signed distance to the hyperplane (computed as the dot product between\\nthe coefficients and the input sample, plus the intercept) is given by\\nSGDClassifier.decision_function:\\nclf.decision_function([[2., 2.]])\\narray([29.6...])\\nThe concrete loss function can be set via the loss\\nparameter. SGDClassifier supports the following loss functions:\\nloss=\"hinge\": (soft-margin) linear Support Vector Machine,\\nloss=\"modified_huber\": smoothed hinge loss,\\nloss=\"log_loss\": logistic regression,\\nand all regression losses below. In this case the target is encoded as -1\\nor 1, and the problem is treated as a regression problem. The predicted\\nclass then correspond to the sign of the predicted target.\\nPlease refer to the mathematical section below for formulas.\\nThe first two loss functions are lazy, they only update the model\\nparameters if an example violates the margin constraint, which makes\\ntraining very efficient and may result in sparser models (i.e. with more zero\\ncoefficients), even when L2 penalty is used.\\nUsing loss=\"log_loss\" or loss=\"modified_huber\" enables the\\npredict_proba method, which gives a vector of probability estimates\\n(P(y|x)) per sample (x):\\nclf = SGDClassifier(loss=\"log_loss\", max_iter=5).fit(X, y)\\nclf.predict_proba([[1., 1.]])\\narray([[0.00..., 0.99...]])\\nThe concrete penalty can be set via the penalty parameter.\\nSGD supports the following penalties:\\npenalty=\"l2\": L2 norm penalty on coef_.\\npenalty=\"l1\": L1 norm penalty on coef_.\\npenalty=\"elasticnet\": Convex combination of L2 and L1;\\n(1 - l1_ratio) * L2 + l1_ratio * L1.\\nThe default setting is penalty=\"l2\". The L1 penalty leads to sparse\\nsolutions, driving most coefficients to zero. The Elastic Net [11] solves\\nsome deficiencies of the L1 penalty in the presence of highly correlated\\nattributes. The parameter l1_ratio controls the convex combination\\nof L1 and L2 penalty.\\nSGDClassifier supports multi-class classification by combining\\nmultiple binary classifiers in a “one versus all” (OVA) scheme. For each\\nof the (K) classes, a binary classifier is learned that discriminates\\nbetween that and all other (K-1) classes. At testing time, we compute the\\nconfidence score (i.e. the signed distances to the hyperplane) for each\\nclassifier and choose the class with the highest confidence. The Figure\\nbelow illustrates the OVA approach on the iris dataset.  The dashed\\nlines represent the three OVA classifiers; the background colors show\\nthe decision surface induced by the three classifiers.\\nIn the case of multi-class classification coef_ is a two-dimensional\\narray of shape (n_classes, n_features) and intercept_ is a\\none-dimensional array of shape (n_classes,). The i-th row of coef_ holds\\nthe weight vector of the OVA classifier for the i-th class; classes are\\nindexed in ascending order (see attribute classes_).\\nNote that, in principle, since they allow to create a probability model,\\nloss=\"log_loss\" and loss=\"modified_huber\" are more suitable for\\none-vs-all classification.\\nSGDClassifier supports both weighted classes and weighted\\ninstances via the fit parameters class_weight and sample_weight. See\\nthe examples below and the docstring of SGDClassifier.fit for\\nfurther information.\\nSGDClassifier supports averaged SGD (ASGD) [10]. Averaging can be\\nenabled by setting average=True. ASGD performs the same updates as the\\nregular SGD (see Mathematical formulation), but instead of using\\nthe last value of the coefficients as the coef_ attribute (i.e. the values\\nof the last update), coef_ is set instead to the average value of the\\ncoefficients across all updates. The same is done for the intercept_\\nattribute. When using ASGD the learning rate can be larger and even constant,\\nleading on some datasets to a speed up in training time.\\nFor classification with a logistic loss, another variant of SGD with an\\naveraging strategy is available with Stochastic Average Gradient (SAG)\\nalgorithm, available as a solver in LogisticRegression.\\nExamples:\\nSGD: Maximum margin separating hyperplane,\\nPlot multi-class SGD on the iris dataset\\nSGD: Weighted samples\\nComparing various online solvers\\nSVM: Separating hyperplane for unbalanced classes\\n(See the Note in the example)\\n- 1.5.2. Regression¶\\nThe class SGDRegressor implements a plain stochastic gradient\\ndescent learning routine which supports different loss functions and\\npenalties to fit linear regression models. SGDRegressor is\\nwell suited for regression problems with a large number of training\\nsamples (> 10.000), for other problems we recommend Ridge,\\nLasso, or ElasticNet.\\nThe concrete loss function can be set via the loss\\nparameter. SGDRegressor supports the following loss functions:\\nloss=\"squared_error\": Ordinary least squares,\\nloss=\"huber\": Huber loss for robust regression,\\nloss=\"epsilon_insensitive\": linear Support Vector Regression.\\nPlease refer to the mathematical section below for formulas.\\nThe Huber and epsilon-insensitive loss functions can be used for\\nrobust regression. The width of the insensitive region has to be\\nspecified via the parameter epsilon. This parameter depends on the\\nscale of the target variables.\\nThe penalty parameter determines the regularization to be used (see\\ndescription above in the classification section).\\nSGDRegressor also supports averaged SGD [10] (here again, see\\ndescription above in the classification section).\\nFor regression with a squared loss and a l2 penalty, another variant of\\nSGD with an averaging strategy is available with Stochastic Average\\nGradient (SAG) algorithm, available as a solver in Ridge.\\n- 1.5.3. Online One-Class SVM¶\\nThe class sklearn.linear_model.SGDOneClassSVM implements an online\\nlinear version of the One-Class SVM using a stochastic gradient descent.\\nCombined with kernel approximation techniques,\\nsklearn.linear_model.SGDOneClassSVM can be used to approximate the\\nsolution of a kernelized One-Class SVM, implemented in\\nsklearn.svm.OneClassSVM, with a linear complexity in the number of\\nsamples. Note that the complexity of a kernelized One-Class SVM is at best\\nquadratic in the number of samples.\\nsklearn.linear_model.SGDOneClassSVM is thus well suited for datasets\\nwith a large number of training samples (> 10,000) for which the SGD\\nvariant can be several orders of magnitude faster.\\nMathematical details\\nClick for more details\\n¶\\nIts implementation is based on the implementation of the stochastic\\ngradient descent. Indeed, the original optimization problem of the One-Class\\nSVM is given by\\n[\\\\begin{split}\\\\begin{aligned}\\n\\\\min_{w, \\\\rho, \\\\xi} & \\\\quad \\\\frac{1}{2}\\\\Vert w \\\\Vert^2 - \\\\rho + \\\\frac{1}{\\\\nu n} \\\\sum_{i=1}^n \\\\xi_i \\\\\\n\\\\text{s.t.} & \\\\quad \\\\langle w, x_i \\\\rangle \\\\geq \\\\rho - \\\\xi_i \\\\quad 1 \\\\leq i \\\\leq n \\\\\\n& \\\\quad \\\\xi_i \\\\geq 0 \\\\quad 1 \\\\leq i \\\\leq n\\n\\\\end{aligned}\\\\end{split}]\\nwhere (\\\\nu \\\\in (0, 1]) is the user-specified parameter controlling the\\nproportion of outliers and the proportion of support vectors. Getting rid of\\nthe slack variables (\\\\xi_i) this problem is equivalent to\\n[\\\\min_{w, \\\\rho} \\\\frac{1}{2}\\\\Vert w \\\\Vert^2 - \\\\rho + \\\\frac{1}{\\\\nu n} \\\\sum_{i=1}^n \\\\max(0, \\\\rho - \\\\langle w, x_i \\\\rangle) \\\\, .]\\nMultiplying by the constant (\\\\nu) and introducing the intercept\\n(b = 1 - \\\\rho) we obtain the following equivalent optimization problem\\n[\\\\min_{w, b} \\\\frac{\\\\nu}{2}\\\\Vert w \\\\Vert^2 + b\\\\nu + \\\\frac{1}{n} \\\\sum_{i=1}^n \\\\max(0, 1 - (\\\\langle w, x_i \\\\rangle + b)) \\\\, .]\\nThis is similar to the optimization problems studied in section\\nMathematical formulation with (y_i = 1, 1 \\\\leq i \\\\leq n) and\\n(\\\\alpha = \\\\nu/2), (L) being the hinge loss function and (R)\\nbeing the L2 norm. We just need to add the term (b\\\\nu) in the\\noptimization loop.\\nAs SGDClassifier and SGDRegressor, SGDOneClassSVM\\nsupports averaged SGD. Averaging can be enabled by setting average=True.\\n- 1.5.4. Stochastic Gradient Descent for sparse data¶\\nNote\\nThe sparse implementation produces slightly different results\\nfrom the dense implementation, due to a shrunk learning rate for the\\nintercept. See Implementation details.\\nThere is built-in support for sparse data given in any matrix in a format\\nsupported by scipy.sparse. For maximum\\nefficiency, however, use the CSR\\nmatrix format as defined in scipy.sparse.csr_matrix.\\nExamples:\\nClassification of text documents using sparse features\\n- 1.5.5. Complexity¶\\nThe major advantage of SGD is its efficiency, which is basically\\nlinear in the number of training examples. If X is a matrix of size (n, p)\\ntraining has a cost of (O(k n \\\\bar p)), where k is the number\\nof iterations (epochs) and (\\\\bar p) is the average number of\\nnon-zero attributes per sample.\\nRecent theoretical results, however, show that the runtime to get some\\ndesired optimization accuracy does not increase as the training set size increases.\\n- 1.5.6. Stopping criterion¶\\nThe classes SGDClassifier and SGDRegressor provide two\\ncriteria to stop the algorithm when a given level of convergence is reached:\\nWith early_stopping=True, the input data is split into a training set\\nand a validation set. The model is then fitted on the training set, and the\\nstopping criterion is based on the prediction score (using the score\\nmethod) computed on the validation set. The size of the validation set\\ncan be changed with the parameter validation_fraction.\\nWith early_stopping=False, the model is fitted on the entire input data\\nand the stopping criterion is based on the objective function computed on\\nthe training data.\\nIn both cases, the criterion is evaluated once by epoch, and the algorithm stops\\nwhen the criterion does not improve n_iter_no_change times in a row. The\\nimprovement is evaluated with absolute tolerance tol, and the algorithm\\nstops in any case after a maximum number of iteration max_iter.\\n- 1.5.7. Tips on Practical Use¶\\nStochastic Gradient Descent is sensitive to feature scaling, so it\\nis highly recommended to scale your data. For example, scale each\\nattribute on the input vector X to [0,1] or [-1,+1], or standardize\\nit to have mean 0 and variance 1. Note that the same scaling must be\\napplied to the test vector to obtain meaningful results. This can be easily\\ndone using StandardScaler:\\nfrom sklearn.preprocessing import StandardScaler\\nscaler = StandardScaler()\\nscaler.fit(X_train)  # Don\\'t cheat - fit only on training data\\nX_train = scaler.transform(X_train)\\nX_test = scaler.transform(X_test)  # apply same transformation to test data\\n\\nOr better yet: use a pipeline!\\n\\nfrom sklearn.pipeline import make_pipeline\\nest = make_pipeline(StandardScaler(), SGDClassifier())\\nest.fit(X_train)\\nest.predict(X_test)\\nIf your attributes have an intrinsic scale (e.g. word frequencies or\\nindicator features) scaling is not needed.\\nFinding a reasonable regularization term (\\\\alpha) is\\nbest done using automatic hyper-parameter search, e.g.\\nGridSearchCV or\\nRandomizedSearchCV, usually in the\\nrange 10.0-np.arange(1,7).\\nEmpirically, we found that SGD converges after observing\\napproximately 10^6 training samples. Thus, a reasonable first guess\\nfor the number of iterations is max_iter = np.ceil(106 / n),\\nwhere n is the size of the training set.\\nIf you apply SGD to features extracted using PCA we found that\\nit is often wise to scale the feature values by some constant c\\nsuch that the average L2 norm of the training data equals one.\\nWe found that Averaged SGD works best with a larger number of features\\nand a higher eta0.\\nReferences:\\n“Efficient BackProp”\\nY. LeCun, L. Bottou, G. Orr, K. Müller - In Neural Networks: Tricks\\nof the Trade 1998.\\n- 1.5.8. Mathematical formulation¶\\nWe describe here the mathematical details of the SGD procedure. A good\\noverview with convergence rates can be found in [12].\\nGiven a set of training examples ((x_1, y_1), \\\\ldots, (x_n, y_n)) where\\n(x_i \\\\in \\\\mathbf{R}^m) and (y_i \\\\in \\\\mathcal{R}) ((y_i \\\\in\\n{-1, 1}) for classification), our goal is to learn a linear scoring function\\n(f(x) = w^T x + b) with model parameters (w \\\\in \\\\mathbf{R}^m) and\\nintercept (b \\\\in \\\\mathbf{R}). In order to make predictions for binary\\nclassification, we simply look at the sign of (f(x)). To find the model\\nparameters, we minimize the regularized training error given by\\n[E(w,b) = \\\\frac{1}{n}\\\\sum_{i=1}^{n} L(y_i, f(x_i)) + \\\\alpha R(w)]\\nwhere (L) is a loss function that measures model (mis)fit and\\n(R) is a regularization term (aka penalty) that penalizes model\\ncomplexity; (\\\\alpha > 0) is a non-negative hyperparameter that controls\\nthe regularization strength.\\nLoss functions details\\nClick for more details\\n¶\\nDifferent choices for (L) entail different classifiers or regressors:\\nHinge (soft-margin): equivalent to Support Vector Classification.\\n(L(y_i, f(x_i)) = \\\\max(0, 1 - y_i f(x_i))).\\nPerceptron:\\n(L(y_i, f(x_i)) = \\\\max(0, - y_i f(x_i))).\\nModified Huber:\\n(L(y_i, f(x_i)) = \\\\max(0, 1 - y_i f(x_i))^2) if (y_i f(x_i) >\\n-1), and (L(y_i, f(x_i)) = -4 y_i f(x_i)) otherwise.\\nLog Loss: equivalent to Logistic Regression.\\n(L(y_i, f(x_i)) = \\\\log(1 + \\\\exp (-y_i f(x_i)))).\\nSquared Error: Linear regression (Ridge or Lasso depending on\\n(R)).\\n(L(y_i, f(x_i)) = \\\\frac{1}{2}(y_i - f(x_i))^2).\\nHuber: less sensitive to outliers than least-squares. It is equivalent to\\nleast squares when (|y_i - f(x_i)| \\\\leq \\\\varepsilon), and\\n(L(y_i, f(x_i)) = \\\\varepsilon |y_i - f(x_i)| - \\\\frac{1}{2}\\n\\\\varepsilon^2) otherwise.\\nEpsilon-Insensitive: (soft-margin) equivalent to Support Vector Regression.\\n(L(y_i, f(x_i)) = \\\\max(0, |y_i - f(x_i)| - \\\\varepsilon)).\\nAll of the above loss functions can be regarded as an upper bound on the\\nmisclassification error (Zero-one loss) as shown in the Figure below.\\nPopular choices for the regularization term (R) (the penalty\\nparameter) include:\\nL2 norm: (R(w) := \\\\frac{1}{2} \\\\sum_{j=1}^{m} w_j^2 = ||w||2^2),\\nL1 norm: (R(w) := \\\\sum{j=1}^{m} |w_j|), which leads to sparse\\nsolutions.\\nElastic Net: (R(w) := \\\\frac{\\\\rho}{2} \\\\sum_{j=1}^{n} w_j^2 +\\n(1-\\\\rho) \\\\sum_{j=1}^{m} |w_j|), a convex combination of L2 and L1, where\\n(\\\\rho) is given by 1 - l1_ratio.\\nThe Figure below shows the contours of the different regularization terms\\nin a 2-dimensional parameter space ((m=2)) when (R(w) = 1).\\n1.5.8.1. SGD¶\\nStochastic gradient descent is an optimization method for unconstrained\\noptimization problems. In contrast to (batch) gradient descent, SGD\\napproximates the true gradient of (E(w,b)) by considering a\\nsingle training example at a time.\\nThe class SGDClassifier implements a first-order SGD learning\\nroutine.  The algorithm iterates over the training examples and for each\\nexample updates the model parameters according to the update rule given by\\n[w \\\\leftarrow w - \\\\eta \\\\left[\\\\alpha \\\\frac{\\\\partial R(w)}{\\\\partial w}\\n+ \\\\frac{\\\\partial L(w^T x_i + b, y_i)}{\\\\partial w}\\\\right]]\\nwhere (\\\\eta) is the learning rate which controls the step-size in\\nthe parameter space.  The intercept (b) is updated similarly but\\nwithout regularization (and with additional decay for sparse matrices, as\\ndetailed in Implementation details).\\nThe learning rate (\\\\eta) can be either constant or gradually decaying. For\\nclassification, the default learning rate schedule (learning_rate=\\'optimal\\')\\nis given by\\n[\\\\eta^{(t)} = \\\\frac {1}{\\\\alpha  (t_0 + t)}]\\nwhere (t) is the time step (there are a total of n_samples * n_iter\\ntime steps), (t_0) is determined based on a heuristic proposed by Léon Bottou\\nsuch that the expected initial updates are comparable with the expected\\nsize of the weights (this assuming that the norm of the training samples is\\napprox. 1). The exact definition can be found in init_t in BaseSGD.\\nFor regression the default learning rate schedule is inverse scaling\\n(learning_rate=\\'invscaling\\'), given by\\n[\\\\eta^{(t)} = \\\\frac{eta_0}{t^{power_t}}]\\nwhere (eta_0) and (power_t) are hyperparameters chosen by the\\nuser via eta0 and power_t, resp.\\nFor a constant learning rate use learning_rate=\\'constant\\' and use eta0\\nto specify the learning rate.\\nFor an adaptively decreasing learning rate, use learning_rate=\\'adaptive\\'\\nand use eta0 to specify the starting learning rate. When the stopping\\ncriterion is reached, the learning rate is divided by 5, and the algorithm\\ndoes not stop. The algorithm stops when the learning rate goes below 1e-6.\\nThe model parameters can be accessed through the coef and\\nintercept_ attributes: coef_ holds the weights (w) and\\nintercept_ holds (b).\\nWhen using Averaged SGD (with the average parameter), coef_ is set to the\\naverage weight across all updates:\\ncoef_ (= \\\\frac{1}{T} \\\\sum_{t=0}^{T-1} w^{(t)}),\\nwhere (T) is the total number of updates, found in the t_ attribute.\\n- 1.5.9. Implementation details¶\\nThe implementation of SGD is influenced by the Stochastic Gradient SVM of\\n[7].\\nSimilar to SvmSGD,\\nthe weight vector is represented as the product of a scalar and a vector\\nwhich allows an efficient weight update in the case of L2 regularization.\\nIn the case of sparse input X, the intercept is updated with a\\nsmaller learning rate (multiplied by 0.01) to account for the fact that\\nit is updated more frequently. Training examples are picked up sequentially\\nand the learning rate is lowered after each observed example. We adopted the\\nlearning rate schedule from [8].\\nFor multi-class classification, a “one versus all” approach is used.\\nWe use the truncated gradient algorithm proposed in [9]\\nfor L1 regularization (and the Elastic Net).\\nThe code is written in Cython.\\nReferences:\\n[7]\\n“Stochastic Gradient Descent” L. Bottou - Website, 2010.\\n[8]\\n“Pegasos: Primal estimated sub-gradient solver for svm”\\nS. Shalev-Shwartz, Y. Singer, N. Srebro - In Proceedings of ICML ‘07.\\n[9]\\n“Stochastic gradient descent training for l1-regularized\\nlog-linear models with cumulative penalty”\\nY. Tsuruoka, J. Tsujii, S. Ananiadou - In Proceedings of the AFNLP/ACL\\n‘09.\\n[10]\\n(1,2)\\n“Towards Optimal One Pass Large Scale Learning with\\nAveraged Stochastic Gradient Descent”\\nXu, Wei (2011)\\n[11]\\n“Regularization and variable selection via the elastic net”\\nH. Zou, T. Hastie - Journal of the Royal Statistical Society Series B,\\n67 (2), 301-320.\\n[12]\\n“Solving large scale linear prediction problems using stochastic\\ngradient descent algorithms”\\nT. Zhang - In Proceedings of ICML ‘04.\\n© 2007 - 2024, scikit-learn developers (BSD License).\\nShow this page source\\n\\n1.6. Nearest Neighbors — scikit-learn 1.4.2 documentation\\n\\nInstall\\nUser Guide\\nAPI\\nExamples\\nCommunity\\nGetting Started\\nTutorial\\nWhat\\'s new\\nGlossary\\nDevelopment\\nFAQ\\nSupport\\nRelated packages\\nRoadmap\\nGovernance\\nAbout us\\nGitHub\\nOther Versions and Download\\nMore\\nGetting Started\\nTutorial\\nWhat\\'s new\\nGlossary\\nDevelopment\\nFAQ\\nSupport\\nRelated packages\\nRoadmap\\nGovernance\\nAbout us\\nGitHub\\nOther Versions and Download\\nToggle Menu\\nPrevUp\\nNext\\nscikit-learn 1.4.2\\nOther versions\\nPlease cite us if you use the software.\\n\\n1.6. Nearest Neighbors\\n\\n1.6.1. Unsupervised Nearest Neighbors\\n1.6.1.1. Finding the Nearest Neighbors\\n1.6.1.2. KDTree and BallTree Classes\\n\\n1.6.2. Nearest Neighbors Classification\\n\\n1.6.3. Nearest Neighbors Regression\\n\\n1.6.4. Nearest Neighbor Algorithms\\n1.6.4.1. Brute Force\\n1.6.4.2. K-D Tree\\n1.6.4.3. Ball Tree\\n1.6.4.4. Choice of Nearest Neighbors Algorithm\\n1.6.4.5. Effect of leaf_size\\n1.6.4.6. Valid Metrics for Nearest Neighbor Algorithms\\n\\n1.6.5. Nearest Centroid Classifier\\n1.6.5.1. Nearest Shrunken Centroid\\n\\n1.6.6. Nearest Neighbors Transformer\\n\\n1.6.7. Neighborhood Components Analysis\\n1.6.7.1. Classification\\n1.6.7.2. Dimensionality reduction\\n1.6.7.3. Mathematical formulation\\n1.6.7.3.1. Mahalanobis distance\\n1.6.7.4. Implementation\\n1.6.7.5. Complexity\\n1.6.7.5.1. Training\\n1.6.7.5.2. Transform\\n\\n1.6. Nearest Neighbors¶\\n\\nsklearn.neighbors provides functionality for unsupervised and\\nsupervised neighbors-based learning methods.  Unsupervised nearest neighbors\\nis the foundation of many other learning methods,\\nnotably manifold learning and spectral clustering.  Supervised neighbors-based\\nlearning comes in two flavors: classification for data with\\ndiscrete labels, and regression for data with continuous labels.\\nThe principle behind nearest neighbor methods is to find a predefined number\\nof training samples closest in distance to the new point, and\\npredict the label from these.  The number of samples can be a user-defined\\nconstant (k-nearest neighbor learning), or vary based\\non the local density of points (radius-based neighbor learning).\\nThe distance can, in general, be any metric measure: standard Euclidean\\ndistance is the most common choice.\\nNeighbors-based methods are known as non-generalizing machine\\nlearning methods, since they simply “remember” all of its training data\\n(possibly transformed into a fast indexing structure such as a\\nBall Tree or KD Tree).\\nDespite its simplicity, nearest neighbors has been successful in a\\nlarge number of classification and regression problems, including\\nhandwritten digits and satellite image scenes. Being a non-parametric method,\\nit is often successful in classification situations where the decision\\nboundary is very irregular.\\nThe classes in sklearn.neighbors can handle either NumPy arrays or\\nscipy.sparse matrices as input.  For dense matrices, a large number of\\npossible distance metrics are supported.  For sparse matrices, arbitrary\\nMinkowski metrics are supported for searches.\\nThere are many learning routines which rely on nearest neighbors at their\\ncore.  One example is kernel density estimation,\\ndiscussed in the density estimation section.\\n- 1.6.1. Unsupervised Nearest Neighbors¶\\nNearestNeighbors implements unsupervised nearest neighbors learning.\\nIt acts as a uniform interface to three different nearest neighbors\\nalgorithms: BallTree, KDTree, and a\\nbrute-force algorithm based on routines in sklearn.metrics.pairwise.\\nThe choice of neighbors search algorithm is controlled through the keyword\\n\\'algorithm\\', which must be one of\\n[\\'auto\\', \\'ball_tree\\', \\'kd_tree\\', \\'brute\\'].  When the default value\\n\\'auto\\' is passed, the algorithm attempts to determine the best approach\\nfrom the training data.  For a discussion of the strengths and weaknesses\\nof each option, see Nearest Neighbor Algorithms.\\nWarning\\nRegarding the Nearest Neighbors algorithms, if two\\nneighbors (k+1) and (k) have identical distances\\nbut different labels, the result will depend on the ordering of the\\ntraining data.\\n1.6.1.1. Finding the Nearest Neighbors¶\\nFor the simple task of finding the nearest neighbors between two sets of\\ndata, the unsupervised algorithms within sklearn.neighbors can be\\nused:\\n\\nfrom sklearn.neighbors import NearestNeighbors\\nimport numpy as np\\nX = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\\nnbrs = NearestNeighbors(n_neighbors=2, algorithm=\\'ball_tree\\').fit(X)\\ndistances, indices = nbrs.kneighbors(X)\\nindices\\narray([[0, 1],\\n[1, 0],\\n[2, 1],\\n[3, 4],\\n[4, 3],\\n[5, 4]]...)\\ndistances\\narray([[0.        , 1.        ],\\n[0.        , 1.        ],\\n[0.        , 1.41421356],\\n[0.        , 1.        ],\\n[0.        , 1.        ],\\n[0.        , 1.41421356]])\\nBecause the query set matches the training set, the nearest neighbor of each\\npoint is the point itself, at a distance of zero.\\nIt is also possible to efficiently produce a sparse graph showing the\\nconnections between neighboring points:\\nnbrs.kneighbors_graph(X).toarray()\\narray([[1., 1., 0., 0., 0., 0.],\\n[1., 1., 0., 0., 0., 0.],\\n[0., 1., 1., 0., 0., 0.],\\n[0., 0., 0., 1., 1., 0.],\\n[0., 0., 0., 1., 1., 0.],\\n[0., 0., 0., 0., 1., 1.]])\\nThe dataset is structured such that points nearby in index order are nearby\\nin parameter space, leading to an approximately block-diagonal matrix of\\nK-nearest neighbors.  Such a sparse graph is useful in a variety of\\ncircumstances which make use of spatial relationships between points for\\nunsupervised learning: in particular, see Isomap,\\nLocallyLinearEmbedding, and\\nSpectralClustering.\\n1.6.1.2. KDTree and BallTree Classes¶\\nAlternatively, one can use the KDTree or BallTree classes\\ndirectly to find nearest neighbors.  This is the functionality wrapped by\\nthe NearestNeighbors class used above.  The Ball Tree and KD Tree\\nhave the same interface; we’ll show an example of using the KD Tree here:\\nfrom sklearn.neighbors import KDTree\\nimport numpy as np\\nX = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\\nkdt = KDTree(X, leaf_size=30, metric=\\'euclidean\\')\\nkdt.query(X, k=2, return_distance=False)\\narray([[0, 1],\\n[1, 0],\\n[2, 1],\\n[3, 4],\\n[4, 3],\\n[5, 4]]...)\\nRefer to the KDTree and BallTree class documentation\\nfor more information on the options available for nearest neighbors searches,\\nincluding specification of query strategies, distance metrics, etc. For a list\\nof valid metrics use KDTree.valid_metrics and BallTree.valid_metrics:\\nfrom sklearn.neighbors import KDTree, BallTree\\nKDTree.valid_metrics\\n[\\'euclidean\\', \\'l2\\', \\'minkowski\\', \\'p\\', \\'manhattan\\', \\'cityblock\\', \\'l1\\', \\'chebyshev\\', \\'infinity\\']\\nBallTree.valid_metrics\\n[\\'euclidean\\', \\'l2\\', \\'minkowski\\', \\'p\\', \\'manhattan\\', \\'cityblock\\', \\'l1\\', \\'chebyshev\\', \\'infinity\\', \\'seuclidean\\', \\'mahalanobis\\', \\'hamming\\', \\'canberra\\', \\'braycurtis\\', \\'jaccard\\', \\'dice\\', \\'rogerstanimoto\\', \\'russellrao\\', \\'sokalmichener\\', \\'sokalsneath\\', \\'haversine\\', \\'pyfunc\\']\\n- 1.6.2. Nearest Neighbors Classification¶\\nNeighbors-based classification is a type of instance-based learning or\\nnon-generalizing learning: it does not attempt to construct a general\\ninternal model, but simply stores instances of the training data.\\nClassification is computed from a simple majority vote of the nearest\\nneighbors of each point: a query point is assigned the data class which\\nhas the most representatives within the nearest neighbors of the point.\\nscikit-learn implements two different nearest neighbors classifiers:\\nKNeighborsClassifier implements learning based on the (k)\\nnearest neighbors of each query point, where (k) is an integer value\\nspecified by the user.  RadiusNeighborsClassifier implements learning\\nbased on the number of neighbors within a fixed radius (r) of each\\ntraining point, where (r) is a floating-point value specified by\\nthe user.\\nThe (k)-neighbors classification in KNeighborsClassifier\\nis the most commonly used technique. The optimal choice of the value (k)\\nis highly data-dependent: in general a larger (k) suppresses the effects\\nof noise, but makes the classification boundaries less distinct.\\nIn cases where the data is not uniformly sampled, radius-based neighbors\\nclassification in RadiusNeighborsClassifier can be a better choice.\\nThe user specifies a fixed radius (r), such that points in sparser\\nneighborhoods use fewer nearest neighbors for the classification.  For\\nhigh-dimensional parameter spaces, this method becomes less effective due\\nto the so-called “curse of dimensionality”.\\nThe basic nearest neighbors classification uses uniform weights: that is, the\\nvalue assigned to a query point is computed from a simple majority vote of\\nthe nearest neighbors.  Under some circumstances, it is better to weight the\\nneighbors such that nearer neighbors contribute more to the fit.  This can\\nbe accomplished through the weights keyword.  The default value,\\nweights = \\'uniform\\', assigns uniform weights to each neighbor.\\nweights = \\'distance\\' assigns weights proportional to the inverse of the\\ndistance from the query point.  Alternatively, a user-defined function of the\\ndistance can be supplied to compute the weights.\\nExamples:\\nNearest Neighbors Classification: an example of\\nclassification using nearest neighbors.\\n- 1.6.3. Nearest Neighbors Regression¶\\nNeighbors-based regression can be used in cases where the data labels are\\ncontinuous rather than discrete variables.  The label assigned to a query\\npoint is computed based on the mean of the labels of its nearest neighbors.\\nscikit-learn implements two different neighbors regressors:\\nKNeighborsRegressor implements learning based on the (k)\\nnearest neighbors of each query point, where (k) is an integer\\nvalue specified by the user.  RadiusNeighborsRegressor implements\\nlearning based on the neighbors within a fixed radius (r) of the\\nquery point, where (r) is a floating-point value specified by the\\nuser.\\nThe basic nearest neighbors regression uses uniform weights: that is,\\neach point in the local neighborhood contributes uniformly to the\\nclassification of a query point.  Under some circumstances, it can be\\nadvantageous to weight points such that nearby points contribute more\\nto the regression than faraway points.  This can be accomplished through\\nthe weights keyword.  The default value, weights = \\'uniform\\',\\nassigns equal weights to all points.  weights = \\'distance\\' assigns\\nweights proportional to the inverse of the distance from the query point.\\nAlternatively, a user-defined function of the distance can be supplied,\\nwhich will be used to compute the weights.\\nThe use of multi-output nearest neighbors for regression is demonstrated in\\nFace completion with a multi-output estimators. In this example, the inputs\\nX are the pixels of the upper half of faces and the outputs Y are the pixels of\\nthe lower half of those faces.\\nExamples:\\nNearest Neighbors regression: an example of regression\\nusing nearest neighbors.\\nFace completion with a multi-output estimators: an example of\\nmulti-output regression using nearest neighbors.\\n- 1.6.4. Nearest Neighbor Algorithms¶\\n1.6.4.1. Brute Force¶\\nFast computation of nearest neighbors is an active area of research in\\nmachine learning. The most naive neighbor search implementation involves\\nthe brute-force computation of distances between all pairs of points in the\\ndataset: for (N) samples in (D) dimensions, this approach scales\\nas (O[D N^2]).  Efficient brute-force neighbors searches can be very\\ncompetitive for small data samples.\\nHowever, as the number of samples (N) grows, the brute-force\\napproach quickly becomes infeasible.  In the classes within\\nsklearn.neighbors, brute-force neighbors searches are specified\\nusing the keyword algorithm = \\'brute\\', and are computed using the\\nroutines available in sklearn.metrics.pairwise.\\n1.6.4.2. K-D Tree¶\\nTo address the computational inefficiencies of the brute-force approach, a\\nvariety of tree-based data structures have been invented.  In general, these\\nstructures attempt to reduce the required number of distance calculations\\nby efficiently encoding aggregate distance information for the sample.\\nThe basic idea is that if point (A) is very distant from point\\n(B), and point (B) is very close to point (C),\\nthen we know that points (A) and (C)\\nare very distant, without having to explicitly calculate their distance.\\nIn this way, the computational cost of a nearest neighbors search can be\\nreduced to (O[D N \\\\log(N)]) or better. This is a significant\\nimprovement over brute-force for large (N).\\nAn early approach to taking advantage of this aggregate information was\\nthe KD tree data structure (short for K-dimensional tree), which\\ngeneralizes two-dimensional Quad-trees and 3-dimensional Oct-trees\\nto an arbitrary number of dimensions.  The KD tree is a binary tree\\nstructure which recursively partitions the parameter space along the data\\naxes, dividing it into nested orthotropic regions into which data points\\nare filed.  The construction of a KD tree is very fast: because partitioning\\nis performed only along the data axes, no (D)-dimensional distances\\nneed to be computed. Once constructed, the nearest neighbor of a query\\npoint can be determined with only (O[\\\\log(N)]) distance computations.\\nThough the KD tree approach is very fast for low-dimensional ((D < 20))\\nneighbors searches, it becomes inefficient as (D) grows very large:\\nthis is one manifestation of the so-called “curse of dimensionality”.\\nIn scikit-learn, KD tree neighbors searches are specified using the\\nkeyword algorithm = \\'kd_tree\\', and are computed using the class\\nKDTree.\\nReferences:\\n“Multidimensional binary search trees used for associative searching”,\\nBentley, J.L., Communications of the ACM (1975)\\n1.6.4.3. Ball Tree¶\\nTo address the inefficiencies of KD Trees in higher dimensions, the ball tree\\ndata structure was developed.  Where KD trees partition data along\\nCartesian axes, ball trees partition data in a series of nesting\\nhyper-spheres.  This makes tree construction more costly than that of the\\nKD tree, but results in a data structure which can be very efficient on\\nhighly structured data, even in very high dimensions.\\nA ball tree recursively divides the data into\\nnodes defined by a centroid (C) and radius (r), such that each\\npoint in the node lies within the hyper-sphere defined by (r) and\\n(C). The number of candidate points for a neighbor search\\nis reduced through use of the triangle inequality:\\n[|x+y| \\\\leq |x| + |y|]\\nWith this setup, a single distance calculation between a test point and\\nthe centroid is sufficient to determine a lower and upper bound on the\\ndistance to all points within the node.\\nBecause of the spherical geometry of the ball tree nodes, it can out-perform\\na KD-tree in high dimensions, though the actual performance is highly\\ndependent on the structure of the training data.\\nIn scikit-learn, ball-tree-based\\nneighbors searches are specified using the keyword algorithm = \\'ball_tree\\',\\nand are computed using the class BallTree.\\nAlternatively, the user can work with the BallTree class directly.\\nReferences:\\n“Five Balltree Construction Algorithms”,\\nOmohundro, S.M., International Computer Science Institute\\nTechnical Report (1989)\\n1.6.4.4. Choice of Nearest Neighbors Algorithm¶\\nThe optimal algorithm for a given dataset is a complicated choice, and\\ndepends on a number of factors:\\nnumber of samples (N) (i.e. n_samples) and dimensionality\\n(D) (i.e. n_features).\\nBrute force query time grows as (O[D N])\\nBall tree query time grows as approximately (O[D \\\\log(N)])\\nKD tree query time changes with (D) in a way that is difficult\\nto precisely characterise.  For small (D) (less than 20 or so)\\nthe cost is approximately (O[D\\\\log(N)]), and the KD tree\\nquery can be very efficient.\\nFor larger (D), the cost increases to nearly (O[DN]), and\\nthe overhead due to the tree\\nstructure can lead to queries which are slower than brute force.\\nFor small data sets ((N) less than 30 or so), (\\\\log(N)) is\\ncomparable to (N), and brute force algorithms can be more efficient\\nthan a tree-based approach.  Both KDTree and BallTree\\naddress this through providing a leaf size parameter: this controls the\\nnumber of samples at which a query switches to brute-force.  This allows both\\nalgorithms to approach the efficiency of a brute-force computation for small\\n(N).\\ndata structure: intrinsic dimensionality of the data and/or sparsity\\nof the data. Intrinsic dimensionality refers to the dimension\\n(d \\\\le D) of a manifold on which the data lies, which can be linearly\\nor non-linearly embedded in the parameter space. Sparsity refers to the\\ndegree to which the data fills the parameter space (this is to be\\ndistinguished from the concept as used in “sparse” matrices.  The data\\nmatrix may have no zero entries, but the structure can still be\\n“sparse” in this sense).\\nBrute force query time is unchanged by data structure.\\nBall tree and KD tree query times can be greatly influenced\\nby data structure.  In general, sparser data with a smaller intrinsic\\ndimensionality leads to faster query times.  Because the KD tree\\ninternal representation is aligned with the parameter axes, it will not\\ngenerally show as much improvement as ball tree for arbitrarily\\nstructured data.\\nDatasets used in machine learning tend to be very structured, and are\\nvery well-suited for tree-based queries.\\nnumber of neighbors (k) requested for a query point.\\nBrute force query time is largely unaffected by the value of (k)\\nBall tree and KD tree query time will become slower as (k)\\nincreases.  This is due to two effects: first, a larger (k) leads\\nto the necessity to search a larger portion of the parameter space.\\nSecond, using (k > 1) requires internal queueing of results\\nas the tree is traversed.\\nAs (k) becomes large compared to (N), the ability to prune\\nbranches in a tree-based query is reduced.  In this situation, Brute force\\nqueries can be more efficient.\\nnumber of query points.  Both the ball tree and the KD Tree\\nrequire a construction phase.  The cost of this construction becomes\\nnegligible when amortized over many queries.  If only a small number of\\nqueries will be performed, however, the construction can make up\\na significant fraction of the total cost.  If very few query points\\nwill be required, brute force is better than a tree-based method.\\nCurrently, algorithm = \\'auto\\' selects \\'brute\\' if any of the following\\nconditions are verified:\\ninput data is sparse\\nmetric = \\'precomputed\\'\\n(D > 15)\\n(k >= N/2)\\neffective_metric_ isn’t in the VALID_METRICS list for either\\n\\'kd_tree\\' or \\'ball_tree\\'\\nOtherwise, it selects the first out of \\'kd_tree\\' and \\'ball_tree\\' that\\nhas effective_metric_ in its VALID_METRICS list. This heuristic is\\nbased on the following assumptions:\\nthe number of query points is at least the same order as the number of\\ntraining points\\nleaf_size is close to its default value of 30\\nwhen (D > 15), the intrinsic dimensionality of the data is generally\\ntoo high for tree-based methods\\n1.6.4.5. Effect of leaf_size¶\\nAs noted above, for small sample sizes a brute force search can be more\\nefficient than a tree-based query.  This fact is accounted for in the ball\\ntree and KD tree by internally switching to brute force searches within\\nleaf nodes.  The level of this switch can be specified with the parameter\\nleaf_size.  This parameter choice has many effects:\\nconstruction timeA larger leaf_size leads to a faster tree construction time, because\\nfewer nodes need to be created\\nquery timeBoth a large or small leaf_size can lead to suboptimal query cost.\\nFor leaf_size approaching 1, the overhead involved in traversing\\nnodes can significantly slow query times.  For leaf_size approaching\\nthe size of the training set, queries become essentially brute force.\\nA good compromise between these is leaf_size = 30, the default value\\nof the parameter.\\nmemoryAs leaf_size increases, the memory required to store a tree structure\\ndecreases.  This is especially important in the case of ball tree, which\\nstores a (D)-dimensional centroid for each node.  The required\\nstorage space for BallTree is approximately 1 / leaf_size times\\nthe size of the training set.\\nleaf_size is not referenced for brute force queries.\\n1.6.4.6. Valid Metrics for Nearest Neighbor Algorithms¶\\nFor a list of available metrics, see the documentation of the\\nDistanceMetric class and the metrics listed in\\nsklearn.metrics.pairwise.PAIRWISE_DISTANCE_FUNCTIONS. Note that the “cosine”\\nmetric uses cosine_distances.\\nA list of valid metrics for any of the above algorithms can be obtained by using their\\nvalid_metric attribute. For example, valid metrics for KDTree can be generated by:\\nfrom sklearn.neighbors import KDTree\\nprint(sorted(KDTree.valid_metrics))\\n[\\'chebyshev\\', \\'cityblock\\', \\'euclidean\\', \\'infinity\\', \\'l1\\', \\'l2\\', \\'manhattan\\', \\'minkowski\\', \\'p\\']\\n- 1.6.5. Nearest Centroid Classifier¶\\nThe NearestCentroid classifier is a simple algorithm that represents\\neach class by the centroid of its members. In effect, this makes it\\nsimilar to the label updating phase of the KMeans algorithm.\\nIt also has no parameters to choose, making it a good baseline classifier. It\\ndoes, however, suffer on non-convex classes, as well as when classes have\\ndrastically different variances, as equal variance in all dimensions is\\nassumed. See Linear Discriminant Analysis (LinearDiscriminantAnalysis)\\nand Quadratic Discriminant Analysis (QuadraticDiscriminantAnalysis)\\nfor more complex methods that do not make this assumption. Usage of the default\\nNearestCentroid is simple:\\nfrom sklearn.neighbors import NearestCentroid\\nimport numpy as np\\nX = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\\ny = np.array([1, 1, 1, 2, 2, 2])\\nclf = NearestCentroid()\\nclf.fit(X, y)\\nNearestCentroid()\\nprint(clf.predict([[-0.8, -1]]))\\n[1]\\n1.6.5.1. Nearest Shrunken Centroid¶\\nThe NearestCentroid classifier has a shrink_threshold parameter,\\nwhich implements the nearest shrunken centroid classifier. In effect, the value\\nof each feature for each centroid is divided by the within-class variance of\\nthat feature. The feature values are then reduced by shrink_threshold. Most\\nnotably, if a particular feature value crosses zero, it is set\\nto zero. In effect, this removes the feature from affecting the classification.\\nThis is useful, for example, for removing noisy features.\\nIn the example below, using a small shrink threshold increases the accuracy of\\nthe model from 0.81 to 0.82.\\nExamples:\\nNearest Centroid Classification: an example of\\nclassification using nearest centroid with different shrink thresholds.\\n- 1.6.6. Nearest Neighbors Transformer¶\\nMany scikit-learn estimators rely on nearest neighbors: Several classifiers and\\nregressors such as KNeighborsClassifier and\\nKNeighborsRegressor, but also some clustering methods such as\\nDBSCAN and\\nSpectralClustering, and some manifold embeddings such\\nas TSNE and Isomap.\\nAll these estimators can compute internally the nearest neighbors, but most of\\nthem also accept precomputed nearest neighbors sparse graph,\\nas given by kneighbors_graph and\\nradius_neighbors_graph. With mode\\nmode=\\'connectivity\\', these functions return a binary adjacency sparse graph\\nas required, for instance, in SpectralClustering.\\nWhereas with mode=\\'distance\\', they return a distance sparse graph as required,\\nfor instance, in DBSCAN. To include these functions in\\na scikit-learn pipeline, one can also use the corresponding classes\\nKNeighborsTransformer and RadiusNeighborsTransformer.\\nThe benefits of this sparse graph API are multiple.\\nFirst, the precomputed graph can be re-used multiple times, for instance while\\nvarying a parameter of the estimator. This can be done manually by the user, or\\nusing the caching properties of the scikit-learn pipeline:\\nimport tempfile\\nfrom sklearn.manifold import Isomap\\nfrom sklearn.neighbors import KNeighborsTransformer\\nfrom sklearn.pipeline import make_pipeline\\nfrom sklearn.datasets import make_regression\\ncache_path = tempfile.gettempdir()  # we use a temporary folder here\\nX, _ = make_regression(n_samples=50, n_features=25, random_state=0)\\nestimator = make_pipeline(\\n...     KNeighborsTransformer(mode=\\'distance\\'),\\n...     Isomap(n_components=3, metric=\\'precomputed\\'),\\n...     memory=cache_path)\\nX_embedded = estimator.fit_transform(X)\\nX_embedded.shape\\n(50, 3)\\nSecond, precomputing the graph can give finer control on the nearest neighbors\\nestimation, for instance enabling multiprocessing though the parameter\\nn_jobs, which might not be available in all estimators.\\nFinally, the precomputation can be performed by custom estimators to use\\ndifferent implementations, such as approximate nearest neighbors methods, or\\nimplementation with special data types. The precomputed neighbors\\nsparse graph needs to be formatted as in\\nradius_neighbors_graph output:\\na CSR matrix (although COO, CSC or LIL will be accepted).\\nonly explicitly store nearest neighborhoods of each sample with respect to the\\ntraining data. This should include those at 0 distance from a query point,\\nincluding the matrix diagonal when computing the nearest neighborhoods\\nbetween the training data and itself.\\neach row’s data should store the distance in increasing order (optional.\\nUnsorted data will be stable-sorted, adding a computational overhead).\\nall values in data should be non-negative.\\nthere should be no duplicate indices in any row\\n(see https://github.com/scipy/scipy/issues/5807).\\nif the algorithm being passed the precomputed matrix uses k nearest neighbors\\n(as opposed to radius neighborhood), at least k neighbors must be stored in\\neach row (or k+1, as explained in the following note).\\nNote\\nWhen a specific number of neighbors is queried (using\\nKNeighborsTransformer), the definition of n_neighbors is ambiguous\\nsince it can either include each training point as its own neighbor, or\\nexclude them. Neither choice is perfect, since including them leads to a\\ndifferent number of non-self neighbors during training and testing, while\\nexcluding them leads to a difference between fit(X).transform(X) and\\nfit_transform(X), which is against scikit-learn API.\\nIn KNeighborsTransformer we use the definition which includes each\\ntraining point as its own neighbor in the count of n_neighbors. However,\\nfor compatibility reasons with other estimators which use the other\\ndefinition, one extra neighbor will be computed when mode == \\'distance\\'.\\nTo maximise compatibility with all estimators, a safe choice is to always\\ninclude one extra neighbor in a custom nearest neighbors estimator, since\\nunnecessary neighbors will be filtered by following estimators.\\nExamples:\\nApproximate nearest neighbors in TSNE:\\nan example of pipelining KNeighborsTransformer and\\nTSNE. Also proposes two custom nearest neighbors\\nestimators based on external packages.\\nCaching nearest neighbors:\\nan example of pipelining KNeighborsTransformer and\\nKNeighborsClassifier to enable caching of the neighbors graph\\nduring a hyper-parameter grid-search.\\n- 1.6.7. Neighborhood Components Analysis¶\\nNeighborhood Components Analysis (NCA, NeighborhoodComponentsAnalysis)\\nis a distance metric learning algorithm which aims to improve the accuracy of\\nnearest neighbors classification compared to the standard Euclidean distance.\\nThe algorithm directly maximizes a stochastic variant of the leave-one-out\\nk-nearest neighbors (KNN) score on the training set. It can also learn a\\nlow-dimensional linear projection of data that can be used for data\\nvisualization and fast classification.\\nIn the above illustrating figure, we consider some points from a randomly\\ngenerated dataset. We focus on the stochastic KNN classification of point no.\\n\\n3. The thickness of a link between sample 3 and another point is proportional\\n\\nto their distance, and can be seen as the relative weight (or probability) that\\na stochastic nearest neighbor prediction rule would assign to this point. In\\nthe original space, sample 3 has many stochastic neighbors from various\\nclasses, so the right class is not very likely. However, in the projected space\\nlearned by NCA, the only stochastic neighbors with non-negligible weight are\\nfrom the same class as sample 3, guaranteeing that the latter will be well\\nclassified. See the mathematical formulation\\nfor more details.\\n1.6.7.1. Classification¶\\nCombined with a nearest neighbors classifier (KNeighborsClassifier),\\nNCA is attractive for classification because it can naturally handle\\nmulti-class problems without any increase in the model size, and does not\\nintroduce additional parameters that require fine-tuning by the user.\\nNCA classification has been shown to work well in practice for data sets of\\nvarying size and difficulty. In contrast to related methods such as Linear\\nDiscriminant Analysis, NCA does not make any assumptions about the class\\ndistributions. The nearest neighbor classification can naturally produce highly\\nirregular decision boundaries.\\nTo use this model for classification, one needs to combine a\\nNeighborhoodComponentsAnalysis instance that learns the optimal\\ntransformation with a KNeighborsClassifier instance that performs the\\nclassification in the projected space. Here is an example using the two\\nclasses:\\n\\nfrom sklearn.neighbors import (NeighborhoodComponentsAnalysis,\\n... KNeighborsClassifier)\\nfrom sklearn.datasets import load_iris\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.pipeline import Pipeline\\nX, y = load_iris(return_X_y=True)\\nX_train, X_test, y_train, y_test = train_test_split(X, y,\\n... stratify=y, test_size=0.7, random_state=42)\\nnca = NeighborhoodComponentsAnalysis(random_state=42)\\nknn = KNeighborsClassifier(n_neighbors=3)\\nnca_pipe = Pipeline([(\\'nca\\', nca), (\\'knn\\', knn)])\\nnca_pipe.fit(X_train, y_train)\\nPipeline(...)\\nprint(nca_pipe.score(X_test, y_test))\\n0.96190476...\\nThe plot shows decision boundaries for Nearest Neighbor Classification and\\nNeighborhood Components Analysis classification on the iris dataset, when\\ntraining and scoring on only two features, for visualisation purposes.\\n1.6.7.2. Dimensionality reduction¶\\nNCA can be used to perform supervised dimensionality reduction. The input data\\nare projected onto a linear subspace consisting of the directions which\\nminimize the NCA objective. The desired dimensionality can be set using the\\nparameter n_components. For instance, the following figure shows a\\ncomparison of dimensionality reduction with Principal Component Analysis\\n(PCA), Linear Discriminant Analysis\\n(LinearDiscriminantAnalysis) and\\nNeighborhood Component Analysis (NeighborhoodComponentsAnalysis) on\\nthe Digits dataset, a dataset with size (n_{samples} = 1797) and\\n(n_{features} = 64). The data set is split into a training and a test set\\nof equal size, then standardized. For evaluation the 3-nearest neighbor\\nclassification accuracy is computed on the 2-dimensional projected points found\\nby each method. Each data sample belongs to one of 10 classes.\\nExamples:\\nComparing Nearest Neighbors with and without Neighborhood Components Analysis\\nDimensionality Reduction with Neighborhood Components Analysis\\nManifold learning on handwritten digits: Locally Linear Embedding, Isomap…\\n1.6.7.3. Mathematical formulation¶\\nThe goal of NCA is to learn an optimal linear transformation matrix of size\\n(n_components, n_features), which maximises the sum over all samples\\n(i) of the probability (p_i) that (i) is correctly\\nclassified, i.e.:\\n[\\\\underset{L}{\\\\arg\\\\max} \\\\sum\\\\limits_{i=0}^{N - 1} p_{i}]\\nwith (N) = n_samples and (p_i) the probability of sample\\n(i) being correctly classified according to a stochastic nearest\\nneighbors rule in the learned embedded space:\\n[p_{i}=\\\\sum\\\\limits_{j \\\\in C_i}{p_{i j}}]\\nwhere (C_i) is the set of points in the same class as sample (i),\\nand (p_{i j}) is the softmax over Euclidean distances in the embedded\\nspace:\\n[p_{i j} = \\\\frac{\\\\exp(-||L x_i - L x_j||^2)}{\\\\sum\\\\limits_{k \\\\ne\\ni} {\\\\exp{-(||L x_i - L x_k||^2)}}} , \\\\quad p_{i i} = 0]\\n1.6.7.3.1. Mahalanobis distance¶\\nNCA can be seen as learning a (squared) Mahalanobis distance metric:\\n[|| L(x_i - x_j)||^2 = (x_i - x_j)^TM(x_i - x_j),]\\nwhere (M = L^T L) is a symmetric positive semi-definite matrix of size\\n(n_features, n_features).\\n1.6.7.4. Implementation¶\\nThis implementation follows what is explained in the original paper [1]. For\\nthe optimisation method, it currently uses scipy’s L-BFGS-B with a full\\ngradient computation at each iteration, to avoid to tune the learning rate and\\nprovide stable learning.\\nSee the examples below and the docstring of\\nNeighborhoodComponentsAnalysis.fit for further information.\\n1.6.7.5. Complexity¶\\n1.6.7.5.1. Training¶\\nNCA stores a matrix of pairwise distances, taking n_samples ** 2 memory.\\nTime complexity depends on the number of iterations done by the optimisation\\nalgorithm. However, one can set the maximum number of iterations with the\\nargument max_iter. For each iteration, time complexity is\\nO(n_components x n_samples x min(n_samples, n_features)).\\n1.6.7.5.2. Transform¶\\nHere the transform operation returns (LX^T), therefore its time\\ncomplexity equals n_components * n_features * n_samples_test. There is no\\nadded space complexity in the operation.\\nReferences:\\n[1]\\n“Neighbourhood Components Analysis”,\\nJ. Goldberger, S. Roweis, G. Hinton, R. Salakhutdinov, Advances in\\nNeural Information Processing Systems, Vol. 17, May 2005, pp. 513-520.\\nWikipedia entry on Neighborhood Components Analysis\\n© 2007 - 2024, scikit-learn developers (BSD License).\\nShow this page source\\n\\n1.7. Gaussian Processes — scikit-learn 1.4.2 documentation\\n\\nInstall\\nUser Guide\\nAPI\\nExamples\\nCommunity\\nGetting Started\\nTutorial\\nWhat\\'s new\\nGlossary\\nDevelopment\\nFAQ\\nSupport\\nRelated packages\\nRoadmap\\nGovernance\\nAbout us\\nGitHub\\nOther Versions and Download\\nMore\\nGetting Started\\nTutorial\\nWhat\\'s new\\nGlossary\\nDevelopment\\nFAQ\\nSupport\\nRelated packages\\nRoadmap\\nGovernance\\nAbout us\\nGitHub\\nOther Versions and Download\\nToggle Menu\\nPrevUp\\nNext\\nscikit-learn 1.4.2\\nOther versions\\nPlease cite us if you use the software.\\n\\n1.7. Gaussian Processes\\n\\n1.7.1. Gaussian Process Regression (GPR)\\n\\n1.7.2. Gaussian Process Classification (GPC)\\n\\n1.7.3. GPC examples\\n1.7.3.1. Probabilistic predictions with GPC\\n1.7.3.2. Illustration of GPC on the XOR dataset\\n1.7.3.3. Gaussian process classification (GPC) on iris dataset\\n\\n1.7.4. Kernels for Gaussian Processes\\n1.7.4.1. Basic kernels\\n1.7.4.2. Kernel operators\\n1.7.4.3. Radial basis function (RBF) kernel\\n1.7.4.4. Matérn kernel\\n1.7.4.5. Rational quadratic kernel\\n1.7.4.6. Exp-Sine-Squared kernel\\n1.7.4.7. Dot-Product kernel\\n1.7.4.8. References\\n\\n1.7. Gaussian Processes¶\\n\\nGaussian Processes (GP) are a nonparametric supervised learning method used\\nto solve regression and probabilistic classification problems.\\nThe advantages of Gaussian processes are:\\nThe prediction interpolates the observations (at least for regular\\nkernels).\\nThe prediction is probabilistic (Gaussian) so that one can compute\\nempirical confidence intervals and decide based on those if one should\\nrefit (online fitting, adaptive fitting) the prediction in some\\nregion of interest.\\nVersatile: different kernels can be specified. Common kernels are provided, but\\nit is also possible to specify custom kernels.\\nThe disadvantages of Gaussian processes include:\\nOur implementation is not sparse, i.e., they use the whole samples/features\\ninformation to perform the prediction.\\nThey lose efficiency in high dimensional spaces – namely when the number\\nof features exceeds a few dozens.\\n- 1.7.1. Gaussian Process Regression (GPR)¶\\nThe GaussianProcessRegressor implements Gaussian processes (GP) for\\nregression purposes. For this, the prior of the GP needs to be specified. GP\\nwill combine this prior and the likelihood function based on training samples.\\nIt allows to give a probabilistic approach to prediction by giving the mean and\\nstandard deviation as output when predicting.\\nThe prior mean is assumed to be constant and zero (for normalize_y=False) or\\nthe training data’s mean (for normalize_y=True). The prior’s covariance is\\nspecified by passing a kernel object. The hyperparameters\\nof the kernel are optimized when fitting the GaussianProcessRegressor\\nby maximizing the log-marginal-likelihood (LML) based on the passed\\noptimizer. As the LML may have multiple local optima, the optimizer can be\\nstarted repeatedly by specifying n_restarts_optimizer. The first run is\\nalways conducted starting from the initial hyperparameter values of the kernel;\\nsubsequent runs are conducted from hyperparameter values that have been chosen\\nrandomly from the range of allowed values. If the initial hyperparameters\\nshould be kept fixed, None can be passed as optimizer.\\nThe noise level in the targets can be specified by passing it via the parameter\\nalpha, either globally as a scalar or per datapoint. Note that a moderate\\nnoise level can also be helpful for dealing with numeric instabilities during\\nfitting as it is effectively implemented as Tikhonov regularization, i.e., by\\nadding it to the diagonal of the kernel matrix. An alternative to specifying\\nthe noise level explicitly is to include a\\nWhiteKernel component into the\\nkernel, which can estimate the global noise level from the data (see example\\nbelow). The figure below shows the effect of noisy target handled by setting\\nthe parameter alpha.\\nThe implementation is based on Algorithm 2.1 of [RW2006]. In addition to\\nthe API of standard scikit-learn estimators, GaussianProcessRegressor:\\nallows prediction without prior fitting (based on the GP prior)\\nprovides an additional method sample_y(X), which evaluates samples\\ndrawn from the GPR (prior or posterior) at given inputs\\nexposes a method log_marginal_likelihood(theta), which can be used\\nexternally for other ways of selecting hyperparameters, e.g., via\\nMarkov chain Monte Carlo.\\nExamples\\nGaussian Processes regression: basic introductory example\\nAbility of Gaussian process regression (GPR) to estimate data noise-level\\nComparison of kernel ridge and Gaussian process regression\\nForecasting of CO2 level on Mona Loa dataset using Gaussian process regression (GPR)\\n- 1.7.2. Gaussian Process Classification (GPC)¶\\nThe GaussianProcessClassifier implements Gaussian processes (GP) for\\nclassification purposes, more specifically for probabilistic classification,\\nwhere test predictions take the form of class probabilities.\\nGaussianProcessClassifier places a GP prior on a latent function (f),\\nwhich is then squashed through a link function to obtain the probabilistic\\nclassification. The latent function (f) is a so-called nuisance function,\\nwhose values are not observed and are not relevant by themselves.\\nIts purpose is to allow a convenient formulation of the model, and (f)\\nis removed (integrated out) during prediction. GaussianProcessClassifier\\nimplements the logistic link function, for which the integral cannot be\\ncomputed analytically but is easily approximated in the binary case.\\nIn contrast to the regression setting, the posterior of the latent function\\n(f) is not Gaussian even for a GP prior since a Gaussian likelihood is\\ninappropriate for discrete class labels. Rather, a non-Gaussian likelihood\\ncorresponding to the logistic link function (logit) is used.\\nGaussianProcessClassifier approximates the non-Gaussian posterior with a\\nGaussian based on the Laplace approximation. More details can be found in\\nChapter 3 of [RW2006].\\nThe GP prior mean is assumed to be zero. The prior’s\\ncovariance is specified by passing a kernel object. The\\nhyperparameters of the kernel are optimized during fitting of\\nGaussianProcessRegressor by maximizing the log-marginal-likelihood (LML) based\\non the passed optimizer. As the LML may have multiple local optima, the\\noptimizer can be started repeatedly by specifying n_restarts_optimizer. The\\nfirst run is always conducted starting from the initial hyperparameter values\\nof the kernel; subsequent runs are conducted from hyperparameter values\\nthat have been chosen randomly from the range of allowed values.\\nIf the initial hyperparameters should be kept fixed, None can be passed as\\noptimizer.\\nGaussianProcessClassifier supports multi-class classification\\nby performing either one-versus-rest or one-versus-one based training and\\nprediction.  In one-versus-rest, one binary Gaussian process classifier is\\nfitted for each class, which is trained to separate this class from the rest.\\nIn “one_vs_one”, one binary Gaussian process classifier is fitted for each pair\\nof classes, which is trained to separate these two classes. The predictions of\\nthese binary predictors are combined into multi-class predictions. See the\\nsection on multi-class classification for more details.\\nIn the case of Gaussian process classification, “one_vs_one” might be\\ncomputationally  cheaper since it has to solve many problems involving only a\\nsubset of the whole training set rather than fewer problems on the whole\\ndataset. Since Gaussian process classification scales cubically with the size\\nof the dataset, this might be considerably faster. However, note that\\n“one_vs_one” does not support predicting probability estimates but only plain\\npredictions. Moreover, note that GaussianProcessClassifier does not\\n(yet) implement a true multi-class Laplace approximation internally, but\\nas discussed above is based on solving several binary classification tasks\\ninternally, which are combined using one-versus-rest or one-versus-one.\\n- 1.7.3. GPC examples¶\\n1.7.3.1. Probabilistic predictions with GPC¶\\nThis example illustrates the predicted probability of GPC for an RBF kernel\\nwith different choices of the hyperparameters. The first figure shows the\\npredicted probability of GPC with arbitrarily chosen hyperparameters and with\\nthe hyperparameters corresponding to the maximum log-marginal-likelihood (LML).\\nWhile the hyperparameters chosen by optimizing LML have a considerably larger\\nLML, they perform slightly worse according to the log-loss on test data. The\\nfigure shows that this is because they exhibit a steep change of the class\\nprobabilities at the class boundaries (which is good) but have predicted\\nprobabilities close to 0.5 far away from the class boundaries (which is bad)\\nThis undesirable effect is caused by the Laplace approximation used\\ninternally by GPC.\\nThe second figure shows the log-marginal-likelihood for different choices of\\nthe kernel’s hyperparameters, highlighting the two choices of the\\nhyperparameters used in the first figure by black dots.\\n1.7.3.2. Illustration of GPC on the XOR dataset¶\\nThis example illustrates GPC on XOR data. Compared are a stationary, isotropic\\nkernel (RBF) and a non-stationary kernel (DotProduct). On\\nthis particular dataset, the DotProduct kernel obtains considerably\\nbetter results because the class-boundaries are linear and coincide with the\\ncoordinate axes. In practice, however, stationary kernels such as RBF\\noften obtain better results.\\n1.7.3.3. Gaussian process classification (GPC) on iris dataset¶\\nThis example illustrates the predicted probability of GPC for an isotropic\\nand anisotropic RBF kernel on a two-dimensional version for the iris-dataset.\\nThis illustrates the applicability of GPC to non-binary classification.\\nThe anisotropic RBF kernel obtains slightly higher log-marginal-likelihood by\\nassigning different length-scales to the two feature dimensions.\\n- 1.7.4. Kernels for Gaussian Processes¶\\nKernels (also called “covariance functions” in the context of GPs) are a crucial\\ningredient of GPs which determine the shape of prior and posterior of the GP.\\nThey encode the assumptions on the function being learned by defining the “similarity”\\nof two datapoints combined with the assumption that similar datapoints should\\nhave similar target values. Two categories of kernels can be distinguished:\\nstationary kernels depend only on the distance of two datapoints and not on their\\nabsolute values (k(x_i, x_j)= k(d(x_i, x_j))) and are thus invariant to\\ntranslations in the input space, while non-stationary kernels\\ndepend also on the specific values of the datapoints. Stationary kernels can further\\nbe subdivided into isotropic and anisotropic kernels, where isotropic kernels are\\nalso invariant to rotations in the input space. For more details, we refer to\\nChapter 4 of [RW2006]. For guidance on how to best combine different kernels,\\nwe refer to [Duv2014].\\nGaussian Process Kernel API\\nClick for more details\\n¶\\nThe main usage of a Kernel is to compute the GP’s covariance between\\ndatapoints. For this, the method call of the kernel can be called. This\\nmethod can either be used to compute the “auto-covariance” of all pairs of\\ndatapoints in a 2d array X, or the “cross-covariance” of all combinations\\nof datapoints of a 2d array X with datapoints in a 2d array Y. The following\\nidentity holds true for all kernels k (except for the WhiteKernel):\\nk(X) == K(X, Y=X)\\nIf only the diagonal of the auto-covariance is being used, the method diag()\\nof a kernel can be called, which is more computationally efficient than the\\nequivalent call to call: np.diag(k(X, X)) == k.diag(X)\\nKernels are parameterized by a vector (\\\\theta) of hyperparameters. These\\nhyperparameters can for instance control length-scales or periodicity of a\\nkernel (see below). All kernels support computing analytic gradients\\nof the kernel’s auto-covariance with respect to (log(\\\\theta)) via setting\\neval_gradient=True in the call method.\\nThat is, a (len(X), len(X), len(theta)) array is returned where the entry\\n[i, j, l] contains (\\\\frac{\\\\partial k_\\\\theta(x_i, x_j)}{\\\\partial log(\\\\theta_l)}).\\nThis gradient is used by the Gaussian process (both regressor and classifier)\\nin computing the gradient of the log-marginal-likelihood, which in turn is used\\nto determine the value of (\\\\theta), which maximizes the log-marginal-likelihood,\\nvia gradient ascent. For each hyperparameter, the initial value and the\\nbounds need to be specified when creating an instance of the kernel. The\\ncurrent value of (\\\\theta) can be get and set via the property\\ntheta of the kernel object. Moreover, the bounds of the hyperparameters can be\\naccessed by the property bounds of the kernel. Note that both properties\\n(theta and bounds) return log-transformed values of the internally used values\\nsince those are typically more amenable to gradient-based optimization.\\nThe specification of each hyperparameter is stored in the form of an instance of\\nHyperparameter in the respective kernel. Note that a kernel using a\\nhyperparameter with name “x” must have the attributes self.x and self.x_bounds.\\nThe abstract base class for all kernels is Kernel. Kernel implements a\\nsimilar interface as BaseEstimator, providing the\\nmethods get_params(), set_params(), and clone(). This allows\\nsetting kernel values also via meta-estimators such as\\nPipeline or\\nGridSearchCV. Note that due to the nested\\nstructure of kernels (by applying kernel operators, see below), the names of\\nkernel parameters might become relatively complicated. In general, for a binary\\nkernel operator, parameters of the left operand are prefixed with k1__ and\\nparameters of the right operand with k2__. An additional convenience method\\nis clone_with_theta(theta), which returns a cloned version of the kernel\\nbut with the hyperparameters set to theta. An illustrative example:\\n\\nfrom sklearn.gaussian_process.kernels import ConstantKernel, RBF\\nkernel = ConstantKernel(constant_value=1.0, constant_value_bounds=(0.0, 10.0)) * RBF(length_scale=0.5, length_scale_bounds=(0.0, 10.0)) + RBF(length_scale=2.0, length_scale_bounds=(0.0, 10.0))\\nfor hyperparameter in kernel.hyperparameters: print(hyperparameter)\\nHyperparameter(name=\\'k1__k1__constant_value\\', value_type=\\'numeric\\', bounds=array([[ 0., 10.]]), n_elements=1, fixed=False)\\nHyperparameter(name=\\'k1__k2__length_scale\\', value_type=\\'numeric\\', bounds=array([[ 0., 10.]]), n_elements=1, fixed=False)\\nHyperparameter(name=\\'k2__length_scale\\', value_type=\\'numeric\\', bounds=array([[ 0., 10.]]), n_elements=1, fixed=False)\\nparams = kernel.get_params()\\nfor key in sorted(params): print(\"%s : %s\" % (key, params[key]))\\nk1 : 12 * RBF(length_scale=0.5)\\nk1__k1 : 12\\nk1__k1__constant_value : 1.0\\nk1__k1__constant_value_bounds : (0.0, 10.0)\\nk1__k2 : RBF(length_scale=0.5)\\nk1__k2__length_scale : 0.5\\nk1__k2__length_scale_bounds : (0.0, 10.0)\\nk2 : RBF(length_scale=2)\\nk2__length_scale : 2.0\\nk2__length_scale_bounds : (0.0, 10.0)\\nprint(kernel.theta)  # Note: log-transformed\\n[ 0.         -0.69314718  0.69314718]\\nprint(kernel.bounds)  # Note: log-transformed\\n[[      -inf 2.30258509]\\n[      -inf 2.30258509]\\n[      -inf 2.30258509]]\\nAll Gaussian process kernels are interoperable with sklearn.metrics.pairwise\\nand vice versa: instances of subclasses of Kernel can be passed as\\nmetric to pairwise_kernels from sklearn.metrics.pairwise. Moreover,\\nkernel functions from pairwise can be used as GP kernels by using the wrapper\\nclass PairwiseKernel. The only caveat is that the gradient of\\nthe hyperparameters is not analytic but numeric and all those kernels support\\nonly isotropic distances. The parameter gamma is considered to be a\\nhyperparameter and may be optimized. The other kernel parameters are set\\ndirectly at initialization and are kept fixed.\\n1.7.4.1. Basic kernels¶\\nThe ConstantKernel kernel can be used as part of a Product\\nkernel where it scales the magnitude of the other factor (kernel) or as part\\nof a Sum kernel, where it modifies the mean of the Gaussian process.\\nIt depends on a parameter (constant_value). It is defined as:\\n[k(x_i, x_j) = constant_value \\\\;\\\\forall\\\\; x_1, x_2]\\nThe main use-case of the WhiteKernel kernel is as part of a\\nsum-kernel where it explains the noise-component of the signal. Tuning its\\nparameter (noise_level) corresponds to estimating the noise-level.\\nIt is defined as:\\n[k(x_i, x_j) = noise_level \\\\text{ if } x_i == x_j \\\\text{ else } 0]\\n1.7.4.2. Kernel operators¶\\nKernel operators take one or two base kernels and combine them into a new\\nkernel. The Sum kernel takes two kernels (k_1) and (k_2)\\nand combines them via (k_{sum}(X, Y) = k_1(X, Y) + k_2(X, Y)).\\nThe  Product kernel takes two kernels (k_1) and (k_2)\\nand combines them via (k_{product}(X, Y) = k_1(X, Y) * k_2(X, Y)).\\nThe Exponentiation kernel takes one base kernel and a scalar parameter\\n(p) and combines them via\\n(k_{exp}(X, Y) = k(X, Y)^p).\\nNote that magic methods add, mul___ and __pow are\\noverridden on the Kernel objects, so one can use e.g. RBF() + RBF() as\\na shortcut for Sum(RBF(), RBF()).\\n1.7.4.3. Radial basis function (RBF) kernel¶\\nThe RBF kernel is a stationary kernel. It is also known as the “squared\\nexponential” kernel. It is parameterized by a length-scale parameter (l>0), which\\ncan either be a scalar (isotropic variant of the kernel) or a vector with the same\\nnumber of dimensions as the inputs (x) (anisotropic variant of the kernel).\\nThe kernel is given by:\\n[k(x_i, x_j) = \\\\text{exp}\\\\left(- \\\\frac{d(x_i, x_j)^2}{2l^2} \\\\right)]\\nwhere (d(\\\\cdot, \\\\cdot)) is the Euclidean distance.\\nThis kernel is infinitely differentiable, which implies that GPs with this\\nkernel as covariance function have mean square derivatives of all orders, and are thus\\nvery smooth. The prior and posterior of a GP resulting from an RBF kernel are shown in\\nthe following figure:\\n1.7.4.4. Matérn kernel¶\\nThe Matern kernel is a stationary kernel and a generalization of the\\nRBF kernel. It has an additional parameter (\\\\nu) which controls\\nthe smoothness of the resulting function. It is parameterized by a length-scale parameter (l>0), which can either be a scalar (isotropic variant of the kernel) or a vector with the same number of dimensions as the inputs (x) (anisotropic variant of the kernel).\\nMathematical implementation of Matérn kernel\\nClick for more details\\n¶\\nThe kernel is given by:\\n[k(x_i, x_j) = \\\\frac{1}{\\\\Gamma(\\\\nu)2^{\\\\nu-1}}\\\\Bigg(\\\\frac{\\\\sqrt{2\\\\nu}}{l} d(x_i , x_j )\\\\Bigg)^\\\\nu K_\\\\nu\\\\Bigg(\\\\frac{\\\\sqrt{2\\\\nu}}{l} d(x_i , x_j )\\\\Bigg),]\\nwhere (d(\\\\cdot,\\\\cdot)) is the Euclidean distance, (K_\\\\nu(\\\\cdot)) is a modified Bessel function and (\\\\Gamma(\\\\cdot)) is the gamma function.\\nAs (\\\\nu\\\\rightarrow\\\\infty), the Matérn kernel converges to the RBF kernel.\\nWhen (\\\\nu = 1/2), the Matérn kernel becomes identical to the absolute\\nexponential kernel, i.e.,\\n[k(x_i, x_j) = \\\\exp \\\\Bigg(- \\\\frac{1}{l} d(x_i , x_j ) \\\\Bigg) \\\\quad \\\\quad \\\\nu= \\\\tfrac{1}{2}]\\nIn particular, (\\\\nu = 3/2):\\n[k(x_i, x_j) =  \\\\Bigg(1 + \\\\frac{\\\\sqrt{3}}{l} d(x_i , x_j )\\\\Bigg) \\\\exp \\\\Bigg(-\\\\frac{\\\\sqrt{3}}{l} d(x_i , x_j ) \\\\Bigg) \\\\quad \\\\quad \\\\nu= \\\\tfrac{3}{2}]\\nand (\\\\nu = 5/2):\\n[k(x_i, x_j) = \\\\Bigg(1 + \\\\frac{\\\\sqrt{5}}{l} d(x_i , x_j ) +\\\\frac{5}{3l} d(x_i , x_j )^2 \\\\Bigg) \\\\exp \\\\Bigg(-\\\\frac{\\\\sqrt{5}}{l} d(x_i , x_j ) \\\\Bigg) \\\\quad \\\\quad \\\\nu= \\\\tfrac{5}{2}]\\nare popular choices for learning functions that are not infinitely\\ndifferentiable (as assumed by the RBF kernel) but at least once ((\\\\nu =\\n3/2)) or twice differentiable ((\\\\nu = 5/2)).\\nThe flexibility of controlling the smoothness of the learned function via (\\\\nu)\\nallows adapting to the properties of the true underlying functional relation.\\nThe prior and posterior of a GP resulting from a Matérn kernel are shown in\\nthe following figure:\\nSee [RW2006], pp84 for further details regarding the\\ndifferent variants of the Matérn kernel.\\n1.7.4.5. Rational quadratic kernel¶\\nThe RationalQuadratic kernel can be seen as a scale mixture (an infinite sum)\\nof RBF kernels with different characteristic length-scales. It is parameterized\\nby a length-scale parameter (l>0) and a scale mixture parameter  (\\\\alpha>0)\\nOnly the isotropic variant where (l) is a scalar is supported at the moment.\\nThe kernel is given by:\\n[k(x_i, x_j) = \\\\left(1 + \\\\frac{d(x_i, x_j)^2}{2\\\\alpha l^2}\\\\right)^{-\\\\alpha}]\\nThe prior and posterior of a GP resulting from a RationalQuadratic kernel are shown in\\nthe following figure:\\n1.7.4.6. Exp-Sine-Squared kernel¶\\nThe ExpSineSquared kernel allows modeling periodic functions.\\nIt is parameterized by a length-scale parameter (l>0) and a periodicity parameter\\n(p>0). Only the isotropic variant where (l) is a scalar is supported at the moment.\\nThe kernel is given by:\\n[k(x_i, x_j) = \\\\text{exp}\\\\left(- \\\\frac{ 2\\\\sin^2(\\\\pi d(x_i, x_j) / p) }{ l^ 2} \\\\right)]\\nThe prior and posterior of a GP resulting from an ExpSineSquared kernel are shown in\\nthe following figure:\\n1.7.4.7. Dot-Product kernel¶\\nThe DotProduct kernel is non-stationary and can be obtained from linear regression\\nby putting (N(0, 1)) priors on the coefficients of (x_d (d = 1, . . . , D)) and\\na prior of (N(0, \\\\sigma_0^2)) on the bias. The DotProduct kernel is invariant to a rotation\\nof the coordinates about the origin, but not translations.\\nIt is parameterized by a parameter (\\\\sigma_0^2). For (\\\\sigma_0^2 = 0), the kernel\\nis called the homogeneous linear kernel, otherwise it is inhomogeneous. The kernel is given by\\n[k(x_i, x_j) = \\\\sigma_0 ^ 2 + x_i \\\\cdot x_j]\\nThe DotProduct kernel is commonly combined with exponentiation. An example with exponent 2 is\\nshown in the following figure:\\n1.7.4.8. References¶\\n[RW2006]\\n(1,2,3,4)\\nCarl E. Rasmussen and Christopher K.I. Williams,\\n“Gaussian Processes for Machine Learning”,\\nMIT Press 2006\\n[Duv2014]\\nDavid Duvenaud, “The Kernel Cookbook: Advice on Covariance functions”, 2014\\n© 2007 - 2024, scikit-learn developers (BSD License).\\nShow this page source\\n\\n1.8. Cross decomposition — scikit-learn 1.4.2 documentation\\n\\nInstall\\nUser Guide\\nAPI\\nExamples\\nCommunity\\nGetting Started\\nTutorial\\nWhat\\'s new\\nGlossary\\nDevelopment\\nFAQ\\nSupport\\nRelated packages\\nRoadmap\\nGovernance\\nAbout us\\nGitHub\\nOther Versions and Download\\nMore\\nGetting Started\\nTutorial\\nWhat\\'s new\\nGlossary\\nDevelopment\\nFAQ\\nSupport\\nRelated packages\\nRoadmap\\nGovernance\\nAbout us\\nGitHub\\nOther Versions and Download\\nToggle Menu\\nPrevUp\\nNext\\nscikit-learn 1.4.2\\nOther versions\\nPlease cite us if you use the software.\\n\\n1.8. Cross decomposition\\n\\n1.8.1. PLSCanonical\\n\\n1.8.2. PLSSVD\\n\\n1.8.3. PLSRegression\\n\\n1.8.4. Canonical Correlation Analysis\\n\\n1.8. Cross decomposition¶\\n\\nThe cross decomposition module contains supervised estimators for\\ndimensionality reduction and regression, belonging to the “Partial Least\\nSquares” family.\\nCross decomposition algorithms find the fundamental relations between two\\nmatrices (X and Y). They are latent variable approaches to modeling the\\ncovariance structures in these two spaces. They will try to find the\\nmultidimensional direction in the X space that explains the maximum\\nmultidimensional variance direction in the Y space. In other words, PLS\\nprojects both X and Y into a lower-dimensional subspace such that the\\ncovariance between transformed(X) and transformed(Y) is maximal.\\nPLS draws similarities with Principal Component Regression (PCR), where\\nthe samples are first projected into a lower-dimensional subspace, and the\\ntargets y are predicted using transformed(X). One issue with PCR is that\\nthe dimensionality reduction is unsupervised, and may lose some important\\nvariables: PCR would keep the features with the most variance, but it’s\\npossible that features with a small variances are relevant from predicting\\nthe target. In a way, PLS allows for the same kind of dimensionality\\nreduction, but by taking into account the targets y. An illustration of\\nthis fact is given in the following example:\\n* Principal Component Regression vs Partial Least Squares Regression.\\nApart from CCA, the PLS estimators are particularly suited when the matrix of\\npredictors has more variables than observations, and when there is\\nmulticollinearity among the features. By contrast, standard linear regression\\nwould fail in these cases unless it is regularized.\\nClasses included in this module are PLSRegression,\\nPLSCanonical, CCA and PLSSVD\\n- 1.8.1. PLSCanonical¶\\nWe here describe the algorithm used in PLSCanonical. The other\\nestimators use variants of this algorithm, and are detailed below.\\nWe recommend section [1] for more details and comparisons between these\\nalgorithms. In [1], PLSCanonical corresponds to “PLSW2A”.\\nGiven two centered matrices (X \\\\in \\\\mathbb{R}^{n \\\\times d}) and\\n(Y \\\\in \\\\mathbb{R}^{n \\\\times t}), and a number of components (K),\\nPLSCanonical proceeds as follows:\\nSet (X_1) to (X) and (Y_1) to (Y). Then, for each\\n(k \\\\in [1, K]):\\na) compute (u_k \\\\in \\\\mathbb{R}^d) and (v_k \\\\in \\\\mathbb{R}^t),\\nthe first left and right singular vectors of the cross-covariance matrix\\n(C = X_k^T Y_k).\\n(u_k) and (v_k) are called the weights.\\nBy definition, (u_k) and (v_k) are\\nchosen so that they maximize the covariance between the projected\\n(X_k) and the projected target, that is (\\\\text{Cov}(X_k u_k,\\nY_k v_k)).\\nb) Project (X_k) and (Y_k) on the singular vectors to obtain\\nscores: (\\\\xi_k = X_k u_k) and (\\\\omega_k = Y_k v_k)\\nc) Regress (X_k) on (\\\\xi_k), i.e. find a vector (\\\\gamma_k\\n\\\\in \\\\mathbb{R}^d) such that the rank-1 matrix (\\\\xi_k \\\\gamma_k^T)\\nis as close as possible to (X_k). Do the same on (Y_k) with\\n(\\\\omega_k) to obtain (\\\\delta_k). The vectors\\n(\\\\gamma_k) and (\\\\delta_k) are called the loadings.\\nd) deflate (X_k) and (Y_k), i.e. subtract the rank-1\\napproximations: (X_{k+1} = X_k - \\\\xi_k \\\\gamma_k^T), and\\n(Y_{k + 1} = Y_k - \\\\omega_k \\\\delta_k^T).\\nAt the end, we have approximated (X) as a sum of rank-1 matrices:\\n(X = \\\\Xi \\\\Gamma^T) where (\\\\Xi \\\\in \\\\mathbb{R}^{n \\\\times K})\\ncontains the scores in its columns, and (\\\\Gamma^T \\\\in \\\\mathbb{R}^{K\\n\\\\times d}) contains the loadings in its rows. Similarly for (Y), we\\nhave (Y = \\\\Omega \\\\Delta^T).\\nNote that the scores matrices (\\\\Xi) and (\\\\Omega) correspond to\\nthe projections of the training data (X) and (Y), respectively.\\nStep a) may be performed in two ways: either by computing the whole SVD of\\n(C) and only retain the singular vectors with the biggest singular\\nvalues, or by directly computing the singular vectors using the power method (cf section 11.3 in [1]),\\nwhich corresponds to the \\'nipals\\' option of the algorithm parameter.\\nTransforming data\\nClick for more details\\n¶\\nTo transform (X) into (\\\\bar{X}), we need to find a projection\\nmatrix (P) such that (\\\\bar{X} = XP). We know that for the\\ntraining data, (\\\\Xi = XP), and (X = \\\\Xi \\\\Gamma^T). Setting\\n(P = U(\\\\Gamma^T U)^{-1}) where (U) is the matrix with the\\n(u_k) in the columns, we have (XP = X U(\\\\Gamma^T U)^{-1} = \\\\Xi\\n(\\\\Gamma^T U) (\\\\Gamma^T U)^{-1} = \\\\Xi) as desired. The rotation matrix\\n(P) can be accessed from the x_rotations_ attribute.\\nSimilarly, (Y) can be transformed using the rotation matrix\\n(V(\\\\Delta^T V)^{-1}), accessed via the y_rotations_ attribute.\\nPredicting the targets Y\\nClick for more details\\n¶\\nTo predict the targets of some data (X), we are looking for a\\ncoefficient matrix (\\\\beta \\\\in R^{d \\\\times t}) such that (Y =\\nX\\\\beta).\\nThe idea is to try to predict the transformed targets (\\\\Omega) as a\\nfunction of the transformed samples (\\\\Xi), by computing (\\\\alpha\\n\\\\in \\\\mathbb{R}) such that (\\\\Omega = \\\\alpha \\\\Xi).\\nThen, we have (Y = \\\\Omega \\\\Delta^T = \\\\alpha \\\\Xi \\\\Delta^T), and since\\n(\\\\Xi) is the transformed training data we have that (Y = X \\\\alpha\\nP \\\\Delta^T), and as a result the coefficient matrix (\\\\beta = \\\\alpha P\\n\\\\Delta^T).\\n(\\\\beta) can be accessed through the coef_ attribute.\\n- 1.8.2. PLSSVD¶\\nPLSSVD is a simplified version of PLSCanonical\\ndescribed earlier: instead of iteratively deflating the matrices (X_k)\\nand (Y_k), PLSSVD computes the SVD of (C = X^TY)\\nonly once, and stores the n_components singular vectors corresponding to\\nthe biggest singular values in the matrices U and V, corresponding to the\\nx_weights_ and y_weights_ attributes. Here, the transformed data is\\nsimply transformed(X) = XU and transformed(Y) = YV.\\nIf n_components == 1, PLSSVD and PLSCanonical are\\nstrictly equivalent.\\n- 1.8.3. PLSRegression¶\\nThe PLSRegression estimator is similar to\\nPLSCanonical with algorithm=\\'nipals\\', with 2 significant\\ndifferences:\\nat step a) in the power method to compute (u_k) and (v_k),\\n(v_k) is never normalized.\\nat step c), the targets (Y_k) are approximated using the projection\\nof (X_k) (i.e. (\\\\xi_k)) instead of the projection of\\n(Y_k) (i.e. (\\\\omega_k)). In other words, the loadings\\ncomputation is different. As a result, the deflation in step d) will also\\nbe affected.\\nThese two modifications affect the output of predict and transform,\\nwhich are not the same as for PLSCanonical. Also, while the number\\nof components is limited by min(n_samples, n_features, n_targets) in\\nPLSCanonical, here the limit is the rank of (X^TX), i.e.\\nmin(n_samples, n_features).\\nPLSRegression is also known as PLS1 (single targets) and PLS2\\n(multiple targets). Much like Lasso,\\nPLSRegression is a form of regularized linear regression where the\\nnumber of components controls the strength of the regularization.\\n- 1.8.4. Canonical Correlation Analysis¶\\nCanonical Correlation Analysis was developed prior and independently to PLS.\\nBut it turns out that CCA is a special case of PLS, and corresponds\\nto PLS in “Mode B” in the literature.\\nCCA differs from PLSCanonical in the way the weights\\n(u_k) and (v_k) are computed in the power method of step a).\\nDetails can be found in section 10 of [1].\\nSince CCA involves the inversion of (X_k^TX_k) and\\n(Y_k^TY_k), this estimator can be unstable if the number of features or\\ntargets is greater than the number of samples.\\nReference\\nClick for more details\\n¶\\n[1]\\n(1,2,3,4)\\nA survey of Partial Least Squares (PLS) methods, with emphasis on\\nthe two-block case\\nJA Wegelin\\nExamples:\\nCompare cross decomposition methods\\nPrincipal Component Regression vs Partial Least Squares Regression\\n© 2007 - 2024, scikit-learn developers (BSD License).\\nShow this page source\\n\\n1.9. Naive Bayes — scikit-learn 1.4.2 documentation\\n\\nInstall\\nUser Guide\\nAPI\\nExamples\\nCommunity\\nGetting Started\\nTutorial\\nWhat\\'s new\\nGlossary\\nDevelopment\\nFAQ\\nSupport\\nRelated packages\\nRoadmap\\nGovernance\\nAbout us\\nGitHub\\nOther Versions and Download\\nMore\\nGetting Started\\nTutorial\\nWhat\\'s new\\nGlossary\\nDevelopment\\nFAQ\\nSupport\\nRelated packages\\nRoadmap\\nGovernance\\nAbout us\\nGitHub\\nOther Versions and Download\\nToggle Menu\\nPrevUp\\nNext\\nscikit-learn 1.4.2\\nOther versions\\nPlease cite us if you use the software.\\n\\n1.9. Naive Bayes\\n\\n1.9.1. Gaussian Naive Bayes\\n\\n1.9.2. Multinomial Naive Bayes\\n\\n1.9.3. Complement Naive Bayes\\n\\n1.9.4. Bernoulli Naive Bayes\\n\\n1.9.5. Categorical Naive Bayes\\n\\n1.9.6. Out-of-core naive Bayes model fitting\\n\\n1.9. Naive Bayes¶\\n\\nNaive Bayes methods are a set of supervised learning algorithms\\nbased on applying Bayes’ theorem with the “naive” assumption of\\nconditional independence between every pair of features given the\\nvalue of the class variable. Bayes’ theorem states the following\\nrelationship, given class variable (y) and dependent feature\\nvector (x_1) through (x_n), :\\n[P(y \\\\mid x_1, \\\\dots, x_n) = \\\\frac{P(y) P(x_1, \\\\dots, x_n \\\\mid y)}\\n{P(x_1, \\\\dots, x_n)}]\\nUsing the naive conditional independence assumption that\\n[P(x_i | y, x_1, \\\\dots, x_{i-1}, x_{i+1}, \\\\dots, x_n) = P(x_i | y),]\\nfor all (i), this relationship is simplified to\\n[P(y \\\\mid x_1, \\\\dots, x_n) = \\\\frac{P(y) \\\\prod_{i=1}^{n} P(x_i \\\\mid y)}\\n{P(x_1, \\\\dots, x_n)}]\\nSince (P(x_1, \\\\dots, x_n)) is constant given the input,\\nwe can use the following classification rule:\\n[ \\\\begin{align}\\\\begin{aligned}P(y \\\\mid x_1, \\\\dots, x_n) \\\\propto P(y) \\\\prod_{i=1}^{n} P(x_i \\\\mid y)\\\\\\\\Downarrow\\\\\\\\hat{y} = \\\\arg\\\\max_y P(y) \\\\prod_{i=1}^{n} P(x_i \\\\mid y),\\\\end{aligned}\\\\end{align} ]\\nand we can use Maximum A Posteriori (MAP) estimation to estimate\\n(P(y)) and (P(x_i \\\\mid y));\\nthe former is then the relative frequency of class (y)\\nin the training set.\\nThe different naive Bayes classifiers differ mainly by the assumptions they\\nmake regarding the distribution of (P(x_i \\\\mid y)).\\nIn spite of their apparently over-simplified assumptions, naive Bayes\\nclassifiers have worked quite well in many real-world situations, famously\\ndocument classification and spam filtering. They require a small amount\\nof training data to estimate the necessary parameters. (For theoretical\\nreasons why naive Bayes works well, and on which types of data it does, see\\nthe references below.)\\nNaive Bayes learners and classifiers can be extremely fast compared to more\\nsophisticated methods.\\nThe decoupling of the class conditional feature distributions means that each\\ndistribution can be independently estimated as a one dimensional distribution.\\nThis in turn helps to alleviate problems stemming from the curse of\\ndimensionality.\\nOn the flip side, although naive Bayes is known as a decent classifier,\\nit is known to be a bad estimator, so the probability outputs from\\npredict_proba are not to be taken too seriously.\\nReferences\\nClick for more details\\n¶\\nH. Zhang (2004). The optimality of Naive Bayes.\\nProc. FLAIRS.\\n- 1.9.1. Gaussian Naive Bayes¶\\nGaussianNB implements the Gaussian Naive Bayes algorithm for\\nclassification. The likelihood of the features is assumed to be Gaussian:\\n[P(x_i \\\\mid y) = \\\\frac{1}{\\\\sqrt{2\\\\pi\\\\sigma^2_y}} \\\\exp\\\\left(-\\\\frac{(x_i - \\\\mu_y)^2}{2\\\\sigma^2_y}\\\\right)]\\nThe parameters (\\\\sigma_y) and (\\\\mu_y)\\nare estimated using maximum likelihood.\\n\\nfrom sklearn.datasets import load_iris\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.naive_bayes import GaussianNB\\nX, y = load_iris(return_X_y=True)\\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=0)\\ngnb = GaussianNB()\\ny_pred = gnb.fit(X_train, y_train).predict(X_test)\\nprint(\"Number of mislabeled points out of a total %d points : %d\"\\n...       % (X_test.shape[0], (y_test != y_pred).sum()))\\nNumber of mislabeled points out of a total 75 points : 4\\n- 1.9.2. Multinomial Naive Bayes¶\\nMultinomialNB implements the naive Bayes algorithm for multinomially\\ndistributed data, and is one of the two classic naive Bayes variants used in\\ntext classification (where the data are typically represented as word vector\\ncounts, although tf-idf vectors are also known to work well in practice).\\nThe distribution is parametrized by vectors\\n(\\\\theta_y = (\\\\theta_{y1},\\\\ldots,\\\\theta_{yn}))\\nfor each class (y), where (n) is the number of features\\n(in text classification, the size of the vocabulary)\\nand (\\\\theta_{yi}) is the probability (P(x_i \\\\mid y))\\nof feature (i) appearing in a sample belonging to class (y).\\nThe parameters (\\\\theta_y) is estimated by a smoothed\\nversion of maximum likelihood, i.e. relative frequency counting:\\n[\\\\hat{\\\\theta}{yi} = \\\\frac{ N{yi} + \\\\alpha}{N_y + \\\\alpha n}]\\nwhere (N_{yi} = \\\\sum_{x \\\\in T} x_i) is\\nthe number of times feature (i) appears in a sample of class (y)\\nin the training set (T),\\nand (N_{y} = \\\\sum_{i=1}^{n} N_{yi}) is the total count of\\nall features for class (y).\\nThe smoothing priors (\\\\alpha \\\\ge 0) accounts for\\nfeatures not present in the learning samples and prevents zero probabilities\\nin further computations.\\nSetting (\\\\alpha = 1) is called Laplace smoothing,\\nwhile (\\\\alpha < 1) is called Lidstone smoothing.\\n- 1.9.3. Complement Naive Bayes¶\\nComplementNB implements the complement naive Bayes (CNB) algorithm.\\nCNB is an adaptation of the standard multinomial naive Bayes (MNB) algorithm\\nthat is particularly suited for imbalanced data sets. Specifically, CNB uses\\nstatistics from the complement of each class to compute the model’s weights.\\nThe inventors of CNB show empirically that the parameter estimates for CNB are\\nmore stable than those for MNB. Further, CNB regularly outperforms MNB (often\\nby a considerable margin) on text classification tasks.\\nWeights calculation\\nClick for more details\\n¶\\nThe procedure for calculating the weights is as follows:\\n[ \\\\begin{align}\\\\begin{aligned}\\\\hat{\\\\theta}{ci} = \\\\frac{\\\\alpha_i + \\\\sum{j:y_j \\\\neq c} d_{ij}}\\n{\\\\alpha + \\\\sum_{j:y_j \\\\neq c} \\\\sum_{k} d_{kj}}\\\\w_{ci} = \\\\log \\\\hat{\\\\theta}{ci}\\\\w{ci} = \\\\frac{w_{ci}}{\\\\sum_{j} |w_{cj}|}\\\\end{aligned}\\\\end{align} ]\\nwhere the summations are over all documents (j) not in class (c),\\n(d_{ij}) is either the count or tf-idf value of term (i) in document\\n(j), (\\\\alpha_i) is a smoothing hyperparameter like that found in\\nMNB, and (\\\\alpha = \\\\sum_{i} \\\\alpha_i). The second normalization addresses\\nthe tendency for longer documents to dominate parameter estimates in MNB. The\\nclassification rule is:\\n[\\\\hat{c} = \\\\arg\\\\min_c \\\\sum_{i} t_i w_{ci}]\\ni.e., a document is assigned to the class that is the poorest complement\\nmatch.\\nReferences\\nClick for more details\\n¶\\nRennie, J. D., Shih, L., Teevan, J., & Karger, D. R. (2003).\\nTackling the poor assumptions of naive bayes text classifiers.\\nIn ICML (Vol. 3, pp. 616-623).\\n- 1.9.4. Bernoulli Naive Bayes¶\\nBernoulliNB implements the naive Bayes training and classification\\nalgorithms for data that is distributed according to multivariate Bernoulli\\ndistributions; i.e., there may be multiple features but each one is assumed\\nto be a binary-valued (Bernoulli, boolean) variable.\\nTherefore, this class requires samples to be represented as binary-valued\\nfeature vectors; if handed any other kind of data, a BernoulliNB instance\\nmay binarize its input (depending on the binarize parameter).\\nThe decision rule for Bernoulli naive Bayes is based on\\n[P(x_i \\\\mid y) = P(x_i = 1 \\\\mid y) x_i + (1 - P(x_i = 1 \\\\mid y)) (1 - x_i)]\\nwhich differs from multinomial NB’s rule\\nin that it explicitly penalizes the non-occurrence of a feature (i)\\nthat is an indicator for class (y),\\nwhere the multinomial variant would simply ignore a non-occurring feature.\\nIn the case of text classification, word occurrence vectors (rather than word\\ncount vectors) may be used to train and use this classifier. BernoulliNB\\nmight perform better on some datasets, especially those with shorter documents.\\nIt is advisable to evaluate both models, if time permits.\\nReferences\\nClick for more details\\n¶\\nC.D. Manning, P. Raghavan and H. Schütze (2008). Introduction to\\nInformation Retrieval. Cambridge University Press, pp. 234-265.\\nA. McCallum and K. Nigam (1998).\\nA comparison of event models for Naive Bayes text classification.\\nProc. AAAI/ICML-98 Workshop on Learning for Text Categorization, pp. 41-48.\\nV. Metsis, I. Androutsopoulos and G. Paliouras (2006).\\nSpam filtering with Naive Bayes – Which Naive Bayes?\\n3rd Conf. on Email and Anti-Spam (CEAS).\\n- 1.9.5. Categorical Naive Bayes¶\\nCategoricalNB implements the categorical naive Bayes\\nalgorithm for categorically distributed data. It assumes that each feature,\\nwhich is described by the index (i), has its own categorical\\ndistribution.\\nFor each feature (i) in the training set (X),\\nCategoricalNB estimates a categorical distribution for each feature i\\nof X conditioned on the class y. The index set of the samples is defined as\\n(J = { 1, \\\\dots, m }), with (m) as the number of samples.\\nProbability calculation\\nClick for more details\\n¶\\nThe probability of category (t) in feature (i) given class\\n(c) is estimated as:\\n[P(x_i = t \\\\mid y = c \\\\: ;\\\\, \\\\alpha) = \\\\frac{ N_{tic} + \\\\alpha}{N_{c} +\\n\\\\alpha n_i},]\\nwhere (N_{tic} = |{j \\\\in J \\\\mid x_{ij} = t, y_j = c}|) is the number\\nof times category (t) appears in the samples (x_{i}), which belong\\nto class (c), (N_{c} = |{ j \\\\in J\\\\mid y_j = c}|) is the number\\nof samples with class c, (\\\\alpha) is a smoothing parameter and\\n(n_i) is the number of available categories of feature (i).\\nCategoricalNB assumes that the sample matrix (X) is encoded (for\\ninstance with the help of OrdinalEncoder) such\\nthat all categories for each feature (i) are represented with numbers\\n(0, ..., n_i - 1) where (n_i) is the number of available categories\\nof feature (i).\\n- 1.9.6. Out-of-core naive Bayes model fitting¶\\nNaive Bayes models can be used to tackle large scale classification problems\\nfor which the full training set might not fit in memory. To handle this case,\\nMultinomialNB, BernoulliNB, and GaussianNB\\nexpose a partial_fit method that can be used\\nincrementally as done with other classifiers as demonstrated in\\nOut-of-core classification of text documents. All naive Bayes\\nclassifiers support sample weighting.\\nContrary to the fit method, the first call to partial_fit needs to be\\npassed the list of all the expected class labels.\\nFor an overview of available strategies in scikit-learn, see also the\\nout-of-core learning documentation.\\nNote\\nThe partial_fit method call of naive Bayes models introduces some\\ncomputational overhead. It is recommended to use data chunk sizes that are as\\nlarge as possible, that is as the available RAM allows.\\n© 2007 - 2024, scikit-learn developers (BSD License).\\nShow this page source\\n\\n1.10. Decision Trees — scikit-learn 1.4.2 documentation\\n\\nInstall\\nUser Guide\\nAPI\\nExamples\\nCommunity\\nGetting Started\\nTutorial\\nWhat\\'s new\\nGlossary\\nDevelopment\\nFAQ\\nSupport\\nRelated packages\\nRoadmap\\nGovernance\\nAbout us\\nGitHub\\nOther Versions and Download\\nMore\\nGetting Started\\nTutorial\\nWhat\\'s new\\nGlossary\\nDevelopment\\nFAQ\\nSupport\\nRelated packages\\nRoadmap\\nGovernance\\nAbout us\\nGitHub\\nOther Versions and Download\\nToggle Menu\\nPrevUp\\nNext\\nscikit-learn 1.4.2\\nOther versions\\nPlease cite us if you use the software.\\n\\n1.10. Decision Trees\\n\\n1.10.1. Classification\\n\\n1.10.2. Regression\\n\\n1.10.3. Multi-output problems\\n\\n1.10.4. Complexity\\n\\n1.10.5. Tips on practical use\\n\\n1.10.6. Tree algorithms: ID3, C4.5, C5.0 and CART\\n\\n1.10.7. Mathematical formulation\\n1.10.7.1. Classification criteria\\n1.10.7.2. Regression criteria\\n\\n1.10.8. Missing Values Support\\n\\n1.10.9. Minimal Cost-Complexity Pruning\\n\\n1.10. Decision Trees¶\\n\\nDecision Trees (DTs) are a non-parametric supervised learning method used\\nfor classification and regression. The goal is to create a model that predicts the value of a\\ntarget variable by learning simple decision rules inferred from the data\\nfeatures. A tree can be seen as a piecewise constant approximation.\\nFor instance, in the example below, decision trees learn from data to\\napproximate a sine curve with a set of if-then-else decision rules. The deeper\\nthe tree, the more complex the decision rules and the fitter the model.\\nSome advantages of decision trees are:\\nSimple to understand and to interpret. Trees can be visualized.\\nRequires little data preparation. Other techniques often require data\\nnormalization, dummy variables need to be created and blank values to\\nbe removed. Some tree and algorithm combinations support\\nmissing values.\\nThe cost of using the tree (i.e., predicting data) is logarithmic in the\\nnumber of data points used to train the tree.\\nAble to handle both numerical and categorical data. However, the scikit-learn\\nimplementation does not support categorical variables for now. Other\\ntechniques are usually specialized in analyzing datasets that have only one type\\nof variable. See algorithms for more\\ninformation.\\nAble to handle multi-output problems.\\nUses a white box model. If a given situation is observable in a model,\\nthe explanation for the condition is easily explained by boolean logic.\\nBy contrast, in a black box model (e.g., in an artificial neural\\nnetwork), results may be more difficult to interpret.\\nPossible to validate a model using statistical tests. That makes it\\npossible to account for the reliability of the model.\\nPerforms well even if its assumptions are somewhat violated by\\nthe true model from which the data were generated.\\nThe disadvantages of decision trees include:\\nDecision-tree learners can create over-complex trees that do not\\ngeneralize the data well. This is called overfitting. Mechanisms\\nsuch as pruning, setting the minimum number of samples required\\nat a leaf node or setting the maximum depth of the tree are\\nnecessary to avoid this problem.\\nDecision trees can be unstable because small variations in the\\ndata might result in a completely different tree being generated.\\nThis problem is mitigated by using decision trees within an\\nensemble.\\nPredictions of decision trees are neither smooth nor continuous, but\\npiecewise constant approximations as seen in the above figure. Therefore,\\nthey are not good at extrapolation.\\nThe problem of learning an optimal decision tree is known to be\\nNP-complete under several aspects of optimality and even for simple\\nconcepts. Consequently, practical decision-tree learning algorithms\\nare based on heuristic algorithms such as the greedy algorithm where\\nlocally optimal decisions are made at each node. Such algorithms\\ncannot guarantee to return the globally optimal decision tree.  This\\ncan be mitigated by training multiple trees in an ensemble learner,\\nwhere the features and samples are randomly sampled with replacement.\\nThere are concepts that are hard to learn because decision trees\\ndo not express them easily, such as XOR, parity or multiplexer problems.\\nDecision tree learners create biased trees if some classes dominate.\\nIt is therefore recommended to balance the dataset prior to fitting\\nwith the decision tree.\\n- 1.10.1. Classification¶\\nDecisionTreeClassifier is a class capable of performing multi-class\\nclassification on a dataset.\\nAs with other classifiers, DecisionTreeClassifier takes as input two arrays:\\nan array X, sparse or dense, of shape (n_samples, n_features) holding the\\ntraining samples, and an array Y of integer values, shape (n_samples,),\\nholding the class labels for the training samples:\\n\\nfrom sklearn import tree\\nX = [[0, 0], [1, 1]]\\nY = [0, 1]\\nclf = tree.DecisionTreeClassifier()\\nclf = clf.fit(X, Y)\\nAfter being fitted, the model can then be used to predict the class of samples:\\nclf.predict([[2., 2.]])\\narray([1])\\nIn case that there are multiple classes with the same and highest\\nprobability, the classifier will predict the class with the lowest index\\namongst those classes.\\nAs an alternative to outputting a specific class, the probability of each class\\ncan be predicted, which is the fraction of training samples of the class in a\\nleaf:\\nclf.predict_proba([[2., 2.]])\\narray([[0., 1.]])\\nDecisionTreeClassifier is capable of both binary (where the\\nlabels are [-1, 1]) classification and multiclass (where the labels are\\n[0, …, K-1]) classification.\\nUsing the Iris dataset, we can construct a tree as follows:\\nfrom sklearn.datasets import load_iris\\nfrom sklearn import tree\\niris = load_iris()\\nX, y = iris.data, iris.target\\nclf = tree.DecisionTreeClassifier()\\nclf = clf.fit(X, y)\\nOnce trained, you can plot the tree with the plot_tree function:\\ntree.plot_tree(clf)\\n[...]\\nAlternative ways to export trees\\nClick for more details\\n¶\\nWe can also export the tree in Graphviz format using the export_graphviz\\nexporter. If you use the conda package manager, the graphviz binaries\\nand the python package can be installed with conda install python-graphviz.\\nAlternatively binaries for graphviz can be downloaded from the graphviz project homepage,\\nand the Python wrapper installed from pypi with pip install graphviz.\\nBelow is an example graphviz export of the above tree trained on the entire\\niris dataset; the results are saved in an output file iris.pdf:\\nimport graphviz\\ndot_data = tree.export_graphviz(clf, out_file=None)\\ngraph = graphviz.Source(dot_data)\\ngraph.render(\"iris\")\\nThe export_graphviz exporter also supports a variety of aesthetic\\noptions, including coloring nodes by their class (or value for regression) and\\nusing explicit variable and class names if desired. Jupyter notebooks also\\nrender these plots inline automatically:\\ndot_data = tree.export_graphviz(clf, out_file=None,\\n...                      feature_names=iris.feature_names,\\n...                      class_names=iris.target_names,\\n...                      filled=True, rounded=True,\\n...                      special_characters=True)\\ngraph = graphviz.Source(dot_data)\\ngraph\\nAlternatively, the tree can also be exported in textual format with the\\nfunction export_text. This method doesn’t require the installation\\nof external libraries and is more compact:\\nfrom sklearn.datasets import load_iris\\nfrom sklearn.tree import DecisionTreeClassifier\\nfrom sklearn.tree import export_text\\niris = load_iris()\\ndecision_tree = DecisionTreeClassifier(random_state=0, max_depth=2)\\ndecision_tree = decision_tree.fit(iris.data, iris.target)\\nr = export_text(decision_tree, feature_names=iris[\\'feature_names\\'])\\nprint(r)\\n|--- petal width (cm) <= 0.80\\n|   |--- class: 0\\n|--- petal width (cm) >  0.80\\n|   |--- petal width (cm) <= 1.75\\n|   |   |--- class: 1\\n|   |--- petal width (cm) >  1.75\\n|   |   |--- class: 2\\nExamples:\\nPlot the decision surface of decision trees trained on the iris dataset\\nUnderstanding the decision tree structure\\n- 1.10.2. Regression¶\\nDecision trees can also be applied to regression problems, using the\\nDecisionTreeRegressor class.\\nAs in the classification setting, the fit method will take as argument arrays X\\nand y, only that in this case y is expected to have floating point values\\ninstead of integer values:\\nfrom sklearn import tree\\nX = [[0, 0], [2, 2]]\\ny = [0.5, 2.5]\\nclf = tree.DecisionTreeRegressor()\\nclf = clf.fit(X, y)\\nclf.predict([[1, 1]])\\narray([0.5])\\nExamples:\\nDecision Tree Regression\\n- 1.10.3. Multi-output problems¶\\nA multi-output problem is a supervised learning problem with several outputs\\nto predict, that is when Y is a 2d array of shape (n_samples, n_outputs).\\nWhen there is no correlation between the outputs, a very simple way to solve\\nthis kind of problem is to build n independent models, i.e. one for each\\noutput, and then to use those models to independently predict each one of the n\\noutputs. However, because it is likely that the output values related to the\\nsame input are themselves correlated, an often better way is to build a single\\nmodel capable of predicting simultaneously all n outputs. First, it requires\\nlower training time since only a single estimator is built. Second, the\\ngeneralization accuracy of the resulting estimator may often be increased.\\nWith regard to decision trees, this strategy can readily be used to support\\nmulti-output problems. This requires the following changes:\\nStore n output values in leaves, instead of 1;\\nUse splitting criteria that compute the average reduction across all\\nn outputs.\\nThis module offers support for multi-output problems by implementing this\\nstrategy in both DecisionTreeClassifier and\\nDecisionTreeRegressor. If a decision tree is fit on an output array Y\\nof shape (n_samples, n_outputs) then the resulting estimator will:\\nOutput n_output values upon predict;\\nOutput a list of n_output arrays of class probabilities upon\\npredict_proba.\\nThe use of multi-output trees for regression is demonstrated in\\nMulti-output Decision Tree Regression. In this example, the input\\nX is a single real value and the outputs Y are the sine and cosine of X.\\nThe use of multi-output trees for classification is demonstrated in\\nFace completion with a multi-output estimators. In this example, the inputs\\nX are the pixels of the upper half of faces and the outputs Y are the pixels of\\nthe lower half of those faces.\\nExamples:\\nMulti-output Decision Tree Regression\\nFace completion with a multi-output estimators\\nReferences\\nClick for more details\\n¶\\nM. Dumont et al,  Fast multi-class image annotation with random subwindows\\nand multiple output randomized trees, International Conference on\\nComputer Vision Theory and Applications 2009\\n- 1.10.4. Complexity¶\\nIn general, the run time cost to construct a balanced binary tree is\\n(O(n_{samples}n_{features}\\\\log(n_{samples}))) and query time\\n(O(\\\\log(n_{samples}))).  Although the tree construction algorithm attempts\\nto generate balanced trees, they will not always be balanced.  Assuming that the\\nsubtrees remain approximately balanced, the cost at each node consists of\\nsearching through (O(n_{features})) to find the feature that offers the\\nlargest reduction in the impurity criterion, e.g. log loss (which is equivalent to an\\ninformation gain). This has a cost of\\n(O(n_{features}n_{samples}\\\\log(n_{samples}))) at each node, leading to a\\ntotal cost over the entire trees (by summing the cost at each node) of\\n(O(n_{features}n_{samples}^{2}\\\\log(n_{samples}))).\\n- 1.10.5. Tips on practical use¶\\nDecision trees tend to overfit on data with a large number of features.\\nGetting the right ratio of samples to number of features is important, since\\na tree with few samples in high dimensional space is very likely to overfit.\\nConsider performing  dimensionality reduction (PCA,\\nICA, or Feature selection) beforehand to\\ngive your tree a better chance of finding features that are discriminative.\\nUnderstanding the decision tree structure will help\\nin gaining more insights about how the decision tree makes predictions, which is\\nimportant for understanding the important features in the data.\\nVisualize your tree as you are training by using the export\\nfunction.  Use max_depth=3 as an initial tree depth to get a feel for\\nhow the tree is fitting to your data, and then increase the depth.\\nRemember that the number of samples required to populate the tree doubles\\nfor each additional level the tree grows to.  Use max_depth to control\\nthe size of the tree to prevent overfitting.\\nUse min_samples_split or min_samples_leaf to ensure that multiple\\nsamples inform every decision in the tree, by controlling which splits will\\nbe considered. A very small number will usually mean the tree will overfit,\\nwhereas a large number will prevent the tree from learning the data. Try\\nmin_samples_leaf=5 as an initial value. If the sample size varies\\ngreatly, a float number can be used as percentage in these two parameters.\\nWhile min_samples_split can create arbitrarily small leaves,\\nmin_samples_leaf guarantees that each leaf has a minimum size, avoiding\\nlow-variance, over-fit leaf nodes in regression problems.  For\\nclassification with few classes, min_samples_leaf=1 is often the best\\nchoice.\\nNote that min_samples_split considers samples directly and independent of\\nsample_weight, if provided (e.g. a node with m weighted samples is still\\ntreated as having exactly m samples). Consider min_weight_fraction_leaf or\\nmin_impurity_decrease if accounting for sample weights is required at splits.\\nBalance your dataset before training to prevent the tree from being biased\\ntoward the classes that are dominant. Class balancing can be done by\\nsampling an equal number of samples from each class, or preferably by\\nnormalizing the sum of the sample weights (sample_weight) for each\\nclass to the same value. Also note that weight-based pre-pruning criteria,\\nsuch as min_weight_fraction_leaf, will then be less biased toward\\ndominant classes than criteria that are not aware of the sample weights,\\nlike min_samples_leaf.\\nIf the samples are weighted, it will be easier to optimize the tree\\nstructure using weight-based pre-pruning criterion such as\\nmin_weight_fraction_leaf, which ensure that leaf nodes contain at least\\na fraction of the overall sum of the sample weights.\\nAll decision trees use np.float32 arrays internally.\\nIf training data is not in this format, a copy of the dataset will be made.\\nIf the input matrix X is very sparse, it is recommended to convert to sparse\\ncsc_matrix before calling fit and sparse csr_matrix before calling\\npredict. Training time can be orders of magnitude faster for a sparse\\nmatrix input compared to a dense matrix when features have zero values in\\nmost of the samples.\\n- 1.10.6. Tree algorithms: ID3, C4.5, C5.0 and CART¶\\nWhat are all the various decision tree algorithms and how do they differ\\nfrom each other? Which one is implemented in scikit-learn?\\nVarious decision tree algorithms\\nClick for more details\\n¶\\nID3 (Iterative Dichotomiser 3) was developed in 1986 by Ross Quinlan.\\nThe algorithm creates a multiway tree, finding for each node (i.e. in\\na greedy manner) the categorical feature that will yield the largest\\ninformation gain for categorical targets. Trees are grown to their\\nmaximum size and then a pruning step is usually applied to improve the\\nability of the tree to generalize to unseen data.\\nC4.5 is the successor to ID3 and removed the restriction that features\\nmust be categorical by dynamically defining a discrete attribute (based\\non numerical variables) that partitions the continuous attribute value\\ninto a discrete set of intervals. C4.5 converts the trained trees\\n(i.e. the output of the ID3 algorithm) into sets of if-then rules.\\nThe accuracy of each rule is then evaluated to determine the order\\nin which they should be applied. Pruning is done by removing a rule’s\\nprecondition if the accuracy of the rule improves without it.\\nC5.0 is Quinlan’s latest version release under a proprietary license.\\nIt uses less memory and builds smaller rulesets than C4.5 while being\\nmore accurate.\\nCART (Classification and Regression Trees) is very similar to C4.5, but\\nit differs in that it supports numerical target variables (regression) and\\ndoes not compute rule sets. CART constructs binary trees using the feature\\nand threshold that yield the largest information gain at each node.\\nscikit-learn uses an optimized version of the CART algorithm; however, the\\nscikit-learn implementation does not support categorical variables for now.\\n- 1.10.7. Mathematical formulation¶\\nGiven training vectors (x_i \\\\in R^n), i=1,…, l and a label vector\\n(y \\\\in R^l), a decision tree recursively partitions the feature space\\nsuch that the samples with the same labels or similar target values are grouped\\ntogether.\\nLet the data at node (m) be represented by (Q_m) with (n_m)\\nsamples. For each candidate split (\\\\theta = (j, t_m)) consisting of a\\nfeature (j) and threshold (t_m), partition the data into\\n(Q_m^{left}(\\\\theta)) and (Q_m^{right}(\\\\theta)) subsets\\n[ \\\\begin{align}\\\\begin{aligned}Q_m^{left}(\\\\theta) = {(x, y) | x_j \\\\leq t_m}\\\\Q_m^{right}(\\\\theta) = Q_m \\\\setminus Q_m^{left}(\\\\theta)\\\\end{aligned}\\\\end{align} ]\\nThe quality of a candidate split of node (m) is then computed using an\\nimpurity function or loss function (H()), the choice of which depends on\\nthe task being solved (classification or regression)\\n[G(Q_m, \\\\theta) = \\\\frac{n_m^{left}}{n_m} H(Q_m^{left}(\\\\theta))\\n+ \\\\frac{n_m^{right}}{n_m} H(Q_m^{right}(\\\\theta))]\\nSelect the parameters that minimises the impurity\\n[\\\\theta^ = \\\\operatorname{argmin}_\\\\theta  G(Q_m, \\\\theta)]\\nRecurse for subsets (Q_m^{left}(\\\\theta^)) and\\n(Q_m^{right}(\\\\theta^*)) until the maximum allowable depth is reached,\\n(n_m < \\\\min_{samples}) or (n_m = 1).\\n1.10.7.1. Classification criteria¶\\nIf a target is a classification outcome taking on values 0,1,…,K-1,\\nfor node (m), let\\n[p_{mk} = \\\\frac{1}{n_m} \\\\sum_{y \\\\in Q_m} I(y = k)]\\nbe the proportion of class k observations in node (m). If (m) is a\\nterminal node, predict_proba for this region is set to (p_{mk}).\\nCommon measures of impurity are the following.\\nGini:\\n[H(Q_m) = \\\\sum_k p_{mk} (1 - p_{mk})]\\nLog Loss or Entropy:\\n[H(Q_m) = - \\\\sum_k p_{mk} \\\\log(p_{mk})]\\nShannon entropy\\nClick for more details\\n¶\\nThe entropy criterion computes the Shannon entropy of the possible classes. It\\ntakes the class frequencies of the training data points that reached a given\\nleaf (m) as their probability. Using the Shannon entropy as tree node\\nsplitting criterion is equivalent to minimizing the log loss (also known as\\ncross-entropy and multinomial deviance) between the true labels (y_i)\\nand the probabilistic predictions (T_k(x_i)) of the tree model (T) for class (k).\\nTo see this, first recall that the log loss of a tree model (T)\\ncomputed on a dataset (D) is defined as follows:\\n[\\\\mathrm{LL}(D, T) = -\\\\frac{1}{n} \\\\sum_{(x_i, y_i) \\\\in D} \\\\sum_k I(y_i = k) \\\\log(T_k(x_i))]\\nwhere (D) is a training dataset of (n) pairs ((x_i, y_i)).\\nIn a classification tree, the predicted class probabilities within leaf nodes\\nare constant, that is: for all ((x_i, y_i) \\\\in Q_m), one has:\\n(T_k(x_i) = p_{mk}) for each class (k).\\nThis property makes it possible to rewrite (\\\\mathrm{LL}(D, T)) as the\\nsum of the Shannon entropies computed for each leaf of (T) weighted by\\nthe number of training data points that reached each leaf:\\n[\\\\mathrm{LL}(D, T) = \\\\sum_{m \\\\in T} \\\\frac{n_m}{n} H(Q_m)]\\n1.10.7.2. Regression criteria¶\\nIf the target is a continuous value, then for node (m), common\\ncriteria to minimize as for determining locations for future splits are Mean\\nSquared Error (MSE or L2 error), Poisson deviance as well as Mean Absolute\\nError (MAE or L1 error). MSE and Poisson deviance both set the predicted value\\nof terminal nodes to the learned mean value (\\\\bar{y}m) of the node\\nwhereas the MAE sets the predicted value of terminal nodes to the median\\n(median(y)_m).\\nMean Squared Error:\\n[ \\\\begin{align}\\\\begin{aligned}\\\\bar{y}_m = \\\\frac{1}{n_m} \\\\sum{y \\\\in Q_m} y\\\\H(Q_m) = \\\\frac{1}{n_m} \\\\sum_{y \\\\in Q_m} (y - \\\\bar{y}m)^2\\\\end{aligned}\\\\end{align} ]\\nHalf Poisson deviance:\\n[H(Q_m) = \\\\frac{1}{n_m} \\\\sum{y \\\\in Q_m} (y \\\\log\\\\frac{y}{\\\\bar{y}m}\\n- y + \\\\bar{y}_m)]\\nSetting criterion=\"poisson\" might be a good choice if your target is a count\\nor a frequency (count per some unit). In any case, (y >= 0) is a\\nnecessary condition to use this criterion. Note that it fits much slower than\\nthe MSE criterion.\\nMean Absolute Error:\\n[ \\\\begin{align}\\\\begin{aligned}median(y)_m = \\\\underset{y \\\\in Q_m}{\\\\mathrm{median}}(y)\\\\H(Q_m) = \\\\frac{1}{n_m} \\\\sum{y \\\\in Q_m} |y - median(y)m|\\\\end{aligned}\\\\end{align} ]\\nNote that it fits much slower than the MSE criterion.\\n- 1.10.8. Missing Values Support¶\\nDecisionTreeClassifier and DecisionTreeRegressor\\nhave built-in support for missing values when splitter=\\'best\\' and criterion is\\n\\'gini\\', \\'entropy’, or \\'log_loss\\', for classification or\\n\\'squared_error\\', \\'friedman_mse\\', or \\'poisson\\' for regression.\\nFor each potential threshold on the non-missing data, the splitter will evaluate\\nthe split with all the missing values going to the left node or the right node.\\nDecisions are made as follows:\\nBy default when predicting, the samples with missing values are classified\\nwith the class used in the split found during training:\\nfrom sklearn.tree import DecisionTreeClassifier\\nimport numpy as np\\nX = np.array([0, 1, 6, np.nan]).reshape(-1, 1)\\ny = [0, 0, 1, 1]\\ntree = DecisionTreeClassifier(random_state=0).fit(X, y)\\ntree.predict(X)\\narray([0, 0, 1, 1])\\nIf the criterion evaluation is the same for both nodes,\\nthen the tie for missing value at predict time is broken by going to the\\nright node. The splitter also checks the split where all the missing\\nvalues go to one child and non-missing values go to the other:\\nfrom sklearn.tree import DecisionTreeClassifier\\nimport numpy as np\\nX = np.array([np.nan, -1, np.nan, 1]).reshape(-1, 1)\\ny = [0, 0, 1, 1]\\ntree = DecisionTreeClassifier(random_state=0).fit(X, y)\\nX_test = np.array([np.nan]).reshape(-1, 1)\\ntree.predict(X_test)\\narray([1])\\nIf no missing values are seen during training for a given feature, then during\\nprediction missing values are mapped to the child with the most samples:\\nfrom sklearn.tree import DecisionTreeClassifier\\nimport numpy as np\\nX = np.array([0, 1, 2, 3]).reshape(-1, 1)\\ny = [0, 1, 1, 1]\\ntree = DecisionTreeClassifier(random_state=0).fit(X, y)\\nX_test = np.array([np.nan]).reshape(-1, 1)\\ntree.predict(X_test)\\narray([1])\\n- 1.10.9. Minimal Cost-Complexity Pruning¶\\nMinimal cost-complexity pruning is an algorithm used to prune a tree to avoid\\nover-fitting, described in Chapter 3 of [BRE]. This algorithm is parameterized\\nby (\\\\alpha\\\\ge0) known as the complexity parameter. The complexity\\nparameter is used to define the cost-complexity measure, (R\\\\alpha(T)) of\\na given tree (T):\\n[R_\\\\alpha(T) = R(T) + \\\\alpha|\\\\widetilde{T}|]\\nwhere (|\\\\widetilde{T}|) is the number of terminal nodes in (T) and (R(T))\\nis traditionally defined as the total misclassification rate of the terminal\\nnodes. Alternatively, scikit-learn uses the total sample weighted impurity of\\nthe terminal nodes for (R(T)). As shown above, the impurity of a node\\ndepends on the criterion. Minimal cost-complexity pruning finds the subtree of\\n(T) that minimizes (R_\\\\alpha(T)).\\nThe cost complexity measure of a single node is\\n(R_\\\\alpha(t)=R(t)+\\\\alpha). The branch, (T_t), is defined to be a\\ntree where node (t) is its root. In general, the impurity of a node\\nis greater than the sum of impurities of its terminal nodes,\\n(R(T_t)<R(t)). However, the cost complexity measure of a node,\\n(t), and its branch, (T_t), can be equal depending on\\n(\\\\alpha). We define the effective (\\\\alpha) of a node to be the\\nvalue where they are equal, (R_\\\\alpha(T_t)=R_\\\\alpha(t)) or\\n(\\\\alpha_{eff}(t)=\\\\frac{R(t)-R(T_t)}{|T|-1}). A non-terminal node\\nwith the smallest value of (\\\\alpha_{eff}) is the weakest link and will\\nbe pruned. This process stops when the pruned tree’s minimal\\n(\\\\alpha_{eff}) is greater than the ccp_alpha parameter.\\nExamples:\\nPost pruning decision trees with cost complexity pruning\\nReferences\\nClick for more details\\n¶\\n[BRE]\\nL. Breiman, J. Friedman, R. Olshen, and C. Stone. Classification\\nand Regression Trees. Wadsworth, Belmont, CA, 1984.\\nhttps://en.wikipedia.org/wiki/Decision_tree_learning\\nhttps://en.wikipedia.org/wiki/Predictive_analytics\\nJ.R. Quinlan. C4. 5: programs for machine learning. Morgan\\nKaufmann, 1993.\\nT. Hastie, R. Tibshirani and J. Friedman. Elements of Statistical\\nLearning, Springer, 2009.\\n© 2007 - 2024, scikit-learn developers (BSD License).\\nShow this page source\\n\\n1.11. Ensembles: Gradient boosting, random forests, bagging, voting, stacking — scikit-learn 1.4.2 documentation\\n\\nInstall\\nUser Guide\\nAPI\\nExamples\\nCommunity\\nGetting Started\\nTutorial\\nWhat\\'s new\\nGlossary\\nDevelopment\\nFAQ\\nSupport\\nRelated packages\\nRoadmap\\nGovernance\\nAbout us\\nGitHub\\nOther Versions and Download\\nMore\\nGetting Started\\nTutorial\\nWhat\\'s new\\nGlossary\\nDevelopment\\nFAQ\\nSupport\\nRelated packages\\nRoadmap\\nGovernance\\nAbout us\\nGitHub\\nOther Versions and Download\\nToggle Menu\\nPrevUp\\nNext\\nscikit-learn 1.4.2\\nOther versions\\nPlease cite us if you use the software.\\n\\n1.11. Ensembles: Gradient boosting, random forests, bagging, voting, stacking\\n\\n1.11.1. Gradient-boosted trees\\n1.11.1.1. Histogram-Based Gradient Boosting\\n1.11.1.1.1. Usage\\n1.11.1.1.2. Missing values support\\n1.11.1.1.3. Sample weight support\\n1.11.1.1.4. Categorical Features Support\\n1.11.1.1.5. Monotonic Constraints\\n1.11.1.1.6. Interaction constraints\\n1.11.1.1.7. Low-level parallelism\\n1.11.1.1.8. Why it’s faster\\n1.11.1.2. GradientBoostingClassifier and GradientBoostingRegressor\\n1.11.1.2.1. Classification\\n1.11.1.2.2. Regression\\n1.11.1.2.3. Fitting additional weak-learners\\n1.11.1.2.4. Controlling the tree size\\n1.11.1.2.5. Mathematical formulation\\n1.11.1.2.5.1. Regression\\n1.11.1.2.5.2. Classification\\n1.11.1.2.6. Loss Functions\\n1.11.1.2.7. Shrinkage via learning rate\\n1.11.1.2.8. Subsampling\\n1.11.1.2.9. Interpretation with feature importance\\n\\n1.11.2. Random forests and other randomized tree ensembles\\n1.11.2.1. Random Forests\\n1.11.2.2. Extremely Randomized Trees\\n1.11.2.3. Parameters\\n1.11.2.4. Parallelization\\n1.11.2.5. Feature importance evaluation\\n1.11.2.6. Totally Random Trees Embedding\\n\\n1.11.3. Bagging meta-estimator\\n\\n1.11.4. Voting Classifier\\n1.11.4.1. Majority Class Labels (Majority/Hard Voting)\\n1.11.4.2. Usage\\n1.11.4.3. Weighted Average Probabilities (Soft Voting)\\n1.11.4.4. Using the VotingClassifier with GridSearchCV\\n1.11.4.5. Usage\\n\\n1.11.5. Voting Regressor\\n1.11.5.1. Usage\\n\\n1.11.6. Stacked generalization\\n\\n1.11.7. AdaBoost\\n1.11.7.1. Usage\\n\\n1.11. Ensembles: Gradient boosting, random forests, bagging, voting, stacking¶\\n\\nEnsemble methods combine the predictions of several\\nbase estimators built with a given learning algorithm in order to improve\\ngeneralizability / robustness over a single estimator.\\nTwo very famous examples of ensemble methods are gradient-boosted trees and random forests.\\nMore generally, ensemble models can be applied to any base learner beyond\\ntrees, in averaging methods such as Bagging methods,\\nmodel stacking, or Voting, or in\\nboosting, as AdaBoost.\\nGradient-boosted trees\\nRandom forests and other randomized tree ensembles\\nBagging meta-estimator\\nVoting Classifier\\nVoting Regressor\\nStacked generalization\\nAdaBoost\\n- 1.11.1. Gradient-boosted trees¶\\nGradient Tree Boosting\\nor Gradient Boosted Decision Trees (GBDT) is a generalization\\nof boosting to arbitrary differentiable loss functions, see the seminal work of\\n[Friedman2001]. GBDT is an excellent model for both regression and\\nclassification, in particular for tabular data.\\nGradientBoostingClassifier vs HistGradientBoostingClassifier\\nScikit-learn provides two implementations of gradient-boosted trees:\\nHistGradientBoostingClassifier vs\\nGradientBoostingClassifier for classification, and the\\ncorresponding classes for regression. The former can be orders of\\nmagnitude faster than the latter when the number of samples is\\nlarger than tens of thousands of samples.\\nMissing values and categorical data are natively supported by the\\nHist… version, removing the need for additional preprocessing such as\\nimputation.\\nGradientBoostingClassifier and\\nGradientBoostingRegressor, might be preferred for small sample\\nsizes since binning may lead to split points that are too approximate\\nin this setting.\\n1.11.1.1. Histogram-Based Gradient Boosting¶\\nScikit-learn 0.21 introduced two new implementations of\\ngradient boosted trees, namely HistGradientBoostingClassifier\\nand HistGradientBoostingRegressor, inspired by\\nLightGBM (See [LightGBM]).\\nThese histogram-based estimators can be orders of magnitude faster\\nthan GradientBoostingClassifier and\\nGradientBoostingRegressor when the number of samples is larger\\nthan tens of thousands of samples.\\nThey also have built-in support for missing values, which avoids the need\\nfor an imputer.\\nThese fast estimators first bin the input samples X into\\ninteger-valued bins (typically 256 bins) which tremendously reduces the\\nnumber of splitting points to consider, and allows the algorithm to\\nleverage integer-based data structures (histograms) instead of relying on\\nsorted continuous values when building the trees. The API of these\\nestimators is slightly different, and some of the features from\\nGradientBoostingClassifier and GradientBoostingRegressor\\nare not yet supported, for instance some loss functions.\\nExamples:\\nPartial Dependence and Individual Conditional Expectation Plots\\n1.11.1.1.1. Usage¶\\nMost of the parameters are unchanged from\\nGradientBoostingClassifier and GradientBoostingRegressor.\\nOne exception is the max_iter parameter that replaces n_estimators, and\\ncontrols the number of iterations of the boosting process:\\n\\nfrom sklearn.ensemble import HistGradientBoostingClassifier\\nfrom sklearn.datasets import make_hastie_10_2\\nX, y = make_hastie_10_2(random_state=0)\\nX_train, X_test = X[:2000], X[2000:]\\ny_train, y_test = y[:2000], y[2000:]\\nclf = HistGradientBoostingClassifier(max_iter=100).fit(X_train, y_train)\\nclf.score(X_test, y_test)\\n0.8965\\nAvailable losses for regression are ‘squared_error’,\\n‘absolute_error’, which is less sensitive to outliers, and\\n‘poisson’, which is well suited to model counts and frequencies. For\\nclassification, ‘log_loss’ is the only option. For binary classification it uses the\\nbinary log loss, also known as binomial deviance or binary cross-entropy. For\\nn_classes >= 3, it uses the multi-class log loss function, with multinomial deviance\\nand categorical cross-entropy as alternative names. The appropriate loss version is\\nselected based on y passed to fit.\\nThe size of the trees can be controlled through the max_leaf_nodes,\\nmax_depth, and min_samples_leaf parameters.\\nThe number of bins used to bin the data is controlled with the max_bins\\nparameter. Using less bins acts as a form of regularization. It is\\ngenerally recommended to use as many bins as possible (256), which is the default.\\nThe l2_regularization parameter is a regularizer on the loss function and\\ncorresponds to (\\\\lambda) in equation (2) of [XGBoost].\\nNote that early-stopping is enabled by default if the number of samples is\\nlarger than 10,000. The early-stopping behaviour is controlled via the\\nearly_stopping, scoring, validation_fraction,\\nn_iter_no_change, and tol parameters. It is possible to early-stop\\nusing an arbitrary scorer, or just the training or validation loss.\\nNote that for technical reasons, using a callable as a scorer is significantly slower\\nthan using the loss. By default, early-stopping is performed if there are at least\\n10,000 samples in the training set, using the validation loss.\\n1.11.1.1.2. Missing values support¶\\nHistGradientBoostingClassifier and\\nHistGradientBoostingRegressor have built-in support for missing\\nvalues (NaNs).\\nDuring training, the tree grower learns at each split point whether samples\\nwith missing values should go to the left or right child, based on the\\npotential gain. When predicting, samples with missing values are assigned to\\nthe left or right child consequently:\\nfrom sklearn.ensemble import HistGradientBoostingClassifier\\nimport numpy as np\\nX = np.array([0, 1, 2, np.nan]).reshape(-1, 1)\\ny = [0, 0, 1, 1]\\ngbdt = HistGradientBoostingClassifier(min_samples_leaf=1).fit(X, y)\\ngbdt.predict(X)\\narray([0, 0, 1, 1])\\nWhen the missingness pattern is predictive, the splits can be performed on\\nwhether the feature value is missing or not:\\nX = np.array([0, np.nan, 1, 2, np.nan]).reshape(-1, 1)\\ny = [0, 1, 0, 0, 1]\\ngbdt = HistGradientBoostingClassifier(min_samples_leaf=1,\\n...                                       max_depth=2,\\n...                                       learning_rate=1,\\n...                                       max_iter=1).fit(X, y)\\ngbdt.predict(X)\\narray([0, 1, 0, 0, 1])\\nIf no missing values were encountered for a given feature during training,\\nthen samples with missing values are mapped to whichever child has the most\\nsamples.\\n1.11.1.1.3. Sample weight support¶\\nHistGradientBoostingClassifier and\\nHistGradientBoostingRegressor support sample weights during\\nfit.\\nThe following toy example demonstrates that samples with a sample weight of zero are ignored:\\nX = [[1, 0],\\n...      [1, 0],\\n...      [1, 0],\\n...      [0, 1]]\\ny = [0, 0, 1, 0]\\n\\nignore the first 2 training samples by setting their weight to 0\\n\\nsample_weight = [0, 0, 1, 1]\\ngb = HistGradientBoostingClassifier(min_samples_leaf=1)\\ngb.fit(X, y, sample_weight=sample_weight)\\nHistGradientBoostingClassifier(...)\\ngb.predict([[1, 0]])\\narray([1])\\ngb.predict_proba([[1, 0]])[0, 1]\\n0.99...\\nAs you can see, the [1, 0] is comfortably classified as 1 since the first\\ntwo samples are ignored due to their sample weights.\\nImplementation detail: taking sample weights into account amounts to\\nmultiplying the gradients (and the hessians) by the sample weights. Note that\\nthe binning stage (specifically the quantiles computation) does not take the\\nweights into account.\\n1.11.1.1.4. Categorical Features Support¶\\nHistGradientBoostingClassifier and\\nHistGradientBoostingRegressor have native support for categorical\\nfeatures: they can consider splits on non-ordered, categorical data.\\nFor datasets with categorical features, using the native categorical support\\nis often better than relying on one-hot encoding\\n(OneHotEncoder), because one-hot encoding\\nrequires more tree depth to achieve equivalent splits. It is also usually\\nbetter to rely on the native categorical support rather than to treat\\ncategorical features as continuous (ordinal), which happens for ordinal-encoded\\ncategorical data, since categories are nominal quantities where order does not\\nmatter.\\nTo enable categorical support, a boolean mask can be passed to the\\ncategorical_features parameter, indicating which feature is categorical. In\\nthe following, the first feature will be treated as categorical and the\\nsecond feature as numerical:\\ngbdt = HistGradientBoostingClassifier(categorical_features=[True, False])\\nEquivalently, one can pass a list of integers indicating the indices of the\\ncategorical features:\\ngbdt = HistGradientBoostingClassifier(categorical_features=[0])\\nWhen the input is a DataFrame, it is also possible to pass a list of column\\nnames:\\ngbdt = HistGradientBoostingClassifier(categorical_features=[\"site\", \"manufacturer\"])\\nFinally, when the input is a DataFrame we can use\\ncategorical_features=\"from_dtype\" in which case all columns with a categorical\\ndtype will be treated as categorical features.\\nThe cardinality of each categorical feature must be less than the max_bins\\nparameter. For an example using histogram-based gradient boosting on categorical\\nfeatures, see\\nCategorical Feature Support in Gradient Boosting.\\nIf there are missing values during training, the missing values will be\\ntreated as a proper category. If there are no missing values during training,\\nthen at prediction time, missing values are mapped to the child node that has\\nthe most samples (just like for continuous features). When predicting,\\ncategories that were not seen during fit time will be treated as missing\\nvalues.\\nSplit finding with categorical features: The canonical way of considering\\ncategorical splits in a tree is to consider\\nall of the (2^{K - 1} - 1) partitions, where (K) is the number of\\ncategories. This can quickly become prohibitive when (K) is large.\\nFortunately, since gradient boosting trees are always regression trees (even\\nfor classification problems), there exist a faster strategy that can yield\\nequivalent splits. First, the categories of a feature are sorted according to\\nthe variance of the target, for each category k. Once the categories are\\nsorted, one can consider continuous partitions, i.e. treat the categories\\nas if they were ordered continuous values (see Fisher [Fisher1958] for a\\nformal proof). As a result, only (K - 1) splits need to be considered\\ninstead of (2^{K - 1} - 1). The initial sorting is a\\n(\\\\mathcal{O}(K \\\\log(K))) operation, leading to a total complexity of\\n(\\\\mathcal{O}(K \\\\log(K) + K)), instead of (\\\\mathcal{O}(2^K)).\\nExamples:\\nCategorical Feature Support in Gradient Boosting\\n1.11.1.1.5. Monotonic Constraints¶\\nDepending on the problem at hand, you may have prior knowledge indicating\\nthat a given feature should in general have a positive (or negative) effect\\non the target value. For example, all else being equal, a higher credit\\nscore should increase the probability of getting approved for a loan.\\nMonotonic constraints allow you to incorporate such prior knowledge into the\\nmodel.\\nFor a predictor (F) with two features:\\na monotonic increase constraint is a constraint of the form:\\n[x_1 \\\\leq x_1\\' \\\\implies F(x_1, x_2) \\\\leq F(x_1\\', x_2)]\\na monotonic decrease constraint is a constraint of the form:\\n[x_1 \\\\leq x_1\\' \\\\implies F(x_1, x_2) \\\\geq F(x_1\\', x_2)]\\nYou can specify a monotonic constraint on each feature using the\\nmonotonic_cst parameter. For each feature, a value of 0 indicates no\\nconstraint, while 1 and -1 indicate a monotonic increase and\\nmonotonic decrease constraint, respectively:\\nfrom sklearn.ensemble import HistGradientBoostingRegressor\\n... # monotonic increase, monotonic decrease, and no constraint on the 3 features\\ngbdt = HistGradientBoostingRegressor(monotonic_cst=[1, -1, 0])\\nIn a binary classification context, imposing a monotonic increase (decrease) constraint means that higher values of the feature are supposed\\nto have a positive (negative) effect on the probability of samples\\nto belong to the positive class.\\nNevertheless, monotonic constraints only marginally constrain feature effects on the output.\\nFor instance, monotonic increase and decrease constraints cannot be used to enforce the\\nfollowing modelling constraint:\\n[x_1 \\\\leq x_1\\' \\\\implies F(x_1, x_2) \\\\leq F(x_1\\', x_2\\')]\\nAlso, monotonic constraints are not supported for multiclass classification.\\nNote\\nSince categories are unordered quantities, it is not possible to enforce\\nmonotonic constraints on categorical features.\\nExamples:\\nMonotonic Constraints\\n1.11.1.1.6. Interaction constraints¶\\nA priori, the histogram gradient boosted trees are allowed to use any feature\\nto split a node into child nodes. This creates so called interactions between\\nfeatures, i.e. usage of different features as split along a branch. Sometimes,\\none wants to restrict the possible interactions, see [Mayer2022]. This can be\\ndone by the parameter interaction_cst, where one can specify the indices\\nof features that are allowed to interact.\\nFor instance, with 3 features in total, interaction_cst=[{0}, {1}, {2}]\\nforbids all interactions.\\nThe constraints [{0, 1}, {1, 2}] specifies two groups of possibly\\ninteracting features. Features 0 and 1 may interact with each other, as well\\nas features 1 and 2. But note that features 0 and 2 are forbidden to interact.\\nThe following depicts a tree and the possible splits of the tree:\\n1      <- Both constraint groups could be applied from now on\\n/ \\\\\\n1   2    <- Left split still fulfills both constraint groups.\\n/ \\\\ / \\\\      Right split at feature 2 has only group {1, 2} from now on.\\nLightGBM uses the same logic for overlapping groups.\\nNote that features not listed in interaction_cst are automatically\\nassigned an interaction group for themselves. With again 3 features, this\\nmeans that [{0}] is equivalent to [{0}, {1, 2}].\\nExamples:\\nPartial Dependence and Individual Conditional Expectation Plots\\nReferences\\n[Mayer2022]\\nM. Mayer, S.C. Bourassa, M. Hoesli, and D.F. Scognamiglio.\\n\\n2022. Machine Learning Applications to Land and Structure Valuation.\\n\\nJournal of Risk and Financial Management 15, no. 5: 193\\n1.11.1.1.7. Low-level parallelism¶\\nHistGradientBoostingClassifier and\\nHistGradientBoostingRegressor use OpenMP\\nfor parallelization through Cython. For more details on how to control the\\nnumber of threads, please refer to our Parallelism notes.\\nThe following parts are parallelized:\\nmapping samples from real values to integer-valued bins (finding the bin\\nthresholds is however sequential)\\nbuilding histograms is parallelized over features\\nfinding the best split point at a node is parallelized over features\\nduring fit, mapping samples into the left and right children is\\nparallelized over samples\\ngradient and hessians computations are parallelized over samples\\npredicting is parallelized over samples\\n1.11.1.1.8. Why it’s faster¶\\nThe bottleneck of a gradient boosting procedure is building the decision\\ntrees. Building a traditional decision tree (as in the other GBDTs\\nGradientBoostingClassifier and GradientBoostingRegressor)\\nrequires sorting the samples at each node (for\\neach feature). Sorting is needed so that the potential gain of a split point\\ncan be computed efficiently. Splitting a single node has thus a complexity\\nof (\\\\mathcal{O}(n_\\\\text{features} \\\\times n \\\\log(n))) where (n)\\nis the number of samples at the node.\\nHistGradientBoostingClassifier and\\nHistGradientBoostingRegressor, in contrast, do not require sorting the\\nfeature values and instead use a data-structure called a histogram, where the\\nsamples are implicitly ordered. Building a histogram has a\\n(\\\\mathcal{O}(n)) complexity, so the node splitting procedure has a\\n(\\\\mathcal{O}(n_\\\\text{features} \\\\times n)) complexity, much smaller\\nthan the previous one. In addition, instead of considering (n) split\\npoints, we consider only max_bins split points, which might be much\\nsmaller.\\nIn order to build histograms, the input data X needs to be binned into\\ninteger-valued bins. This binning procedure does require sorting the feature\\nvalues, but it only happens once at the very beginning of the boosting process\\n(not at each node, like in GradientBoostingClassifier and\\nGradientBoostingRegressor).\\nFinally, many parts of the implementation of\\nHistGradientBoostingClassifier and\\nHistGradientBoostingRegressor are parallelized.\\nReferences\\n[XGBoost]\\nTianqi Chen, Carlos Guestrin, “XGBoost: A Scalable Tree\\nBoosting System”\\n[LightGBM]\\nKe et. al. “LightGBM: A Highly Efficient Gradient\\nBoostingDecision Tree”\\n[Fisher1958]\\nFisher, W.D. (1958). “On Grouping for Maximum Homogeneity”\\nJournal of the American Statistical Association, 53, 789-798.\\n1.11.1.2. GradientBoostingClassifier and GradientBoostingRegressor¶\\nThe usage and the parameters of GradientBoostingClassifier and\\nGradientBoostingRegressor are described below. The 2 most important\\nparameters of these estimators are n_estimators and learning_rate.\\n1.11.1.2.1. Classification¶\\nGradientBoostingClassifier supports both binary and multi-class\\nclassification.\\nThe following example shows how to fit a gradient boosting classifier\\nwith 100 decision stumps as weak learners:\\n\\nfrom sklearn.datasets import make_hastie_10_2\\nfrom sklearn.ensemble import GradientBoostingClassifier\\nX, y = make_hastie_10_2(random_state=0)\\nX_train, X_test = X[:2000], X[2000:]\\ny_train, y_test = y[:2000], y[2000:]\\nclf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,\\n...     max_depth=1, random_state=0).fit(X_train, y_train)\\nclf.score(X_test, y_test)\\n0.913...\\nThe number of weak learners (i.e. regression trees) is controlled by the\\nparameter n_estimators; The size of each tree can be controlled either by setting the tree\\ndepth via max_depth or by setting the number of leaf nodes via\\nmax_leaf_nodes. The learning_rate is a hyper-parameter in the range\\n(0.0, 1.0] that controls overfitting via shrinkage .\\nNote\\nClassification with more than 2 classes requires the induction\\nof n_classes regression trees at each iteration,\\nthus, the total number of induced trees equals\\nn_classes * n_estimators. For datasets with a large number\\nof classes we strongly recommend to use\\nHistGradientBoostingClassifier as an alternative to\\nGradientBoostingClassifier .\\n1.11.1.2.2. Regression¶\\nGradientBoostingRegressor supports a number of\\ndifferent loss functions\\nfor regression which can be specified via the argument\\nloss; the default loss function for regression is squared error\\n(\\'squared_error\\').\\nimport numpy as np\\nfrom sklearn.metrics import mean_squared_error\\nfrom sklearn.datasets import make_friedman1\\nfrom sklearn.ensemble import GradientBoostingRegressor\\nX, y = make_friedman1(n_samples=1200, random_state=0, noise=1.0)\\nX_train, X_test = X[:200], X[200:]\\ny_train, y_test = y[:200], y[200:]\\nest = GradientBoostingRegressor(\\n...     n_estimators=100, learning_rate=0.1, max_depth=1, random_state=0,\\n...     loss=\\'squared_error\\'\\n... ).fit(X_train, y_train)\\nmean_squared_error(y_test, est.predict(X_test))\\n5.00...\\nThe figure below shows the results of applying GradientBoostingRegressor\\nwith least squares loss and 500 base learners to the diabetes dataset\\n(sklearn.datasets.load_diabetes).\\nThe plot shows the train and test error at each iteration.\\nThe train error at each iteration is stored in the\\ntrain_score_ attribute of the gradient boosting model.\\nThe test error at each iterations can be obtained\\nvia the staged_predict method which returns a\\ngenerator that yields the predictions at each stage. Plots like these can be used\\nto determine the optimal number of trees (i.e. n_estimators) by early stopping.\\nExamples:\\nGradient Boosting regression\\nGradient Boosting Out-of-Bag estimates\\n1.11.1.2.3. Fitting additional weak-learners¶\\nBoth GradientBoostingRegressor and GradientBoostingClassifier\\nsupport warm_start=True which allows you to add more estimators to an already\\nfitted model.\\n_ = est.set_params(n_estimators=200, warm_start=True)  # set warm_start and new nr of trees\\n_ = est.fit(X_train, y_train) # fit additional 100 trees to est\\nmean_squared_error(y_test, est.predict(X_test))\\n3.84...\\n1.11.1.2.4. Controlling the tree size¶\\nThe size of the regression tree base learners defines the level of variable\\ninteractions that can be captured by the gradient boosting model. In general,\\na tree of depth h can capture interactions of order h .\\nThere are two ways in which the size of the individual regression trees can\\nbe controlled.\\nIf you specify max_depth=h then complete binary trees\\nof depth h will be grown. Such trees will have (at most) 2h leaf nodes\\nand 2h - 1 split nodes.\\nAlternatively, you can control the tree size by specifying the number of\\nleaf nodes via the parameter max_leaf_nodes. In this case,\\ntrees will be grown using best-first search where nodes with the highest improvement\\nin impurity will be expanded first.\\nA tree with max_leaf_nodes=k has k - 1 split nodes and thus can\\nmodel interactions of up to order max_leaf_nodes - 1 .\\nWe found that max_leaf_nodes=k gives comparable results to max_depth=k-1\\nbut is significantly faster to train at the expense of a slightly higher\\ntraining error.\\nThe parameter max_leaf_nodes corresponds to the variable J in the\\nchapter on gradient boosting in [Friedman2001] and is related to the parameter\\ninteraction.depth in R’s gbm package where max_leaf_nodes == interaction.depth + 1 .\\n1.11.1.2.5. Mathematical formulation¶\\nWe first present GBRT for regression, and then detail the classification\\ncase.\\n1.11.1.2.5.1. Regression¶\\nGBRT regressors are additive models whose prediction (\\\\hat{y}i) for a\\ngiven input (x_i) is of the following form:\\n[\\\\hat{y}_i = F_M(x_i) = \\\\sum{m=1}^{M} h_m(x_i)]\\nwhere the (h_m) are estimators called weak learners in the context\\nof boosting. Gradient Tree Boosting uses decision tree regressors of fixed size as weak learners. The constant M corresponds to the\\nn_estimators parameter.\\nSimilar to other boosting algorithms, a GBRT is built in a greedy fashion:\\n[F_m(x) = F_{m-1}(x) + h_m(x),]\\nwhere the newly added tree (h_m) is fitted in order to minimize a sum\\nof losses (L_m), given the previous ensemble (F_{m-1}):\\n[h_m =  \\\\arg\\\\min_{h} L_m = \\\\arg\\\\min_{h} \\\\sum_{i=1}^{n}\\nl(y_i, F_{m-1}(x_i) + h(x_i)),]\\nwhere (l(y_i, F(x_i))) is defined by the loss parameter, detailed\\nin the next section.\\nBy default, the initial model (F_{0}) is chosen as the constant that\\nminimizes the loss: for a least-squares loss, this is the empirical mean of\\nthe target values. The initial model can also be specified via the init\\nargument.\\nUsing a first-order Taylor approximation, the value of (l) can be\\napproximated as follows:\\n[l(y_i, F_{m-1}(x_i) + h_m(x_i)) \\\\approx\\nl(y_i, F_{m-1}(x_i))\\n+ h_m(x_i)\\n\\\\left[ \\\\frac{\\\\partial l(y_i, F(x_i))}{\\\\partial F(x_i)} \\\\right]{F=F{m - 1}}.]\\nNote\\nBriefly, a first-order Taylor approximation says that\\n(l(z) \\\\approx l(a) + (z - a) \\\\frac{\\\\partial l}{\\\\partial z}(a)).\\nHere, (z) corresponds to (F_{m - 1}(x_i) + h_m(x_i)), and\\n(a) corresponds to (F_{m-1}(x_i))\\nThe quantity (\\\\left[ \\\\frac{\\\\partial l(y_i, F(x_i))}{\\\\partial F(x_i)}\\n\\\\right]{F=F{m - 1}}) is the derivative of the loss with respect to its\\nsecond parameter, evaluated at (F_{m-1}(x)). It is easy to compute for\\nany given (F_{m - 1}(x_i)) in a closed form since the loss is\\ndifferentiable. We will denote it by (g_i).\\nRemoving the constant terms, we have:\\n[h_m \\\\approx \\\\arg\\\\min_{h} \\\\sum_{i=1}^{n} h(x_i) g_i]\\nThis is minimized if (h(x_i)) is fitted to predict a value that is\\nproportional to the negative gradient (-g_i). Therefore, at each\\niteration, the estimator (h_m) is fitted to predict the negative\\ngradients of the samples. The gradients are updated at each iteration.\\nThis can be considered as some kind of gradient descent in a functional\\nspace.\\nNote\\nFor some losses, e.g. \\'absolute_error\\' where the gradients\\nare (\\\\pm 1), the values predicted by a fitted (h_m) are not\\naccurate enough: the tree can only output integer values. As a result, the\\nleaves values of the tree (h_m) are modified once the tree is\\nfitted, such that the leaves values minimize the loss (L_m). The\\nupdate is loss-dependent: for the absolute error loss, the value of\\na leaf is updated to the median of the samples in that leaf.\\n1.11.1.2.5.2. Classification¶\\nGradient boosting for classification is very similar to the regression case.\\nHowever, the sum of the trees (F_M(x_i) = \\\\sum_m h_m(x_i)) is not\\nhomogeneous to a prediction: it cannot be a class, since the trees predict\\ncontinuous values.\\nThe mapping from the value (F_M(x_i)) to a class or a probability is\\nloss-dependent. For the log-loss, the probability that\\n(x_i) belongs to the positive class is modeled as (p(y_i = 1 |\\nx_i) = \\\\sigma(F_M(x_i))) where (\\\\sigma) is the sigmoid or expit function.\\nFor multiclass classification, K trees (for K classes) are built at each of\\nthe (M) iterations. The probability that (x_i) belongs to class\\nk is modeled as a softmax of the (F_{M,k}(x_i)) values.\\nNote that even for a classification task, the (h_m) sub-estimator is\\nstill a regressor, not a classifier. This is because the sub-estimators are\\ntrained to predict (negative) gradients, which are always continuous\\nquantities.\\n1.11.1.2.6. Loss Functions¶\\nThe following loss functions are supported and can be specified using\\nthe parameter loss:\\nRegression\\nSquared error (\\'squared_error\\'): The natural choice for regression\\ndue to its superior computational properties. The initial model is\\ngiven by the mean of the target values.\\nAbsolute error (\\'absolute_error\\'): A robust loss function for\\nregression. The initial model is given by the median of the\\ntarget values.\\nHuber (\\'huber\\'): Another robust loss function that combines\\nleast squares and least absolute deviation; use alpha to\\ncontrol the sensitivity with regards to outliers (see [Friedman2001] for\\nmore details).\\nQuantile (\\'quantile\\'): A loss function for quantile regression.\\nUse 0 < alpha < 1 to specify the quantile. This loss function\\ncan be used to create prediction intervals\\n(see Prediction Intervals for Gradient Boosting Regression).\\nClassification\\nBinary log-loss (\\'log-loss\\'): The binomial\\nnegative log-likelihood loss function for binary classification. It provides\\nprobability estimates.  The initial model is given by the\\nlog odds-ratio.\\nMulti-class log-loss (\\'log-loss\\'): The multinomial\\nnegative log-likelihood loss function for multi-class classification with\\nn_classes mutually exclusive classes. It provides\\nprobability estimates.  The initial model is given by the\\nprior probability of each class. At each iteration n_classes\\nregression trees have to be constructed which makes GBRT rather\\ninefficient for data sets with a large number of classes.\\nExponential loss (\\'exponential\\'): The same loss function\\nas AdaBoostClassifier. Less robust to mislabeled\\nexamples than \\'log-loss\\'; can only be used for binary\\nclassification.\\n1.11.1.2.7. Shrinkage via learning rate¶\\n[Friedman2001] proposed a simple regularization strategy that scales\\nthe contribution of each weak learner by a constant factor (\\\\nu):\\n[F_m(x) = F_{m-1}(x) + \\\\nu h_m(x)]\\nThe parameter (\\\\nu) is also called the learning rate because\\nit scales the step length the gradient descent procedure; it can\\nbe set via the learning_rate parameter.\\nThe parameter learning_rate strongly interacts with the parameter\\nn_estimators, the number of weak learners to fit. Smaller values\\nof learning_rate require larger numbers of weak learners to maintain\\na constant training error. Empirical evidence suggests that small\\nvalues of learning_rate favor better test error. [HTF]\\nrecommend to set the learning rate to a small constant\\n(e.g. learning_rate <= 0.1) and choose n_estimators large enough\\nthat early stopping applies,\\nsee Early stopping in Gradient Boosting\\nfor a more detailed discussion of the interaction between\\nlearning_rate and n_estimators see [R2007].\\n1.11.1.2.8. Subsampling¶\\n[Friedman2002] proposed stochastic gradient boosting, which combines gradient\\nboosting with bootstrap averaging (bagging). At each iteration\\nthe base classifier is trained on a fraction subsample of\\nthe available training data. The subsample is drawn without replacement.\\nA typical value of subsample is 0.5.\\nThe figure below illustrates the effect of shrinkage and subsampling\\non the goodness-of-fit of the model. We can clearly see that shrinkage\\noutperforms no-shrinkage. Subsampling with shrinkage can further increase\\nthe accuracy of the model. Subsampling without shrinkage, on the other hand,\\ndoes poorly.\\nAnother strategy to reduce the variance is by subsampling the features\\nanalogous to the random splits in RandomForestClassifier.\\nThe number of subsampled features can be controlled via the max_features\\nparameter.\\nNote\\nUsing a small max_features value can significantly decrease the runtime.\\nStochastic gradient boosting allows to compute out-of-bag estimates of the\\ntest deviance by computing the improvement in deviance on the examples that are\\nnot included in the bootstrap sample (i.e. the out-of-bag examples).\\nThe improvements are stored in the attribute oob_improvement_.\\noob_improvement_[i] holds the improvement in terms of the loss on the OOB samples\\nif you add the i-th stage to the current predictions.\\nOut-of-bag estimates can be used for model selection, for example to determine\\nthe optimal number of iterations. OOB estimates are usually very pessimistic thus\\nwe recommend to use cross-validation instead and only use OOB if cross-validation\\nis too time consuming.\\nExamples:\\nGradient Boosting regularization\\nGradient Boosting Out-of-Bag estimates\\nOOB Errors for Random Forests\\n1.11.1.2.9. Interpretation with feature importance¶\\nIndividual decision trees can be interpreted easily by simply\\nvisualizing the tree structure. Gradient boosting models, however,\\ncomprise hundreds of regression trees thus they cannot be easily\\ninterpreted by visual inspection of the individual trees. Fortunately,\\na number of techniques have been proposed to summarize and interpret\\ngradient boosting models.\\nOften features do not contribute equally to predict the target\\nresponse; in many situations the majority of the features are in fact\\nirrelevant.\\nWhen interpreting a model, the first question usually is: what are\\nthose important features and how do they contributing in predicting\\nthe target response?\\nIndividual decision trees intrinsically perform feature selection by selecting\\nappropriate split points. This information can be used to measure the\\nimportance of each feature; the basic idea is: the more often a\\nfeature is used in the split points of a tree the more important that\\nfeature is. This notion of importance can be extended to decision tree\\nensembles by simply averaging the impurity-based feature importance of each tree (see\\nFeature importance evaluation for more details).\\nThe feature importance scores of a fit gradient boosting model can be\\naccessed via the feature_importances_ property:\\nfrom sklearn.datasets import make_hastie_10_2\\nfrom sklearn.ensemble import GradientBoostingClassifier\\nX, y = make_hastie_10_2(random_state=0)\\nclf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,\\n...     max_depth=1, random_state=0).fit(X, y)\\nclf.feature_importances_\\narray([0.10..., 0.10..., 0.11..., ...\\nNote that this computation of feature importance is based on entropy, and it\\nis distinct from sklearn.inspection.permutation_importance which is\\nbased on permutation of the features.\\nExamples:\\nGradient Boosting regression\\nReferences\\n[Friedman2001]\\n(1,2,3,4)\\nFriedman, J.H. (2001). Greedy function approximation: A gradient\\nboosting machine.\\nAnnals of Statistics, 29, 1189-1232.\\n[Friedman2002]\\nFriedman, J.H. (2002). Stochastic gradient boosting..\\nComputational Statistics & Data Analysis, 38, 367-378.\\n[R2007]\\nG. Ridgeway (2006). Generalized Boosted Models: A guide to the gbm\\npackage\\n- 1.11.2. Random forests and other randomized tree ensembles¶\\nThe sklearn.ensemble module includes two averaging algorithms based\\non randomized decision trees: the RandomForest algorithm\\nand the Extra-Trees method. Both algorithms are perturb-and-combine\\ntechniques [B1998] specifically designed for trees. This means a diverse\\nset of classifiers is created by introducing randomness in the classifier\\nconstruction.  The prediction of the ensemble is given as the averaged\\nprediction of the individual classifiers.\\nAs other classifiers, forest classifiers have to be fitted with two\\narrays: a sparse or dense array X of shape (n_samples, n_features)\\nholding the training samples, and an array Y of shape (n_samples,)\\nholding the target values (class labels) for the training samples:\\nfrom sklearn.ensemble import RandomForestClassifier\\nX = [[0, 0], [1, 1]]\\nY = [0, 1]\\nclf = RandomForestClassifier(n_estimators=10)\\nclf = clf.fit(X, Y)\\nLike decision trees, forests of trees also extend to\\nmulti-output problems  (if Y is an array\\nof shape (n_samples, n_outputs)).\\n1.11.2.1. Random Forests¶\\nIn random forests (see RandomForestClassifier and\\nRandomForestRegressor classes), each tree in the ensemble is built\\nfrom a sample drawn with replacement (i.e., a bootstrap sample) from the\\ntraining set.\\nFurthermore, when splitting each node during the construction of a tree, the\\nbest split is found through an exhaustive search of the features values of\\neither all input features or a random subset of size max_features.\\n(See the parameter tuning guidelines for more details.)\\nThe purpose of these two sources of randomness is to decrease the variance of\\nthe forest estimator. Indeed, individual decision trees typically exhibit high\\nvariance and tend to overfit. The injected randomness in forests yield decision\\ntrees with somewhat decoupled prediction errors. By taking an average of those\\npredictions, some errors can cancel out. Random forests achieve a reduced\\nvariance by combining diverse trees, sometimes at the cost of a slight increase\\nin bias. In practice the variance reduction is often significant hence yielding\\nan overall better model.\\nIn contrast to the original publication [B2001], the scikit-learn\\nimplementation combines classifiers by averaging their probabilistic\\nprediction, instead of letting each classifier vote for a single class.\\nA competitive alternative to random forests are\\nHistogram-Based Gradient Boosting (HGBT) models:\\nBuilding trees: Random forests typically rely on deep trees (that overfit\\nindividually) which uses much computational resources, as they require\\nseveral splittings and evaluations of candidate splits. Boosting models\\nbuild shallow trees (that underfit individually) which are faster to fit\\nand predict.\\nSequential boosting: In HGBT, the decision trees are built sequentially,\\nwhere each tree is trained to correct the errors made by the previous ones.\\nThis allows them to iteratively improve the model’s performance using\\nrelatively few trees. In contrast, random forests use a majority vote to\\npredict the outcome, which can require a larger number of trees to achieve\\nthe same level of accuracy.\\nEfficient binning: HGBT uses an efficient binning algorithm that can handle\\nlarge datasets with a high number of features. The binning algorithm can\\npre-process the data to speed up the subsequent tree construction (see\\nWhy it’s faster). In contrast, the scikit-learn\\nimplementation of random forests does not use binning and relies on exact\\nsplitting, which can be computationally expensive.\\nOverall, the computational cost of HGBT versus RF depends on the specific\\ncharacteristics of the dataset and the modeling task. It’s a good idea\\nto try both models and compare their performance and computational efficiency\\non your specific problem to determine which model is the best fit.\\nExamples:\\nComparing Random Forests and Histogram Gradient Boosting models\\n1.11.2.2. Extremely Randomized Trees¶\\nIn extremely randomized trees (see ExtraTreesClassifier\\nand ExtraTreesRegressor classes), randomness goes one step\\nfurther in the way splits are computed. As in random forests, a random\\nsubset of candidate features is used, but instead of looking for the\\nmost discriminative thresholds, thresholds are drawn at random for each\\ncandidate feature and the best of these randomly-generated thresholds is\\npicked as the splitting rule. This usually allows to reduce the variance\\nof the model a bit more, at the expense of a slightly greater increase\\nin bias:\\nfrom sklearn.model_selection import cross_val_score\\nfrom sklearn.datasets import make_blobs\\nfrom sklearn.ensemble import RandomForestClassifier\\nfrom sklearn.ensemble import ExtraTreesClassifier\\nfrom sklearn.tree import DecisionTreeClassifier\\nX, y = make_blobs(n_samples=10000, n_features=10, centers=100,\\n...     random_state=0)\\nclf = DecisionTreeClassifier(max_depth=None, min_samples_split=2,\\n...     random_state=0)\\nscores = cross_val_score(clf, X, y, cv=5)\\nscores.mean()\\n0.98...\\nclf = RandomForestClassifier(n_estimators=10, max_depth=None,\\n...     min_samples_split=2, random_state=0)\\nscores = cross_val_score(clf, X, y, cv=5)\\nscores.mean()\\n0.999...\\nclf = ExtraTreesClassifier(n_estimators=10, max_depth=None,\\n...     min_samples_split=2, random_state=0)\\nscores = cross_val_score(clf, X, y, cv=5)\\nscores.mean() > 0.999\\nTrue\\n1.11.2.3. Parameters¶\\nThe main parameters to adjust when using these methods is n_estimators and\\nmax_features. The former is the number of trees in the forest. The larger\\nthe better, but also the longer it will take to compute. In addition, note that\\nresults will stop getting significantly better beyond a critical number of\\ntrees. The latter is the size of the random subsets of features to consider\\nwhen splitting a node. The lower the greater the reduction of variance, but\\nalso the greater the increase in bias. Empirical good default values are\\nmax_features=1.0 or equivalently max_features=None (always considering\\nall features instead of a random subset) for regression problems, and\\nmax_features=\"sqrt\" (using a random subset of size sqrt(n_features))\\nfor classification tasks (where n_features is the number of features in\\nthe data). The default value of max_features=1.0 is equivalent to bagged\\ntrees and more randomness can be achieved by setting smaller values (e.g. 0.3\\nis a typical default in the literature). Good results are often achieved when\\nsetting max_depth=None in combination with min_samples_split=2 (i.e.,\\nwhen fully developing the trees). Bear in mind though that these values are\\nusually not optimal, and might result in models that consume a lot of RAM.\\nThe best parameter values should always be cross-validated. In addition, note\\nthat in random forests, bootstrap samples are used by default\\n(bootstrap=True) while the default strategy for extra-trees is to use the\\nwhole dataset (bootstrap=False). When using bootstrap sampling the\\ngeneralization error can be estimated on the left out or out-of-bag samples.\\nThis can be enabled by setting oob_score=True.\\nNote\\nThe size of the model with the default parameters is (O( M * N * log (N) )),\\nwhere (M) is the number of trees and (N) is the number of samples.\\nIn order to reduce the size of the model, you can change these parameters:\\nmin_samples_split, max_leaf_nodes, max_depth and min_samples_leaf.\\n1.11.2.4. Parallelization¶\\nFinally, this module also features the parallel construction of the trees\\nand the parallel computation of the predictions through the n_jobs\\nparameter. If n_jobs=k then computations are partitioned into\\nk jobs, and run on k cores of the machine. If n_jobs=-1\\nthen all cores available on the machine are used. Note that because of\\ninter-process communication overhead, the speedup might not be linear\\n(i.e., using k jobs will unfortunately not be k times as\\nfast). Significant speedup can still be achieved though when building\\na large number of trees, or when building a single tree requires a fair\\namount of time (e.g., on large datasets).\\nExamples:\\nPlot the decision surfaces of ensembles of trees on the iris dataset\\nPixel importances with a parallel forest of trees\\nFace completion with a multi-output estimators\\nReferences\\n[B2001]\\nBreiman, “Random Forests”, Machine Learning, 45(1), 5-32, 2001.\\n[B1998]\\nBreiman, “Arcing Classifiers”, Annals of Statistics 1998.\\nP. Geurts, D. Ernst., and L. Wehenkel, “Extremely randomized\\ntrees”, Machine Learning, 63(1), 3-42, 2006.\\n1.11.2.5. Feature importance evaluation¶\\nThe relative rank (i.e. depth) of a feature used as a decision node in a\\ntree can be used to assess the relative importance of that feature with\\nrespect to the predictability of the target variable. Features used at\\nthe top of the tree contribute to the final prediction decision of a\\nlarger fraction of the input samples. The expected fraction of the\\nsamples they contribute to can thus be used as an estimate of the\\nrelative importance of the features. In scikit-learn, the fraction of\\nsamples a feature contributes to is combined with the decrease in impurity\\nfrom splitting them to create a normalized estimate of the predictive power\\nof that feature.\\nBy averaging the estimates of predictive ability over several randomized\\ntrees one can reduce the variance of such an estimate and use it\\nfor feature selection. This is known as the mean decrease in impurity, or MDI.\\nRefer to [L2014] for more information on MDI and feature importance\\nevaluation with Random Forests.\\nWarning\\nThe impurity-based feature importances computed on tree-based models suffer\\nfrom two flaws that can lead to misleading conclusions. First they are\\ncomputed on statistics derived from the training dataset and therefore do\\nnot necessarily inform us on which features are most important to make good\\npredictions on held-out dataset. Secondly, they favor high cardinality\\nfeatures, that is features with many unique values.\\nPermutation feature importance is an alternative to impurity-based feature\\nimportance that does not suffer from these flaws. These two methods of\\nobtaining feature importance are explored in:\\nPermutation Importance vs Random Forest Feature Importance (MDI).\\nThe following example shows a color-coded representation of the relative\\nimportances of each individual pixel for a face recognition task using\\na ExtraTreesClassifier model.\\nIn practice those estimates are stored as an attribute named\\nfeature_importances_ on the fitted model. This is an array with shape\\n(n_features,) whose values are positive and sum to 1.0. The higher\\nthe value, the more important is the contribution of the matching feature\\nto the prediction function.\\nExamples:\\nPixel importances with a parallel forest of trees\\nFeature importances with a forest of trees\\nReferences\\n[L2014]\\nG. Louppe, “Understanding Random Forests: From Theory to\\nPractice”,\\nPhD Thesis, U. of Liege, 2014.\\n1.11.2.6. Totally Random Trees Embedding¶\\nRandomTreesEmbedding implements an unsupervised transformation of the\\ndata.  Using a forest of completely random trees, RandomTreesEmbedding\\nencodes the data by the indices of the leaves a data point ends up in.  This\\nindex is then encoded in a one-of-K manner, leading to a high dimensional,\\nsparse binary coding.\\nThis coding can be computed very efficiently and can then be used as a basis\\nfor other learning tasks.\\nThe size and sparsity of the code can be influenced by choosing the number of\\ntrees and the maximum depth per tree. For each tree in the ensemble, the coding\\ncontains one entry of one. The size of the coding is at most n_estimators * 2\\n** max_depth, the maximum number of leaves in the forest.\\nAs neighboring data points are more likely to lie within the same leaf of a\\ntree, the transformation performs an implicit, non-parametric density\\nestimation.\\nExamples:\\nHashing feature transformation using Totally Random Trees\\nManifold learning on handwritten digits: Locally Linear Embedding, Isomap… compares non-linear\\ndimensionality reduction techniques on handwritten digits.\\nFeature transformations with ensembles of trees compares\\nsupervised and unsupervised tree based feature transformations.\\nSee also\\nManifold learning techniques can also be useful to derive non-linear\\nrepresentations of feature space, also these approaches focus also on\\ndimensionality reduction.\\n- 1.11.3. Bagging meta-estimator¶\\nIn ensemble algorithms, bagging methods form a class of algorithms which build\\nseveral instances of a black-box estimator on random subsets of the original\\ntraining set and then aggregate their individual predictions to form a final\\nprediction. These methods are used as a way to reduce the variance of a base\\nestimator (e.g., a decision tree), by introducing randomization into its\\nconstruction procedure and then making an ensemble out of it. In many cases,\\nbagging methods constitute a very simple way to improve with respect to a\\nsingle model, without making it necessary to adapt the underlying base\\nalgorithm. As they provide a way to reduce overfitting, bagging methods work\\nbest with strong and complex models (e.g., fully developed decision trees), in\\ncontrast with boosting methods which usually work best with weak models (e.g.,\\nshallow decision trees).\\nBagging methods come in many flavours but mostly differ from each other by the\\nway they draw random subsets of the training set:\\nWhen random subsets of the dataset are drawn as random subsets of the\\nsamples, then this algorithm is known as Pasting [B1999].\\nWhen samples are drawn with replacement, then the method is known as\\nBagging [B1996].\\nWhen random subsets of the dataset are drawn as random subsets of\\nthe features, then the method is known as Random Subspaces [H1998].\\nFinally, when base estimators are built on subsets of both samples and\\nfeatures, then the method is known as Random Patches [LG2012].\\nIn scikit-learn, bagging methods are offered as a unified\\nBaggingClassifier meta-estimator  (resp. BaggingRegressor),\\ntaking as input a user-specified estimator along with parameters\\nspecifying the strategy to draw random subsets. In particular, max_samples\\nand max_features control the size of the subsets (in terms of samples and\\nfeatures), while bootstrap and bootstrap_features control whether\\nsamples and features are drawn with or without replacement. When using a subset\\nof the available samples the generalization accuracy can be estimated with the\\nout-of-bag samples by setting oob_score=True. As an example, the\\nsnippet below illustrates how to instantiate a bagging ensemble of\\nKNeighborsClassifier estimators, each built on random\\nsubsets of 50% of the samples and 50% of the features.\\nfrom sklearn.ensemble import BaggingClassifier\\nfrom sklearn.neighbors import KNeighborsClassifier\\nbagging = BaggingClassifier(KNeighborsClassifier(),\\n...                             max_samples=0.5, max_features=0.5)\\nExamples:\\nSingle estimator versus bagging: bias-variance decomposition\\nReferences\\n[B1999]\\nL. Breiman, “Pasting small votes for classification in large\\ndatabases and on-line”, Machine Learning, 36(1), 85-103, 1999.\\n[B1996]\\nL. Breiman, “Bagging predictors”, Machine Learning, 24(2),\\n123-140, 1996.\\n[H1998]\\nT. Ho, “The random subspace method for constructing decision\\nforests”, Pattern Analysis and Machine Intelligence, 20(8), 832-844,\\n1998.\\n[LG2012]\\nG. Louppe and P. Geurts, “Ensembles on Random Patches”,\\nMachine Learning and Knowledge Discovery in Databases, 346-361, 2012.\\n- 1.11.4. Voting Classifier¶\\nThe idea behind the VotingClassifier is to combine\\nconceptually different machine learning classifiers and use a majority vote\\nor the average predicted probabilities (soft vote) to predict the class labels.\\nSuch a classifier can be useful for a set of equally well performing models\\nin order to balance out their individual weaknesses.\\n1.11.4.1. Majority Class Labels (Majority/Hard Voting)¶\\nIn majority voting, the predicted class label for a particular sample is\\nthe class label that represents the majority (mode) of the class labels\\npredicted by each individual classifier.\\nE.g., if the prediction for a given sample is\\nclassifier 1 -> class 1\\nclassifier 2 -> class 1\\nclassifier 3 -> class 2\\nthe VotingClassifier (with voting=\\'hard\\') would classify the sample\\nas “class 1” based on the majority class label.\\nIn the cases of a tie, the VotingClassifier will select the class\\nbased on the ascending sort order. E.g., in the following scenario\\nclassifier 1 -> class 2\\nclassifier 2 -> class 1\\nthe class label 1 will be assigned to the sample.\\n1.11.4.2. Usage¶\\nThe following example shows how to fit the majority rule classifier:\\nfrom sklearn import datasets\\nfrom sklearn.model_selection import cross_val_score\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.naive_bayes import GaussianNB\\nfrom sklearn.ensemble import RandomForestClassifier\\nfrom sklearn.ensemble import VotingClassifier\\niris = datasets.load_iris()\\nX, y = iris.data[:, 1:3], iris.target\\nclf1 = LogisticRegression(random_state=1)\\nclf2 = RandomForestClassifier(n_estimators=50, random_state=1)\\nclf3 = GaussianNB()\\neclf = VotingClassifier(\\n...     estimators=[(\\'lr\\', clf1), (\\'rf\\', clf2), (\\'gnb\\', clf3)],\\n...     voting=\\'hard\\')\\nfor clf, label in zip([clf1, clf2, clf3, eclf], [\\'Logistic Regression\\', \\'Random Forest\\', \\'naive Bayes\\', \\'Ensemble\\']):\\n...     scores = cross_val_score(clf, X, y, scoring=\\'accuracy\\', cv=5)\\n...     print(\"Accuracy: %0.2f (+/- %0.2f) [%s]\" % (scores.mean(), scores.std(), label))\\nAccuracy: 0.95 (+/- 0.04) [Logistic Regression]\\nAccuracy: 0.94 (+/- 0.04) [Random Forest]\\nAccuracy: 0.91 (+/- 0.04) [naive Bayes]\\nAccuracy: 0.95 (+/- 0.04) [Ensemble]\\n1.11.4.3. Weighted Average Probabilities (Soft Voting)¶\\nIn contrast to majority voting (hard voting), soft voting\\nreturns the class label as argmax of the sum of predicted probabilities.\\nSpecific weights can be assigned to each classifier via the weights\\nparameter. When weights are provided, the predicted class probabilities\\nfor each classifier are collected, multiplied by the classifier weight,\\nand averaged. The final class label is then derived from the class label\\nwith the highest average probability.\\nTo illustrate this with a simple example, let’s assume we have 3\\nclassifiers and a 3-class classification problems where we assign\\nequal weights to all classifiers: w1=1, w2=1, w3=1.\\nThe weighted average probabilities for a sample would then be\\ncalculated as follows:\\nclassifier\\nclass 1\\nclass 2\\nclass 3\\nclassifier 1\\nw1 * 0.2\\nw1 * 0.5\\nw1 * 0.3\\nclassifier 2\\nw2 * 0.6\\nw2 * 0.3\\nw2 * 0.1\\nclassifier 3\\nw3 * 0.3\\nw3 * 0.4\\nw3 * 0.3\\nweighted average\\n0.37\\n0.4\\n0.23\\nHere, the predicted class label is 2, since it has the\\nhighest average probability.\\nThe following example illustrates how the decision regions may change\\nwhen a soft VotingClassifier is used based on a linear Support\\nVector Machine, a Decision Tree, and a K-nearest neighbor classifier:\\nfrom sklearn import datasets\\nfrom sklearn.tree import DecisionTreeClassifier\\nfrom sklearn.neighbors import KNeighborsClassifier\\nfrom sklearn.svm import SVC\\nfrom itertools import product\\nfrom sklearn.ensemble import VotingClassifier\\n\\nLoading some example data\\n\\niris = datasets.load_iris()\\nX = iris.data[:, [0, 2]]\\ny = iris.target\\n\\nTraining classifiers\\n\\nclf1 = DecisionTreeClassifier(max_depth=4)\\nclf2 = KNeighborsClassifier(n_neighbors=7)\\nclf3 = SVC(kernel=\\'rbf\\', probability=True)\\neclf = VotingClassifier(estimators=[(\\'dt\\', clf1), (\\'knn\\', clf2), (\\'svc\\', clf3)],\\n...                         voting=\\'soft\\', weights=[2, 1, 2])\\nclf1 = clf1.fit(X, y)\\nclf2 = clf2.fit(X, y)\\nclf3 = clf3.fit(X, y)\\neclf = eclf.fit(X, y)\\n1.11.4.4. Using the VotingClassifier with GridSearchCV¶\\nThe VotingClassifier can also be used together with\\nGridSearchCV in order to tune the\\nhyperparameters of the individual estimators:\\nfrom sklearn.model_selection import GridSearchCV\\nclf1 = LogisticRegression(random_state=1)\\nclf2 = RandomForestClassifier(random_state=1)\\nclf3 = GaussianNB()\\neclf = VotingClassifier(\\n...     estimators=[(\\'lr\\', clf1), (\\'rf\\', clf2), (\\'gnb\\', clf3)],\\n...     voting=\\'soft\\'\\n... )\\nparams = {\\'lr__C\\': [1.0, 100.0], \\'rf__n_estimators\\': [20, 200]}\\ngrid = GridSearchCV(estimator=eclf, param_grid=params, cv=5)\\ngrid = grid.fit(iris.data, iris.target)\\n1.11.4.5. Usage¶\\nIn order to predict the class labels based on the predicted\\nclass-probabilities (scikit-learn estimators in the VotingClassifier\\nmust support predict_proba method):\\neclf = VotingClassifier(\\n...     estimators=[(\\'lr\\', clf1), (\\'rf\\', clf2), (\\'gnb\\', clf3)],\\n...     voting=\\'soft\\'\\n... )\\nOptionally, weights can be provided for the individual classifiers:\\neclf = VotingClassifier(\\n...     estimators=[(\\'lr\\', clf1), (\\'rf\\', clf2), (\\'gnb\\', clf3)],\\n...     voting=\\'soft\\', weights=[2,5,1]\\n... )\\n- 1.11.5. Voting Regressor¶\\nThe idea behind the VotingRegressor is to combine conceptually\\ndifferent machine learning regressors and return the average predicted values.\\nSuch a regressor can be useful for a set of equally well performing models\\nin order to balance out their individual weaknesses.\\n1.11.5.1. Usage¶\\nThe following example shows how to fit the VotingRegressor:\\nfrom sklearn.datasets import load_diabetes\\nfrom sklearn.ensemble import GradientBoostingRegressor\\nfrom sklearn.ensemble import RandomForestRegressor\\nfrom sklearn.linear_model import LinearRegression\\nfrom sklearn.ensemble import VotingRegressor\\n\\nLoading some example data\\n\\nX, y = load_diabetes(return_X_y=True)\\n\\nTraining classifiers\\n\\nreg1 = GradientBoostingRegressor(random_state=1)\\nreg2 = RandomForestRegressor(random_state=1)\\nreg3 = LinearRegression()\\nereg = VotingRegressor(estimators=[(\\'gb\\', reg1), (\\'rf\\', reg2), (\\'lr\\', reg3)])\\nereg = ereg.fit(X, y)\\nExamples:\\nPlot individual and voting regression predictions\\n- 1.11.6. Stacked generalization¶\\nStacked generalization is a method for combining estimators to reduce their\\nbiases [W1992] [HTF]. More precisely, the predictions of each individual\\nestimator are stacked together and used as input to a final estimator to\\ncompute the prediction. This final estimator is trained through\\ncross-validation.\\nThe StackingClassifier and StackingRegressor provide such\\nstrategies which can be applied to classification and regression problems.\\nThe estimators parameter corresponds to the list of the estimators which\\nare stacked together in parallel on the input data. It should be given as a\\nlist of names and estimators:\\nfrom sklearn.linear_model import RidgeCV, LassoCV\\nfrom sklearn.neighbors import KNeighborsRegressor\\nestimators = [(\\'ridge\\', RidgeCV()),\\n...               (\\'lasso\\', LassoCV(random_state=42)),\\n...               (\\'knr\\', KNeighborsRegressor(n_neighbors=20,\\n...                                           metric=\\'euclidean\\'))]\\nThe final_estimator will use the predictions of the estimators as input. It\\nneeds to be a classifier or a regressor when using StackingClassifier\\nor StackingRegressor, respectively:\\nfrom sklearn.ensemble import GradientBoostingRegressor\\nfrom sklearn.ensemble import StackingRegressor\\nfinal_estimator = GradientBoostingRegressor(\\n...     n_estimators=25, subsample=0.5, min_samples_leaf=25, max_features=1,\\n...     random_state=42)\\nreg = StackingRegressor(\\n...     estimators=estimators,\\n...     final_estimator=final_estimator)\\nTo train the estimators and final_estimator, the fit method needs\\nto be called on the training data:\\nfrom sklearn.datasets import load_diabetes\\nX, y = load_diabetes(return_X_y=True)\\nfrom sklearn.model_selection import train_test_split\\nX_train, X_test, y_train, y_test = train_test_split(X, y,\\n...                                                     random_state=42)\\nreg.fit(X_train, y_train)\\nStackingRegressor(...)\\nDuring training, the estimators are fitted on the whole training data\\nX_train. They will be used when calling predict or predict_proba. To\\ngeneralize and avoid over-fitting, the final_estimator is trained on\\nout-samples using sklearn.model_selection.cross_val_predict internally.\\nFor StackingClassifier, note that the output of the estimators is\\ncontrolled by the parameter stack_method and it is called by each estimator.\\nThis parameter is either a string, being estimator method names, or \\'auto\\'\\nwhich will automatically identify an available method depending on the\\navailability, tested in the order of preference: predict_proba,\\ndecision_function and predict.\\nA StackingRegressor and StackingClassifier can be used as\\nany other regressor or classifier, exposing a predict, predict_proba, and\\ndecision_function methods, e.g.:\\ny_pred = reg.predict(X_test)\\nfrom sklearn.metrics import r2_score\\nprint(\\'R2 score: {:.2f}\\'.format(r2_score(y_test, y_pred)))\\nR2 score: 0.53\\nNote that it is also possible to get the output of the stacked\\nestimators using the transform method:\\nreg.transform(X_test[:5])\\narray([[142..., 138..., 146...],\\n[179..., 182..., 151...],\\n[139..., 132..., 158...],\\n[286..., 292..., 225...],\\n[126..., 124..., 164...]])\\nIn practice, a stacking predictor predicts as good as the best predictor of the\\nbase layer and even sometimes outperforms it by combining the different\\nstrengths of the these predictors. However, training a stacking predictor is\\ncomputationally expensive.\\nNote\\nFor StackingClassifier, when using stack_method_=\\'predict_proba\\',\\nthe first column is dropped when the problem is a binary classification\\nproblem. Indeed, both probability columns predicted by each estimator are\\nperfectly collinear.\\nNote\\nMultiple stacking layers can be achieved by assigning final_estimator to\\na StackingClassifier or StackingRegressor:\\nfinal_layer_rfr = RandomForestRegressor(\\n...     n_estimators=10, max_features=1, max_leaf_nodes=5,random_state=42)\\nfinal_layer_gbr = GradientBoostingRegressor(\\n...     n_estimators=10, max_features=1, max_leaf_nodes=5,random_state=42)\\nfinal_layer = StackingRegressor(\\n...     estimators=[(\\'rf\\', final_layer_rfr),\\n...                 (\\'gbrt\\', final_layer_gbr)],\\n...     final_estimator=RidgeCV()\\n...     )\\nmulti_layer_regressor = StackingRegressor(\\n...     estimators=[(\\'ridge\\', RidgeCV()),\\n...                 (\\'lasso\\', LassoCV(random_state=42)),\\n...                 (\\'knr\\', KNeighborsRegressor(n_neighbors=20,\\n...                                             metric=\\'euclidean\\'))],\\n...     final_estimator=final_layer\\n... )\\nmulti_layer_regressor.fit(X_train, y_train)\\nStackingRegressor(...)\\nprint(\\'R2 score: {:.2f}\\'\\n...       .format(multi_layer_regressor.score(X_test, y_test)))\\nR2 score: 0.53\\nReferences\\n[W1992]\\nWolpert, David H. “Stacked generalization.” Neural networks 5.2\\n(1992): 241-259.\\n- 1.11.7. AdaBoost¶\\nThe module sklearn.ensemble includes the popular boosting algorithm\\nAdaBoost, introduced in 1995 by Freund and Schapire [FS1995].\\nThe core principle of AdaBoost is to fit a sequence of weak learners (i.e.,\\nmodels that are only slightly better than random guessing, such as small\\ndecision trees) on repeatedly modified versions of the data. The predictions\\nfrom all of them are then combined through a weighted majority vote (or sum) to\\nproduce the final prediction. The data modifications at each so-called boosting\\niteration consists of applying weights (w_1), (w_2), …, (w_N)\\nto each of the training samples. Initially, those weights are all set to\\n(w_i = 1/N), so that the first step simply trains a weak learner on the\\noriginal data. For each successive iteration, the sample weights are\\nindividually modified and the learning algorithm is reapplied to the reweighted\\ndata. At a given step, those training examples that were incorrectly predicted\\nby the boosted model induced at the previous step have their weights increased,\\nwhereas the weights are decreased for those that were predicted correctly. As\\niterations proceed, examples that are difficult to predict receive\\never-increasing influence. Each subsequent weak learner is thereby forced to\\nconcentrate on the examples that are missed by the previous ones in the sequence\\n[HTF].\\nAdaBoost can be used both for classification and regression problems:\\nFor multi-class classification, AdaBoostClassifier implements\\nAdaBoost.SAMME [ZZRH2009].\\nFor regression, AdaBoostRegressor implements AdaBoost.R2 [D1997].\\n1.11.7.1. Usage¶\\nThe following example shows how to fit an AdaBoost classifier with 100 weak\\nlearners:\\nfrom sklearn.model_selection import cross_val_score\\nfrom sklearn.datasets import load_iris\\nfrom sklearn.ensemble import AdaBoostClassifier\\nX, y = load_iris(return_X_y=True)\\nclf = AdaBoostClassifier(n_estimators=100, algorithm=\"SAMME\",)\\nscores = cross_val_score(clf, X, y, cv=5)\\nscores.mean()\\n0.9...\\nThe number of weak learners is controlled by the parameter n_estimators. The\\nlearning_rate parameter controls the contribution of the weak learners in\\nthe final combination. By default, weak learners are decision stumps. Different\\nweak learners can be specified through the estimator parameter.\\nThe main parameters to tune to obtain good results are n_estimators and\\nthe complexity of the base estimators (e.g., its depth max_depth or\\nminimum required number of samples to consider a split min_samples_split).\\nExamples:\\nMulti-class AdaBoosted Decision Trees shows the performance\\nof AdaBoost on a multi-class problem.\\nTwo-class AdaBoost shows the decision boundary\\nand decision function values for a non-linearly separable two-class problem\\nusing AdaBoost-SAMME.\\nDecision Tree Regression with AdaBoost demonstrates regression\\nwith the AdaBoost.R2 algorithm.\\nReferences\\n[FS1995]\\nY. Freund, and R. Schapire, “A Decision-Theoretic Generalization of\\nOn-Line Learning and an Application to Boosting”, 1997.\\n[ZZRH2009]\\nJ. Zhu, H. Zou, S. Rosset, T. Hastie. “Multi-class AdaBoost”,\\n2009.\\n[D1997]\\nDrucker. “Improving Regressors using Boosting Techniques”, 1997.\\n[HTF]\\n(1,2,3)\\nT. Hastie, R. Tibshirani and J. Friedman, “Elements of\\nStatistical Learning Ed. 2”, Springer, 2009.\\n© 2007 - 2024, scikit-learn developers (BSD License).\\nShow this page source\\n\\n1.12. Multiclass and multioutput algorithms — scikit-learn 1.4.2 documentation\\n\\nInstall\\nUser Guide\\nAPI\\nExamples\\nCommunity\\nGetting Started\\nTutorial\\nWhat\\'s new\\nGlossary\\nDevelopment\\nFAQ\\nSupport\\nRelated packages\\nRoadmap\\nGovernance\\nAbout us\\nGitHub\\nOther Versions and Download\\nMore\\nGetting Started\\nTutorial\\nWhat\\'s new\\nGlossary\\nDevelopment\\nFAQ\\nSupport\\nRelated packages\\nRoadmap\\nGovernance\\nAbout us\\nGitHub\\nOther Versions and Download\\nToggle Menu\\nPrevUp\\nNext\\nscikit-learn 1.4.2\\nOther versions\\nPlease cite us if you use the software.\\n\\n1.12. Multiclass and multioutput algorithms\\n\\n1.12.1. Multiclass classification\\n1.12.1.1. Target format\\n1.12.1.2. OneVsRestClassifier\\n1.12.1.3. OneVsOneClassifier\\n1.12.1.4. OutputCodeClassifier\\n\\n1.12.2. Multilabel classification\\n1.12.2.1. Target format\\n1.12.2.2. MultiOutputClassifier\\n1.12.2.3. ClassifierChain\\n\\n1.12.3. Multiclass-multioutput classification\\n1.12.3.1. Target format\\n\\n1.12.4. Multioutput regression\\n1.12.4.1. Target format\\n1.12.4.2. MultiOutputRegressor\\n1.12.4.3. RegressorChain\\n\\n1.12. Multiclass and multioutput algorithms¶\\n\\nThis section of the user guide covers functionality related to multi-learning\\nproblems, including multiclass, multilabel, and\\nmultioutput classification and regression.\\nThe modules in this section implement meta-estimators, which require a\\nbase estimator to be provided in their constructor. Meta-estimators extend the\\nfunctionality of the base estimator to support multi-learning problems, which\\nis accomplished by transforming the multi-learning problem into a set of\\nsimpler problems, then fitting one estimator per problem.\\nThis section covers two modules: sklearn.multiclass and\\nsklearn.multioutput. The chart below demonstrates the problem types\\nthat each module is responsible for, and the corresponding meta-estimators\\nthat each module provides.\\nThe table below provides a quick reference on the differences between problem\\ntypes. More detailed explanations can be found in subsequent sections of this\\nguide.\\nNumber of targets\\nTarget cardinality\\nValid\\ntype_of_target\\nMulticlass\\nclassification\\n1\\n\\n2\\n‘multiclass’\\nMultilabel\\nclassification\\n1\\n2 (0 or 1)\\n‘multilabel-indicator’\\nMulticlass-multioutput\\nclassification\\n1\\n2\\n‘multiclass-multioutput’\\nMultioutput\\nregression\\n1\\nContinuous\\n‘continuous-multioutput’\\nBelow is a summary of scikit-learn estimators that have multi-learning support\\nbuilt-in, grouped by strategy. You don’t need the meta-estimators provided by\\nthis section if you’re using one of these estimators. However, meta-estimators\\ncan provide additional strategies beyond what is built-in:\\nInherently multiclass:\\nnaive_bayes.BernoulliNB\\ntree.DecisionTreeClassifier\\ntree.ExtraTreeClassifier\\nensemble.ExtraTreesClassifier\\nnaive_bayes.GaussianNB\\nneighbors.KNeighborsClassifier\\nsemi_supervised.LabelPropagation\\nsemi_supervised.LabelSpreading\\ndiscriminant_analysis.LinearDiscriminantAnalysis\\nsvm.LinearSVC (setting multi_class=”crammer_singer”)\\nlinear_model.LogisticRegression (setting multi_class=”multinomial”)\\nlinear_model.LogisticRegressionCV (setting multi_class=”multinomial”)\\nneural_network.MLPClassifier\\nneighbors.NearestCentroid\\ndiscriminant_analysis.QuadraticDiscriminantAnalysis\\nneighbors.RadiusNeighborsClassifier\\nensemble.RandomForestClassifier\\nlinear_model.RidgeClassifier\\nlinear_model.RidgeClassifierCV\\nMulticlass as One-Vs-One:\\nsvm.NuSVC\\nsvm.SVC.\\ngaussian_process.GaussianProcessClassifier (setting multi_class = “one_vs_one”)\\nMulticlass as One-Vs-The-Rest:\\nensemble.GradientBoostingClassifier\\ngaussian_process.GaussianProcessClassifier (setting multi_class = “one_vs_rest”)\\nsvm.LinearSVC (setting multi_class=”ovr”)\\nlinear_model.LogisticRegression (setting multi_class=”ovr”)\\nlinear_model.LogisticRegressionCV (setting multi_class=”ovr”)\\nlinear_model.SGDClassifier\\nlinear_model.Perceptron\\nlinear_model.PassiveAggressiveClassifier\\nSupport multilabel:\\ntree.DecisionTreeClassifier\\ntree.ExtraTreeClassifier\\nensemble.ExtraTreesClassifier\\nneighbors.KNeighborsClassifier\\nneural_network.MLPClassifier\\nneighbors.RadiusNeighborsClassifier\\nensemble.RandomForestClassifier\\nlinear_model.RidgeClassifier\\nlinear_model.RidgeClassifierCV\\nSupport multiclass-multioutput:\\ntree.DecisionTreeClassifier\\ntree.ExtraTreeClassifier\\nensemble.ExtraTreesClassifier\\nneighbors.KNeighborsClassifier\\nneighbors.RadiusNeighborsClassifier\\nensemble.RandomForestClassifier\\n- 1.12.1. Multiclass classification¶\\nWarning\\nAll classifiers in scikit-learn do multiclass classification\\nout-of-the-box. You don’t need to use the sklearn.multiclass module\\nunless you want to experiment with different multiclass strategies.\\nMulticlass classification is a classification task with more than two\\nclasses. Each sample can only be labeled as one class.\\nFor example, classification using features extracted from a set of images of\\nfruit, where each image may either be of an orange, an apple, or a pear.\\nEach image is one sample and is labeled as one of the 3 possible classes.\\nMulticlass classification makes the assumption that each sample is assigned\\nto one and only one label - one sample cannot, for example, be both a pear\\nand an apple.\\nWhile all scikit-learn classifiers are capable of multiclass classification,\\nthe meta-estimators offered by sklearn.multiclass\\npermit changing the way they handle more than two classes\\nbecause this may have an effect on classifier performance\\n(either in terms of generalization error or required computational resources).\\n1.12.1.1. Target format¶\\nValid multiclass representations for\\ntype_of_target (y) are:\\n1d or column vector containing more than two discrete values. An\\nexample of a vector y for 4 samples:\\n\\nimport numpy as np\\ny = np.array([\\'apple\\', \\'pear\\', \\'apple\\', \\'orange\\'])\\nprint(y)\\n[\\'apple\\' \\'pear\\' \\'apple\\' \\'orange\\']\\nDense or sparse binary matrix of shape (n_samples, n_classes)\\nwith a single sample per row, where each column represents one class. An\\nexample of both a dense and sparse binary matrix y for 4\\nsamples, where the columns, in order, are apple, orange, and pear:\\nimport numpy as np\\nfrom sklearn.preprocessing import LabelBinarizer\\ny = np.array([\\'apple\\', \\'pear\\', \\'apple\\', \\'orange\\'])\\ny_dense = LabelBinarizer().fit_transform(y)\\nprint(y_dense)\\n[[1 0 0]\\n[0 0 1]\\n[1 0 0]\\n[0 1 0]]\\nfrom scipy import sparse\\ny_sparse = sparse.csr_matrix(y_dense)\\nprint(y_sparse)\\n(0, 0)    1\\n(1, 2)    1\\n(2, 0)    1\\n(3, 1)    1\\nFor more information about LabelBinarizer,\\nrefer to Transforming the prediction target (y).\\n1.12.1.2. OneVsRestClassifier¶\\nThe one-vs-rest strategy, also known as one-vs-all, is implemented in\\nOneVsRestClassifier.  The strategy consists in\\nfitting one classifier per class. For each classifier, the class is fitted\\nagainst all the other classes. In addition to its computational efficiency\\n(only n_classes classifiers are needed), one advantage of this approach is\\nits interpretability. Since each class is represented by one and only one\\nclassifier, it is possible to gain knowledge about the class by inspecting its\\ncorresponding classifier. This is the most commonly used strategy and is a fair\\ndefault choice.\\nBelow is an example of multiclass learning using OvR:\\nfrom sklearn import datasets\\nfrom sklearn.multiclass import OneVsRestClassifier\\nfrom sklearn.svm import LinearSVC\\nX, y = datasets.load_iris(return_X_y=True)\\nOneVsRestClassifier(LinearSVC(dual=\"auto\", random_state=0)).fit(X, y).predict(X)\\narray([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\\n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\\n0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\\n1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1,\\n1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\\n2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 1, 2, 2, 2, 2,\\n2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])\\nOneVsRestClassifier also supports multilabel\\nclassification. To use this feature, feed the classifier an indicator matrix,\\nin which cell [i, j] indicates the presence of label j in sample i.\\nExamples:\\nMultilabel classification\\n1.12.1.3. OneVsOneClassifier¶\\nOneVsOneClassifier constructs one classifier per\\npair of classes. At prediction time, the class which received the most votes\\nis selected. In the event of a tie (among two classes with an equal number of\\nvotes), it selects the class with the highest aggregate classification\\nconfidence by summing over the pair-wise classification confidence levels\\ncomputed by the underlying binary classifiers.\\nSince it requires to fit n_classes * (n_classes - 1) / 2 classifiers,\\nthis method is usually slower than one-vs-the-rest, due to its\\nO(n_classes^2) complexity. However, this method may be advantageous for\\nalgorithms such as kernel algorithms which don’t scale well with\\nn_samples. This is because each individual learning problem only involves\\na small subset of the data whereas, with one-vs-the-rest, the complete\\ndataset is used n_classes times. The decision function is the result\\nof a monotonic transformation of the one-versus-one classification.\\nBelow is an example of multiclass learning using OvO:\\nfrom sklearn import datasets\\nfrom sklearn.multiclass import OneVsOneClassifier\\nfrom sklearn.svm import LinearSVC\\nX, y = datasets.load_iris(return_X_y=True)\\nOneVsOneClassifier(LinearSVC(dual=\"auto\", random_state=0)).fit(X, y).predict(X)\\narray([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\\n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\\n0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\\n1, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1,\\n1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\\n2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\\n2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])\\nReferences:\\n“Pattern Recognition and Machine Learning. Springer”,\\nChristopher M. Bishop, page 183, (First Edition)\\n1.12.1.4. OutputCodeClassifier¶\\nError-Correcting Output Code-based strategies are fairly different from\\none-vs-the-rest and one-vs-one. With these strategies, each class is\\nrepresented in a Euclidean space, where each dimension can only be 0 or 1.\\nAnother way to put it is that each class is represented by a binary code (an\\narray of 0 and 1). The matrix which keeps track of the location/code of each\\nclass is called the code book. The code size is the dimensionality of the\\naforementioned space. Intuitively, each class should be represented by a code\\nas unique as possible and a good code book should be designed to optimize\\nclassification accuracy. In this implementation, we simply use a\\nrandomly-generated code book as advocated in [3] although more elaborate\\nmethods may be added in the future.\\nAt fitting time, one binary classifier per bit in the code book is fitted.\\nAt prediction time, the classifiers are used to project new points in the\\nclass space and the class closest to the points is chosen.\\nIn OutputCodeClassifier, the code_size\\nattribute allows the user to control the number of classifiers which will be\\nused. It is a percentage of the total number of classes.\\nA number between 0 and 1 will require fewer classifiers than\\none-vs-the-rest. In theory, log2(n_classes) / n_classes is sufficient to\\nrepresent each class unambiguously. However, in practice, it may not lead to\\ngood accuracy since log2(n_classes) is much smaller than n_classes.\\nA number greater than 1 will require more classifiers than\\none-vs-the-rest. In this case, some classifiers will in theory correct for\\nthe mistakes made by other classifiers, hence the name “error-correcting”.\\nIn practice, however, this may not happen as classifier mistakes will\\ntypically be correlated. The error-correcting output codes have a similar\\neffect to bagging.\\nBelow is an example of multiclass learning using Output-Codes:\\nfrom sklearn import datasets\\nfrom sklearn.multiclass import OutputCodeClassifier\\nfrom sklearn.svm import LinearSVC\\nX, y = datasets.load_iris(return_X_y=True)\\nclf = OutputCodeClassifier(LinearSVC(dual=\"auto\", random_state=0),\\n...                            code_size=2, random_state=0)\\nclf.fit(X, y).predict(X)\\narray([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\\n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\\n0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1,\\n1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 2, 2, 2, 1, 1, 1, 1, 1, 1,\\n1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\\n2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 1, 1, 2, 2, 2,\\n2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])\\nReferences:\\n“Solving multiclass learning problems via error-correcting output codes”,\\nDietterich T., Bakiri G.,\\nJournal of Artificial Intelligence Research 2,\\n1995.\\n[3]\\n“The error coding method and PICTs”,\\nJames G., Hastie T.,\\nJournal of Computational and Graphical statistics 7,\\n1998.\\n“The Elements of Statistical Learning”,\\nHastie T., Tibshirani R., Friedman J., page 606 (second-edition)\\n2008.\\n- 1.12.2. Multilabel classification¶\\nMultilabel classification (closely related to multioutput\\nclassification) is a classification task labeling each sample with m\\nlabels from n_classes possible classes, where m can be 0 to\\nn_classes inclusive. This can be thought of as predicting properties of a\\nsample that are not mutually exclusive. Formally, a binary output is assigned\\nto each class, for every sample. Positive classes are indicated with 1 and\\nnegative classes with 0 or -1. It is thus comparable to running n_classes\\nbinary classification tasks, for example with\\nMultiOutputClassifier. This approach treats\\neach label independently whereas multilabel classifiers may treat the\\nmultiple classes simultaneously, accounting for correlated behavior among\\nthem.\\nFor example, prediction of the topics relevant to a text document or video.\\nThe document or video may be about one of ‘religion’, ‘politics’, ‘finance’\\nor ‘education’, several of the topic classes or all of the topic classes.\\n1.12.2.1. Target format¶\\nA valid representation of multilabel y is an either dense or sparse\\nbinary matrix of shape (n_samples, n_classes). Each column\\nrepresents a class. The 1’s in each row denote the positive classes a\\nsample has been labeled with. An example of a dense matrix y for 3\\nsamples:\\ny = np.array([[1, 0, 0, 1], [0, 0, 1, 1], [0, 0, 0, 0]])\\nprint(y)\\n[[1 0 0 1]\\n[0 0 1 1]\\n[0 0 0 0]]\\nDense binary matrices can also be created using\\nMultiLabelBinarizer. For more information,\\nrefer to Transforming the prediction target (y).\\nAn example of the same y in sparse matrix form:\\ny_sparse = sparse.csr_matrix(y)\\nprint(y_sparse)\\n(0, 0)      1\\n(0, 3)      1\\n(1, 2)      1\\n(1, 3)      1\\n1.12.2.2. MultiOutputClassifier¶\\nMultilabel classification support can be added to any classifier with\\nMultiOutputClassifier. This strategy consists of\\nfitting one classifier per target.  This allows multiple target variable\\nclassifications. The purpose of this class is to extend estimators\\nto be able to estimate a series of target functions (f1,f2,f3…,fn)\\nthat are trained on a single X predictor matrix to predict a series\\nof responses (y1,y2,y3…,yn).\\nYou can find a usage example for\\nMultiOutputClassifier\\nas part of the section on Multiclass-multioutput classification\\nsince it is a generalization of multilabel classification to\\nmulticlass outputs instead of binary outputs.\\n1.12.2.3. ClassifierChain¶\\nClassifier chains (see ClassifierChain) are a way\\nof combining a number of binary classifiers into a single multi-label model\\nthat is capable of exploiting correlations among targets.\\nFor a multi-label classification problem with N classes, N binary\\nclassifiers are assigned an integer between 0 and N-1. These integers\\ndefine the order of models in the chain. Each classifier is then fit on the\\navailable training data plus the true labels of the classes whose\\nmodels were assigned a lower number.\\nWhen predicting, the true labels will not be available. Instead the\\npredictions of each model are passed on to the subsequent models in the\\nchain to be used as features.\\nClearly the order of the chain is important. The first model in the chain\\nhas no information about the other labels while the last model in the chain\\nhas features indicating the presence of all of the other labels. In general\\none does not know the optimal ordering of the models in the chain so\\ntypically many randomly ordered chains are fit and their predictions are\\naveraged together.\\nReferences:\\nJesse Read, Bernhard Pfahringer, Geoff Holmes, Eibe Frank,“Classifier Chains for Multi-label Classification”, 2009.\\n- 1.12.3. Multiclass-multioutput classification¶\\nMulticlass-multioutput classification\\n(also known as multitask classification) is a\\nclassification task which labels each sample with a set of non-binary\\nproperties. Both the number of properties and the number of\\nclasses per property is greater than 2. A single estimator thus\\nhandles several joint classification tasks. This is both a generalization of\\nthe multilabel classification task, which only considers binary\\nattributes, as well as a generalization of the multiclass classification\\ntask, where only one property is considered.\\nFor example, classification of the properties “type of fruit” and “colour”\\nfor a set of images of fruit. The property “type of fruit” has the possible\\nclasses: “apple”, “pear” and “orange”. The property “colour” has the\\npossible classes: “green”, “red”, “yellow” and “orange”. Each sample is an\\nimage of a fruit, a label is output for both properties and each label is\\none of the possible classes of the corresponding property.\\nNote that all classifiers handling multiclass-multioutput (also known as\\nmultitask classification) tasks, support the multilabel classification task\\nas a special case. Multitask classification is similar to the multioutput\\nclassification task with different model formulations. For more information,\\nsee the relevant estimator documentation.\\nBelow is an example of multiclass-multioutput classification:\\nfrom sklearn.datasets import make_classification\\nfrom sklearn.multioutput import MultiOutputClassifier\\nfrom sklearn.ensemble import RandomForestClassifier\\nfrom sklearn.utils import shuffle\\nimport numpy as np\\nX, y1 = make_classification(n_samples=10, n_features=100,\\n...                             n_informative=30, n_classes=3,\\n...                             random_state=1)\\ny2 = shuffle(y1, random_state=1)\\ny3 = shuffle(y1, random_state=2)\\nY = np.vstack((y1, y2, y3)).T\\nn_samples, n_features = X.shape # 10,100\\nn_outputs = Y.shape[1] # 3\\nn_classes = 3\\nforest = RandomForestClassifier(random_state=1)\\nmulti_target_forest = MultiOutputClassifier(forest, n_jobs=2)\\nmulti_target_forest.fit(X, Y).predict(X)\\narray([[2, 2, 0],\\n[1, 2, 1],\\n[2, 1, 0],\\n[0, 0, 2],\\n[0, 2, 1],\\n[0, 0, 2],\\n[1, 1, 0],\\n[1, 1, 1],\\n[0, 0, 2],\\n[2, 0, 0]])\\nWarning\\nAt present, no metric in sklearn.metrics\\nsupports the multiclass-multioutput classification task.\\n1.12.3.1. Target format¶\\nA valid representation of multioutput y is a dense matrix of shape\\n(n_samples, n_classes) of class labels. A column wise concatenation of 1d\\nmulticlass variables. An example of y for 3 samples:\\ny = np.array([[\\'apple\\', \\'green\\'], [\\'orange\\', \\'orange\\'], [\\'pear\\', \\'green\\']])\\nprint(y)\\n[[\\'apple\\' \\'green\\']\\n[\\'orange\\' \\'orange\\']\\n[\\'pear\\' \\'green\\']]\\n- 1.12.4. Multioutput regression¶\\nMultioutput regression predicts multiple numerical properties for each\\nsample. Each property is a numerical variable and the number of properties\\nto be predicted for each sample is greater than or equal to 2. Some estimators\\nthat support multioutput regression are faster than just running n_output\\nestimators.\\nFor example, prediction of both wind speed and wind direction, in degrees,\\nusing data obtained at a certain location. Each sample would be data\\nobtained at one location and both wind speed and direction would be\\noutput for each sample.\\n1.12.4.1. Target format¶\\nA valid representation of multioutput y is a dense matrix of shape\\n(n_samples, n_output) of floats. A column wise concatenation of\\ncontinuous variables. An example of y for 3 samples:\\ny = np.array([[31.4, 94], [40.5, 109], [25.0, 30]])\\nprint(y)\\n[[ 31.4  94. ]\\n[ 40.5 109. ]\\n[ 25.   30. ]]\\n1.12.4.2. MultiOutputRegressor¶\\nMultioutput regression support can be added to any regressor with\\nMultiOutputRegressor.  This strategy consists of\\nfitting one regressor per target. Since each target is represented by exactly\\none regressor it is possible to gain knowledge about the target by\\ninspecting its corresponding regressor. As\\nMultiOutputRegressor fits one regressor per\\ntarget it can not take advantage of correlations between targets.\\nBelow is an example of multioutput regression:\\nfrom sklearn.datasets import make_regression\\nfrom sklearn.multioutput import MultiOutputRegressor\\nfrom sklearn.ensemble import GradientBoostingRegressor\\nX, y = make_regression(n_samples=10, n_targets=3, random_state=1)\\nMultiOutputRegressor(GradientBoostingRegressor(random_state=0)).fit(X, y).predict(X)\\narray([[-154.75474165, -147.03498585,  -50.03812219],\\n[   7.12165031,    5.12914884,  -81.46081961],\\n[-187.8948621 , -100.44373091,   13.88978285],\\n[-141.62745778,   95.02891072, -191.48204257],\\n[  97.03260883,  165.34867495,  139.52003279],\\n[ 123.92529176,   21.25719016,   -7.84253   ],\\n[-122.25193977,  -85.16443186, -107.12274212],\\n[ -30.170388  ,  -94.80956739,   12.16979946],\\n[ 140.72667194,  176.50941682,  -17.50447799],\\n[ 149.37967282,  -81.15699552,   -5.72850319]])\\n1.12.4.3. RegressorChain¶\\nRegressor chains (see RegressorChain) is\\nanalogous to ClassifierChain as a way of\\ncombining a number of regressions into a single multi-target model that is\\ncapable of exploiting correlations among targets.\\n© 2007 - 2024, scikit-learn developers (BSD License).\\nShow this page source\\n\\n1.13. Feature selection — scikit-learn 1.4.2 documentation\\n\\nInstall\\nUser Guide\\nAPI\\nExamples\\nCommunity\\nGetting Started\\nTutorial\\nWhat\\'s new\\nGlossary\\nDevelopment\\nFAQ\\nSupport\\nRelated packages\\nRoadmap\\nGovernance\\nAbout us\\nGitHub\\nOther Versions and Download\\nMore\\nGetting Started\\nTutorial\\nWhat\\'s new\\nGlossary\\nDevelopment\\nFAQ\\nSupport\\nRelated packages\\nRoadmap\\nGovernance\\nAbout us\\nGitHub\\nOther Versions and Download\\nToggle Menu\\nPrevUp\\nNext\\nscikit-learn 1.4.2\\nOther versions\\nPlease cite us if you use the software.\\n\\n1.13. Feature selection\\n\\n1.13.1. Removing features with low variance\\n\\n1.13.2. Univariate feature selection\\n\\n1.13.3. Recursive feature elimination\\n\\n1.13.4. Feature selection using SelectFromModel\\n1.13.4.1. L1-based feature selection\\n1.13.4.2. Tree-based feature selection\\n\\n1.13.5. Sequential Feature Selection\\n\\n1.13.6. Feature selection as part of a pipeline\\n\\n1.13. Feature selection¶\\n\\nThe classes in the sklearn.feature_selection module can be used\\nfor feature selection/dimensionality reduction on sample sets, either to\\nimprove estimators’ accuracy scores or to boost their performance on very\\nhigh-dimensional datasets.\\n- 1.13.1. Removing features with low variance¶\\nVarianceThreshold is a simple baseline approach to feature selection.\\nIt removes all features whose variance doesn’t meet some threshold.\\nBy default, it removes all zero-variance features,\\ni.e. features that have the same value in all samples.\\nAs an example, suppose that we have a dataset with boolean features,\\nand we want to remove all features that are either one or zero (on or off)\\nin more than 80% of the samples.\\nBoolean features are Bernoulli random variables,\\nand the variance of such variables is given by\\n[\\\\mathrm{Var}[X] = p(1 - p)]\\nso we can select using the threshold .8 * (1 - .8):\\n\\nfrom sklearn.feature_selection import VarianceThreshold\\nX = [[0, 0, 1], [0, 1, 0], [1, 0, 0], [0, 1, 1], [0, 1, 0], [0, 1, 1]]\\nsel = VarianceThreshold(threshold=(.8 * (1 - .8)))\\nsel.fit_transform(X)\\narray([[0, 1],\\n[1, 0],\\n[0, 0],\\n[1, 1],\\n[1, 0],\\n[1, 1]])\\nAs expected, VarianceThreshold has removed the first column,\\nwhich has a probability (p = 5/6 > .8) of containing a zero.\\n- 1.13.2. Univariate feature selection¶\\nUnivariate feature selection works by selecting the best features based on\\nunivariate statistical tests. It can be seen as a preprocessing step\\nto an estimator. Scikit-learn exposes feature selection routines\\nas objects that implement the transform method:\\nSelectKBest removes all but the (k) highest scoring features\\nSelectPercentile removes all but a user-specified highest scoring\\npercentage of features\\nusing common univariate statistical tests for each feature:\\nfalse positive rate SelectFpr, false discovery rate\\nSelectFdr, or family wise error SelectFwe.\\nGenericUnivariateSelect allows to perform univariate feature\\nselection with a configurable strategy. This allows to select the best\\nunivariate selection strategy with hyper-parameter search estimator.\\nFor instance, we can use a F-test to retrieve the two\\nbest features for a dataset as follows:\\nfrom sklearn.datasets import load_iris\\nfrom sklearn.feature_selection import SelectKBest\\nfrom sklearn.feature_selection import f_classif\\nX, y = load_iris(return_X_y=True)\\nX.shape\\n(150, 4)\\nX_new = SelectKBest(f_classif, k=2).fit_transform(X, y)\\nX_new.shape\\n(150, 2)\\nThese objects take as input a scoring function that returns univariate scores\\nand p-values (or only scores for SelectKBest and\\nSelectPercentile):\\nFor regression: r_regression, f_regression, mutual_info_regression\\nFor classification: chi2, f_classif, mutual_info_classif\\nThe methods based on F-test estimate the degree of linear dependency between\\ntwo random variables. On the other hand, mutual information methods can capture\\nany kind of statistical dependency, but being nonparametric, they require more\\nsamples for accurate estimation. Note that the (\\\\chi^2)-test should only be\\napplied to non-negative features, such as frequencies.\\nFeature selection with sparse data\\nIf you use sparse data (i.e. data represented as sparse matrices),\\nchi2, mutual_info_regression, mutual_info_classif\\nwill deal with the data without making it dense.\\nWarning\\nBeware not to use a regression scoring function with a classification\\nproblem, you will get useless results.\\nNote\\nThe SelectPercentile and SelectKBest support unsupervised\\nfeature selection as well. One needs to provide a score_func where y=None.\\nThe score_func should use internally X to compute the scores.\\nExamples:\\nUnivariate Feature Selection\\nComparison of F-test and mutual information\\n- 1.13.3. Recursive feature elimination¶\\nGiven an external estimator that assigns weights to features (e.g., the\\ncoefficients of a linear model), the goal of recursive feature elimination (RFE)\\nis to select features by recursively considering smaller and smaller sets of\\nfeatures. First, the estimator is trained on the initial set of features and\\nthe importance of each feature is obtained either through any specific attribute\\n(such as coef_, feature_importances_) or callable. Then, the least important\\nfeatures are pruned from current set of features. That procedure is recursively\\nrepeated on the pruned set until the desired number of features to select is\\neventually reached.\\nRFECV performs RFE in a cross-validation loop to find the optimal\\nnumber of features. In more details, the number of features selected is tuned\\nautomatically by fitting an RFE selector on the different\\ncross-validation splits (provided by the cv parameter). The performance\\nof the RFE selector are evaluated using scorer for different number\\nof selected features and aggregated together. Finally, the scores are averaged\\nacross folds and the number of features selected is set to the number of\\nfeatures that maximize the cross-validation score.\\nExamples:\\nRecursive feature elimination: A recursive feature elimination example\\nshowing the relevance of pixels in a digit classification task.\\nRecursive feature elimination with cross-validation: A recursive feature\\nelimination example with automatic tuning of the number of features\\nselected with cross-validation.\\n- 1.13.4. Feature selection using SelectFromModel¶\\nSelectFromModel is a meta-transformer that can be used alongside any\\nestimator that assigns importance to each feature through a specific attribute (such as\\ncoef_, feature_importances_) or via an importance_getter callable after fitting.\\nThe features are considered unimportant and removed if the corresponding\\nimportance of the feature values are below the provided\\nthreshold parameter. Apart from specifying the threshold numerically,\\nthere are built-in heuristics for finding a threshold using a string argument.\\nAvailable heuristics are “mean”, “median” and float multiples of these like\\n“0.1*mean”. In combination with the threshold criteria, one can use the\\nmax_features parameter to set a limit on the number of features to select.\\nFor examples on how it is to be used refer to the sections below.\\nExamples\\nModel-based and sequential feature selection\\n1.13.4.1. L1-based feature selection¶\\nLinear models penalized with the L1 norm have\\nsparse solutions: many of their estimated coefficients are zero. When the goal\\nis to reduce the dimensionality of the data to use with another classifier,\\nthey can be used along with SelectFromModel\\nto select the non-zero coefficients. In particular, sparse estimators useful\\nfor this purpose are the Lasso for regression, and\\nof LogisticRegression and LinearSVC\\nfor classification:\\nfrom sklearn.svm import LinearSVC\\nfrom sklearn.datasets import load_iris\\nfrom sklearn.feature_selection import SelectFromModel\\nX, y = load_iris(return_X_y=True)\\nX.shape\\n(150, 4)\\nlsvc = LinearSVC(C=0.01, penalty=\"l1\", dual=False).fit(X, y)\\nmodel = SelectFromModel(lsvc, prefit=True)\\nX_new = model.transform(X)\\nX_new.shape\\n(150, 3)\\nWith SVMs and logistic-regression, the parameter C controls the sparsity:\\nthe smaller C the fewer features selected. With Lasso, the higher the\\nalpha parameter, the fewer features selected.\\nExamples:\\nLasso on dense and sparse data.\\nL1-recovery and compressive sensing\\nClick for more details\\n¶\\nFor a good choice of alpha, the Lasso can fully recover the\\nexact set of non-zero variables using only few observations, provided\\ncertain specific conditions are met. In particular, the number of\\nsamples should be “sufficiently large”, or L1 models will perform at\\nrandom, where “sufficiently large” depends on the number of non-zero\\ncoefficients, the logarithm of the number of features, the amount of\\nnoise, the smallest absolute value of non-zero coefficients, and the\\nstructure of the design matrix X. In addition, the design matrix must\\ndisplay certain specific properties, such as not being too correlated.\\nThere is no general rule to select an alpha parameter for recovery of\\nnon-zero coefficients. It can by set by cross-validation\\n(LassoCV or\\nLassoLarsCV), though this may lead to\\nunder-penalized models: including a small number of non-relevant variables\\nis not detrimental to prediction score. BIC\\n(LassoLarsIC) tends, on the opposite, to set\\nhigh values of alpha.\\nReference\\nRichard G. Baraniuk “Compressive Sensing”, IEEE Signal\\nProcessing Magazine [120] July 2007\\nhttp://users.isr.ist.utl.pt/~aguiar/CS_notes.pdf\\n1.13.4.2. Tree-based feature selection¶\\nTree-based estimators (see the sklearn.tree module and forest\\nof trees in the sklearn.ensemble module) can be used to compute\\nimpurity-based feature importances, which in turn can be used to discard irrelevant\\nfeatures (when coupled with the SelectFromModel\\nmeta-transformer):\\nfrom sklearn.ensemble import ExtraTreesClassifier\\nfrom sklearn.datasets import load_iris\\nfrom sklearn.feature_selection import SelectFromModel\\nX, y = load_iris(return_X_y=True)\\nX.shape\\n(150, 4)\\nclf = ExtraTreesClassifier(n_estimators=50)\\nclf = clf.fit(X, y)\\nclf.feature_importances_\\narray([ 0.04...,  0.05...,  0.4...,  0.4...])\\nmodel = SelectFromModel(clf, prefit=True)\\nX_new = model.transform(X)\\nX_new.shape\\n(150, 2)\\nExamples:\\nFeature importances with a forest of trees: example on\\nsynthetic data showing the recovery of the actually meaningful\\nfeatures.\\nPixel importances with a parallel forest of trees: example\\non face recognition data.\\n- 1.13.5. Sequential Feature Selection¶\\nSequential Feature Selection [sfs] (SFS) is available in the\\nSequentialFeatureSelector transformer.\\nSFS can be either forward or backward:\\nForward-SFS is a greedy procedure that iteratively finds the best new feature\\nto add to the set of selected features. Concretely, we initially start with\\nzero features and find the one feature that maximizes a cross-validated score\\nwhen an estimator is trained on this single feature. Once that first feature\\nis selected, we repeat the procedure by adding a new feature to the set of\\nselected features. The procedure stops when the desired number of selected\\nfeatures is reached, as determined by the n_features_to_select parameter.\\nBackward-SFS follows the same idea but works in the opposite direction:\\ninstead of starting with no features and greedily adding features, we start\\nwith all the features and greedily remove features from the set. The\\ndirection parameter controls whether forward or backward SFS is used.\\nDetail on Sequential Feature Selection\\nClick for more details\\n¶\\nIn general, forward and backward selection do not yield equivalent results.\\nAlso, one may be much faster than the other depending on the requested number\\nof selected features: if we have 10 features and ask for 7 selected features,\\nforward selection would need to perform 7 iterations while backward selection\\nwould only need to perform 3.\\nSFS differs from RFE and\\nSelectFromModel in that it does not\\nrequire the underlying model to expose a coef_ or feature_importances_\\nattribute. It may however be slower considering that more models need to be\\nevaluated, compared to the other approaches. For example in backward\\nselection, the iteration going from m features to m - 1 features using k-fold\\ncross-validation requires fitting m * k models, while\\nRFE would require only a single fit, and\\nSelectFromModel always just does a single\\nfit and requires no iterations.\\nReference\\n[sfs]\\nFerri et al, Comparative study of techniques for\\nlarge-scale feature selection.\\nExamples\\nModel-based and sequential feature selection\\n- 1.13.6. Feature selection as part of a pipeline¶\\nFeature selection is usually used as a pre-processing step before doing\\nthe actual learning. The recommended way to do this in scikit-learn is\\nto use a Pipeline:\\nclf = Pipeline([\\n(\\'feature_selection\\', SelectFromModel(LinearSVC(dual=\"auto\", penalty=\"l1\"))),\\n(\\'classification\\', RandomForestClassifier())\\n])\\nclf.fit(X, y)\\nIn this snippet we make use of a LinearSVC\\ncoupled with SelectFromModel\\nto evaluate feature importances and select the most relevant features.\\nThen, a RandomForestClassifier is trained on the\\ntransformed output, i.e. using only relevant features. You can perform\\nsimilar operations with the other feature selection methods and also\\nclassifiers that provide a way to evaluate feature importances of course.\\nSee the Pipeline examples for more details.\\n© 2007 - 2024, scikit-learn developers (BSD License).\\nShow this page source\\n\\n1.14. Semi-supervised learning — scikit-learn 1.4.2 documentation\\n\\nInstall\\nUser Guide\\nAPI\\nExamples\\nCommunity\\nGetting Started\\nTutorial\\nWhat\\'s new\\nGlossary\\nDevelopment\\nFAQ\\nSupport\\nRelated packages\\nRoadmap\\nGovernance\\nAbout us\\nGitHub\\nOther Versions and Download\\nMore\\nGetting Started\\nTutorial\\nWhat\\'s new\\nGlossary\\nDevelopment\\nFAQ\\nSupport\\nRelated packages\\nRoadmap\\nGovernance\\nAbout us\\nGitHub\\nOther Versions and Download\\nToggle Menu\\nPrevUp\\nNext\\nscikit-learn 1.4.2\\nOther versions\\nPlease cite us if you use the software.\\n\\n1.14. Semi-supervised learning\\n\\n1.14.1. Self Training\\n\\n1.14.2. Label Propagation\\n\\n1.14. Semi-supervised learning¶\\n\\nSemi-supervised learning is a situation\\nin which in your training data some of the samples are not labeled. The\\nsemi-supervised estimators in sklearn.semi_supervised are able to\\nmake use of this additional unlabeled data to better capture the shape of\\nthe underlying data distribution and generalize better to new samples.\\nThese algorithms can perform well when we have a very small amount of\\nlabeled points and a large amount of unlabeled points.\\nUnlabeled entries in y\\nIt is important to assign an identifier to unlabeled points along with the\\nlabeled data when training the model with the fit method. The\\nidentifier that this implementation uses is the integer value (-1).\\nNote that for string labels, the dtype of y should be object so that it\\ncan contain both strings and integers.\\nNote\\nSemi-supervised algorithms need to make assumptions about the distribution\\nof the dataset in order to achieve performance gains. See here\\nfor more details.\\n- 1.14.1. Self Training¶\\nThis self-training implementation is based on Yarowsky’s [1] algorithm. Using\\nthis algorithm, a given supervised classifier can function as a semi-supervised\\nclassifier, allowing it to learn from unlabeled data.\\nSelfTrainingClassifier can be called with any classifier that\\nimplements predict_proba, passed as the parameter base_classifier. In\\neach iteration, the base_classifier predicts labels for the unlabeled\\nsamples and adds a subset of these labels to the labeled dataset.\\nThe choice of this subset is determined by the selection criterion. This\\nselection can be done using a threshold on the prediction probabilities, or\\nby choosing the k_best samples according to the prediction probabilities.\\nThe labels used for the final fit as well as the iteration in which each sample\\nwas labeled are available as attributes. The optional max_iter parameter\\nspecifies how many times the loop is executed at most.\\nThe max_iter parameter may be set to None, causing the algorithm to iterate\\nuntil all samples have labels or no new samples are selected in that iteration.\\nNote\\nWhen using the self-training classifier, the\\ncalibration of the classifier is important.\\nExamples\\nEffect of varying threshold for self-training\\nDecision boundary of semi-supervised classifiers versus SVM on the Iris dataset\\nReferences\\n[1]\\n“Unsupervised word sense disambiguation rivaling supervised methods”\\nDavid Yarowsky, Proceedings of the 33rd annual meeting on Association for\\nComputational Linguistics (ACL ‘95). Association for Computational Linguistics,\\nStroudsburg, PA, USA, 189-196.\\n- 1.14.2. Label Propagation¶\\nLabel propagation denotes a few variations of semi-supervised graph\\ninference algorithms.\\nA few features available in this model:\\nUsed for classification tasks\\nKernel methods to project data into alternate dimensional spaces\\nscikit-learn provides two label propagation models:\\nLabelPropagation and LabelSpreading. Both work by\\nconstructing a similarity graph over all items in the input dataset.\\nAn illustration of label-propagation: the structure of unlabeled\\nobservations is consistent with the class structure, and thus the\\nclass label can be propagated to the unlabeled observations of the\\ntraining set.¶\\nLabelPropagation and LabelSpreading\\ndiffer in modifications to the similarity matrix that graph and the\\nclamping effect on the label distributions.\\nClamping allows the algorithm to change the weight of the true ground labeled\\ndata to some degree. The LabelPropagation algorithm performs hard\\nclamping of input labels, which means (\\\\alpha=0). This clamping factor\\ncan be relaxed, to say (\\\\alpha=0.2), which means that we will always\\nretain 80 percent of our original label distribution, but the algorithm gets to\\nchange its confidence of the distribution within 20 percent.\\nLabelPropagation uses the raw similarity matrix constructed from\\nthe data with no modifications. In contrast, LabelSpreading\\nminimizes a loss function that has regularization properties, as such it\\nis often more robust to noise. The algorithm iterates on a modified\\nversion of the original graph and normalizes the edge weights by\\ncomputing the normalized graph Laplacian matrix. This procedure is also\\nused in Spectral clustering.\\nLabel propagation models have two built-in kernel methods. Choice of kernel\\neffects both scalability and performance of the algorithms. The following are\\navailable:\\nrbf ((\\\\exp(-\\\\gamma |x-y|^2), \\\\gamma > 0)). (\\\\gamma) is\\nspecified by keyword gamma.\\nknn ((1[x\\' \\\\in kNN(x)])). (k) is specified by keyword\\nn_neighbors.\\nThe RBF kernel will produce a fully connected graph which is represented in memory\\nby a dense matrix. This matrix may be very large and combined with the cost of\\nperforming a full matrix multiplication calculation for each iteration of the\\nalgorithm can lead to prohibitively long running times. On the other hand,\\nthe KNN kernel will produce a much more memory-friendly sparse matrix\\nwhich can drastically reduce running times.\\nExamples\\nDecision boundary of semi-supervised classifiers versus SVM on the Iris dataset\\nLabel Propagation learning a complex structure\\nLabel Propagation digits: Demonstrating performance\\nLabel Propagation digits active learning\\nReferences\\n[2] Yoshua Bengio, Olivier Delalleau, Nicolas Le Roux. In Semi-Supervised\\nLearning (2006), pp. 193-216\\n[3] Olivier Delalleau, Yoshua Bengio, Nicolas Le Roux. Efficient\\nNon-Parametric Function Induction in Semi-Supervised Learning. AISTAT 2005\\nhttps://www.gatsby.ucl.ac.uk/aistats/fullpapers/204.pdf\\n© 2007 - 2024, scikit-learn developers (BSD License).\\nShow this page source\\n\\n1.15. Isotonic regression — scikit-learn 1.4.2 documentation\\n\\nInstall\\nUser Guide\\nAPI\\nExamples\\nCommunity\\nGetting Started\\nTutorial\\nWhat\\'s new\\nGlossary\\nDevelopment\\nFAQ\\nSupport\\nRelated packages\\nRoadmap\\nGovernance\\nAbout us\\nGitHub\\nOther Versions and Download\\nMore\\nGetting Started\\nTutorial\\nWhat\\'s new\\nGlossary\\nDevelopment\\nFAQ\\nSupport\\nRelated packages\\nRoadmap\\nGovernance\\nAbout us\\nGitHub\\nOther Versions and Download\\nToggle Menu\\nPrevUp\\nNext\\nscikit-learn 1.4.2\\nOther versions\\nPlease cite us if you use the software.\\n\\n1.15. Isotonic regression\\n\\n1.15. Isotonic regression¶\\n\\nThe class IsotonicRegression fits a non-decreasing real function to\\n1-dimensional data. It solves the following problem:\\n[\\\\min \\\\sum_i w_i (y_i - \\\\hat{y}_i)^2]\\nsubject to (\\\\hat{y}_i \\\\le \\\\hat{y}_j) whenever (X_i \\\\le X_j),\\nwhere the weights (w_i) are strictly positive, and both X and y are\\narbitrary real quantities.\\nThe increasing parameter changes the constraint to\\n(\\\\hat{y}_i \\\\ge \\\\hat{y}_j) whenever (X_i \\\\le X_j). Setting it to\\n‘auto’ will automatically choose the constraint based on Spearman’s rank\\ncorrelation coefficient.\\nIsotonicRegression produces a series of predictions\\n(\\\\hat{y}_i) for the training data which are the closest to the targets\\n(y) in terms of mean squared error. These predictions are interpolated\\nfor predicting to unseen data. The predictions of IsotonicRegression\\nthus form a function that is piecewise linear:\\n© 2007 - 2024, scikit-learn developers (BSD License).\\nShow this page source\\n\\n1.16. Probability calibration — scikit-learn 1.4.2 documentation\\n\\nInstall\\nUser Guide\\nAPI\\nExamples\\nCommunity\\nGetting Started\\nTutorial\\nWhat\\'s new\\nGlossary\\nDevelopment\\nFAQ\\nSupport\\nRelated packages\\nRoadmap\\nGovernance\\nAbout us\\nGitHub\\nOther Versions and Download\\nMore\\nGetting Started\\nTutorial\\nWhat\\'s new\\nGlossary\\nDevelopment\\nFAQ\\nSupport\\nRelated packages\\nRoadmap\\nGovernance\\nAbout us\\nGitHub\\nOther Versions and Download\\nToggle Menu\\nPrevUp\\nNext\\nscikit-learn 1.4.2\\nOther versions\\nPlease cite us if you use the software.\\n\\n1.16. Probability calibration\\n\\n1.16.1. Calibration curves\\n\\n1.16.2. Calibrating a classifier\\n\\n1.16.3. Usage\\n1.16.3.1. Sigmoid\\n1.16.3.2. Isotonic\\n1.16.3.3. Multiclass support\\n\\n1.16. Probability calibration¶\\n\\nWhen performing classification you often want not only to predict the class\\nlabel, but also obtain a probability of the respective label. This probability\\ngives you some kind of confidence on the prediction. Some models can give you\\npoor estimates of the class probabilities and some even do not support\\nprobability prediction (e.g., some instances of\\nSGDClassifier).\\nThe calibration module allows you to better calibrate\\nthe probabilities of a given model, or to add support for probability\\nprediction.\\nWell calibrated classifiers are probabilistic classifiers for which the output\\nof the predict_proba method can be directly interpreted as a confidence\\nlevel.\\nFor instance, a well calibrated (binary) classifier should classify the samples such\\nthat among the samples to which it gave a predict_proba value close to, say,\\n0.8, approximately 80% actually belong to the positive class.\\nBefore we show how to re-calibrate a classifier, we first need a way to detect how\\ngood a classifier is calibrated.\\nNote\\nStrictly proper scoring rules for probabilistic predictions like\\nsklearn.metrics.brier_score_loss and\\nsklearn.metrics.log_loss assess calibration (reliability) and\\ndiscriminative power (resolution) of a model, as well as the randomness of the data\\n(uncertainty) at the same time. This follows from the well-known Brier score\\ndecomposition of Murphy [1]. As it is not clear which term dominates, the score is\\nof limited use for assessing calibration alone (unless one computes each term of\\nthe decomposition). A lower Brier loss, for instance, does not necessarily\\nmean a better calibrated model, it could also mean a worse calibrated model with much\\nmore discriminatory power, e.g. using many more features.\\n- 1.16.1. Calibration curves¶\\nCalibration curves, also referred to as reliability diagrams (Wilks 1995 [2]),\\ncompare how well the probabilistic predictions of a binary classifier are calibrated.\\nIt plots the frequency of the positive label (to be more precise, an estimation of the\\nconditional event probability (P(Y=1|\\\\text{predict_proba}))) on the y-axis\\nagainst the predicted probability predict_proba of a model on the x-axis.\\nThe tricky part is to get values for the y-axis.\\nIn scikit-learn, this is accomplished by binning the predictions such that the x-axis\\nrepresents the average predicted probability in each bin.\\nThe y-axis is then the fraction of positives given the predictions of that bin, i.e.\\nthe proportion of samples whose class is the positive class (in each bin).\\nThe top calibration curve plot is created with\\nCalibrationDisplay.from_estimator, which uses calibration_curve to\\ncalculate the per bin average predicted probabilities and fraction of positives.\\nCalibrationDisplay.from_estimator\\ntakes as input a fitted classifier, which is used to calculate the predicted\\nprobabilities. The classifier thus must have predict_proba method. For\\nthe few classifiers that do not have a predict_proba method, it is\\npossible to use CalibratedClassifierCV to calibrate the classifier\\noutputs to probabilities.\\nThe bottom histogram gives some insight into the behavior of each classifier\\nby showing the number of samples in each predicted probability bin.\\nLogisticRegression returns well calibrated predictions by default as it has a\\ncanonical link function for its loss, i.e. the logit-link for the Log loss.\\nThis leads to the so-called balance property, see [8] and\\nLogistic regression.\\nIn contrast to that, the other shown models return biased probabilities; with\\ndifferent biases per model.\\nGaussianNB (Naive Bayes) tends to push probabilities to 0 or 1 (note the counts\\nin the histograms). This is mainly because it makes the assumption that\\nfeatures are conditionally independent given the class, which is not the\\ncase in this dataset which contains 2 redundant features.\\nRandomForestClassifier shows the opposite behavior: the histograms\\nshow peaks at probabilities approximately 0.2 and 0.9, while probabilities\\nclose to 0 or 1 are very rare. An explanation for this is given by\\nNiculescu-Mizil and Caruana [3]: “Methods such as bagging and random\\nforests that average predictions from a base set of models can have\\ndifficulty making predictions near 0 and 1 because variance in the\\nunderlying base models will bias predictions that should be near zero or one\\naway from these values. Because predictions are restricted to the interval\\n[0,1], errors caused by variance tend to be one-sided near zero and one. For\\nexample, if a model should predict p = 0 for a case, the only way bagging\\ncan achieve this is if all bagged trees predict zero. If we add noise to the\\ntrees that bagging is averaging over, this noise will cause some trees to\\npredict values larger than 0 for this case, thus moving the average\\nprediction of the bagged ensemble away from 0. We observe this effect most\\nstrongly with random forests because the base-level trees trained with\\nrandom forests have relatively high variance due to feature subsetting.” As\\na result, the calibration curve shows a characteristic sigmoid shape, indicating that\\nthe classifier could trust its “intuition” more and return probabilities closer\\nto 0 or 1 typically.\\nLinearSVC (SVC) shows an even more sigmoid curve than the random forest, which\\nis typical for maximum-margin methods (compare Niculescu-Mizil and Caruana [3]), which\\nfocus on difficult to classify samples that are close to the decision boundary (the\\nsupport vectors).\\n- 1.16.2. Calibrating a classifier¶\\nCalibrating a classifier consists of fitting a regressor (called a\\ncalibrator) that maps the output of the classifier (as given by\\ndecision_function or predict_proba) to a calibrated probability\\nin [0, 1]. Denoting the output of the classifier for a given sample by (f_i),\\nthe calibrator tries to predict the conditional event probability\\n(P(y_i = 1 | f_i)).\\nIdeally, the calibrator is fit on a dataset independent of the training data used to\\nfit the classifier in the first place.\\nThis is because performance of the classifier on its training data would be\\nbetter than for novel data. Using the classifier output of training data\\nto fit the calibrator would thus result in a biased calibrator that maps to\\nprobabilities closer to 0 and 1 than it should.\\n- 1.16.3. Usage¶\\nThe CalibratedClassifierCV class is used to calibrate a classifier.\\nCalibratedClassifierCV uses a cross-validation approach to ensure\\nunbiased data is always used to fit the calibrator. The data is split into k\\n(train_set, test_set) couples (as determined by cv). When ensemble=True\\n(default), the following procedure is repeated independently for each\\ncross-validation split: a clone of base_estimator is first trained on the\\ntrain subset. Then its predictions on the test subset are used to fit a\\ncalibrator (either a sigmoid or isotonic regressor). This results in an\\nensemble of k (classifier, calibrator) couples where each calibrator maps\\nthe output of its corresponding classifier into [0, 1]. Each couple is exposed\\nin the calibrated_classifiers_ attribute, where each entry is a calibrated\\nclassifier with a predict_proba method that outputs calibrated\\nprobabilities. The output of predict_proba for the main\\nCalibratedClassifierCV instance corresponds to the average of the\\npredicted probabilities of the k estimators in the calibrated_classifiers_\\nlist. The output of predict is the class that has the highest\\nprobability.\\nWhen ensemble=False, cross-validation is used to obtain ‘unbiased’\\npredictions for all the data, via\\ncross_val_predict.\\nThese unbiased predictions are then used to train the calibrator. The attribute\\ncalibrated_classifiers_ consists of only one (classifier, calibrator)\\ncouple where the classifier is the base_estimator trained on all the data.\\nIn this case the output of predict_proba for\\nCalibratedClassifierCV is the predicted probabilities obtained\\nfrom the single (classifier, calibrator) couple.\\nThe main advantage of ensemble=True is to benefit from the traditional\\nensembling effect (similar to Bagging meta-estimator). The resulting ensemble should\\nboth be well calibrated and slightly more accurate than with ensemble=False.\\nThe main advantage of using ensemble=False is computational: it reduces the\\noverall fit time by training only a single base classifier and calibrator\\npair, decreases the final model size and increases prediction speed.\\nAlternatively an already fitted classifier can be calibrated by setting\\ncv=\"prefit\". In this case, the data is not split and all of it is used to\\nfit the regressor. It is up to the user to\\nmake sure that the data used for fitting the classifier is disjoint from the\\ndata used for fitting the regressor.\\nCalibratedClassifierCV supports the use of two regression techniques\\nfor calibration via the method parameter: \"sigmoid\" and \"isotonic\".\\n1.16.3.1. Sigmoid¶\\nThe sigmoid regressor, method=\"sigmoid\" is based on Platt’s logistic model [4]:\\n[p(y_i = 1 | f_i) = \\\\frac{1}{1 + \\\\exp(A f_i + B)} \\\\,,]\\nwhere (y_i) is the true label of sample (i) and (f_i)\\nis the output of the un-calibrated classifier for sample (i). (A)\\nand (B) are real numbers to be determined when fitting the regressor via\\nmaximum likelihood.\\nThe sigmoid method assumes the calibration curve\\ncan be corrected by applying a sigmoid function to the raw predictions. This\\nassumption has been empirically justified in the case of Support Vector Machines with\\ncommon kernel functions on various benchmark datasets in section 2.1 of Platt\\n1999 [4] but does not necessarily hold in general. Additionally, the\\nlogistic model works best if the calibration error is symmetrical, meaning\\nthe classifier output for each binary class is normally distributed with\\nthe same variance [7]. This can be a problem for highly imbalanced\\nclassification problems, where outputs do not have equal variance.\\nIn general this method is most effective for small sample sizes or when the\\nun-calibrated model is under-confident and has similar calibration errors for both\\nhigh and low outputs.\\n1.16.3.2. Isotonic¶\\nThe method=\"isotonic\" fits a non-parametric isotonic regressor, which outputs\\na step-wise non-decreasing function, see sklearn.isotonic. It minimizes:\\n[\\\\sum_{i=1}^{n} (y_i - \\\\hat{f}_i)^2]\\nsubject to (\\\\hat{f}_i \\\\geq \\\\hat{f}_j) whenever\\n(f_i \\\\geq f_j). (y_i) is the true\\nlabel of sample (i) and (\\\\hat{f}_i) is the output of the\\ncalibrated classifier for sample (i) (i.e., the calibrated probability).\\nThis method is more general when compared to ‘sigmoid’ as the only restriction\\nis that the mapping function is monotonically increasing. It is thus more\\npowerful as it can correct any monotonic distortion of the un-calibrated model.\\nHowever, it is more prone to overfitting, especially on small datasets [6].\\nOverall, ‘isotonic’ will perform as well as or better than ‘sigmoid’ when\\nthere is enough data (greater than ~ 1000 samples) to avoid overfitting [3].\\nNote\\nImpact on ranking metrics like AUC\\nIt is generally expected that calibration does not affect ranking metrics such as\\nROC-AUC. However, these metrics might differ after calibration when using\\nmethod=\"isotonic\" since isotonic regression introduces ties in the predicted\\nprobabilities. This can be seen as within the uncertainty of the model predictions.\\nIn case, you strictly want to keep the ranking and thus AUC scores, use\\nmethod=\"sigmoid\" which is a strictly monotonic transformation and thus keeps\\nthe ranking.\\n1.16.3.3. Multiclass support¶\\nBoth isotonic and sigmoid regressors only\\nsupport 1-dimensional data (e.g., binary classification output) but are\\nextended for multiclass classification if the base_estimator supports\\nmulticlass predictions. For multiclass predictions,\\nCalibratedClassifierCV calibrates for\\neach class separately in a OneVsRestClassifier fashion [5]. When\\npredicting\\nprobabilities, the calibrated probabilities for each class\\nare predicted separately. As those probabilities do not necessarily sum to\\none, a postprocessing is performed to normalize them.\\nExamples:\\nProbability Calibration curves\\nProbability Calibration for 3-class classification\\nProbability calibration of classifiers\\nComparison of Calibration of Classifiers\\nReferences:\\n[1]\\nAllan H. Murphy (1973).\\n“A New Vector Partition of the Probability Score”\\nJournal of Applied Meteorology and Climatology\\n[2]\\nOn the combination of forecast probabilities for\\nconsecutive precipitation periods.\\nWea. Forecasting, 5, 640–650., Wilks, D. S., 1990a\\n[3]\\n(1,2,3)\\nPredicting Good Probabilities with Supervised Learning,\\nA. Niculescu-Mizil & R. Caruana, ICML 2005\\n[4]\\n(1,2)\\nProbabilistic Outputs for Support Vector Machines and Comparisons\\nto Regularized Likelihood Methods.\\nJ. Platt, (1999)\\n[5]\\nTransforming Classifier Scores into Accurate Multiclass\\nProbability Estimates.\\nB. Zadrozny & C. Elkan, (KDD 2002)\\n[6]\\nPredicting accurate probabilities with a ranking loss.\\nMenon AK, Jiang XJ, Vembu S, Elkan C, Ohno-Machado L.\\nProc Int Conf Mach Learn. 2012;2012:703-710\\n[7]\\nBeyond sigmoids: How to obtain well-calibrated probabilities from\\nbinary classifiers with beta calibration\\nKull, M., Silva Filho, T. M., & Flach, P. (2017).\\n[8]\\nMario V. Wüthrich, Michael Merz (2023).\\n“Statistical Foundations of Actuarial Learning and its Applications”\\nSpringer Actuarial\\n© 2007 - 2024, scikit-learn developers (BSD License).\\nShow this page source\\n\\n1.17. Neural network models (supervised) — scikit-learn 1.4.2 documentation\\n\\nInstall\\nUser Guide\\nAPI\\nExamples\\nCommunity\\nGetting Started\\nTutorial\\nWhat\\'s new\\nGlossary\\nDevelopment\\nFAQ\\nSupport\\nRelated packages\\nRoadmap\\nGovernance\\nAbout us\\nGitHub\\nOther Versions and Download\\nMore\\nGetting Started\\nTutorial\\nWhat\\'s new\\nGlossary\\nDevelopment\\nFAQ\\nSupport\\nRelated packages\\nRoadmap\\nGovernance\\nAbout us\\nGitHub\\nOther Versions and Download\\nToggle Menu\\nPrevUp\\nNext\\nscikit-learn 1.4.2\\nOther versions\\nPlease cite us if you use the software.\\n\\n1.17. Neural network models (supervised)\\n\\n1.17.1. Multi-layer Perceptron\\n\\n1.17.2. Classification\\n\\n1.17.3. Regression\\n\\n1.17.4. Regularization\\n\\n1.17.5. Algorithms\\n\\n1.17.6. Complexity\\n\\n1.17.7. Mathematical formulation\\n\\n1.17.8. Tips on Practical Use\\n\\n1.17.9. More control with warm_start\\n\\n1.17. Neural network models (supervised)¶\\n\\nWarning\\nThis implementation is not intended for large-scale applications. In particular,\\nscikit-learn offers no GPU support. For much faster, GPU-based implementations,\\nas well as frameworks offering much more flexibility to build deep learning\\narchitectures, see  Related Projects.\\n- 1.17.1. Multi-layer Perceptron¶\\nMulti-layer Perceptron (MLP) is a supervised learning algorithm that learns\\na function (f(\\\\cdot): R^m \\\\rightarrow R^o) by training on a dataset,\\nwhere (m) is the number of dimensions for input and (o) is the\\nnumber of dimensions for output. Given a set of features (X = {x_1, x_2, ..., x_m})\\nand a target (y), it can learn a non-linear function approximator for either\\nclassification or regression. It is different from logistic regression, in that\\nbetween the input and the output layer, there can be one or more non-linear\\nlayers, called hidden layers. Figure 1 shows a one hidden layer MLP with scalar\\noutput.\\nFigure 1 : One hidden layer MLP.¶\\nThe leftmost layer, known as the input layer, consists of a set of neurons\\n({x_i | x_1, x_2, ..., x_m}) representing the input features. Each\\nneuron in the hidden layer transforms the values from the previous layer with\\na weighted linear summation (w_1x_1 + w_2x_2 + ... + w_mx_m), followed\\nby a non-linear activation function (g(\\\\cdot):R \\\\rightarrow R) - like\\nthe hyperbolic tan function. The output layer receives the values from the\\nlast hidden layer and transforms them into output values.\\nThe module contains the public attributes coefs_ and intercepts_.\\ncoefs_ is a list of weight matrices, where weight matrix at index\\n(i) represents the weights between layer (i) and layer\\n(i+1). intercepts_ is a list of bias vectors, where the vector\\nat index (i) represents the bias values added to layer (i+1).\\nThe advantages of Multi-layer Perceptron are:\\nCapability to learn non-linear models.\\nCapability to learn models in real-time (on-line learning)\\nusing partial_fit.\\nThe disadvantages of Multi-layer Perceptron (MLP) include:\\nMLP with hidden layers have a non-convex loss function where there exists\\nmore than one local minimum. Therefore different random weight\\ninitializations can lead to different validation accuracy.\\nMLP requires tuning a number of hyperparameters such as the number of\\nhidden neurons, layers, and iterations.\\nMLP is sensitive to feature scaling.\\nPlease see Tips on Practical Use section that addresses\\nsome of these disadvantages.\\n- 1.17.2. Classification¶\\nClass MLPClassifier implements a multi-layer perceptron (MLP) algorithm\\nthat trains using Backpropagation.\\nMLP trains on two arrays: array X of size (n_samples, n_features), which holds\\nthe training samples represented as floating point feature vectors; and array\\ny of size (n_samples,), which holds the target values (class labels) for the\\ntraining samples:\\n\\nfrom sklearn.neural_network import MLPClassifier\\nX = [[0., 0.], [1., 1.]]\\ny = [0, 1]\\nclf = MLPClassifier(solver=\\'lbfgs\\', alpha=1e-5,\\n...                     hidden_layer_sizes=(5, 2), random_state=1)\\n...\\nclf.fit(X, y)\\nMLPClassifier(alpha=1e-05, hidden_layer_sizes=(5, 2), random_state=1,\\nsolver=\\'lbfgs\\')\\nAfter fitting (training), the model can predict labels for new samples:\\nclf.predict([[2., 2.], [-1., -2.]])\\narray([1, 0])\\nMLP can fit a non-linear model to the training data. clf.coefs_\\ncontains the weight matrices that constitute the model parameters:\\n[coef.shape for coef in clf.coefs_]\\n[(2, 5), (5, 2), (2, 1)]\\nCurrently, MLPClassifier supports only the\\nCross-Entropy loss function, which allows probability estimates by running the\\npredict_proba method.\\nMLP trains using Backpropagation. More precisely, it trains using some form of\\ngradient descent and the gradients are calculated using Backpropagation. For\\nclassification, it minimizes the Cross-Entropy loss function, giving a vector\\nof probability estimates (P(y|x)) per sample (x):\\nclf.predict_proba([[2., 2.], [1., 2.]])\\narray([[1.967...e-04, 9.998...-01],\\n[1.967...e-04, 9.998...-01]])\\nMLPClassifier supports multi-class classification by\\napplying Softmax\\nas the output function.\\nFurther, the model supports multi-label classification\\nin which a sample can belong to more than one class. For each class, the raw\\noutput passes through the logistic function. Values larger or equal to 0.5\\nare rounded to 1, otherwise to 0. For a predicted output of a sample, the\\nindices where the value is 1 represents the assigned classes of that sample:\\nX = [[0., 0.], [1., 1.]]\\ny = [[0, 1], [1, 1]]\\nclf = MLPClassifier(solver=\\'lbfgs\\', alpha=1e-5,\\n...                     hidden_layer_sizes=(15,), random_state=1)\\n...\\nclf.fit(X, y)\\nMLPClassifier(alpha=1e-05, hidden_layer_sizes=(15,), random_state=1,\\nsolver=\\'lbfgs\\')\\nclf.predict([[1., 2.]])\\narray([[1, 1]])\\nclf.predict([[0., 0.]])\\narray([[0, 1]])\\nSee the examples below and the docstring of\\nMLPClassifier.fit for further information.\\nExamples:\\nCompare Stochastic learning strategies for MLPClassifier\\nVisualization of MLP weights on MNIST\\n- 1.17.3. Regression¶\\nClass MLPRegressor implements a multi-layer perceptron (MLP) that\\ntrains using backpropagation with no activation function in the output layer,\\nwhich can also be seen as using the identity function as activation function.\\nTherefore, it uses the square error as the loss function, and the output is a\\nset of continuous values.\\nMLPRegressor also supports multi-output regression, in\\nwhich a sample can have more than one target.\\n- 1.17.4. Regularization¶\\nBoth MLPRegressor and MLPClassifier use parameter alpha\\nfor regularization (L2 regularization) term which helps in avoiding overfitting\\nby penalizing weights with large magnitudes. Following plot displays varying\\ndecision function with value of alpha.\\nSee the examples below for further information.\\nExamples:\\nVarying regularization in Multi-layer Perceptron\\n- 1.17.5. Algorithms¶\\nMLP trains using Stochastic Gradient Descent,\\nAdam, or\\nL-BFGS.\\nStochastic Gradient Descent (SGD) updates parameters using the gradient of the\\nloss function with respect to a parameter that needs adaptation, i.e.\\n[w \\\\leftarrow w - \\\\eta (\\\\alpha \\\\frac{\\\\partial R(w)}{\\\\partial w}\\n+ \\\\frac{\\\\partial Loss}{\\\\partial w})]\\nwhere (\\\\eta) is the learning rate which controls the step-size in\\nthe parameter space search.  (Loss) is the loss function used\\nfor the network.\\nMore details can be found in the documentation of\\nSGD\\nAdam is similar to SGD in a sense that it is a stochastic optimizer, but it can\\nautomatically adjust the amount to update parameters based on adaptive estimates\\nof lower-order moments.\\nWith SGD or Adam, training supports online and mini-batch learning.\\nL-BFGS is a solver that approximates the Hessian matrix which represents the\\nsecond-order partial derivative of a function. Further it approximates the\\ninverse of the Hessian matrix to perform parameter updates. The implementation\\nuses the Scipy version of L-BFGS.\\nIf the selected solver is ‘L-BFGS’, training does not support online nor\\nmini-batch learning.\\n- 1.17.6. Complexity¶\\nSuppose there are (n) training samples, (m) features, (k)\\nhidden layers, each containing (h) neurons - for simplicity, and (o)\\noutput neurons.  The time complexity of backpropagation is\\n(O(n\\\\cdot m \\\\cdot h^k \\\\cdot o \\\\cdot i)), where (i) is the number\\nof iterations. Since backpropagation has a high time complexity, it is advisable\\nto start with smaller number of hidden neurons and few hidden layers for\\ntraining.\\n- 1.17.7. Mathematical formulation¶\\nGiven a set of training examples ((x_1, y_1), (x_2, y_2), \\\\ldots, (x_n, y_n))\\nwhere (x_i \\\\in \\\\mathbf{R}^n) and (y_i \\\\in {0, 1}), a one hidden\\nlayer one hidden neuron MLP learns the function (f(x) = W_2 g(W_1^T x + b_1) + b_2)\\nwhere (W_1 \\\\in \\\\mathbf{R}^m) and (W_2, b_1, b_2 \\\\in \\\\mathbf{R}) are\\nmodel parameters. (W_1, W_2) represent the weights of the input layer and\\nhidden layer, respectively; and (b_1, b_2) represent the bias added to\\nthe hidden layer and the output layer, respectively.\\n(g(\\\\cdot) : R \\\\rightarrow R) is the activation function, set by default as\\nthe hyperbolic tan. It is given as,\\n[g(z)= \\\\frac{e^z-e^{-z}}{e^z+e^{-z}}]\\nFor binary classification, (f(x)) passes through the logistic function\\n(g(z)=1/(1+e^{-z})) to obtain output values between zero and one. A\\nthreshold, set to 0.5, would assign samples of outputs larger or equal 0.5\\nto the positive class, and the rest to the negative class.\\nIf there are more than two classes, (f(x)) itself would be a vector of\\nsize (n_classes,). Instead of passing through logistic function, it passes\\nthrough the softmax function, which is written as,\\n[\\\\text{softmax}(z)i = \\\\frac{\\\\exp(z_i)}{\\\\sum{l=1}^k\\\\exp(z_l)}]\\nwhere (z_i) represents the (i) th element of the input to softmax,\\nwhich corresponds to class (i), and (K) is the number of classes.\\nThe result is a vector containing the probabilities that sample (x)\\nbelong to each class. The output is the class with the highest probability.\\nIn regression, the output remains as (f(x)); therefore, output activation\\nfunction is just the identity function.\\nMLP uses different loss functions depending on the problem type. The loss\\nfunction for classification is Average Cross-Entropy, which in binary case is\\ngiven as,\\n[Loss(\\\\hat{y},y,W) = -\\\\dfrac{1}{n}\\\\sum_{i=0}^n(y_i \\\\ln {\\\\hat{y_i}} + (1-y_i) \\\\ln{(1-\\\\hat{y_i})}) + \\\\dfrac{\\\\alpha}{2n} ||W||2^2]\\nwhere (\\\\alpha ||W||_2^2) is an L2-regularization term (aka penalty)\\nthat penalizes complex models; and (\\\\alpha > 0) is a non-negative\\nhyperparameter that controls the magnitude of the penalty.\\nFor regression, MLP uses the Mean Square Error loss function; written as,\\n[Loss(\\\\hat{y},y,W) = \\\\frac{1}{2n}\\\\sum{i=0}^n||\\\\hat{y}i - y_i ||_2^2 + \\\\frac{\\\\alpha}{2n} ||W||_2^2]\\nStarting from initial random weights, multi-layer perceptron (MLP) minimizes\\nthe loss function by repeatedly updating these weights. After computing the\\nloss, a backward pass propagates it from the output layer to the previous\\nlayers, providing each weight parameter with an update value meant to decrease\\nthe loss.\\nIn gradient descent, the gradient (\\\\nabla Loss{W}) of the loss with respect\\nto the weights is computed and deducted from (W).\\nMore formally, this is expressed as,\\n[W^{i+1} = W^i - \\\\epsilon \\\\nabla {Loss}_{W}^{i}]\\nwhere (i) is the iteration step, and (\\\\epsilon) is the learning rate\\nwith a value larger than 0.\\nThe algorithm stops when it reaches a preset maximum number of iterations; or\\nwhen the improvement in loss is below a certain, small number.\\n- 1.17.8. Tips on Practical Use¶\\nMulti-layer Perceptron is sensitive to feature scaling, so it\\nis highly recommended to scale your data. For example, scale each\\nattribute on the input vector X to [0, 1] or [-1, +1], or standardize\\nit to have mean 0 and variance 1. Note that you must apply the same\\nscaling to the test set for meaningful results.\\nYou can use StandardScaler for standardization.\\nfrom sklearn.preprocessing import StandardScaler\\nscaler = StandardScaler()\\n\\nDon\\'t cheat - fit only on training data\\n\\nscaler.fit(X_train)\\nX_train = scaler.transform(X_train)\\n\\napply same transformation to test data\\n\\nX_test = scaler.transform(X_test)\\nAn alternative and recommended approach is to use\\nStandardScaler in a\\nPipeline\\nFinding a reasonable regularization parameter (\\\\alpha) is best done\\nusing GridSearchCV, usually in the range\\n10.0 ** -np.arange(1, 7).\\nEmpirically, we observed that L-BFGS converges faster and\\nwith better solutions on small datasets. For relatively large\\ndatasets, however, Adam is very robust. It usually converges\\nquickly and gives pretty good performance. SGD with momentum or\\nnesterov’s momentum, on the other hand, can perform better than\\nthose two algorithms if learning rate is correctly tuned.\\n- 1.17.9. More control with warm_start¶\\nIf you want more control over stopping criteria or learning rate in SGD,\\nor want to do additional monitoring, using warm_start=True and\\nmax_iter=1 and iterating yourself can be helpful:\\nX = [[0., 0.], [1., 1.]]\\ny = [0, 1]\\nclf = MLPClassifier(hidden_layer_sizes=(15,), random_state=1, max_iter=1, warm_start=True)\\nfor i in range(10):\\n...     clf.fit(X, y)\\n...     # additional monitoring / inspection\\nMLPClassifier(...\\nReferences:\\n“Learning representations by back-propagating errors.”\\nRumelhart, David E., Geoffrey E. Hinton, and Ronald J. Williams.\\n“Stochastic Gradient Descent” L. Bottou - Website, 2010.\\n“Backpropagation”\\nAndrew Ng, Jiquan Ngiam, Chuan Yu Foo, Yifan Mai, Caroline Suen - Website, 2011.\\n“Efficient BackProp”\\nY. LeCun, L. Bottou, G. Orr, K. Müller - In Neural Networks: Tricks\\nof the Trade 1998.\\n“Adam: A method for stochastic optimization.”\\nKingma, Diederik, and Jimmy Ba (2014)\\n© 2007 - 2024, scikit-learn developers (BSD License).\\nShow this page source\\n\\n2.1. Gaussian mixture models — scikit-learn 1.4.2 documentation\\n\\nInstall\\nUser Guide\\nAPI\\nExamples\\nCommunity\\nGetting Started\\nTutorial\\nWhat\\'s new\\nGlossary\\nDevelopment\\nFAQ\\nSupport\\nRelated packages\\nRoadmap\\nGovernance\\nAbout us\\nGitHub\\nOther Versions and Download\\nMore\\nGetting Started\\nTutorial\\nWhat\\'s new\\nGlossary\\nDevelopment\\nFAQ\\nSupport\\nRelated packages\\nRoadmap\\nGovernance\\nAbout us\\nGitHub\\nOther Versions and Download\\nToggle Menu\\nPrevUp\\nNext\\nscikit-learn 1.4.2\\nOther versions\\nPlease cite us if you use the software.\\n\\n2.1. Gaussian mixture models\\n\\n2.1.1. Gaussian Mixture\\n\\n2.1.2. Variational Bayesian Gaussian Mixture\\n2.1.2.1. The Dirichlet Process\\n\\n2.1. Gaussian mixture models¶\\n\\nsklearn.mixture is a package which enables one to learn\\nGaussian Mixture Models (diagonal, spherical, tied and full covariance\\nmatrices supported), sample them, and estimate them from\\ndata. Facilities to help determine the appropriate number of\\ncomponents are also provided.\\nTwo-component Gaussian mixture model: data points, and equi-probability\\nsurfaces of the model.¶\\nA Gaussian mixture model is a probabilistic model that assumes all the\\ndata points are generated from a mixture of a finite number of\\nGaussian distributions with unknown parameters. One can think of\\nmixture models as generalizing k-means clustering to incorporate\\ninformation about the covariance structure of the data as well as the\\ncenters of the latent Gaussians.\\nScikit-learn implements different classes to estimate Gaussian\\nmixture models, that correspond to different estimation strategies,\\ndetailed below.\\n- 2.1.1. Gaussian Mixture¶\\nThe GaussianMixture object implements the\\nexpectation-maximization (EM)\\nalgorithm for fitting mixture-of-Gaussian models. It can also draw\\nconfidence ellipsoids for multivariate models, and compute the\\nBayesian Information Criterion to assess the number of clusters in the\\ndata. A GaussianMixture.fit method is provided that learns a Gaussian\\nMixture Model from train data. Given test data, it can assign to each\\nsample the Gaussian it most probably belongs to using\\nthe GaussianMixture.predict method.\\nThe GaussianMixture comes with different options to constrain the\\ncovariance of the difference classes estimated: spherical, diagonal, tied or\\nfull covariance.\\nExamples:\\nSee GMM covariances for an example of\\nusing the Gaussian mixture as clustering on the iris dataset.\\nSee Density Estimation for a Gaussian mixture for an example on plotting the\\ndensity estimation.\\nPros and cons of class GaussianMixture\\nClick for more details\\n¶\\nPros:\\nSpeed:\\nIt is the fastest algorithm for learning mixture models\\nAgnostic:\\nAs this algorithm maximizes only the likelihood, it\\nwill not bias the means towards zero, or bias the cluster sizes to\\nhave specific structures that might or might not apply.\\nCons:\\nSingularities:\\nWhen one has insufficiently many points per\\nmixture, estimating the covariance matrices becomes difficult,\\nand the algorithm is known to diverge and find solutions with\\ninfinite likelihood unless one regularizes the covariances artificially.\\nNumber of components:\\nThis algorithm will always use all the\\ncomponents it has access to, needing held-out data\\nor information theoretical criteria to decide how many components to use\\nin the absence of external cues.\\nSelecting the number of components in a classical Gaussian Mixture model\\nClick for more details\\n¶\\nThe BIC criterion can be used to select the number of components in a Gaussian\\nMixture in an efficient way. In theory, it recovers the true number of\\ncomponents only in the asymptotic regime (i.e. if much data is available and\\nassuming that the data was actually generated i.i.d. from a mixture of Gaussian\\ndistribution). Note that using a Variational Bayesian Gaussian mixture\\navoids the specification of the number of components for a Gaussian mixture\\nmodel.\\nExamples:\\nSee Gaussian Mixture Model Selection for an example\\nof model selection performed with classical Gaussian mixture.\\nEstimation algorithm expectation-maximization\\nClick for more details\\n¶\\nThe main difficulty in learning Gaussian mixture models from unlabeled\\ndata is that one usually doesn’t know which points came from\\nwhich latent component (if one has access to this information it gets\\nvery easy to fit a separate Gaussian distribution to each set of\\npoints). Expectation-maximization\\nis a well-founded statistical\\nalgorithm to get around this problem by an iterative process. First\\none assumes random components (randomly centered on data points,\\nlearned from k-means, or even just normally distributed around the\\norigin) and computes for each point a probability of being generated by\\neach component of the model. Then, one tweaks the\\nparameters to maximize the likelihood of the data given those\\nassignments. Repeating this process is guaranteed to always converge\\nto a local optimum.\\nChoice of the Initialization method\\nClick for more details\\n¶\\nThere is a choice of four initialization methods (as well as inputting user defined\\ninitial means) to generate the initial centers for the model components:\\nk-means (default)This applies a traditional k-means clustering algorithm.\\nThis can be computationally expensive compared to other initialization methods.\\nk-means++This uses the initialization method of k-means clustering: k-means++.\\nThis will pick the first center at random from the data. Subsequent centers will be\\nchosen from a weighted distribution of the data favouring points further away from\\nexisting centers. k-means++ is the default initialization for k-means so will be\\nquicker than running a full k-means but can still take a significant amount of\\ntime for large data sets with many components.\\nrandom_from_dataThis will pick random data points from the input data as the initial\\ncenters. This is a very fast method of initialization but can produce non-convergent\\nresults if the chosen points are too close to each other.\\nrandomCenters are chosen as a small perturbation away from the mean of all data.\\nThis method is simple but can lead to the model taking longer to converge.\\nExamples:\\nSee GMM Initialization Methods for an example of\\nusing different initializations in Gaussian Mixture.\\n- 2.1.2. Variational Bayesian Gaussian Mixture¶\\nThe BayesianGaussianMixture object implements a variant of the\\nGaussian mixture model with variational inference algorithms. The API is\\nsimilar to the one defined by GaussianMixture.\\nEstimation algorithm: variational inference\\nVariational inference is an extension of expectation-maximization that\\nmaximizes a lower bound on model evidence (including\\npriors) instead of data likelihood. The principle behind\\nvariational methods is the same as expectation-maximization (that is\\nboth are iterative algorithms that alternate between finding the\\nprobabilities for each point to be generated by each mixture and\\nfitting the mixture to these assigned points), but variational\\nmethods add regularization by integrating information from prior\\ndistributions. This avoids the singularities often found in\\nexpectation-maximization solutions but introduces some subtle biases\\nto the model. Inference is often notably slower, but not usually as\\nmuch so as to render usage unpractical.\\nDue to its Bayesian nature, the variational algorithm needs more hyperparameters\\nthan expectation-maximization, the most important of these being the\\nconcentration parameter weight_concentration_prior. Specifying a low value\\nfor the concentration prior will make the model put most of the weight on a few\\ncomponents and set the remaining components’ weights very close to zero. High\\nvalues of the concentration prior will allow a larger number of components to\\nbe active in the mixture.\\nThe parameters implementation of the BayesianGaussianMixture class\\nproposes two types of prior for the weights distribution: a finite mixture model\\nwith Dirichlet distribution and an infinite mixture model with the Dirichlet\\nProcess. In practice Dirichlet Process inference algorithm is approximated and\\nuses a truncated distribution with a fixed maximum number of components (called\\nthe Stick-breaking representation). The number of components actually used\\nalmost always depends on the data.\\nThe next figure compares the results obtained for the different type of the\\nweight concentration prior (parameter weight_concentration_prior_type)\\nfor different values of weight_concentration_prior.\\nHere, we can see the value of the weight_concentration_prior parameter\\nhas a strong impact on the effective number of active components obtained. We\\ncan also notice that large values for the concentration weight prior lead to\\nmore uniform weights when the type of prior is ‘dirichlet_distribution’ while\\nthis is not necessarily the case for the ‘dirichlet_process’ type (used by\\ndefault).\\nThe examples below compare Gaussian mixture models with a fixed number of\\ncomponents, to the variational Gaussian mixture models with a Dirichlet process\\nprior. Here, a classical Gaussian mixture is fitted with 5 components on a\\ndataset composed of 2 clusters. We can see that the variational Gaussian mixture\\nwith a Dirichlet process prior is able to limit itself to only 2 components\\nwhereas the Gaussian mixture fits the data with a fixed number of components\\nthat has to be set a priori by the user. In this case the user has selected\\nn_components=5 which does not match the true generative distribution of this\\ntoy dataset. Note that with very little observations, the variational Gaussian\\nmixture models with a Dirichlet process prior can take a conservative stand, and\\nfit only one component.\\nOn the following figure we are fitting a dataset not well-depicted by a\\nGaussian mixture. Adjusting the weight_concentration_prior, parameter of the\\nBayesianGaussianMixture controls the number of components used to fit\\nthis data. We also present on the last two plots a random sampling generated\\nfrom the two resulting mixtures.\\nExamples:\\nSee Gaussian Mixture Model Ellipsoids for an example on\\nplotting the confidence ellipsoids for both GaussianMixture\\nand BayesianGaussianMixture.\\nGaussian Mixture Model Sine Curve shows using\\nGaussianMixture and BayesianGaussianMixture to fit a\\nsine wave.\\nSee Concentration Prior Type Analysis of Variation Bayesian Gaussian Mixture\\nfor an example plotting the confidence ellipsoids for the\\nBayesianGaussianMixture with different\\nweight_concentration_prior_type for different values of the parameter\\nweight_concentration_prior.\\nPros and cons of variational inference with BayesianGaussianMixture\\nClick for more details\\n¶\\nPros:\\nAutomatic selection:\\nwhen weight_concentration_prior is small enough and\\nn_components is larger than what is found necessary by the model, the\\nVariational Bayesian mixture model has a natural tendency to set some mixture\\nweights values close to zero. This makes it possible to let the model choose\\na suitable number of effective components automatically. Only an upper bound\\nof this number needs to be provided. Note however that the “ideal” number of\\nactive components is very application specific and is typically ill-defined\\nin a data exploration setting.\\nLess sensitivity to the number of parameters:\\nunlike finite models, which will\\nalmost always use all components as much as they can, and hence will produce\\nwildly different solutions for different numbers of components, the\\nvariational inference with a Dirichlet process prior\\n(weight_concentration_prior_type=\\'dirichlet_process\\') won’t change much\\nwith changes to the parameters, leading to more stability and less tuning.\\nRegularization:\\ndue to the incorporation of prior information,\\nvariational solutions have less pathological special cases than\\nexpectation-maximization solutions.\\nCons:\\nSpeed:\\nthe extra parametrization necessary for variational inference makes\\ninference slower, although not by much.\\nHyperparameters:\\nthis algorithm needs an extra hyperparameter\\nthat might need experimental tuning via cross-validation.\\nBias:\\nthere are many implicit biases in the inference algorithms (and also in\\nthe Dirichlet process if used), and whenever there is a mismatch between\\nthese biases and the data it might be possible to fit better models using a\\nfinite mixture.\\n2.1.2.1. The Dirichlet Process¶\\nHere we describe variational inference algorithms on Dirichlet process\\nmixture. The Dirichlet process is a prior probability distribution on\\nclusterings with an infinite, unbounded, number of partitions.\\nVariational techniques let us incorporate this prior structure on\\nGaussian mixture models at almost no penalty in inference time, comparing\\nwith a finite Gaussian mixture model.\\nAn important question is how can the Dirichlet process use an infinite,\\nunbounded number of clusters and still be consistent. While a full explanation\\ndoesn’t fit this manual, one can think of its stick breaking process\\nanalogy to help understanding it. The stick breaking process is a generative\\nstory for the Dirichlet process. We start with a unit-length stick and in each\\nstep we break off a portion of the remaining stick. Each time, we associate the\\nlength of the piece of the stick to the proportion of points that falls into a\\ngroup of the mixture. At the end, to represent the infinite mixture, we\\nassociate the last remaining piece of the stick to the proportion of points\\nthat don’t fall into all the other groups. The length of each piece is a random\\nvariable with probability proportional to the concentration parameter. Smaller\\nvalues of the concentration will divide the unit-length into larger pieces of\\nthe stick (defining more concentrated distribution). Larger concentration\\nvalues will create smaller pieces of the stick (increasing the number of\\ncomponents with non zero weights).\\nVariational inference techniques for the Dirichlet process still work\\nwith a finite approximation to this infinite mixture model, but\\ninstead of having to specify a priori how many components one wants to\\nuse, one just specifies the concentration parameter and an upper bound\\non the number of mixture components (this upper bound, assuming it is\\nhigher than the “true” number of components, affects only algorithmic\\ncomplexity, not the actual number of components used).\\n© 2007 - 2024, scikit-learn developers (BSD License).\\nShow this page source\\n\\n2.2. Manifold learning — scikit-learn 1.4.2 documentation\\n\\nInstall\\nUser Guide\\nAPI\\nExamples\\nCommunity\\nGetting Started\\nTutorial\\nWhat\\'s new\\nGlossary\\nDevelopment\\nFAQ\\nSupport\\nRelated packages\\nRoadmap\\nGovernance\\nAbout us\\nGitHub\\nOther Versions and Download\\nMore\\nGetting Started\\nTutorial\\nWhat\\'s new\\nGlossary\\nDevelopment\\nFAQ\\nSupport\\nRelated packages\\nRoadmap\\nGovernance\\nAbout us\\nGitHub\\nOther Versions and Download\\nToggle Menu\\nPrevUp\\nNext\\nscikit-learn 1.4.2\\nOther versions\\nPlease cite us if you use the software.\\n\\n2.2. Manifold learning\\n\\n2.2.1. Introduction\\n\\n2.2.2. Isomap\\n\\n2.2.3. Locally Linear Embedding\\n\\n2.2.4. Modified Locally Linear Embedding\\n\\n2.2.5. Hessian Eigenmapping\\n\\n2.2.6. Spectral Embedding\\n\\n2.2.7. Local Tangent Space Alignment\\n\\n2.2.8. Multi-dimensional Scaling (MDS)\\n\\n2.2.9. t-distributed Stochastic Neighbor Embedding (t-SNE)\\n\\n2.2.10. Tips on practical use\\n\\n2.2. Manifold learning¶\\n\\nLook for the bare necessities\\nThe simple bare necessities\\nForget about your worries and your strife\\nI mean the bare necessities\\nOld Mother Nature’s recipes\\nThat bring the bare necessities of life\\n– Baloo’s song [The Jungle Book]\\nManifold learning is an approach to non-linear dimensionality reduction.\\nAlgorithms for this task are based on the idea that the dimensionality of\\nmany data sets is only artificially high.\\n- 2.2.1. Introduction¶\\nHigh-dimensional datasets can be very difficult to visualize.  While data\\nin two or three dimensions can be plotted to show the inherent\\nstructure of the data, equivalent high-dimensional plots are much less\\nintuitive.  To aid visualization of the structure of a dataset, the\\ndimension must be reduced in some way.\\nThe simplest way to accomplish this dimensionality reduction is by taking\\na random projection of the data.  Though this allows some degree of\\nvisualization of the data structure, the randomness of the choice leaves much\\nto be desired.  In a random projection, it is likely that the more\\ninteresting structure within the data will be lost.\\nTo address this concern, a number of supervised and unsupervised linear\\ndimensionality reduction frameworks have been designed, such as Principal\\nComponent Analysis (PCA), Independent Component Analysis, Linear\\nDiscriminant Analysis, and others.  These algorithms define specific\\nrubrics to choose an “interesting” linear projection of the data.\\nThese methods can be powerful, but often miss important non-linear\\nstructure in the data.\\nManifold Learning can be thought of as an attempt to generalize linear\\nframeworks like PCA to be sensitive to non-linear structure in data. Though\\nsupervised variants exist, the typical manifold learning problem is\\nunsupervised: it learns the high-dimensional structure of the data\\nfrom the data itself, without the use of predetermined classifications.\\nExamples:\\nSee Manifold learning on handwritten digits: Locally Linear Embedding, Isomap… for an example of\\ndimensionality reduction on handwritten digits.\\nSee Comparison of Manifold Learning methods for an example of\\ndimensionality reduction on a toy “S-curve” dataset.\\nThe manifold learning implementations available in scikit-learn are\\nsummarized below\\n- 2.2.2. Isomap¶\\nOne of the earliest approaches to manifold learning is the Isomap\\nalgorithm, short for Isometric Mapping.  Isomap can be viewed as an\\nextension of Multi-dimensional Scaling (MDS) or Kernel PCA.\\nIsomap seeks a lower-dimensional embedding which maintains geodesic\\ndistances between all points.  Isomap can be performed with the object\\nIsomap.\\nComplexity\\nClick for more details\\n¶\\nThe Isomap algorithm comprises three stages:\\nNearest neighbor search.  Isomap uses\\nBallTree for efficient neighbor search.\\nThe cost is approximately (O[D \\\\log(k) N \\\\log(N)]), for (k)\\nnearest neighbors of (N) points in (D) dimensions.\\nShortest-path graph search.  The most efficient known algorithms\\nfor this are Dijkstra’s Algorithm, which is approximately\\n(O[N^2(k + \\\\log(N))]), or the Floyd-Warshall algorithm, which\\nis (O[N^3]).  The algorithm can be selected by the user with\\nthe path_method keyword of Isomap.  If unspecified, the code\\nattempts to choose the best algorithm for the input data.\\nPartial eigenvalue decomposition.  The embedding is encoded in the\\neigenvectors corresponding to the (d) largest eigenvalues of the\\n(N \\\\times N) isomap kernel.  For a dense solver, the cost is\\napproximately (O[d N^2]).  This cost can often be improved using\\nthe ARPACK solver.  The eigensolver can be specified by the user\\nwith the eigen_solver keyword of Isomap.  If unspecified, the\\ncode attempts to choose the best algorithm for the input data.\\nThe overall complexity of Isomap is\\n(O[D \\\\log(k) N \\\\log(N)] + O[N^2(k + \\\\log(N))] + O[d N^2]).\\n(N) : number of training data points\\n(D) : input dimension\\n(k) : number of nearest neighbors\\n(d) : output dimension\\nReferences:\\n“A global geometric framework for nonlinear dimensionality reduction”\\nTenenbaum, J.B.; De Silva, V.; & Langford, J.C.  Science 290 (5500)\\n- 2.2.3. Locally Linear Embedding¶\\nLocally linear embedding (LLE) seeks a lower-dimensional projection of the data\\nwhich preserves distances within local neighborhoods.  It can be thought\\nof as a series of local Principal Component Analyses which are globally\\ncompared to find the best non-linear embedding.\\nLocally linear embedding can be performed with function\\nlocally_linear_embedding or its object-oriented counterpart\\nLocallyLinearEmbedding.\\nComplexity\\nClick for more details\\n¶\\nThe standard LLE algorithm comprises three stages:\\nNearest Neighbors Search.  See discussion under Isomap above.\\nWeight Matrix Construction. (O[D N k^3]).\\nThe construction of the LLE weight matrix involves the solution of a\\n(k \\\\times k) linear equation for each of the (N) local\\nneighborhoods\\nPartial Eigenvalue Decomposition. See discussion under Isomap above.\\nThe overall complexity of standard LLE is\\n(O[D \\\\log(k) N \\\\log(N)] + O[D N k^3] + O[d N^2]).\\n(N) : number of training data points\\n(D) : input dimension\\n(k) : number of nearest neighbors\\n(d) : output dimension\\nReferences:\\n“Nonlinear dimensionality reduction by locally linear embedding”\\nRoweis, S. & Saul, L.  Science 290:2323 (2000)\\n- 2.2.4. Modified Locally Linear Embedding¶\\nOne well-known issue with LLE is the regularization problem.  When the number\\nof neighbors is greater than the number of input dimensions, the matrix\\ndefining each local neighborhood is rank-deficient.  To address this, standard\\nLLE applies an arbitrary regularization parameter (r), which is chosen\\nrelative to the trace of the local weight matrix.  Though it can be shown\\nformally that as (r \\\\to 0), the solution converges to the desired\\nembedding, there is no guarantee that the optimal solution will be found\\nfor (r > 0).  This problem manifests itself in embeddings which distort\\nthe underlying geometry of the manifold.\\nOne method to address the regularization problem is to use multiple weight\\nvectors in each neighborhood.  This is the essence of modified locally\\nlinear embedding (MLLE).  MLLE can be  performed with function\\nlocally_linear_embedding or its object-oriented counterpart\\nLocallyLinearEmbedding, with the keyword method = \\'modified\\'.\\nIt requires n_neighbors > n_components.\\nComplexity\\nClick for more details\\n¶\\nThe MLLE algorithm comprises three stages:\\nNearest Neighbors Search.  Same as standard LLE\\nWeight Matrix Construction. Approximately\\n(O[D N k^3] + O[N (k-D) k^2]).  The first term is exactly equivalent\\nto that of standard LLE.  The second term has to do with constructing the\\nweight matrix from multiple weights.  In practice, the added cost of\\nconstructing the MLLE weight matrix is relatively small compared to the\\ncost of stages 1 and 3.\\nPartial Eigenvalue Decomposition. Same as standard LLE\\nThe overall complexity of MLLE is\\n(O[D \\\\log(k) N \\\\log(N)] + O[D N k^3] + O[N (k-D) k^2] + O[d N^2]).\\n(N) : number of training data points\\n(D) : input dimension\\n(k) : number of nearest neighbors\\n(d) : output dimension\\nReferences:\\n“MLLE: Modified Locally Linear Embedding Using Multiple Weights”\\nZhang, Z. & Wang, J.\\n- 2.2.5. Hessian Eigenmapping¶\\nHessian Eigenmapping (also known as Hessian-based LLE: HLLE) is another method\\nof solving the regularization problem of LLE.  It revolves around a\\nhessian-based quadratic form at each neighborhood which is used to recover\\nthe locally linear structure.  Though other implementations note its poor\\nscaling with data size, sklearn implements some algorithmic\\nimprovements which make its cost comparable to that of other LLE variants\\nfor small output dimension.  HLLE can be  performed with function\\nlocally_linear_embedding or its object-oriented counterpart\\nLocallyLinearEmbedding, with the keyword method = \\'hessian\\'.\\nIt requires n_neighbors > n_components * (n_components + 3) / 2.\\nComplexity\\nClick for more details\\n¶\\nThe HLLE algorithm comprises three stages:\\nNearest Neighbors Search.  Same as standard LLE\\nWeight Matrix Construction. Approximately\\n(O[D N k^3] + O[N d^6]).  The first term reflects a similar\\ncost to that of standard LLE.  The second term comes from a QR\\ndecomposition of the local hessian estimator.\\nPartial Eigenvalue Decomposition. Same as standard LLE\\nThe overall complexity of standard HLLE is\\n(O[D \\\\log(k) N \\\\log(N)] + O[D N k^3] + O[N d^6] + O[d N^2]).\\n(N) : number of training data points\\n(D) : input dimension\\n(k) : number of nearest neighbors\\n(d) : output dimension\\nReferences:\\n“Hessian Eigenmaps: Locally linear embedding techniques for\\nhigh-dimensional data”\\nDonoho, D. & Grimes, C. Proc Natl Acad Sci USA. 100:5591 (2003)\\n- 2.2.6. Spectral Embedding¶\\nSpectral Embedding is an approach to calculating a non-linear embedding.\\nScikit-learn implements Laplacian Eigenmaps, which finds a low dimensional\\nrepresentation of the data using a spectral decomposition of the graph\\nLaplacian. The graph generated can be considered as a discrete approximation of\\nthe low dimensional manifold in the high dimensional space. Minimization of a\\ncost function based on the graph ensures that points close to each other on\\nthe manifold are mapped close to each other in the low dimensional space,\\npreserving local distances. Spectral embedding can be  performed with the\\nfunction spectral_embedding or its object-oriented counterpart\\nSpectralEmbedding.\\nComplexity\\nClick for more details\\n¶\\nThe Spectral Embedding (Laplacian Eigenmaps) algorithm comprises three stages:\\nWeighted Graph Construction. Transform the raw input data into\\ngraph representation using affinity (adjacency) matrix representation.\\nGraph Laplacian Construction. unnormalized Graph Laplacian\\nis constructed as (L = D - A) for and normalized one as\\n(L = D^{-\\\\frac{1}{2}} (D - A) D^{-\\\\frac{1}{2}}).\\nPartial Eigenvalue Decomposition. Eigenvalue decomposition is\\ndone on graph Laplacian\\nThe overall complexity of spectral embedding is\\n(O[D \\\\log(k) N \\\\log(N)] + O[D N k^3] + O[d N^2]).\\n(N) : number of training data points\\n(D) : input dimension\\n(k) : number of nearest neighbors\\n(d) : output dimension\\nReferences:\\n“Laplacian Eigenmaps for Dimensionality Reduction\\nand Data Representation”\\nM. Belkin, P. Niyogi, Neural Computation, June 2003; 15 (6):1373-1396\\n- 2.2.7. Local Tangent Space Alignment¶\\nThough not technically a variant of LLE, Local tangent space alignment (LTSA)\\nis algorithmically similar enough to LLE that it can be put in this category.\\nRather than focusing on preserving neighborhood distances as in LLE, LTSA\\nseeks to characterize the local geometry at each neighborhood via its\\ntangent space, and performs a global optimization to align these local\\ntangent spaces to learn the embedding.  LTSA can be performed with function\\nlocally_linear_embedding or its object-oriented counterpart\\nLocallyLinearEmbedding, with the keyword method = \\'ltsa\\'.\\nComplexity\\nClick for more details\\n¶\\nThe LTSA algorithm comprises three stages:\\nNearest Neighbors Search.  Same as standard LLE\\nWeight Matrix Construction. Approximately\\n(O[D N k^3] + O[k^2 d]).  The first term reflects a similar\\ncost to that of standard LLE.\\nPartial Eigenvalue Decomposition. Same as standard LLE\\nThe overall complexity of standard LTSA is\\n(O[D \\\\log(k) N \\\\log(N)] + O[D N k^3] + O[k^2 d] + O[d N^2]).\\n(N) : number of training data points\\n(D) : input dimension\\n(k) : number of nearest neighbors\\n(d) : output dimension\\nReferences:\\n“Principal manifolds and nonlinear dimensionality reduction via\\ntangent space alignment”\\nZhang, Z. & Zha, H. Journal of Shanghai Univ. 8:406 (2004)\\n- 2.2.8. Multi-dimensional Scaling (MDS)¶\\nMultidimensional scaling\\n(MDS) seeks a low-dimensional\\nrepresentation of the data in which the distances respect well the\\ndistances in the original high-dimensional space.\\nIn general, MDS is a technique used for analyzing similarity or\\ndissimilarity data. It attempts to model similarity or dissimilarity data as\\ndistances in a geometric spaces. The data can be ratings of similarity between\\nobjects, interaction frequencies of molecules, or trade indices between\\ncountries.\\nThere exists two types of MDS algorithm: metric and non metric. In\\nscikit-learn, the class MDS implements both. In Metric MDS, the input\\nsimilarity matrix arises from a metric (and thus respects the triangular\\ninequality), the distances between output two points are then set to be as\\nclose as possible to the similarity or dissimilarity data. In the non-metric\\nversion, the algorithms will try to preserve the order of the distances, and\\nhence seek for a monotonic relationship between the distances in the embedded\\nspace and the similarities/dissimilarities.\\nLet (S) be the similarity matrix, and (X) the coordinates of the\\n(n) input points. Disparities (\\\\hat{d}{ij}) are transformation of\\nthe similarities chosen in some optimal ways. The objective, called the\\nstress, is then defined by (\\\\sum{i < j} d_{ij}(X) - \\\\hat{d}{ij}(X))\\nMetric MDS\\nClick for more details\\n¶\\nThe simplest metric MDS model, called absolute MDS, disparities are defined by\\n(\\\\hat{d}{ij} = S_{ij}). With absolute MDS, the value (S_{ij})\\nshould then correspond exactly to the distance between point (i) and\\n(j) in the embedding point.\\nMost commonly, disparities are set to (\\\\hat{d}{ij} = b S{ij}).\\nNonmetric MDS\\nClick for more details\\n¶\\nNon metric MDS focuses on the ordination of the data. If\\n(S_{ij} > S_{jk}), then the embedding should enforce (d_{ij} <\\nd_{jk}). For this reason, we discuss it in terms of dissimilarities\\n((\\\\delta_{ij})) instead of similarities ((S_{ij})). Note that\\ndissimilarities can easily be obtained from similarities through a simple\\ntransform, e.g. (\\\\delta_{ij}=c_1-c_2 S_{ij}) for some real constants\\n(c_1, c_2). A simple algorithm to enforce proper ordination is to use a\\nmonotonic regression of (d_{ij}) on (\\\\delta_{ij}), yielding\\ndisparities (\\\\hat{d}{ij}) in the same order as (\\\\delta{ij}).\\nA trivial solution to this problem is to set all the points on the origin. In\\norder to avoid that, the disparities (\\\\hat{d}{ij}) are normalized. Note\\nthat since we only care about relative ordering, our objective should be\\ninvariant to simple translation and scaling, however the stress used in metric\\nMDS is sensitive to scaling. To address this, non-metric MDS may use a\\nnormalized stress, known as Stress-1 defined as\\n[\\\\sqrt{\\\\frac{\\\\sum{i < j} (d_{ij} - \\\\hat{d}{ij})^2}{\\\\sum{i < j} d_{ij}^2}}.]\\nThe use of normalized Stress-1 can be enabled by setting normalized_stress=True,\\nhowever it is only compatible with the non-metric MDS problem and will be ignored\\nin the metric case.\\nReferences:\\n“Modern Multidimensional Scaling - Theory and Applications”\\nBorg, I.; Groenen P. Springer Series in Statistics (1997)\\n“Nonmetric multidimensional scaling: a numerical method”\\nKruskal, J. Psychometrika, 29 (1964)\\n“Multidimensional scaling by optimizing goodness of fit to a nonmetric hypothesis”\\nKruskal, J. Psychometrika, 29, (1964)\\n- 2.2.9. t-distributed Stochastic Neighbor Embedding (t-SNE)¶\\nt-SNE (TSNE) converts affinities of data points to probabilities.\\nThe affinities in the original space are represented by Gaussian joint\\nprobabilities and the affinities in the embedded space are represented by\\nStudent’s t-distributions. This allows t-SNE to be particularly sensitive\\nto local structure and has a few other advantages over existing techniques:\\nRevealing the structure at many scales on a single map\\nRevealing data that lie in multiple, different, manifolds or clusters\\nReducing the tendency to crowd points together at the center\\nWhile Isomap, LLE and variants are best suited to unfold a single continuous\\nlow dimensional manifold, t-SNE will focus on the local structure of the data\\nand will tend to extract clustered local groups of samples as highlighted on\\nthe S-curve example. This ability to group samples based on the local structure\\nmight be beneficial to visually disentangle a dataset that comprises several\\nmanifolds at once as is the case in the digits dataset.\\nThe Kullback-Leibler (KL) divergence of the joint\\nprobabilities in the original space and the embedded space will be minimized\\nby gradient descent. Note that the KL divergence is not convex, i.e.\\nmultiple restarts with different initializations will end up in local minima\\nof the KL divergence. Hence, it is sometimes useful to try different seeds\\nand select the embedding with the lowest KL divergence.\\nThe disadvantages to using t-SNE are roughly:\\nt-SNE is computationally expensive, and can take several hours on million-sample\\ndatasets where PCA will finish in seconds or minutes\\nThe Barnes-Hut t-SNE method is limited to two or three dimensional embeddings.\\nThe algorithm is stochastic and multiple restarts with different seeds can\\nyield different embeddings. However, it is perfectly legitimate to pick the\\nembedding with the least error.\\nGlobal structure is not explicitly preserved. This problem is mitigated by\\ninitializing points with PCA (using init=\\'pca\\').\\nOptimizing t-SNE\\nClick for more details\\n¶\\nThe main purpose of t-SNE is visualization of high-dimensional data. Hence,\\nit works best when the data will be embedded on two or three dimensions.\\nOptimizing the KL divergence can be a little bit tricky sometimes. There are\\nfive parameters that control the optimization of t-SNE and therefore possibly\\nthe quality of the resulting embedding:\\nperplexity\\nearly exaggeration factor\\nlearning rate\\nmaximum number of iterations\\nangle (not used in the exact method)\\nThe perplexity is defined as (k=2^{(S)}) where (S) is the Shannon\\nentropy of the conditional probability distribution. The perplexity of a\\n(k)-sided die is (k), so that (k) is effectively the number of\\nnearest neighbors t-SNE considers when generating the conditional probabilities.\\nLarger perplexities lead to more nearest neighbors and less sensitive to small\\nstructure. Conversely a lower perplexity considers a smaller number of\\nneighbors, and thus ignores more global information in favour of the\\nlocal neighborhood. As dataset sizes get larger more points will be\\nrequired to get a reasonable sample of the local neighborhood, and hence\\nlarger perplexities may be required. Similarly noisier datasets will require\\nlarger perplexity values to encompass enough local neighbors to see beyond\\nthe background noise.\\nThe maximum number of iterations is usually high enough and does not need\\nany tuning. The optimization consists of two phases: the early exaggeration\\nphase and the final optimization. During early exaggeration the joint\\nprobabilities in the original space will be artificially increased by\\nmultiplication with a given factor. Larger factors result in larger gaps\\nbetween natural clusters in the data. If the factor is too high, the KL\\ndivergence could increase during this phase. Usually it does not have to be\\ntuned. A critical parameter is the learning rate. If it is too low gradient\\ndescent will get stuck in a bad local minimum. If it is too high the KL\\ndivergence will increase during optimization. A heuristic suggested in\\nBelkina et al. (2019) is to set the learning rate to the sample size\\ndivided by the early exaggeration factor. We implement this heuristic\\nas learning_rate=\\'auto\\' argument. More tips can be found in\\nLaurens van der Maaten’s FAQ (see references). The last parameter, angle,\\nis a tradeoff between performance and accuracy. Larger angles imply that we\\ncan approximate larger regions by a single point, leading to better speed\\nbut less accurate results.\\n“How to Use t-SNE Effectively”\\nprovides a good discussion of the effects of the various parameters, as well\\nas interactive plots to explore the effects of different parameters.\\nBarnes-Hut t-SNE\\nClick for more details\\n¶\\nThe Barnes-Hut t-SNE that has been implemented here is usually much slower than\\nother manifold learning algorithms. The optimization is quite difficult\\nand the computation of the gradient is (O[d N log(N)]), where (d)\\nis the number of output dimensions and (N) is the number of samples. The\\nBarnes-Hut method improves on the exact method where t-SNE complexity is\\n(O[d N^2]), but has several other notable differences:\\nThe Barnes-Hut implementation only works when the target dimensionality is 3\\nor less. The 2D case is typical when building visualizations.\\nBarnes-Hut only works with dense input data. Sparse data matrices can only be\\nembedded with the exact method or can be approximated by a dense low rank\\nprojection for instance using PCA\\nBarnes-Hut is an approximation of the exact method. The approximation is\\nparameterized with the angle parameter, therefore the angle parameter is\\nunused when method=”exact”\\nBarnes-Hut is significantly more scalable. Barnes-Hut can be used to embed\\nhundred of thousands of data points while the exact method can handle\\nthousands of samples before becoming computationally intractable\\nFor visualization purpose (which is the main use case of t-SNE), using the\\nBarnes-Hut method is strongly recommended. The exact t-SNE method is useful\\nfor checking the theoretically properties of the embedding possibly in higher\\ndimensional space but limit to small datasets due to computational constraints.\\nAlso note that the digits labels roughly match the natural grouping found by\\nt-SNE while the linear 2D projection of the PCA model yields a representation\\nwhere label regions largely overlap. This is a strong clue that this data can\\nbe well separated by non linear methods that focus on the local structure (e.g.\\nan SVM with a Gaussian RBF kernel). However, failing to visualize well\\nseparated homogeneously labeled groups with t-SNE in 2D does not necessarily\\nimply that the data cannot be correctly classified by a supervised model. It\\nmight be the case that 2 dimensions are not high enough to accurately represent\\nthe internal structure of the data.\\nReferences:\\n“Visualizing High-Dimensional Data Using t-SNE”\\nvan der Maaten, L.J.P.; Hinton, G. Journal of Machine Learning Research\\n(2008)\\n“t-Distributed Stochastic Neighbor Embedding”\\nvan der Maaten, L.J.P.\\n“Accelerating t-SNE using Tree-Based Algorithms”\\nvan der Maaten, L.J.P.; Journal of Machine Learning Research 15(Oct):3221-3245, 2014.\\n“Automated optimized parameters for T-distributed stochastic neighbor\\nembedding improve visualization and analysis of large datasets”\\nBelkina, A.C., Ciccolella, C.O., Anno, R., Halpert, R., Spidlen, J.,\\nSnyder-Cappione, J.E., Nature Communications 10, 5415 (2019).\\n- 2.2.10. Tips on practical use¶\\nMake sure the same scale is used over all features. Because manifold\\nlearning methods are based on a nearest-neighbor search, the algorithm\\nmay perform poorly otherwise.  See StandardScaler\\nfor convenient ways of scaling heterogeneous data.\\nThe reconstruction error computed by each routine can be used to choose\\nthe optimal output dimension.  For a (d)-dimensional manifold embedded\\nin a (D)-dimensional parameter space, the reconstruction error will\\ndecrease as n_components is increased until n_components == d.\\nNote that noisy data can “short-circuit” the manifold, in essence acting\\nas a bridge between parts of the manifold that would otherwise be\\nwell-separated.  Manifold learning on noisy and/or incomplete data is\\nan active area of research.\\nCertain input configurations can lead to singular weight matrices, for\\nexample when more than two points in the dataset are identical, or when\\nthe data is split into disjointed groups.  In this case, solver=\\'arpack\\'\\nwill fail to find the null space.  The easiest way to address this is to\\nuse solver=\\'dense\\' which will work on a singular matrix, though it may\\nbe very slow depending on the number of input points.  Alternatively, one\\ncan attempt to understand the source of the singularity: if it is due to\\ndisjoint sets, increasing n_neighbors may help.  If it is due to\\nidentical points in the dataset, removing these points may help.\\nSee also\\nTotally Random Trees Embedding can also be useful to derive non-linear\\nrepresentations of feature space, also it does not perform\\ndimensionality reduction.\\n\\n2.3. Clustering — scikit-learn 1.4.2 documentation\\n\\nInstall\\nUser Guide\\nAPI\\nExamples\\nCommunity\\nGetting Started\\nTutorial\\nWhat\\'s new\\nGlossary\\nDevelopment\\nFAQ\\nSupport\\nRelated packages\\nRoadmap\\nGovernance\\nAbout us\\nGitHub\\nOther Versions and Download\\nMore\\nGetting Started\\nTutorial\\nWhat\\'s new\\nGlossary\\nDevelopment\\nFAQ\\nSupport\\nRelated packages\\nRoadmap\\nGovernance\\nAbout us\\nGitHub\\nOther Versions and Download\\nToggle Menu\\nPrevUp\\nNext\\nscikit-learn 1.4.2\\nOther versions\\nPlease cite us if you use the software.\\n\\n2.3. Clustering\\n\\n2.3.1. Overview of clustering methods\\n\\n2.3.2. K-means\\n2.3.2.1. Low-level parallelism\\n2.3.2.2. Mini Batch K-Means\\n\\n2.3.3. Affinity Propagation\\n\\n2.3.4. Mean Shift\\n\\n2.3.5. Spectral clustering\\n2.3.5.1. Different label assignment strategies\\n2.3.5.2. Spectral Clustering Graphs\\n\\n2.3.6. Hierarchical clustering\\n2.3.6.1. Different linkage type: Ward, complete, average, and single linkage\\n2.3.6.2. Visualization of cluster hierarchy\\n2.3.6.3. Adding connectivity constraints\\n2.3.6.4. Varying the metric\\n2.3.6.5. Bisecting K-Means\\n\\n2.3.7. DBSCAN\\n\\n2.3.8. HDBSCAN\\n2.3.8.1. Mutual Reachability Graph\\n2.3.8.2. Hierarchical Clustering\\n\\n2.3.9. OPTICS\\n\\n2.3.10. BIRCH\\n\\n2.3.11. Clustering performance evaluation\\n2.3.11.1. Rand index\\n2.3.11.1.1. Advantages\\n2.3.11.1.2. Drawbacks\\n2.3.11.1.3. Mathematical formulation\\n2.3.11.2. Mutual Information based scores\\n2.3.11.2.1. Advantages\\n2.3.11.2.2. Drawbacks\\n2.3.11.2.3. Mathematical formulation\\n2.3.11.3. Homogeneity, completeness and V-measure\\n2.3.11.3.1. Advantages\\n2.3.11.3.2. Drawbacks\\n2.3.11.3.3. Mathematical formulation\\n2.3.11.4. Fowlkes-Mallows scores\\n2.3.11.4.1. Advantages\\n2.3.11.4.2. Drawbacks\\n2.3.11.5. Silhouette Coefficient\\n2.3.11.5.1. Advantages\\n2.3.11.5.2. Drawbacks\\n2.3.11.6. Calinski-Harabasz Index\\n2.3.11.6.1. Advantages\\n2.3.11.6.2. Drawbacks\\n2.3.11.6.3. Mathematical formulation\\n2.3.11.7. Davies-Bouldin Index\\n2.3.11.7.1. Advantages\\n2.3.11.7.2. Drawbacks\\n2.3.11.7.3. Mathematical formulation\\n2.3.11.8. Contingency Matrix\\n2.3.11.8.1. Advantages\\n2.3.11.8.2. Drawbacks\\n2.3.11.9. Pair Confusion Matrix\\n\\n2.3. Clustering¶\\n\\nClustering of\\nunlabeled data can be performed with the module sklearn.cluster.\\nEach clustering algorithm comes in two variants: a class, that implements\\nthe fit method to learn the clusters on train data, and a function,\\nthat, given train data, returns an array of integer labels corresponding\\nto the different clusters. For the class, the labels over the training\\ndata can be found in the labels_ attribute.\\nInput data\\nOne important thing to note is that the algorithms implemented in\\nthis module can take different kinds of matrix as input. All the\\nmethods accept standard data matrices of shape (n_samples, n_features).\\nThese can be obtained from the classes in the sklearn.feature_extraction\\nmodule. For AffinityPropagation, SpectralClustering\\nand DBSCAN one can also input similarity matrices of shape\\n(n_samples, n_samples). These can be obtained from the functions\\nin the sklearn.metrics.pairwise module.\\n- 2.3.1. Overview of clustering methods¶\\nA comparison of the clustering algorithms in scikit-learn¶\\nMethod name\\nParameters\\nScalability\\nUsecase\\nGeometry (metric used)\\nK-Means\\nnumber of clusters\\nVery large n_samples, medium n_clusters with\\nMiniBatch code\\nGeneral-purpose, even cluster size, flat geometry,\\nnot too many clusters, inductive\\nDistances between points\\nAffinity propagation\\ndamping, sample preference\\nNot scalable with n_samples\\nMany clusters, uneven cluster size, non-flat geometry, inductive\\nGraph distance (e.g. nearest-neighbor graph)\\nMean-shift\\nbandwidth\\nNot scalable with n_samples\\nMany clusters, uneven cluster size, non-flat geometry, inductive\\nDistances between points\\nSpectral clustering\\nnumber of clusters\\nMedium n_samples, small n_clusters\\nFew clusters, even cluster size, non-flat geometry, transductive\\nGraph distance (e.g. nearest-neighbor graph)\\nWard hierarchical clustering\\nnumber of clusters or distance threshold\\nLarge n_samples and n_clusters\\nMany clusters, possibly connectivity constraints, transductive\\nDistances between points\\nAgglomerative clustering\\nnumber of clusters or distance threshold, linkage type, distance\\nLarge n_samples and n_clusters\\nMany clusters, possibly connectivity constraints, non Euclidean\\ndistances, transductive\\nAny pairwise distance\\nDBSCAN\\nneighborhood size\\nVery large n_samples, medium n_clusters\\nNon-flat geometry, uneven cluster sizes, outlier removal,\\ntransductive\\nDistances between nearest points\\nHDBSCAN\\nminimum cluster membership, minimum point neighbors\\nlarge n_samples, medium n_clusters\\nNon-flat geometry, uneven cluster sizes, outlier removal,\\ntransductive, hierarchical, variable cluster density\\nDistances between nearest points\\nOPTICS\\nminimum cluster membership\\nVery large n_samples, large n_clusters\\nNon-flat geometry, uneven cluster sizes, variable cluster density,\\noutlier removal, transductive\\nDistances between points\\nGaussian mixtures\\nmany\\nNot scalable\\nFlat geometry, good for density estimation, inductive\\nMahalanobis distances to  centers\\nBIRCH\\nbranching factor, threshold, optional global clusterer.\\nLarge n_clusters and n_samples\\nLarge dataset, outlier removal, data reduction, inductive\\nEuclidean distance between points\\nBisecting K-Means\\nnumber of clusters\\nVery large n_samples, medium n_clusters\\nGeneral-purpose, even cluster size, flat geometry,\\nno empty clusters, inductive, hierarchical\\nDistances between points\\nNon-flat geometry clustering is useful when the clusters have a specific\\nshape, i.e. a non-flat manifold, and the standard euclidean distance is\\nnot the right metric. This case arises in the two top rows of the figure\\nabove.\\nGaussian mixture models, useful for clustering, are described in\\nanother chapter of the documentation dedicated to\\nmixture models. KMeans can be seen as a special case of Gaussian mixture\\nmodel with equal covariance per component.\\nTransductive clustering methods (in contrast to\\ninductive clustering methods) are not designed to be applied to new,\\nunseen data.\\n- 2.3.2. K-means¶\\nThe KMeans algorithm clusters data by trying to separate samples in n\\ngroups of equal variance, minimizing a criterion known as the inertia or\\nwithin-cluster sum-of-squares (see below). This algorithm requires the number\\nof clusters to be specified. It scales well to large numbers of samples and has\\nbeen used across a large range of application areas in many different fields.\\nThe k-means algorithm divides a set of (N) samples (X) into\\n(K) disjoint clusters (C), each described by the mean (\\\\mu_j)\\nof the samples in the cluster. The means are commonly called the cluster\\n“centroids”; note that they are not, in general, points from (X),\\nalthough they live in the same space.\\nThe K-means algorithm aims to choose centroids that minimise the inertia,\\nor within-cluster sum-of-squares criterion:\\n[\\\\sum_{i=0}^{n}\\\\min_{\\\\mu_j \\\\in C}(||x_i - \\\\mu_j||^2)]\\nInertia can be recognized as a measure of how internally coherent clusters are.\\nIt suffers from various drawbacks:\\nInertia makes the assumption that clusters are convex and isotropic,\\nwhich is not always the case. It responds poorly to elongated clusters,\\nor manifolds with irregular shapes.\\nInertia is not a normalized metric: we just know that lower values are\\nbetter and zero is optimal. But in very high-dimensional spaces, Euclidean\\ndistances tend to become inflated\\n(this is an instance of the so-called “curse of dimensionality”).\\nRunning a dimensionality reduction algorithm such as Principal component analysis (PCA) prior to\\nk-means clustering can alleviate this problem and speed up the\\ncomputations.\\nFor more detailed descriptions of the issues shown above and how to address them,\\nrefer to the examples Demonstration of k-means assumptions\\nand Selecting the number of clusters with silhouette analysis on KMeans clustering.\\nK-means is often referred to as Lloyd’s algorithm. In basic terms, the\\nalgorithm has three steps. The first step chooses the initial centroids, with\\nthe most basic method being to choose (k) samples from the dataset\\n(X). After initialization, K-means consists of looping between the\\ntwo other steps. The first step assigns each sample to its nearest centroid.\\nThe second step creates new centroids by taking the mean value of all of the\\nsamples assigned to each previous centroid. The difference between the old\\nand the new centroids are computed and the algorithm repeats these last two\\nsteps until this value is less than a threshold. In other words, it repeats\\nuntil the centroids do not move significantly.\\nK-means is equivalent to the expectation-maximization algorithm\\nwith a small, all-equal, diagonal covariance matrix.\\nThe algorithm can also be understood through the concept of Voronoi diagrams. First the Voronoi diagram of\\nthe points is calculated using the current centroids. Each segment in the\\nVoronoi diagram becomes a separate cluster. Secondly, the centroids are updated\\nto the mean of each segment. The algorithm then repeats this until a stopping\\ncriterion is fulfilled. Usually, the algorithm stops when the relative decrease\\nin the objective function between iterations is less than the given tolerance\\nvalue. This is not the case in this implementation: iteration stops when\\ncentroids move less than the tolerance.\\nGiven enough time, K-means will always converge, however this may be to a local\\nminimum. This is highly dependent on the initialization of the centroids.\\nAs a result, the computation is often done several times, with different\\ninitializations of the centroids. One method to help address this issue is the\\nk-means++ initialization scheme, which has been implemented in scikit-learn\\n(use the init=\\'k-means++\\' parameter). This initializes the centroids to be\\n(generally) distant from each other, leading to probably better results than\\nrandom initialization, as shown in the reference. For a detailed example of\\ncomaparing different initialization schemes, refer to\\nA demo of K-Means clustering on the handwritten digits data.\\nK-means++ can also be called independently to select seeds for other\\nclustering algorithms, see sklearn.cluster.kmeans_plusplus for details\\nand example usage.\\nThe algorithm supports sample weights, which can be given by a parameter\\nsample_weight. This allows to assign more weight to some samples when\\ncomputing cluster centers and values of inertia. For example, assigning a\\nweight of 2 to a sample is equivalent to adding a duplicate of that sample\\nto the dataset (X).\\nK-means can be used for vector quantization. This is achieved using the\\ntransform method of a trained model of KMeans. For an example of\\nperforming vector quantization on an image refer to\\nColor Quantization using K-Means.\\nExamples:\\nK-means Clustering: Example usage of\\nKMeans using the iris dataset\\nClustering text documents using k-means: Document clustering\\nusing KMeans and MiniBatchKMeans based on sparse data\\n2.3.2.1. Low-level parallelism¶\\nKMeans benefits from OpenMP based parallelism through Cython. Small\\nchunks of data (256 samples) are processed in parallel, which in addition\\nyields a low memory footprint. For more details on how to control the number of\\nthreads, please refer to our Parallelism notes.\\nExamples:\\nDemonstration of k-means assumptions: Demonstrating when\\nk-means performs intuitively and when it does not\\nA demo of K-Means clustering on the handwritten digits data: Clustering handwritten digits\\nReferences:\\n“k-means++: The advantages of careful seeding”\\nArthur, David, and Sergei Vassilvitskii,\\nProceedings of the eighteenth annual ACM-SIAM symposium on Discrete\\nalgorithms, Society for Industrial and Applied Mathematics (2007)\\n2.3.2.2. Mini Batch K-Means¶\\nThe MiniBatchKMeans is a variant of the KMeans algorithm\\nwhich uses mini-batches to reduce the computation time, while still attempting\\nto optimise the same objective function. Mini-batches are subsets of the input\\ndata, randomly sampled in each training iteration. These mini-batches\\ndrastically reduce the amount of computation required to converge to a local\\nsolution. In contrast to other algorithms that reduce the convergence time of\\nk-means, mini-batch k-means produces results that are generally only slightly\\nworse than the standard algorithm.\\nThe algorithm iterates between two major steps, similar to vanilla k-means.\\nIn the first step, (b) samples are drawn randomly from the dataset, to form\\na mini-batch. These are then assigned to the nearest centroid. In the second\\nstep, the centroids are updated. In contrast to k-means, this is done on a\\nper-sample basis. For each sample in the mini-batch, the assigned centroid\\nis updated by taking the streaming average of the sample and all previous\\nsamples assigned to that centroid. This has the effect of decreasing the\\nrate of change for a centroid over time. These steps are performed until\\nconvergence or a predetermined number of iterations is reached.\\nMiniBatchKMeans converges faster than KMeans, but the quality\\nof the results is reduced. In practice this difference in quality can be quite\\nsmall, as shown in the example and cited reference.\\nExamples:\\nComparison of the K-Means and MiniBatchKMeans clustering algorithms: Comparison of\\nKMeans and MiniBatchKMeans\\nClustering text documents using k-means: Document clustering\\nusing KMeans and MiniBatchKMeans based on sparse data\\nOnline learning of a dictionary of parts of faces\\nReferences:\\n“Web Scale K-Means clustering”\\nD. Sculley, Proceedings of the 19th international conference on World\\nwide web (2010)\\n- 2.3.3. Affinity Propagation¶\\nAffinityPropagation creates clusters by sending messages between\\npairs of samples until convergence. A dataset is then described using a small\\nnumber of exemplars, which are identified as those most representative of other\\nsamples. The messages sent between pairs represent the suitability for one\\nsample to be the exemplar of the other, which is updated in response to the\\nvalues from other pairs. This updating happens iteratively until convergence,\\nat which point the final exemplars are chosen, and hence the final clustering\\nis given.\\nAffinity Propagation can be interesting as it chooses the number of\\nclusters based on the data provided. For this purpose, the two important\\nparameters are the preference, which controls how many exemplars are\\nused, and the damping factor which damps the responsibility and\\navailability messages to avoid numerical oscillations when updating these\\nmessages.\\nThe main drawback of Affinity Propagation is its complexity. The\\nalgorithm has a time complexity of the order (O(N^2 T)), where (N)\\nis the number of samples and (T) is the number of iterations until\\nconvergence. Further, the memory complexity is of the order\\n(O(N^2)) if a dense similarity matrix is used, but reducible if a\\nsparse similarity matrix is used. This makes Affinity Propagation most\\nappropriate for small to medium sized datasets.\\nExamples:\\nDemo of affinity propagation clustering algorithm: Affinity\\nPropagation on a synthetic 2D datasets with 3 classes.\\nVisualizing the stock market structure Affinity Propagation on\\nFinancial time series to find groups of companies\\nAlgorithm description:\\nThe messages sent between points belong to one of two categories. The first is\\nthe responsibility (r(i, k)),\\nwhich is the accumulated evidence that sample (k)\\nshould be the exemplar for sample (i).\\nThe second is the availability (a(i, k))\\nwhich is the accumulated evidence that sample (i)\\nshould choose sample (k) to be its exemplar,\\nand considers the values for all other samples that (k) should\\nbe an exemplar. In this way, exemplars are chosen by samples if they are (1)\\nsimilar enough to many samples and (2) chosen by many samples to be\\nrepresentative of themselves.\\nMore formally, the responsibility of a sample (k)\\nto be the exemplar of sample (i) is given by:\\n[r(i, k) \\\\leftarrow s(i, k) - max [ a(i, k\\') + s(i, k\\') \\\\forall k\\' \\\\neq k ]]\\nWhere (s(i, k)) is the similarity between samples (i) and (k).\\nThe availability of sample (k)\\nto be the exemplar of sample (i) is given by:\\n[a(i, k) \\\\leftarrow min [0, r(k, k) + \\\\sum_{i\\'~s.t.~i\\' \\\\notin {i, k}}{r(i\\', k)}]]\\nTo begin with, all values for (r) and (a) are set to zero,\\nand the calculation of each iterates until convergence.\\nAs discussed above, in order to avoid numerical oscillations when updating the\\nmessages, the damping factor (\\\\lambda) is introduced to iteration process:\\n[r_{t+1}(i, k) = \\\\lambda\\\\cdot r_{t}(i, k) + (1-\\\\lambda)\\\\cdot r_{t+1}(i, k)]\\n[a_{t+1}(i, k) = \\\\lambda\\\\cdot a_{t}(i, k) + (1-\\\\lambda)\\\\cdot a_{t+1}(i, k)]\\nwhere (t) indicates the iteration times.\\n- 2.3.4. Mean Shift¶\\nMeanShift clustering aims to discover blobs in a smooth density of\\nsamples. It is a centroid based algorithm, which works by updating candidates\\nfor centroids to be the mean of the points within a given region. These\\ncandidates are then filtered in a post-processing stage to eliminate\\nnear-duplicates to form the final set of centroids.\\nThe position of centroid candidates is iteratively adjusted using a technique called hill\\nclimbing, which finds local maxima of the estimated probability density.\\nGiven a candidate centroid (x) for iteration (t), the candidate\\nis updated according to the following equation:\\n[x^{t+1} = x^t + m(x^t)]\\nWhere (m) is the mean shift vector that is computed for each\\ncentroid that points towards a region of the maximum increase in the density of points.\\nTo compute (m) we define (N(x)) as the neighborhood of samples within\\na given distance around (x). Then (m) is computed using the following\\nequation, effectively updating a centroid to be the mean of the samples within\\nits neighborhood:\\n[m(x) = \\\\frac{1}{|N(x)|} \\\\sum_{x_j \\\\in N(x)}x_j - x]\\nIn general, the equation for (m) depends on a kernel used for density estimation.\\nThe generic formula is:\\n[m(x) = \\\\frac{\\\\sum_{x_j \\\\in N(x)}K(x_j - x)x_j}{\\\\sum_{x_j \\\\in N(x)}K(x_j - x)} - x]\\nIn our implementation, (K(x)) is equal to 1 if (x) is small enough and is\\nequal to 0 otherwise. Effectively (K(y - x)) indicates whether (y) is in\\nthe neighborhood of (x).\\nThe algorithm automatically sets the number of clusters, instead of relying on a\\nparameter bandwidth, which dictates the size of the region to search through.\\nThis parameter can be set manually, but can be estimated using the provided\\nestimate_bandwidth function, which is called if the bandwidth is not set.\\nThe algorithm is not highly scalable, as it requires multiple nearest neighbor\\nsearches during the execution of the algorithm. The algorithm is guaranteed to\\nconverge, however the algorithm will stop iterating when the change in centroids\\nis small.\\nLabelling a new sample is performed by finding the nearest centroid for a\\ngiven sample.\\nExamples:\\nA demo of the mean-shift clustering algorithm: Mean Shift clustering\\non a synthetic 2D datasets with 3 classes.\\nReferences:\\n“Mean shift: A robust approach toward feature space analysis”\\nD. Comaniciu and P. Meer, IEEE Transactions on Pattern Analysis and Machine Intelligence (2002)\\n- 2.3.5. Spectral clustering¶\\nSpectralClustering performs a low-dimension embedding of the\\naffinity matrix between samples, followed by clustering, e.g., by KMeans,\\nof the components of the eigenvectors in the low dimensional space.\\nIt is especially computationally efficient if the affinity matrix is sparse\\nand the amg solver is used for the eigenvalue problem (Note, the amg solver\\nrequires that the pyamg module is installed.)\\nThe present version of SpectralClustering requires the number of clusters\\nto be specified in advance. It works well for a small number of clusters,\\nbut is not advised for many clusters.\\nFor two clusters, SpectralClustering solves a convex relaxation of the\\nnormalized cuts\\nproblem on the similarity graph: cutting the graph in two so that the weight of\\nthe edges cut is small compared to the weights of the edges inside each\\ncluster. This criteria is especially interesting when working on images, where\\ngraph vertices are pixels, and weights of the edges of the similarity graph are\\ncomputed using a function of a gradient of the image.\\nWarning\\nTransforming distance to well-behaved similarities\\nNote that if the values of your similarity matrix are not well\\ndistributed, e.g. with negative values or with a distance matrix\\nrather than a similarity, the spectral problem will be singular and\\nthe problem not solvable. In which case it is advised to apply a\\ntransformation to the entries of the matrix. For instance, in the\\ncase of a signed distance matrix, is common to apply a heat kernel:\\nsimilarity = np.exp(-beta * distance / distance.std())\\nSee the examples for such an application.\\nExamples:\\nSpectral clustering for image segmentation: Segmenting objects\\nfrom a noisy background using spectral clustering.\\nSegmenting the picture of greek coins in regions: Spectral clustering\\nto split the image of coins in regions.\\n2.3.5.1. Different label assignment strategies¶\\nDifferent label assignment strategies can be used, corresponding to the\\nassign_labels parameter of SpectralClustering.\\n\"kmeans\" strategy can match finer details, but can be unstable.\\nIn particular, unless you control the random_state, it may not be\\nreproducible from run-to-run, as it depends on random initialization.\\nThe alternative \"discretize\" strategy is 100% reproducible, but tends\\nto create parcels of fairly even and geometrical shape.\\nThe recently added \"cluster_qr\" option is a deterministic alternative that\\ntends to create the visually best partitioning on the example application\\nbelow.\\nassign_labels=\"kmeans\"\\nassign_labels=\"discretize\"\\nassign_labels=\"cluster_qr\"\\nReferences:\\n“Multiclass spectral clustering”\\nStella X. Yu, Jianbo Shi, 2003\\n“Simple, direct, and efficient multi-way spectral clustering”\\nAnil Damle, Victor Minden, Lexing Ying, 2019\\n2.3.5.2. Spectral Clustering Graphs¶\\nSpectral Clustering can also be used to partition graphs via their spectral\\nembeddings.  In this case, the affinity matrix is the adjacency matrix of the\\ngraph, and SpectralClustering is initialized with affinity=\\'precomputed\\':\\n\\nfrom sklearn.cluster import SpectralClustering\\nsc = SpectralClustering(3, affinity=\\'precomputed\\', n_init=100,\\n...                         assign_labels=\\'discretize\\')\\nsc.fit_predict(adjacency_matrix)\\nReferences:\\n“A Tutorial on Spectral Clustering”\\nUlrike von Luxburg, 2007\\n“Normalized cuts and image segmentation”\\nJianbo Shi, Jitendra Malik, 2000\\n“A Random Walks View of Spectral Segmentation”\\nMarina Meila, Jianbo Shi, 2001\\n“On Spectral Clustering: Analysis and an algorithm”\\nAndrew Y. Ng, Michael I. Jordan, Yair Weiss, 2001\\n“Preconditioned Spectral Clustering for Stochastic\\nBlock Partition Streaming Graph Challenge”\\nDavid Zhuzhunashvili, Andrew Knyazev\\n- 2.3.6. Hierarchical clustering¶\\nHierarchical clustering is a general family of clustering algorithms that\\nbuild nested clusters by merging or splitting them successively. This\\nhierarchy of clusters is represented as a tree (or dendrogram). The root of the\\ntree is the unique cluster that gathers all the samples, the leaves being the\\nclusters with only one sample. See the Wikipedia page for more details.\\nThe AgglomerativeClustering object performs a hierarchical clustering\\nusing a bottom up approach: each observation starts in its own cluster, and\\nclusters are successively merged together. The linkage criteria determines the\\nmetric used for the merge strategy:\\nWard minimizes the sum of squared differences within all clusters. It is a\\nvariance-minimizing approach and in this sense is similar to the k-means\\nobjective function but tackled with an agglomerative hierarchical\\napproach.\\nMaximum or complete linkage minimizes the maximum distance between\\nobservations of pairs of clusters.\\nAverage linkage minimizes the average of the distances between all\\nobservations of pairs of clusters.\\nSingle linkage minimizes the distance between the closest\\nobservations of pairs of clusters.\\nAgglomerativeClustering can also scale to large number of samples\\nwhen it is used jointly with a connectivity matrix, but is computationally\\nexpensive when no connectivity constraints are added between samples: it\\nconsiders at each step all the possible merges.\\nFeatureAgglomeration\\nThe FeatureAgglomeration uses agglomerative clustering to\\ngroup together features that look very similar, thus decreasing the\\nnumber of features. It is a dimensionality reduction tool, see\\nUnsupervised dimensionality reduction.\\n2.3.6.1. Different linkage type: Ward, complete, average, and single linkage¶\\nAgglomerativeClustering supports Ward, single, average, and complete\\nlinkage strategies.\\nAgglomerative cluster has a “rich get richer” behavior that leads to\\nuneven cluster sizes. In this regard, single linkage is the worst\\nstrategy, and Ward gives the most regular sizes. However, the affinity\\n(or distance used in clustering) cannot be varied with Ward, thus for non\\nEuclidean metrics, average linkage is a good alternative. Single linkage,\\nwhile not robust to noisy data, can be computed very efficiently and can\\ntherefore be useful to provide hierarchical clustering of larger datasets.\\nSingle linkage can also perform well on non-globular data.\\nExamples:\\nVarious Agglomerative Clustering on a 2D embedding of digits: exploration of the\\ndifferent linkage strategies in a real dataset.\\n2.3.6.2. Visualization of cluster hierarchy¶\\nIt’s possible to visualize the tree representing the hierarchical merging of clusters\\nas a dendrogram. Visual inspection can often be useful for understanding the structure\\nof the data, though more so in the case of small sample sizes.\\n2.3.6.3. Adding connectivity constraints¶\\nAn interesting aspect of AgglomerativeClustering is that\\nconnectivity constraints can be added to this algorithm (only adjacent\\nclusters can be merged together), through a connectivity matrix that defines\\nfor each sample the neighboring samples following a given structure of the\\ndata. For instance, in the swiss-roll example below, the connectivity\\nconstraints forbid the merging of points that are not adjacent on the swiss\\nroll, and thus avoid forming clusters that extend across overlapping folds of\\nthe roll.\\nThese constraint are useful to impose a certain local structure, but they\\nalso make the algorithm faster, especially when the number of the samples\\nis high.\\nThe connectivity constraints are imposed via an connectivity matrix: a\\nscipy sparse matrix that has elements only at the intersection of a row\\nand a column with indices of the dataset that should be connected. This\\nmatrix can be constructed from a-priori information: for instance, you\\nmay wish to cluster web pages by only merging pages with a link pointing\\nfrom one to another. It can also be learned from the data, for instance\\nusing sklearn.neighbors.kneighbors_graph to restrict\\nmerging to nearest neighbors as in this example, or\\nusing sklearn.feature_extraction.image.grid_to_graph to\\nenable only merging of neighboring pixels on an image, as in the\\ncoin example.\\nExamples:\\nA demo of structured Ward hierarchical clustering on an image of coins: Ward clustering\\nto split the image of coins in regions.\\nHierarchical clustering: structured vs unstructured ward: Example of\\nWard algorithm on a swiss-roll, comparison of structured approaches\\nversus unstructured approaches.\\nFeature agglomeration vs. univariate selection:\\nExample of dimensionality reduction with feature agglomeration based on\\nWard hierarchical clustering.\\nAgglomerative clustering with and without structure\\nWarning\\nConnectivity constraints with single, average and complete linkage\\nConnectivity constraints and single, complete or average linkage can enhance\\nthe ‘rich getting richer’ aspect of agglomerative clustering,\\nparticularly so if they are built with\\nsklearn.neighbors.kneighbors_graph. In the limit of a small\\nnumber of clusters, they tend to give a few macroscopically occupied\\nclusters and almost empty ones. (see the discussion in\\nAgglomerative clustering with and without structure).\\nSingle linkage is the most brittle linkage option with regard to this issue.\\n2.3.6.4. Varying the metric¶\\nSingle, average and complete linkage can be used with a variety of distances (or\\naffinities), in particular Euclidean distance (l2), Manhattan distance\\n(or Cityblock, or l1), cosine distance, or any precomputed affinity\\nmatrix.\\nl1 distance is often good for sparse features, or sparse noise: i.e.\\nmany of the features are zero, as in text mining using occurrences of\\nrare words.\\ncosine distance is interesting because it is invariant to global\\nscalings of the signal.\\nThe guidelines for choosing a metric is to use one that maximizes the\\ndistance between samples in different classes, and minimizes that within\\neach class.\\nExamples:\\nAgglomerative clustering with different metrics\\n2.3.6.5. Bisecting K-Means¶\\nThe BisectingKMeans is an iterative variant of KMeans, using\\ndivisive hierarchical clustering. Instead of creating all centroids at once, centroids\\nare picked progressively based on a previous clustering: a cluster is split into two\\nnew clusters repeatedly until the target number of clusters is reached.\\nBisectingKMeans is more efficient than KMeans when the number of\\nclusters is large since it only works on a subset of the data at each bisection\\nwhile KMeans always works on the entire dataset.\\nAlthough BisectingKMeans can’t benefit from the advantages of the \"k-means++\"\\ninitialization by design, it will still produce comparable results than\\nKMeans(init=\"k-means++\") in terms of inertia at cheaper computational costs, and will\\nlikely produce better results than KMeans with a random initialization.\\nThis variant is more efficient to agglomerative clustering if the number of clusters is\\nsmall compared to the number of data points.\\nThis variant also does not produce empty clusters.\\nThere exist two strategies for selecting the cluster to split:\\nbisecting_strategy=\"largest_cluster\" selects the cluster having the most points\\nbisecting_strategy=\"biggest_inertia\" selects the cluster with biggest inertia\\n(cluster with biggest Sum of Squared Errors within)\\nPicking by largest amount of data points in most cases produces result as\\naccurate as picking by inertia and is faster (especially for larger amount of data\\npoints, where calculating error may be costly).\\nPicking by largest amount of data points will also likely produce clusters of similar\\nsizes while KMeans is known to produce clusters of different sizes.\\nDifference between Bisecting K-Means and regular K-Means can be seen on example\\nBisecting K-Means and Regular K-Means Performance Comparison.\\nWhile the regular K-Means algorithm tends to create non-related clusters,\\nclusters from Bisecting K-Means are well ordered and create quite a visible hierarchy.\\nReferences:\\n“A Comparison of Document Clustering Techniques”\\nMichael Steinbach, George Karypis and Vipin Kumar,\\nDepartment of Computer Science and Egineering, University of Minnesota\\n(June 2000)\\n“Performance Analysis of K-Means and Bisecting K-Means Algorithms in Weblog Data”\\nK.Abirami and Dr.P.Mayilvahanan,\\nInternational Journal of Emerging Technologies in Engineering Research (IJETER)\\nVolume 4, Issue 8, (August 2016)\\n“Bisecting K-means Algorithm Based on K-valued Self-determining\\nand Clustering Center Optimization”\\nJian Di, Xinyue Gou\\nSchool of Control and Computer Engineering,North China Electric Power University,\\nBaoding, Hebei, China (August 2017)\\n- 2.3.7. DBSCAN¶\\nThe DBSCAN algorithm views clusters as areas of high density\\nseparated by areas of low density. Due to this rather generic view, clusters\\nfound by DBSCAN can be any shape, as opposed to k-means which assumes that\\nclusters are convex shaped. The central component to the DBSCAN is the concept\\nof core samples, which are samples that are in areas of high density. A\\ncluster is therefore a set of core samples, each close to each other\\n(measured by some distance measure)\\nand a set of non-core samples that are close to a core sample (but are not\\nthemselves core samples). There are two parameters to the algorithm,\\nmin_samples and eps,\\nwhich define formally what we mean when we say dense.\\nHigher min_samples or lower eps\\nindicate higher density necessary to form a cluster.\\nMore formally, we define a core sample as being a sample in the dataset such\\nthat there exist min_samples other samples within a distance of\\neps, which are defined as neighbors of the core sample. This tells\\nus that the core sample is in a dense area of the vector space. A cluster\\nis a set of core samples that can be built by recursively taking a core\\nsample, finding all of its neighbors that are core samples, finding all of\\ntheir neighbors that are core samples, and so on. A cluster also has a\\nset of non-core samples, which are samples that are neighbors of a core sample\\nin the cluster but are not themselves core samples. Intuitively, these samples\\nare on the fringes of a cluster.\\nAny core sample is part of a cluster, by definition. Any sample that is not a\\ncore sample, and is at least eps in distance from any core sample, is\\nconsidered an outlier by the algorithm.\\nWhile the parameter min_samples primarily controls how tolerant the\\nalgorithm is towards noise (on noisy and large data sets it may be desirable\\nto increase this parameter), the parameter eps is crucial to choose\\nappropriately for the data set and distance function and usually cannot be\\nleft at the default value. It controls the local neighborhood of the points.\\nWhen chosen too small, most data will not be clustered at all (and labeled\\nas -1 for “noise”). When chosen too large, it causes close clusters to\\nbe merged into one cluster, and eventually the entire data set to be returned\\nas a single cluster. Some heuristics for choosing this parameter have been\\ndiscussed in the literature, for example based on a knee in the nearest neighbor\\ndistances plot (as discussed in the references below).\\nIn the figure below, the color indicates cluster membership, with large circles\\nindicating core samples found by the algorithm. Smaller circles are non-core\\nsamples that are still part of a cluster. Moreover, the outliers are indicated\\nby black points below.\\nExamples:\\nDemo of DBSCAN clustering algorithm\\nImplementation\\nThe DBSCAN algorithm is deterministic, always generating the same clusters\\nwhen given the same data in the same order.  However, the results can differ when\\ndata is provided in a different order. First, even though the core samples\\nwill always be assigned to the same clusters, the labels of those clusters\\nwill depend on the order in which those samples are encountered in the data.\\nSecond and more importantly, the clusters to which non-core samples are assigned\\ncan differ depending on the data order.  This would happen when a non-core sample\\nhas a distance lower than eps to two core samples in different clusters. By the\\ntriangular inequality, those two core samples must be more distant than\\neps from each other, or they would be in the same cluster. The non-core\\nsample is assigned to whichever cluster is generated first in a pass\\nthrough the data, and so the results will depend on the data ordering.\\nThe current implementation uses ball trees and kd-trees\\nto determine the neighborhood of points,\\nwhich avoids calculating the full distance matrix\\n(as was done in scikit-learn versions before 0.14).\\nThe possibility to use custom metrics is retained;\\nfor details, see NearestNeighbors.\\nMemory consumption for large sample sizes\\nThis implementation is by default not memory efficient because it constructs\\na full pairwise similarity matrix in the case where kd-trees or ball-trees cannot\\nbe used (e.g., with sparse matrices). This matrix will consume (n^2) floats.\\nA couple of mechanisms for getting around this are:\\nUse OPTICS clustering in conjunction with the\\nextract_dbscan method. OPTICS clustering also calculates the full\\npairwise matrix, but only keeps one row in memory at a time (memory\\ncomplexity n).\\nA sparse radius neighborhood graph (where missing entries are presumed to\\nbe out of eps) can be precomputed in a memory-efficient way and dbscan\\ncan be run over this with metric=\\'precomputed\\'.  See\\nsklearn.neighbors.NearestNeighbors.radius_neighbors_graph.\\nThe dataset can be compressed, either by removing exact duplicates if\\nthese occur in your data, or by using BIRCH. Then you only have a\\nrelatively small number of representatives for a large number of points.\\nYou can then provide a sample_weight when fitting DBSCAN.\\nReferences:\\n“A Density-Based Algorithm for Discovering Clusters in Large Spatial Databases\\nwith Noise”\\nEster, M., H. P. Kriegel, J. Sander, and X. Xu,\\nIn Proceedings of the 2nd International Conference on Knowledge Discovery\\nand Data Mining, Portland, OR, AAAI Press, pp. 226–231. 1996\\n“DBSCAN revisited, revisited: why and how you should (still) use DBSCAN.”\\nSchubert, E., Sander, J., Ester, M., Kriegel, H. P., & Xu, X. (2017).\\nIn ACM Transactions on Database Systems (TODS), 42(3), 19.\\n- 2.3.8. HDBSCAN¶\\nThe HDBSCAN algorithm can be seen as an extension of DBSCAN\\nand OPTICS. Specifically, DBSCAN assumes that the clustering\\ncriterion (i.e. density requirement) is globally homogeneous.\\nIn other words, DBSCAN may struggle to successfully capture clusters\\nwith different densities.\\nHDBSCAN alleviates this assumption and explores all possible density\\nscales by building an alternative representation of the clustering problem.\\nNote\\nThis implementation is adapted from the original implementation of HDBSCAN,\\nscikit-learn-contrib/hdbscan based on [LJ2017].\\n2.3.8.1. Mutual Reachability Graph¶\\nHDBSCAN first defines (d_c(x_p)), the core distance of a sample (x_p), as the\\ndistance to its min_samples th-nearest neighbor, counting itself. For example,\\nif min_samples=5 and (x_) is the 5th-nearest neighbor of (x_p)\\nthen the core distance is:\\n[d_c(x_p)=d(x_p, x_).]\\nNext it defines (d_m(x_p, x_q)), the mutual reachability distance of two points\\n(x_p, x_q), as:\\n[d_m(x_p, x_q) = \\\\max{d_c(x_p), d_c(x_q), d(x_p, x_q)}]\\nThese two notions allow us to construct the mutual reachability graph\\n(G_{ms}) defined for a fixed choice of min_samples by associating each\\nsample (x_p) with a vertex of the graph, and thus edges between points\\n(x_p, x_q) are the mutual reachability distance (d_m(x_p, x_q))\\nbetween them. We may build subsets of this graph, denoted as\\n(G_{ms,\\\\varepsilon}), by removing any edges with value greater than (\\\\varepsilon):\\nfrom the original graph. Any points whose core distance is less than (\\\\varepsilon):\\nare at this staged marked as noise. The remaining points are then clustered by\\nfinding the connected components of this trimmed graph.\\nNote\\nTaking the connected components of a trimmed graph (G_{ms,\\\\varepsilon}) is\\nequivalent to running DBSCAN with min_samples and (\\\\varepsilon). DBSCAN is a\\nslightly modified version of DBSCAN mentioned in [CM2013].\\n2.3.8.2. Hierarchical Clustering¶\\nHDBSCAN can be seen as an algorithm which performs DBSCAN clustering across all\\nvalues of (\\\\varepsilon). As mentioned prior, this is equivalent to finding the connected\\ncomponents of the mutual reachability graphs for all values of (\\\\varepsilon). To do this\\nefficiently, HDBSCAN first extracts a minimum spanning tree (MST) from the fully\\n-connected mutual reachability graph, then greedily cuts the edges with highest\\nweight. An outline of the HDBSCAN algorithm is as follows:\\nExtract the MST of (G_{ms}).\\nExtend the MST by adding a “self edge” for each vertex, with weight equal\\nto the core distance of the underlying sample.\\nInitialize a single cluster and label for the MST.\\nRemove the edge with the greatest weight from the MST (ties are\\nremoved simultaneously).\\nAssign cluster labels to the connected components which contain the\\nend points of the now-removed edge. If the component does not have at least\\none edge it is instead assigned a “null” label marking it as noise.\\nRepeat 4-5 until there are no more connected components.\\nHDBSCAN is therefore able to obtain all possible partitions achievable by\\nDBSCAN for a fixed choice of min_samples in a hierarchical fashion.\\nIndeed, this allows HDBSCAN to perform clustering across multiple densities\\nand as such it no longer needs (\\\\varepsilon) to be given as a hyperparameter. Instead\\nit relies solely on the choice of min_samples, which tends to be a more robust\\nhyperparameter.\\nHDBSCAN can be smoothed with an additional hyperparameter min_cluster_size\\nwhich specifies that during the hierarchical clustering, components with fewer\\nthan minimum_cluster_size many samples are considered noise. In practice, one\\ncan set minimum_cluster_size = min_samples to couple the parameters and\\nsimplify the hyperparameter space.\\nReferences:\\n[CM2013]\\nCampello, R.J.G.B., Moulavi, D., Sander, J. (2013). Density-Based Clustering\\nBased on Hierarchical Density Estimates. In: Pei, J., Tseng, V.S., Cao, L.,\\nMotoda, H., Xu, G. (eds) Advances in Knowledge Discovery and Data Mining.\\nPAKDD 2013. Lecture Notes in Computer Science(), vol 7819. Springer, Berlin,\\nHeidelberg.\\nDensity-Based Clustering Based on Hierarchical Density Estimates\\n[LJ2017]\\nL. McInnes and J. Healy, (2017). Accelerated Hierarchical Density Based\\nClustering. In: IEEE International Conference on Data Mining Workshops (ICDMW),\\n2017, pp. 33-42.\\nAccelerated Hierarchical Density Based Clustering\\n- 2.3.9. OPTICS¶\\nThe OPTICS algorithm shares many similarities with the DBSCAN\\nalgorithm, and can be considered a generalization of DBSCAN that relaxes the\\neps requirement from a single value to a value range. The key difference\\nbetween DBSCAN and OPTICS is that the OPTICS algorithm builds a reachability\\ngraph, which assigns each sample both a reachability_ distance, and a spot\\nwithin the cluster ordering_ attribute; these two attributes are assigned\\nwhen the model is fitted, and are used to determine cluster membership. If\\nOPTICS is run with the default value of inf set for max_eps, then DBSCAN\\nstyle cluster extraction can be performed repeatedly in linear time for any\\ngiven eps value using the cluster_optics_dbscan method. Setting\\nmax_eps to a lower value will result in shorter run times, and can be\\nthought of as the maximum neighborhood radius from each point to find other\\npotential reachable points.\\nThe reachability distances generated by OPTICS allow for variable density\\nextraction of clusters within a single data set. As shown in the above plot,\\ncombining reachability distances and data set ordering_ produces a\\nreachability plot, where point density is represented on the Y-axis, and\\npoints are ordered such that nearby points are adjacent. ‘Cutting’ the\\nreachability plot at a single value produces DBSCAN like results; all points\\nabove the ‘cut’ are classified as noise, and each time that there is a break\\nwhen reading from left to right signifies a new cluster. The default cluster\\nextraction with OPTICS looks at the steep slopes within the graph to find\\nclusters, and the user can define what counts as a steep slope using the\\nparameter xi. There are also other possibilities for analysis on the graph\\nitself, such as generating hierarchical representations of the data through\\nreachability-plot dendrograms, and the hierarchy of clusters detected by the\\nalgorithm can be accessed through the cluster_hierarchy_ parameter. The\\nplot above has been color-coded so that cluster colors in planar space match\\nthe linear segment clusters of the reachability plot. Note that the blue and\\nred clusters are adjacent in the reachability plot, and can be hierarchically\\nrepresented as children of a larger parent cluster.\\nExamples:\\nDemo of OPTICS clustering algorithm\\nComparison with DBSCAN\\nThe results from OPTICS cluster_optics_dbscan method and DBSCAN are\\nvery similar, but not always identical; specifically, labeling of periphery\\nand noise points. This is in part because the first samples of each dense\\narea processed by OPTICS have a large reachability value while being close\\nto other points in their area, and will thus sometimes be marked as noise\\nrather than periphery. This affects adjacent points when they are\\nconsidered as candidates for being marked as either periphery or noise.\\nNote that for any single value of eps, DBSCAN will tend to have a\\nshorter run time than OPTICS; however, for repeated runs at varying eps\\nvalues, a single run of OPTICS may require less cumulative runtime than\\nDBSCAN. It is also important to note that OPTICS’ output is close to\\nDBSCAN’s only if eps and max_eps are close.\\nComputational Complexity\\nSpatial indexing trees are used to avoid calculating the full distance\\nmatrix, and allow for efficient memory usage on large sets of samples.\\nDifferent distance metrics can be supplied via the metric keyword.\\nFor large datasets, similar (but not identical) results can be obtained via\\nHDBSCAN. The HDBSCAN implementation is\\nmultithreaded, and has better algorithmic runtime complexity than OPTICS,\\nat the cost of worse memory scaling. For extremely large datasets that\\nexhaust system memory using HDBSCAN, OPTICS will maintain (n) (as opposed\\nto (n^2)) memory scaling; however, tuning of the max_eps parameter\\nwill likely need to be used to give a solution in a reasonable amount of\\nwall time.\\nReferences:\\n“OPTICS: ordering points to identify the clustering structure.”\\nAnkerst, Mihael, Markus M. Breunig, Hans-Peter Kriegel, and Jörg Sander.\\nIn ACM Sigmod Record, vol. 28, no. 2, pp. 49-60. ACM, 1999.\\n- 2.3.10. BIRCH¶\\nThe Birch builds a tree called the Clustering Feature Tree (CFT)\\nfor the given data. The data is essentially lossy compressed to a set of\\nClustering Feature nodes (CF Nodes). The CF Nodes have a number of\\nsubclusters called Clustering Feature subclusters (CF Subclusters)\\nand these CF Subclusters located in the non-terminal CF Nodes\\ncan have CF Nodes as children.\\nThe CF Subclusters hold the necessary information for clustering which prevents\\nthe need to hold the entire input data in memory. This information includes:\\nNumber of samples in a subcluster.\\nLinear Sum - An n-dimensional vector holding the sum of all samples\\nSquared Sum - Sum of the squared L2 norm of all samples.\\nCentroids - To avoid recalculation linear sum / n_samples.\\nSquared norm of the centroids.\\nThe BIRCH algorithm has two parameters, the threshold and the branching factor.\\nThe branching factor limits the number of subclusters in a node and the\\nthreshold limits the distance between the entering sample and the existing\\nsubclusters.\\nThis algorithm can be viewed as an instance or data reduction method,\\nsince it reduces the input data to a set of subclusters which are obtained directly\\nfrom the leaves of the CFT. This reduced data can be further processed by feeding\\nit into a global clusterer. This global clusterer can be set by n_clusters.\\nIf n_clusters is set to None, the subclusters from the leaves are directly\\nread off, otherwise a global clustering step labels these subclusters into global\\nclusters (labels) and the samples are mapped to the global label of the nearest subcluster.\\nAlgorithm description:\\nA new sample is inserted into the root of the CF Tree which is a CF Node.\\nIt is then merged with the subcluster of the root, that has the smallest\\nradius after merging, constrained by the threshold and branching factor conditions.\\nIf the subcluster has any child node, then this is done repeatedly till it reaches\\na leaf. After finding the nearest subcluster in the leaf, the properties of this\\nsubcluster and the parent subclusters are recursively updated.\\nIf the radius of the subcluster obtained by merging the new sample and the\\nnearest subcluster is greater than the square of the threshold and if the\\nnumber of subclusters is greater than the branching factor, then a space is temporarily\\nallocated to this new sample. The two farthest subclusters are taken and\\nthe subclusters are divided into two groups on the basis of the distance\\nbetween these subclusters.\\nIf this split node has a parent subcluster and there is room\\nfor a new subcluster, then the parent is split into two. If there is no room,\\nthen this node is again split into two and the process is continued\\nrecursively, till it reaches the root.\\nBIRCH or MiniBatchKMeans?\\nBIRCH does not scale very well to high dimensional data. As a rule of thumb if\\nn_features is greater than twenty, it is generally better to use MiniBatchKMeans.\\nIf the number of instances of data needs to be reduced, or if one wants a\\nlarge number of subclusters either as a preprocessing step or otherwise,\\nBIRCH is more useful than MiniBatchKMeans.\\nHow to use partial_fit?\\nTo avoid the computation of global clustering, for every call of partial_fit\\nthe user is advised\\nTo set n_clusters=None initially\\nTrain all data by multiple calls to partial_fit.\\nSet n_clusters to a required value using\\nbrc.set_params(n_clusters=n_clusters).\\nCall partial_fit finally with no arguments, i.e. brc.partial_fit()\\nwhich performs the global clustering.\\nReferences:\\nTian Zhang, Raghu Ramakrishnan, Maron Livny\\nBIRCH: An efficient data clustering method for large databases.\\nhttps://www.cs.sfu.ca/CourseCentral/459/han/papers/zhang96.pdf\\nRoberto Perdisci\\nJBirch - Java implementation of BIRCH clustering algorithm\\nhttps://code.google.com/archive/p/jbirch\\n- 2.3.11. Clustering performance evaluation¶\\nEvaluating the performance of a clustering algorithm is not as trivial as\\ncounting the number of errors or the precision and recall of a supervised\\nclassification algorithm. In particular any evaluation metric should not\\ntake the absolute values of the cluster labels into account but rather\\nif this clustering define separations of the data similar to some ground\\ntruth set of classes or satisfying some assumption such that members\\nbelong to the same class are more similar than members of different\\nclasses according to some similarity metric.\\n2.3.11.1. Rand index¶\\nGiven the knowledge of the ground truth class assignments\\nlabels_true and our clustering algorithm assignments of the same\\nsamples labels_pred, the (adjusted or unadjusted) Rand index\\nis a function that measures the similarity of the two assignments,\\nignoring permutations:\\nfrom sklearn import metrics\\nlabels_true = [0, 0, 0, 1, 1, 1]\\nlabels_pred = [0, 0, 1, 1, 2, 2]\\nmetrics.rand_score(labels_true, labels_pred)\\n0.66...\\nThe Rand index does not ensure to obtain a value close to 0.0 for a\\nrandom labelling. The adjusted Rand index corrects for chance and\\nwill give such a baseline.\\nmetrics.adjusted_rand_score(labels_true, labels_pred)\\n0.24...\\nAs with all clustering metrics, one can permute 0 and 1 in the predicted\\nlabels, rename 2 to 3, and get the same score:\\nlabels_pred = [1, 1, 0, 0, 3, 3]\\nmetrics.rand_score(labels_true, labels_pred)\\n0.66...\\nmetrics.adjusted_rand_score(labels_true, labels_pred)\\n0.24...\\nFurthermore, both rand_score adjusted_rand_score are\\nsymmetric: swapping the argument does not change the scores. They can\\nthus be used as consensus measures:\\nmetrics.rand_score(labels_pred, labels_true)\\n0.66...\\nmetrics.adjusted_rand_score(labels_pred, labels_true)\\n0.24...\\nPerfect labeling is scored 1.0:\\nlabels_pred = labels_true[:]\\nmetrics.rand_score(labels_true, labels_pred)\\n1.0\\nmetrics.adjusted_rand_score(labels_true, labels_pred)\\n1.0\\nPoorly agreeing labels (e.g. independent labelings) have lower scores,\\nand for the adjusted Rand index the score will be negative or close to\\nzero. However, for the unadjusted Rand index the score, while lower,\\nwill not necessarily be close to zero.:\\nlabels_true = [0, 0, 0, 0, 0, 0, 1, 1]\\nlabels_pred = [0, 1, 2, 3, 4, 5, 5, 6]\\nmetrics.rand_score(labels_true, labels_pred)\\n0.39...\\nmetrics.adjusted_rand_score(labels_true, labels_pred)\\n-0.07...\\n2.3.11.1.1. Advantages¶\\nInterpretability: The unadjusted Rand index is proportional\\nto the number of sample pairs whose labels are the same in both\\nlabels_pred and labels_true, or are different in both.\\nRandom (uniform) label assignments have an adjusted Rand index\\nscore close to 0.0 for any value of n_clusters and\\nn_samples (which is not the case for the unadjusted Rand index\\nor the V-measure for instance).\\nBounded range: Lower values indicate different labelings,\\nsimilar clusterings have a high (adjusted or unadjusted) Rand index,\\n1.0 is the perfect match score. The score range is [0, 1] for the\\nunadjusted Rand index and [-1, 1] for the adjusted Rand index.\\nNo assumption is made on the cluster structure: The (adjusted or\\nunadjusted) Rand index can be used to compare all kinds of\\nclustering algorithms, and can be used to compare clustering\\nalgorithms such as k-means which assumes isotropic blob shapes with\\nresults of spectral clustering algorithms which can find cluster\\nwith “folded” shapes.\\n2.3.11.1.2. Drawbacks¶\\nContrary to inertia, the (adjusted or unadjusted) Rand index\\nrequires knowledge of the ground truth classes which is almost\\nnever available in practice or requires manual assignment by human\\nannotators (as in the supervised learning setting).\\nHowever (adjusted or unadjusted) Rand index can also be useful in a\\npurely unsupervised setting as a building block for a Consensus\\nIndex that can be used for clustering model selection (TODO).\\nThe unadjusted Rand index is often close to 1.0 even if the\\nclusterings themselves differ significantly. This can be understood\\nwhen interpreting the Rand index as the accuracy of element pair\\nlabeling resulting from the clusterings: In practice there often is\\na majority of element pairs that are assigned the different pair\\nlabel under both the predicted and the ground truth clustering\\nresulting in a high proportion of pair labels that agree, which\\nleads subsequently to a high score.\\nExamples:\\nAdjustment for chance in clustering performance evaluation:\\nAnalysis of the impact of the dataset size on the value of\\nclustering measures for random assignments.\\n2.3.11.1.3. Mathematical formulation¶\\nIf C is a ground truth class assignment and K the clustering, let us\\ndefine (a) and (b) as:\\n(a), the number of pairs of elements that are in the same set\\nin C and in the same set in K\\n(b), the number of pairs of elements that are in different sets\\nin C and in different sets in K\\nThe unadjusted Rand index is then given by:\\n[\\\\text{RI} = \\\\frac{a + b}{C_2^{n_{samples}}}]\\nwhere (C_2^{n_{samples}}) is the total number of possible pairs\\nin the dataset. It does not matter if the calculation is performed on\\nordered pairs or unordered pairs as long as the calculation is\\nperformed consistently.\\nHowever, the Rand index does not guarantee that random label assignments\\nwill get a value close to zero (esp. if the number of clusters is in\\nthe same order of magnitude as the number of samples).\\nTo counter this effect we can discount the expected RI (E[\\\\text{RI}]) of\\nrandom labelings by defining the adjusted Rand index as follows:\\n[\\\\text{ARI} = \\\\frac{\\\\text{RI} - E[\\\\text{RI}]}{\\\\max(\\\\text{RI}) - E[\\\\text{RI}]}]\\nReferences\\nComparing Partitions\\nL. Hubert and P. Arabie, Journal of Classification 1985\\nProperties of the Hubert-Arabie adjusted Rand index\\nD. Steinley, Psychological Methods 2004\\nWikipedia entry for the Rand index\\nWikipedia entry for the adjusted Rand index\\n2.3.11.2. Mutual Information based scores¶\\nGiven the knowledge of the ground truth class assignments labels_true and\\nour clustering algorithm assignments of the same samples labels_pred, the\\nMutual Information is a function that measures the agreement of the two\\nassignments, ignoring permutations.  Two different normalized versions of this\\nmeasure are available, Normalized Mutual Information (NMI) and Adjusted\\nMutual Information (AMI). NMI is often used in the literature, while AMI was\\nproposed more recently and is normalized against chance:\\nfrom sklearn import metrics\\nlabels_true = [0, 0, 0, 1, 1, 1]\\nlabels_pred = [0, 0, 1, 1, 2, 2]\\nmetrics.adjusted_mutual_info_score(labels_true, labels_pred)\\n0.22504...\\nOne can permute 0 and 1 in the predicted labels, rename 2 to 3 and get\\nthe same score:\\nlabels_pred = [1, 1, 0, 0, 3, 3]\\nmetrics.adjusted_mutual_info_score(labels_true, labels_pred)\\n0.22504...\\nAll, mutual_info_score, adjusted_mutual_info_score and\\nnormalized_mutual_info_score are symmetric: swapping the argument does\\nnot change the score. Thus they can be used as a consensus measure:\\nmetrics.adjusted_mutual_info_score(labels_pred, labels_true)\\n0.22504...\\nPerfect labeling is scored 1.0:\\nlabels_pred = labels_true[:]\\nmetrics.adjusted_mutual_info_score(labels_true, labels_pred)\\n1.0\\nmetrics.normalized_mutual_info_score(labels_true, labels_pred)\\n1.0\\nThis is not true for mutual_info_score, which is therefore harder to judge:\\nmetrics.mutual_info_score(labels_true, labels_pred)\\n0.69...\\nBad (e.g. independent labelings) have non-positive scores:\\nlabels_true = [0, 1, 2, 0, 3, 4, 5, 1]\\nlabels_pred = [1, 1, 0, 0, 2, 2, 2, 2]\\nmetrics.adjusted_mutual_info_score(labels_true, labels_pred)\\n-0.10526...\\n2.3.11.2.1. Advantages¶\\nRandom (uniform) label assignments have a AMI score close to 0.0\\nfor any value of n_clusters and n_samples (which is not the\\ncase for raw Mutual Information or the V-measure for instance).\\nUpper bound  of 1:  Values close to zero indicate two label\\nassignments that are largely independent, while values close to one\\nindicate significant agreement. Further, an AMI of exactly 1 indicates\\nthat the two label assignments are equal (with or without permutation).\\n2.3.11.2.2. Drawbacks¶\\nContrary to inertia, MI-based measures require the knowledge\\nof the ground truth classes while almost never available in practice or\\nrequires manual assignment by human annotators (as in the supervised learning\\nsetting).\\nHowever MI-based measures can also be useful in purely unsupervised setting as a\\nbuilding block for a Consensus Index that can be used for clustering\\nmodel selection.\\nNMI and MI are not adjusted against chance.\\nExamples:\\nAdjustment for chance in clustering performance evaluation: Analysis of\\nthe impact of the dataset size on the value of clustering measures\\nfor random assignments. This example also includes the Adjusted Rand\\nIndex.\\n2.3.11.2.3. Mathematical formulation¶\\nAssume two label assignments (of the same N objects), (U) and (V).\\nTheir entropy is the amount of uncertainty for a partition set, defined by:\\n[H(U) = - \\\\sum_{i=1}^{|U|}P(i)\\\\log(P(i))]\\nwhere (P(i) = |U_i| / N) is the probability that an object picked at\\nrandom from (U) falls into class (U_i). Likewise for (V):\\n[H(V) = - \\\\sum_{j=1}^{|V|}P\\'(j)\\\\log(P\\'(j))]\\nWith (P\\'(j) = |V_j| / N). The mutual information (MI) between (U)\\nand (V) is calculated by:\\n[\\\\text{MI}(U, V) = \\\\sum_{i=1}^{|U|}\\\\sum_{j=1}^{|V|}P(i, j)\\\\log\\\\left(\\\\frac{P(i,j)}{P(i)P\\'(j)}\\\\right)]\\nwhere (P(i, j) = |U_i \\\\cap V_j| / N) is the probability that an object\\npicked at random falls into both classes (U_i) and (V_j).\\nIt also can be expressed in set cardinality formulation:\\n[\\\\text{MI}(U, V) = \\\\sum_{i=1}^{|U|} \\\\sum_{j=1}^{|V|} \\\\frac{|U_i \\\\cap V_j|}{N}\\\\log\\\\left(\\\\frac{N|U_i \\\\cap V_j|}{|U_i||V_j|}\\\\right)]\\nThe normalized mutual information is defined as\\n[\\\\text{NMI}(U, V) = \\\\frac{\\\\text{MI}(U, V)}{\\\\text{mean}(H(U), H(V))}]\\nThis value of the mutual information and also the normalized variant is not\\nadjusted for chance and will tend to increase as the number of different labels\\n(clusters) increases, regardless of the actual amount of “mutual information”\\nbetween the label assignments.\\nThe expected value for the mutual information can be calculated using the\\nfollowing equation [VEB2009]. In this equation,\\n(a_i = |U_i|) (the number of elements in (U_i)) and\\n(b_j = |V_j|) (the number of elements in (V_j)).\\n[E[\\\\text{MI}(U,V)]=\\\\sum_{i=1}^{|U|} \\\\sum_{j=1}^{|V|} \\\\sum_{n_{ij}=(a_i+b_j-N)^+\\n}^{\\\\min(a_i, b_j)} \\\\frac{n_{ij}}{N}\\\\log \\\\left( \\\\frac{ N.n_{ij}}{a_i b_j}\\\\right)\\n\\\\frac{a_i!b_j!(N-a_i)!(N-b_j)!}{N!n_{ij}!(a_i-n_{ij})!(b_j-n_{ij})!\\n(N-a_i-b_j+n_{ij})!}]\\nUsing the expected value, the adjusted mutual information can then be\\ncalculated using a similar form to that of the adjusted Rand index:\\n[\\\\text{AMI} = \\\\frac{\\\\text{MI} - E[\\\\text{MI}]}{\\\\text{mean}(H(U), H(V)) - E[\\\\text{MI}]}]\\nFor normalized mutual information and adjusted mutual information, the normalizing\\nvalue is typically some generalized mean of the entropies of each clustering.\\nVarious generalized means exist, and no firm rules exist for preferring one over the\\nothers.  The decision is largely a field-by-field basis; for instance, in community\\ndetection, the arithmetic mean is most common. Each\\nnormalizing method provides “qualitatively similar behaviours” [YAT2016]. In our\\nimplementation, this is controlled by the average_method parameter.\\nVinh et al. (2010) named variants of NMI and AMI by their averaging method [VEB2010]. Their\\n‘sqrt’ and ‘sum’ averages are the geometric and arithmetic means; we use these\\nmore broadly common names.\\nReferences\\nStrehl, Alexander, and Joydeep Ghosh (2002). “Cluster ensembles – a\\nknowledge reuse framework for combining multiple partitions”. Journal of\\nMachine Learning Research 3: 583–617.\\ndoi:10.1162/153244303321897735.\\nWikipedia entry for the (normalized) Mutual Information\\nWikipedia entry for the Adjusted Mutual Information\\n[VEB2009]\\nVinh, Epps, and Bailey, (2009). “Information theoretic measures\\nfor clusterings comparison”. Proceedings of the 26th Annual International\\nConference on Machine Learning - ICML ‘09.\\ndoi:10.1145/1553374.1553511.\\nISBN 9781605585161.\\n[VEB2010]\\nVinh, Epps, and Bailey, (2010). “Information Theoretic Measures for\\nClusterings Comparison: Variants, Properties, Normalization and\\nCorrection for Chance”. JMLR\\nhttps://jmlr.csail.mit.edu/papers/volume11/vinh10a/vinh10a.pdf\\n[YAT2016]\\nYang, Algesheimer, and Tessone, (2016). “A comparative analysis of\\ncommunity\\ndetection algorithms on artificial networks”. Scientific Reports 6: 30750.\\ndoi:10.1038/srep30750.\\n2.3.11.3. Homogeneity, completeness and V-measure¶\\nGiven the knowledge of the ground truth class assignments of the samples,\\nit is possible to define some intuitive metric using conditional entropy\\nanalysis.\\nIn particular Rosenberg and Hirschberg (2007) define the following two\\ndesirable objectives for any cluster assignment:\\nhomogeneity: each cluster contains only members of a single class.\\ncompleteness: all members of a given class are assigned to the same\\ncluster.\\nWe can turn those concept as scores homogeneity_score and\\ncompleteness_score. Both are bounded below by 0.0 and above by\\n1.0 (higher is better):\\nfrom sklearn import metrics\\nlabels_true = [0, 0, 0, 1, 1, 1]\\nlabels_pred = [0, 0, 1, 1, 2, 2]\\nmetrics.homogeneity_score(labels_true, labels_pred)\\n0.66...\\nmetrics.completeness_score(labels_true, labels_pred)\\n0.42...\\nTheir harmonic mean called V-measure is computed by\\nv_measure_score:\\nmetrics.v_measure_score(labels_true, labels_pred)\\n0.51...\\nThis function’s formula is as follows:\\n[v = \\\\frac{(1 + \\\\beta) \\\\times \\\\text{homogeneity} \\\\times \\\\text{completeness}}{(\\\\beta \\\\times \\\\text{homogeneity} + \\\\text{completeness})}]\\nbeta defaults to a value of 1.0, but for using a value less than 1 for beta:\\nmetrics.v_measure_score(labels_true, labels_pred, beta=0.6)\\n0.54...\\nmore weight will be attributed to homogeneity, and using a value greater than 1:\\nmetrics.v_measure_score(labels_true, labels_pred, beta=1.8)\\n0.48...\\nmore weight will be attributed to completeness.\\nThe V-measure is actually equivalent to the mutual information (NMI)\\ndiscussed above, with the aggregation function being the arithmetic mean [B2011].\\nHomogeneity, completeness and V-measure can be computed at once using\\nhomogeneity_completeness_v_measure as follows:\\nmetrics.homogeneity_completeness_v_measure(labels_true, labels_pred)\\n(0.66..., 0.42..., 0.51...)\\nThe following clustering assignment is slightly better, since it is\\nhomogeneous but not complete:\\nlabels_pred = [0, 0, 0, 1, 2, 2]\\nmetrics.homogeneity_completeness_v_measure(labels_true, labels_pred)\\n(1.0, 0.68..., 0.81...)\\nNote\\nv_measure_score is symmetric: it can be used to evaluate\\nthe agreement of two independent assignments on the same dataset.\\nThis is not the case for completeness_score and\\nhomogeneity_score: both are bound by the relationship:\\nhomogeneity_score(a, b) == completeness_score(b, a)\\n2.3.11.3.1. Advantages¶\\nBounded scores: 0.0 is as bad as it can be, 1.0 is a perfect score.\\nIntuitive interpretation: clustering with bad V-measure can be\\nqualitatively analyzed in terms of homogeneity and completeness\\nto better feel what ‘kind’ of mistakes is done by the assignment.\\nNo assumption is made on the cluster structure: can be used\\nto compare clustering algorithms such as k-means which assumes isotropic\\nblob shapes with results of spectral clustering algorithms which can\\nfind cluster with “folded” shapes.\\n2.3.11.3.2. Drawbacks¶\\nThe previously introduced metrics are not normalized with regards to\\nrandom labeling: this means that depending on the number of samples,\\nclusters and ground truth classes, a completely random labeling will\\nnot always yield the same values for homogeneity, completeness and\\nhence v-measure. In particular random labeling won’t yield zero\\nscores especially when the number of clusters is large.\\nThis problem can safely be ignored when the number of samples is more\\nthan a thousand and the number of clusters is less than 10. For\\nsmaller sample sizes or larger number of clusters it is safer to use\\nan adjusted index such as the Adjusted Rand Index (ARI).\\nThese metrics require the knowledge of the ground truth classes while\\nalmost never available in practice or requires manual assignment by\\nhuman annotators (as in the supervised learning setting).\\nExamples:\\nAdjustment for chance in clustering performance evaluation: Analysis of\\nthe impact of the dataset size on the value of clustering measures\\nfor random assignments.\\n2.3.11.3.3. Mathematical formulation¶\\nHomogeneity and completeness scores are formally given by:\\n[h = 1 - \\\\frac{H(C|K)}{H(C)}]\\n[c = 1 - \\\\frac{H(K|C)}{H(K)}]\\nwhere (H(C|K)) is the conditional entropy of the classes given\\nthe cluster assignments and is given by:\\n[H(C|K) = - \\\\sum_{c=1}^{|C|} \\\\sum_{k=1}^{|K|} \\\\frac{n_{c,k}}{n}\\n\\\\cdot \\\\log\\\\left(\\\\frac{n_{c,k}}{n_k}\\\\right)]\\nand (H(C)) is the entropy of the classes and is given by:\\n[H(C) = - \\\\sum_{c=1}^{|C|} \\\\frac{n_c}{n} \\\\cdot \\\\log\\\\left(\\\\frac{n_c}{n}\\\\right)]\\nwith (n) the total number of samples, (n_c) and (n_k)\\nthe number of samples respectively belonging to class (c) and\\ncluster (k), and finally (n_{c,k}) the number of samples\\nfrom class (c) assigned to cluster (k).\\nThe conditional entropy of clusters given class (H(K|C)) and the\\nentropy of clusters (H(K)) are defined in a symmetric manner.\\nRosenberg and Hirschberg further define V-measure as the harmonic\\nmean of homogeneity and completeness:\\n[v = 2 \\\\cdot \\\\frac{h \\\\cdot c}{h + c}]\\nReferences\\nV-Measure: A conditional entropy-based external cluster evaluation\\nmeasure\\nAndrew Rosenberg and Julia Hirschberg, 2007\\n[B2011]\\nIdentification and Characterization of Events in Social Media, Hila\\nBecker, PhD Thesis.\\n2.3.11.4. Fowlkes-Mallows scores¶\\nThe Fowlkes-Mallows index (sklearn.metrics.fowlkes_mallows_score) can be\\nused when the ground truth class assignments of the samples is known. The\\nFowlkes-Mallows score FMI is defined as the geometric mean of the\\npairwise precision and recall:\\n[\\\\text{FMI} = \\\\frac{\\\\text{TP}}{\\\\sqrt{(\\\\text{TP} + \\\\text{FP}) (\\\\text{TP} + \\\\text{FN})}}]\\nWhere TP is the number of True Positive (i.e. the number of pair\\nof points that belong to the same clusters in both the true labels and the\\npredicted labels), FP is the number of False Positive (i.e. the number\\nof pair of points that belong to the same clusters in the true labels and not\\nin the predicted labels) and FN is the number of False Negative (i.e. the\\nnumber of pair of points that belongs in the same clusters in the predicted\\nlabels and not in the true labels).\\nThe score ranges from 0 to 1. A high value indicates a good similarity\\nbetween two clusters.\\nfrom sklearn import metrics\\nlabels_true = [0, 0, 0, 1, 1, 1]\\nlabels_pred = [0, 0, 1, 1, 2, 2]\\nmetrics.fowlkes_mallows_score(labels_true, labels_pred)\\n0.47140...\\nOne can permute 0 and 1 in the predicted labels, rename 2 to 3 and get\\nthe same score:\\nlabels_pred = [1, 1, 0, 0, 3, 3]\\nmetrics.fowlkes_mallows_score(labels_true, labels_pred)\\n0.47140...\\nPerfect labeling is scored 1.0:\\nlabels_pred = labels_true[:]\\nmetrics.fowlkes_mallows_score(labels_true, labels_pred)\\n1.0\\nBad (e.g. independent labelings) have zero scores:\\nlabels_true = [0, 1, 2, 0, 3, 4, 5, 1]\\nlabels_pred = [1, 1, 0, 0, 2, 2, 2, 2]\\nmetrics.fowlkes_mallows_score(labels_true, labels_pred)\\n0.0\\n2.3.11.4.1. Advantages¶\\nRandom (uniform) label assignments have a FMI score close to 0.0\\nfor any value of n_clusters and n_samples (which is not the\\ncase for raw Mutual Information or the V-measure for instance).\\nUpper-bounded at 1:  Values close to zero indicate two label\\nassignments that are largely independent, while values close to one\\nindicate significant agreement. Further, values of exactly 0 indicate\\npurely independent label assignments and a FMI of exactly 1 indicates\\nthat the two label assignments are equal (with or without permutation).\\nNo assumption is made on the cluster structure: can be used\\nto compare clustering algorithms such as k-means which assumes isotropic\\nblob shapes with results of spectral clustering algorithms which can\\nfind cluster with “folded” shapes.\\n2.3.11.4.2. Drawbacks¶\\nContrary to inertia, FMI-based measures require the knowledge\\nof the ground truth classes while almost never available in practice or\\nrequires manual assignment by human annotators (as in the supervised learning\\nsetting).\\nReferences\\nE. B. Fowkles and C. L. Mallows, 1983. “A method for comparing two\\nhierarchical clusterings”. Journal of the American Statistical Association.\\nhttps://www.tandfonline.com/doi/abs/10.1080/01621459.1983.10478008\\nWikipedia entry for the Fowlkes-Mallows Index\\n2.3.11.5. Silhouette Coefficient¶\\nIf the ground truth labels are not known, evaluation must be performed using\\nthe model itself. The Silhouette Coefficient\\n(sklearn.metrics.silhouette_score)\\nis an example of such an evaluation, where a\\nhigher Silhouette Coefficient score relates to a model with better defined\\nclusters. The Silhouette Coefficient is defined for each sample and is composed\\nof two scores:\\na: The mean distance between a sample and all other points in the same\\nclass.\\nb: The mean distance between a sample and all other points in the next\\nnearest cluster.\\nThe Silhouette Coefficient s for a single sample is then given as:\\n[s = \\\\frac{b - a}{max(a, b)}]\\nThe Silhouette Coefficient for a set of samples is given as the mean of the\\nSilhouette Coefficient for each sample.\\nfrom sklearn import metrics\\nfrom sklearn.metrics import pairwise_distances\\nfrom sklearn import datasets\\nX, y = datasets.load_iris(return_X_y=True)\\nIn normal usage, the Silhouette Coefficient is applied to the results of a\\ncluster analysis.\\nimport numpy as np\\nfrom sklearn.cluster import KMeans\\nkmeans_model = KMeans(n_clusters=3, random_state=1).fit(X)\\nlabels = kmeans_model.labels_\\nmetrics.silhouette_score(X, labels, metric=\\'euclidean\\')\\n0.55...\\nReferences\\nPeter J. Rousseeuw (1987). “Silhouettes: a Graphical Aid to the\\nInterpretation and Validation of Cluster Analysis”\\n. Computational and Applied Mathematics 20: 53–65.\\n2.3.11.5.1. Advantages¶\\nThe score is bounded between -1 for incorrect clustering and +1 for highly\\ndense clustering. Scores around zero indicate overlapping clusters.\\nThe score is higher when clusters are dense and well separated, which relates\\nto a standard concept of a cluster.\\n2.3.11.5.2. Drawbacks¶\\nThe Silhouette Coefficient is generally higher for convex clusters than other\\nconcepts of clusters, such as density based clusters like those obtained\\nthrough DBSCAN.\\nExamples:\\nSelecting the number of clusters with silhouette analysis on KMeans clustering : In this example\\nthe silhouette analysis is used to choose an optimal value for n_clusters.\\n2.3.11.6. Calinski-Harabasz Index¶\\nIf the ground truth labels are not known, the Calinski-Harabasz index\\n(sklearn.metrics.calinski_harabasz_score) - also known as the Variance\\nRatio Criterion - can be used to evaluate the model, where a higher\\nCalinski-Harabasz score relates to a model with better defined clusters.\\nThe index is the ratio of the sum of between-clusters dispersion and of\\nwithin-cluster dispersion for all clusters (where dispersion is defined as the\\nsum of distances squared):\\nfrom sklearn import metrics\\nfrom sklearn.metrics import pairwise_distances\\nfrom sklearn import datasets\\nX, y = datasets.load_iris(return_X_y=True)\\nIn normal usage, the Calinski-Harabasz index is applied to the results of a\\ncluster analysis:\\nimport numpy as np\\nfrom sklearn.cluster import KMeans\\nkmeans_model = KMeans(n_clusters=3, random_state=1).fit(X)\\nlabels = kmeans_model.labels_\\nmetrics.calinski_harabasz_score(X, labels)\\n561.59...\\n2.3.11.6.1. Advantages¶\\nThe score is higher when clusters are dense and well separated, which relates\\nto a standard concept of a cluster.\\nThe score is fast to compute.\\n2.3.11.6.2. Drawbacks¶\\nThe Calinski-Harabasz index is generally higher for convex clusters than other\\nconcepts of clusters, such as density based clusters like those obtained\\nthrough DBSCAN.\\n2.3.11.6.3. Mathematical formulation¶\\nFor a set of data (E) of size (n_E) which has been clustered into\\n(k) clusters, the Calinski-Harabasz score (s) is defined as the\\nratio of the between-clusters dispersion mean and the within-cluster dispersion:\\n[s = \\\\frac{\\\\mathrm{tr}(B_k)}{\\\\mathrm{tr}(W_k)} \\\\times \\\\frac{n_E - k}{k - 1}]\\nwhere (\\\\mathrm{tr}(B_k)) is trace of the between group dispersion matrix\\nand (\\\\mathrm{tr}(W_k)) is the trace of the within-cluster dispersion\\nmatrix defined by:\\n[W_k = \\\\sum_{q=1}^k \\\\sum_{x \\\\in C_q} (x - c_q) (x - c_q)^T]\\n[B_k = \\\\sum_{q=1}^k n_q (c_q - c_E) (c_q - c_E)^T]\\nwith (C_q) the set of points in cluster (q), (c_q) the center\\nof cluster (q), (c_E) the center of (E), and (n_q) the\\nnumber of points in cluster (q).\\nReferences\\nCaliński, T., & Harabasz, J. (1974).\\n“A Dendrite Method for Cluster Analysis”.\\nCommunications in Statistics-theory and Methods 3: 1-27.\\n2.3.11.7. Davies-Bouldin Index¶\\nIf the ground truth labels are not known, the Davies-Bouldin index\\n(sklearn.metrics.davies_bouldin_score) can be used to evaluate the\\nmodel, where a lower Davies-Bouldin index relates to a model with better\\nseparation between the clusters.\\nThis index signifies the average ‘similarity’ between clusters, where the\\nsimilarity is a measure that compares the distance between clusters with the\\nsize of the clusters themselves.\\nZero is the lowest possible score. Values closer to zero indicate a better\\npartition.\\nIn normal usage, the Davies-Bouldin index is applied to the results of a\\ncluster analysis as follows:\\nfrom sklearn import datasets\\niris = datasets.load_iris()\\nX = iris.data\\nfrom sklearn.cluster import KMeans\\nfrom sklearn.metrics import davies_bouldin_score\\nkmeans = KMeans(n_clusters=3, random_state=1).fit(X)\\nlabels = kmeans.labels_\\ndavies_bouldin_score(X, labels)\\n0.666...\\n2.3.11.7.1. Advantages¶\\nThe computation of Davies-Bouldin is simpler than that of Silhouette scores.\\nThe index is solely based on quantities and features inherent to the dataset\\nas its computation only uses point-wise distances.\\n2.3.11.7.2. Drawbacks¶\\nThe Davies-Boulding index is generally higher for convex clusters than other\\nconcepts of clusters, such as density based clusters like those obtained from\\nDBSCAN.\\nThe usage of centroid distance limits the distance metric to Euclidean space.\\n2.3.11.7.3. Mathematical formulation¶\\nThe index is defined as the average similarity between each cluster (C_i)\\nfor (i=1, ..., k) and its most similar one (C_j). In the context of\\nthis index, similarity is defined as a measure (R_{ij}) that trades off:\\n(s_i), the average distance between each point of cluster (i) and\\nthe centroid of that cluster – also know as cluster diameter.\\n(d_{ij}), the distance between cluster centroids (i) and (j).\\nA simple choice to construct (R_{ij}) so that it is nonnegative and\\nsymmetric is:\\n[R_{ij} = \\\\frac{s_i + s_j}{d_{ij}}]\\nThen the Davies-Bouldin index is defined as:\\n[DB = \\\\frac{1}{k} \\\\sum_{i=1}^k \\\\max_{i \\\\neq j} R_{ij}]\\nReferences\\nDavies, David L.; Bouldin, Donald W. (1979).\\n“A Cluster Separation Measure”\\nIEEE Transactions on Pattern Analysis and Machine Intelligence.\\nPAMI-1 (2): 224-227.\\nHalkidi, Maria; Batistakis, Yannis; Vazirgiannis, Michalis (2001).\\n“On Clustering Validation Techniques”\\nJournal of Intelligent Information Systems, 17(2-3), 107-145.\\nWikipedia entry for Davies-Bouldin index.\\n2.3.11.8. Contingency Matrix¶\\nContingency matrix (sklearn.metrics.cluster.contingency_matrix)\\nreports the intersection cardinality for every true/predicted cluster pair.\\nThe contingency matrix provides sufficient statistics for all clustering\\nmetrics where the samples are independent and identically distributed and\\none doesn’t need to account for some instances not being clustered.\\nHere is an example:\\nfrom sklearn.metrics.cluster import contingency_matrix\\nx = [\"a\", \"a\", \"a\", \"b\", \"b\", \"b\"]\\ny = [0, 0, 1, 1, 2, 2]\\ncontingency_matrix(x, y)\\narray([[2, 1, 0],\\n[0, 1, 2]])\\nThe first row of output array indicates that there are three samples whose\\ntrue cluster is “a”. Of them, two are in predicted cluster 0, one is in 1,\\nand none is in 2. And the second row indicates that there are three samples\\nwhose true cluster is “b”. Of them, none is in predicted cluster 0, one is in\\n1 and two are in 2.\\nA confusion matrix for classification is a square\\ncontingency matrix where the order of rows and columns correspond to a list\\nof classes.\\n2.3.11.8.1. Advantages¶\\nAllows to examine the spread of each true cluster across predicted\\nclusters and vice versa.\\nThe contingency table calculated is typically utilized in the calculation\\nof a similarity statistic (like the others listed in this document) between\\nthe two clusterings.\\n2.3.11.8.2. Drawbacks¶\\nContingency matrix is easy to interpret for a small number of clusters, but\\nbecomes very hard to interpret for a large number of clusters.\\nIt doesn’t give a single metric to use as an objective for clustering\\noptimisation.\\nReferences\\nWikipedia entry for contingency matrix\\n2.3.11.9. Pair Confusion Matrix¶\\nThe pair confusion matrix\\n(sklearn.metrics.cluster.pair_confusion_matrix) is a 2x2\\nsimilarity matrix\\n[\\\\begin{split}C = \\\\left[\\\\begin{matrix}\\nC_{00} & C_{01} \\\\\\nC_{10} & C_{11}\\n\\\\end{matrix}\\\\right]\\\\end{split}]\\nbetween two clusterings computed by considering all pairs of samples and\\ncounting pairs that are assigned into the same or into different clusters\\nunder the true and predicted clusterings.\\nIt has the following entries:\\n(C_{00}) : number of pairs with both clusterings having the samples\\nnot clustered together\\n(C_{10}) : number of pairs with the true label clustering having the\\nsamples clustered together but the other clustering not having the samples\\nclustered together\\n(C_{01}) : number of pairs with the true label clustering not having\\nthe samples clustered together but the other clustering having the samples\\nclustered together\\n(C_{11}) : number of pairs with both clusterings having the samples\\nclustered together\\nConsidering a pair of samples that is clustered together a positive pair,\\nthen as in binary classification the count of true negatives is\\n(C_{00}), false negatives is (C_{10}), true positives is\\n(C_{11}) and false positives is (C_{01}).\\nPerfectly matching labelings have all non-zero entries on the\\ndiagonal regardless of actual label values:\\nfrom sklearn.metrics.cluster import pair_confusion_matrix\\npair_confusion_matrix([0, 0, 1, 1], [0, 0, 1, 1])\\narray([[8, 0],\\n[0, 4]])\\npair_confusion_matrix([0, 0, 1, 1], [1, 1, 0, 0])\\narray([[8, 0],\\n[0, 4]])\\nLabelings that assign all classes members to the same clusters\\nare complete but may not always be pure, hence penalized, and\\nhave some off-diagonal non-zero entries:\\npair_confusion_matrix([0, 0, 1, 2], [0, 0, 1, 1])\\narray([[8, 2],\\n[0, 2]])\\nThe matrix is not symmetric:\\npair_confusion_matrix([0, 0, 1, 1], [0, 0, 1, 2])\\narray([[8, 0],\\n[2, 2]])\\nIf classes members are completely split across different clusters, the\\nassignment is totally incomplete, hence the matrix has all zero\\ndiagonal entries:\\npair_confusion_matrix([0, 0, 0, 0], [0, 1, 2, 3])\\narray([[ 0,  0],\\n[12,  0]])\\nReferences\\n“Comparing Partitions”\\nL. Hubert and P. Arabie, Journal of Classification 1985\\n© 2007 - 2024, scikit-learn developers (BSD License).\\nShow this page source\\n\\n2.4. Biclustering — scikit-learn 1.4.2 documentation\\n\\nInstall\\nUser Guide\\nAPI\\nExamples\\nCommunity\\nGetting Started\\nTutorial\\nWhat\\'s new\\nGlossary\\nDevelopment\\nFAQ\\nSupport\\nRelated packages\\nRoadmap\\nGovernance\\nAbout us\\nGitHub\\nOther Versions and Download\\nMore\\nGetting Started\\nTutorial\\nWhat\\'s new\\nGlossary\\nDevelopment\\nFAQ\\nSupport\\nRelated packages\\nRoadmap\\nGovernance\\nAbout us\\nGitHub\\nOther Versions and Download\\nToggle Menu\\nPrevUp\\nNext\\nscikit-learn 1.4.2\\nOther versions\\nPlease cite us if you use the software.\\n\\n2.4. Biclustering\\n\\n2.4.1. Spectral Co-Clustering\\n2.4.1.1. Mathematical formulation\\n\\n2.4.2. Spectral Biclustering\\n2.4.2.1. Mathematical formulation\\n\\n2.4.3. Biclustering evaluation\\n\\n2.4. Biclustering¶\\n\\nBiclustering algorithms simultaneously\\ncluster rows and columns of a data matrix. These clusters of rows and\\ncolumns are known as biclusters. Each determines a submatrix of the\\noriginal data matrix with some desired properties.\\nFor instance, given a matrix of shape (10, 10), one possible bicluster\\nwith three rows and two columns induces a submatrix of shape (3, 2):\\n\\nimport numpy as np\\ndata = np.arange(100).reshape(10, 10)\\nrows = np.array([0, 2, 3])[:, np.newaxis]\\ncolumns = np.array([1, 2])\\ndata[rows, columns]\\narray([[ 1,  2],\\n[21, 22],\\n[31, 32]])\\nFor visualization purposes, given a bicluster, the rows and columns of\\nthe data matrix may be rearranged to make the bicluster contiguous.\\nAlgorithms differ in how they define biclusters. Some of the\\ncommon types include:\\nconstant values, constant rows, or constant columns\\nunusually high or low values\\nsubmatrices with low variance\\ncorrelated rows or columns\\nAlgorithms also differ in how rows and columns may be assigned to\\nbiclusters, which leads to different bicluster structures. Block\\ndiagonal or checkerboard structures occur when rows and columns are\\ndivided into partitions.\\nIf each row and each column belongs to exactly one bicluster, then\\nrearranging the rows and columns of the data matrix reveals the\\nbiclusters on the diagonal. Here is an example of this structure\\nwhere biclusters have higher average values than the other rows and\\ncolumns:\\nAn example of biclusters formed by partitioning rows and columns.¶\\nIn the checkerboard case, each row belongs to all column clusters, and\\neach column belongs to all row clusters. Here is an example of this\\nstructure where the variance of the values within each bicluster is\\nsmall:\\nAn example of checkerboard biclusters.¶\\nAfter fitting a model, row and column cluster membership can be found\\nin the rows_ and columns_ attributes. rows_[i] is a binary vector\\nwith nonzero entries corresponding to rows that belong to bicluster\\ni. Similarly, columns_[i] indicates which columns belong to\\nbicluster i.\\nSome models also have row_labels_ and column_labels_ attributes.\\nThese models partition the rows and columns, such as in the block\\ndiagonal and checkerboard bicluster structures.\\nNote\\nBiclustering has many other names in different fields including\\nco-clustering, two-mode clustering, two-way clustering, block\\nclustering, coupled two-way clustering, etc. The names of some\\nalgorithms, such as the Spectral Co-Clustering algorithm, reflect\\nthese alternate names.\\n- 2.4.1. Spectral Co-Clustering¶\\nThe SpectralCoclustering algorithm finds biclusters with\\nvalues higher than those in the corresponding other rows and columns.\\nEach row and each column belongs to exactly one bicluster, so\\nrearranging the rows and columns to make partitions contiguous reveals\\nthese high values along the diagonal:\\nNote\\nThe algorithm treats the input data matrix as a bipartite graph: the\\nrows and columns of the matrix correspond to the two sets of vertices,\\nand each entry corresponds to an edge between a row and a column. The\\nalgorithm approximates the normalized cut of this graph to find heavy\\nsubgraphs.\\n2.4.1.1. Mathematical formulation¶\\nAn approximate solution to the optimal normalized cut may be found via\\nthe generalized eigenvalue decomposition of the Laplacian of the\\ngraph. Usually this would mean working directly with the Laplacian\\nmatrix. If the original data matrix (A) has shape (m\\n\\\\times n), the Laplacian matrix for the corresponding bipartite graph\\nhas shape ((m + n) \\\\times (m + n)). However, in this case it is\\npossible to work directly with (A), which is smaller and more\\nefficient.\\nThe input matrix (A) is preprocessed as follows:\\n[A_n = R^{-1/2} A C^{-1/2}]\\nWhere (R) is the diagonal matrix with entry (i) equal to\\n(\\\\sum_{j} A_{ij}) and (C) is the diagonal matrix with\\nentry (j) equal to (\\\\sum_{i} A_{ij}).\\nThe singular value decomposition, (A_n = U \\\\Sigma V^\\\\top),\\nprovides the partitions of the rows and columns of (A). A subset\\nof the left singular vectors gives the row partitions, and a subset\\nof the right singular vectors gives the column partitions.\\nThe (\\\\ell = \\\\lceil \\\\log_2 k \\\\rceil) singular vectors, starting\\nfrom the second, provide the desired partitioning information. They\\nare used to form the matrix (Z):\\n[\\\\begin{split}Z = \\\\begin{bmatrix} R^{-1/2} U \\\\\\\\\\nC^{-1/2} V\\n\\\\end{bmatrix}\\\\end{split}]\\nwhere the columns of (U) are (u_2, \\\\dots, u_{\\\\ell +\\n1}), and similarly for (V).\\nThen the rows of (Z) are clustered using k-means. The first n_rows labels provide the row partitioning,\\nand the remaining n_columns labels provide the column partitioning.\\nExamples:\\nA demo of the Spectral Co-Clustering algorithm: A simple example\\nshowing how to generate a data matrix with biclusters and apply\\nthis method to it.\\nBiclustering documents with the Spectral Co-clustering algorithm: An example of finding\\nbiclusters in the twenty newsgroup dataset.\\nReferences:\\nDhillon, Inderjit S, 2001. Co-clustering documents and words using\\nbipartite spectral graph partitioning\\n- 2.4.2. Spectral Biclustering¶\\nThe SpectralBiclustering algorithm assumes that the input\\ndata matrix has a hidden checkerboard structure. The rows and columns\\nof a matrix with this structure may be partitioned so that the entries\\nof any bicluster in the Cartesian product of row clusters and column\\nclusters are approximately constant. For instance, if there are two\\nrow partitions and three column partitions, each row will belong to\\nthree biclusters, and each column will belong to two biclusters.\\nThe algorithm partitions the rows and columns of a matrix so that a\\ncorresponding blockwise-constant checkerboard matrix provides a good\\napproximation to the original matrix.\\n2.4.2.1. Mathematical formulation¶\\nThe input matrix (A) is first normalized to make the\\ncheckerboard pattern more obvious. There are three possible methods:\\nIndependent row and column normalization, as in Spectral\\nCo-Clustering. This method makes the rows sum to a constant and the\\ncolumns sum to a different constant.\\nBistochastization: repeated row and column normalization until\\nconvergence. This method makes both rows and columns sum to the\\nsame constant.\\nLog normalization: the log of the data matrix is computed: (L =\\n\\\\log A). Then the column mean (\\\\overline{L_{i \\\\cdot}}), row mean\\n(\\\\overline{L_{\\\\cdot j}}), and overall mean (\\\\overline{L_{\\\\cdot\\n\\\\cdot}}) of (L) are computed. The final matrix is computed\\naccording to the formula\\n[K_{ij} = L_{ij} - \\\\overline{L_{i \\\\cdot}} - \\\\overline{L_{\\\\cdot\\nj}} + \\\\overline{L_{\\\\cdot \\\\cdot}}]\\nAfter normalizing, the first few singular vectors are computed, just\\nas in the Spectral Co-Clustering algorithm.\\nIf log normalization was used, all the singular vectors are\\nmeaningful. However, if independent normalization or bistochastization\\nwere used, the first singular vectors, (u_1) and (v_1).\\nare discarded. From now on, the “first” singular vectors refers to\\n(u_2 \\\\dots u_{p+1}) and (v_2 \\\\dots v_{p+1}) except in the\\ncase of log normalization.\\nGiven these singular vectors, they are ranked according to which can\\nbe best approximated by a piecewise-constant vector. The\\napproximations for each vector are found using one-dimensional k-means\\nand scored using the Euclidean distance. Some subset of the best left\\nand right singular vector are selected. Next, the data is projected to\\nthis best subset of singular vectors and clustered.\\nFor instance, if (p) singular vectors were calculated, the\\n(q) best are found as described, where (q<p). Let\\n(U) be the matrix with columns the (q) best left singular\\nvectors, and similarly (V) for the right. To partition the rows,\\nthe rows of (A) are projected to a (q) dimensional space:\\n(A * V). Treating the (m) rows of this (m \\\\times q)\\nmatrix as samples and clustering using k-means yields the row labels.\\nSimilarly, projecting the columns to (A^{\\\\top} * U) and\\nclustering this (n \\\\times q) matrix yields the column labels.\\nExamples:\\nA demo of the Spectral Biclustering algorithm: a simple example\\nshowing how to generate a checkerboard matrix and bicluster it.\\nReferences:\\nKluger, Yuval, et. al., 2003. Spectral biclustering of microarray\\ndata: coclustering genes and conditions\\n- 2.4.3. Biclustering evaluation¶\\nThere are two ways of evaluating a biclustering result: internal and\\nexternal. Internal measures, such as cluster stability, rely only on\\nthe data and the result themselves. Currently there are no internal\\nbicluster measures in scikit-learn. External measures refer to an\\nexternal source of information, such as the true solution. When\\nworking with real data the true solution is usually unknown, but\\nbiclustering artificial data may be useful for evaluating algorithms\\nprecisely because the true solution is known.\\nTo compare a set of found biclusters to the set of true biclusters,\\ntwo similarity measures are needed: a similarity measure for\\nindividual biclusters, and a way to combine these individual\\nsimilarities into an overall score.\\nTo compare individual biclusters, several measures have been used. For\\nnow, only the Jaccard index is implemented:\\n[J(A, B) = \\\\frac{|A \\\\cap B|}{|A| + |B| - |A \\\\cap B|}]\\nwhere (A) and (B) are biclusters, (|A \\\\cap B|) is\\nthe number of elements in their intersection. The Jaccard index\\nachieves its minimum of 0 when the biclusters to not overlap at all\\nand its maximum of 1 when they are identical.\\nSeveral methods have been developed to compare two sets of biclusters.\\nFor now, only consensus_score (Hochreiter et. al., 2010) is\\navailable:\\nCompute bicluster similarities for pairs of biclusters, one in each\\nset, using the Jaccard index or a similar measure.\\nAssign biclusters from one set to another in a one-to-one fashion\\nto maximize the sum of their similarities. This step is performed\\nusing the Hungarian algorithm.\\nThe final sum of similarities is divided by the size of the larger\\nset.\\nThe minimum consensus score, 0, occurs when all pairs of biclusters\\nare totally dissimilar. The maximum score, 1, occurs when both sets\\nare identical.\\nReferences:\\nHochreiter, Bodenhofer, et. al., 2010. FABIA: factor analysis\\nfor bicluster acquisition.\\n\\n2.5. Decomposing signals in components (matrix factorization problems) — scikit-learn 1.4.2 documentation\\n\\nInstall\\nUser Guide\\nAPI\\nExamples\\nCommunity\\nGetting Started\\nTutorial\\nWhat\\'s new\\nGlossary\\nDevelopment\\nFAQ\\nSupport\\nRelated packages\\nRoadmap\\nGovernance\\nAbout us\\nGitHub\\nOther Versions and Download\\nMore\\nGetting Started\\nTutorial\\nWhat\\'s new\\nGlossary\\nDevelopment\\nFAQ\\nSupport\\nRelated packages\\nRoadmap\\nGovernance\\nAbout us\\nGitHub\\nOther Versions and Download\\nToggle Menu\\nPrevUp\\nNext\\nscikit-learn 1.4.2\\nOther versions\\nPlease cite us if you use the software.\\n\\n2.5. Decomposing signals in components (matrix factorization problems)\\n\\n2.5.1. Principal component analysis (PCA)\\n2.5.1.1. Exact PCA and probabilistic interpretation\\n2.5.1.2. Incremental PCA\\n2.5.1.3. PCA using randomized SVD\\n2.5.1.4. Sparse principal components analysis (SparsePCA and MiniBatchSparsePCA)\\n\\n2.5.2. Kernel Principal Component Analysis (kPCA)\\n2.5.2.1. Exact Kernel PCA\\n2.5.2.2. Choice of solver for Kernel PCA\\n\\n2.5.3. Truncated singular value decomposition and latent semantic analysis\\n\\n2.5.4. Dictionary Learning\\n2.5.4.1. Sparse coding with a precomputed dictionary\\n2.5.4.2. Generic dictionary learning\\n2.5.4.3. Mini-batch dictionary learning\\n\\n2.5.5. Factor Analysis\\n\\n2.5.6. Independent component analysis (ICA)\\n\\n2.5.7. Non-negative matrix factorization (NMF or NNMF)\\n2.5.7.1. NMF with the Frobenius norm\\n2.5.7.2. NMF with a beta-divergence\\n2.5.7.3. Mini-batch Non Negative Matrix Factorization\\n\\n2.5.8. Latent Dirichlet Allocation (LDA)\\n\\n2.5. Decomposing signals in components (matrix factorization problems)¶\\n\\n2.5.1. Principal component analysis (PCA)¶\\n2.5.1.1. Exact PCA and probabilistic interpretation¶\\nPCA is used to decompose a multivariate dataset in a set of successive\\northogonal components that explain a maximum amount of the variance. In\\nscikit-learn, PCA is implemented as a transformer object\\nthat learns (n) components in its fit method, and can be used on new\\ndata to project it on these components.\\nPCA centers but does not scale the input data for each feature before\\napplying the SVD. The optional parameter whiten=True makes it\\npossible to project the data onto the singular space while scaling each\\ncomponent to unit variance. This is often useful if the models down-stream make\\nstrong assumptions on the isotropy of the signal: this is for example the case\\nfor Support Vector Machines with the RBF kernel and the K-Means clustering\\nalgorithm.\\nBelow is an example of the iris dataset, which is comprised of 4\\nfeatures, projected on the 2 dimensions that explain most variance:\\nThe PCA object also provides a\\nprobabilistic interpretation of the PCA that can give a likelihood of\\ndata based on the amount of variance it explains. As such it implements a\\nscore method that can be used in cross-validation:\\nExamples:\\nPCA example with Iris Data-set\\nComparison of LDA and PCA 2D projection of Iris dataset\\nModel selection with Probabilistic PCA and Factor Analysis (FA)\\n2.5.1.2. Incremental PCA¶\\nThe PCA object is very useful, but has certain limitations for\\nlarge datasets. The biggest limitation is that PCA only supports\\nbatch processing, which means all of the data to be processed must fit in main\\nmemory. The IncrementalPCA object uses a different form of\\nprocessing and allows for partial computations which almost\\nexactly match the results of PCA while processing the data in a\\nminibatch fashion. IncrementalPCA makes it possible to implement\\nout-of-core Principal Component Analysis either by:\\nUsing its partial_fit method on chunks of data fetched sequentially\\nfrom the local hard drive or a network database.\\nCalling its fit method on a memory mapped file using\\nnumpy.memmap.\\nIncrementalPCA only stores estimates of component and noise variances,\\nin order update explained_variance_ratio_ incrementally. This is why\\nmemory usage depends on the number of samples per batch, rather than the\\nnumber of samples to be processed in the dataset.\\nAs in PCA, IncrementalPCA centers but does not scale the\\ninput data for each feature before applying the SVD.\\nExamples:\\nIncremental PCA\\n2.5.1.3. PCA using randomized SVD¶\\nIt is often interesting to project data to a lower-dimensional\\nspace that preserves most of the variance, by dropping the singular vector\\nof components associated with lower singular values.\\nFor instance, if we work with 64x64 pixel gray-level pictures\\nfor face recognition,\\nthe dimensionality of the data is 4096 and it is slow to train an\\nRBF support vector machine on such wide data. Furthermore we know that\\nthe intrinsic dimensionality of the data is much lower than 4096 since all\\npictures of human faces look somewhat alike.\\nThe samples lie on a manifold of much lower\\ndimension (say around 200 for instance). The PCA algorithm can be used\\nto linearly transform the data while both reducing the dimensionality\\nand preserve most of the explained variance at the same time.\\nThe class PCA used with the optional parameter\\nsvd_solver=\\'randomized\\' is very useful in that case: since we are going\\nto drop most of the singular vectors it is much more efficient to limit the\\ncomputation to an approximated estimate of the singular vectors we will keep\\nto actually perform the transform.\\nFor instance, the following shows 16 sample portraits (centered around\\n0.0) from the Olivetti dataset. On the right hand side are the first 16\\nsingular vectors reshaped as portraits. Since we only require the top\\n16 singular vectors of a dataset with size (n_{samples} = 400)\\nand (n_{features} = 64 \\\\times 64 = 4096), the computation time is\\nless than 1s:\\nIf we note (n_{\\\\max} = \\\\max(n_{\\\\mathrm{samples}}, n_{\\\\mathrm{features}})) and\\n(n_{\\\\min} = \\\\min(n_{\\\\mathrm{samples}}, n_{\\\\mathrm{features}})), the time complexity\\nof the randomized PCA is (O(n_{\\\\max}^2 \\\\cdot n_{\\\\mathrm{components}}))\\ninstead of (O(n_{\\\\max}^2 \\\\cdot n_{\\\\min})) for the exact method\\nimplemented in PCA.\\nThe memory footprint of randomized PCA is also proportional to\\n(2 \\\\cdot n_{\\\\max} \\\\cdot n_{\\\\mathrm{components}}) instead of (n_{\\\\max}\\n\\\\cdot n_{\\\\min}) for the exact method.\\nNote: the implementation of inverse_transform in PCA with\\nsvd_solver=\\'randomized\\' is not the exact inverse transform of\\ntransform even when whiten=False (default).\\nExamples:\\nFaces recognition example using eigenfaces and SVMs\\nFaces dataset decompositions\\nReferences:\\nAlgorithm 4.3 in\\n“Finding structure with randomness: Stochastic algorithms for\\nconstructing approximate matrix decompositions”\\nHalko, et al., 2009\\n“An implementation of a randomized algorithm for principal component\\nanalysis” A. Szlam et al. 2014\\n2.5.1.4. Sparse principal components analysis (SparsePCA and MiniBatchSparsePCA)¶\\nSparsePCA is a variant of PCA, with the goal of extracting the\\nset of sparse components that best reconstruct the data.\\nMini-batch sparse PCA (MiniBatchSparsePCA) is a variant of\\nSparsePCA that is faster but less accurate. The increased speed is\\nreached by iterating over small chunks of the set of features, for a given\\nnumber of iterations.\\nPrincipal component analysis (PCA) has the disadvantage that the\\ncomponents extracted by this method have exclusively dense expressions, i.e.\\nthey have non-zero coefficients when expressed as linear combinations of the\\noriginal variables. This can make interpretation difficult. In many cases,\\nthe real underlying components can be more naturally imagined as sparse\\nvectors; for example in face recognition, components might naturally map to\\nparts of faces.\\nSparse principal components yields a more parsimonious, interpretable\\nrepresentation, clearly emphasizing which of the original features contribute\\nto the differences between samples.\\nThe following example illustrates 16 components extracted using sparse PCA from\\nthe Olivetti faces dataset.  It can be seen how the regularization term induces\\nmany zeros. Furthermore, the natural structure of the data causes the non-zero\\ncoefficients to be vertically adjacent. The model does not enforce this\\nmathematically: each component is a vector (h \\\\in \\\\mathbf{R}^{4096}), and\\nthere is no notion of vertical adjacency except during the human-friendly\\nvisualization as 64x64 pixel images. The fact that the components shown below\\nappear local is the effect of the inherent structure of the data, which makes\\nsuch local patterns minimize reconstruction error. There exist sparsity-inducing\\nnorms that take into account adjacency and different kinds of structure; see\\n[Jen09] for a review of such methods.\\nFor more details on how to use Sparse PCA, see the Examples section, below.\\nNote that there are many different formulations for the Sparse PCA\\nproblem. The one implemented here is based on [Mrl09] . The optimization\\nproblem solved is a PCA problem (dictionary learning) with an\\n(\\\\ell_1) penalty on the components:\\n[\\\\begin{split}(U^, V^) = \\\\underset{U, V}{\\\\operatorname{arg\\\\,min\\\\,}} & \\\\frac{1}{2}\\n||X-UV||{\\\\text{Fro}}^2+\\\\alpha||V||{1,1} \\\\\\n\\\\text{subject to } & ||U_k||2 <= 1 \\\\text{ for all }\\n0 \\\\leq k < n{components}\\\\end{split}]\\n(||.||{\\\\text{Fro}}) stands for the Frobenius norm and (||.||{1,1})\\nstands for the entry-wise matrix norm which is the sum of the absolute values\\nof all the entries in the matrix.\\nThe sparsity-inducing (||.||_{1,1}) matrix norm also prevents learning\\ncomponents from noise when few training samples are available. The degree\\nof penalization (and thus sparsity) can be adjusted through the\\nhyperparameter alpha. Small values lead to a gently regularized\\nfactorization, while larger values shrink many coefficients to zero.\\nNote\\nWhile in the spirit of an online algorithm, the class\\nMiniBatchSparsePCA does not implement partial_fit because\\nthe algorithm is online along the features direction, not the samples\\ndirection.\\nExamples:\\nFaces dataset decompositions\\nReferences:\\n[Mrl09]\\n“Online Dictionary Learning for Sparse Coding”\\nJ. Mairal, F. Bach, J. Ponce, G. Sapiro, 2009\\n[Jen09]\\n“Structured Sparse Principal Component Analysis”\\nR. Jenatton, G. Obozinski, F. Bach, 2009\\n\\n2.5.2. Kernel Principal Component Analysis (kPCA)¶\\n2.5.2.1. Exact Kernel PCA¶\\nKernelPCA is an extension of PCA which achieves non-linear\\ndimensionality reduction through the use of kernels (see Pairwise metrics, Affinities and Kernels) [Scholkopf1997]. It\\nhas many applications including denoising, compression and structured\\nprediction (kernel dependency estimation). KernelPCA supports both\\ntransform and inverse_transform.\\nNote\\nKernelPCA.inverse_transform relies on a kernel ridge to learn the\\nfunction mapping samples from the PCA basis into the original feature\\nspace [Bakir2003]. Thus, the reconstruction obtained with\\nKernelPCA.inverse_transform is an approximation. See the example\\nlinked below for more details.\\nExamples:\\nKernel PCA\\nReferences:\\n[Scholkopf1997]\\nSchölkopf, Bernhard, Alexander Smola, and Klaus-Robert Müller.\\n“Kernel principal component analysis.”\\nInternational conference on artificial neural networks.\\nSpringer, Berlin, Heidelberg, 1997.\\n[Bakir2003]\\nBakır, Gökhan H., Jason Weston, and Bernhard Schölkopf.\\n“Learning to find pre-images.”\\nAdvances in neural information processing systems 16 (2003): 449-456.\\n2.5.2.2. Choice of solver for Kernel PCA¶\\nWhile in PCA the number of components is bounded by the number of\\nfeatures, in KernelPCA the number of components is bounded by the\\nnumber of samples. Many real-world datasets have large number of samples! In\\nthese cases finding all the components with a full kPCA is a waste of\\ncomputation time, as data is mostly described by the first few components\\n(e.g. n_components<=100). In other words, the centered Gram matrix that\\nis eigendecomposed in the Kernel PCA fitting process has an effective rank that\\nis much smaller than its size. This is a situation where approximate\\neigensolvers can provide speedup with very low precision loss.\\nEigensolvers\\nClick for more details\\n¶\\nThe optional parameter eigen_solver=\\'randomized\\' can be used to\\nsignificantly reduce the computation time when the number of requested\\nn_components is small compared with the number of samples. It relies on\\nrandomized decomposition methods to find an approximate solution in a shorter\\ntime.\\nThe time complexity of the randomized KernelPCA is\\n(O(n_{\\\\mathrm{samples}}^2 \\\\cdot n_{\\\\mathrm{components}}))\\ninstead of (O(n_{\\\\mathrm{samples}}^3)) for the exact method\\nimplemented with eigen_solver=\\'dense\\'.\\nThe memory footprint of randomized KernelPCA is also proportional to\\n(2 \\\\cdot n_{\\\\mathrm{samples}} \\\\cdot n_{\\\\mathrm{components}}) instead of\\n(n_{\\\\mathrm{samples}}^2) for the exact method.\\nNote: this technique is the same as in PCA using randomized SVD.\\nIn addition to the above two solvers, eigen_solver=\\'arpack\\' can be used as\\nan alternate way to get an approximate decomposition. In practice, this method\\nonly provides reasonable execution times when the number of components to find\\nis extremely small. It is enabled by default when the desired number of\\ncomponents is less than 10 (strict) and the number of samples is more than 200\\n(strict). See KernelPCA for details.\\nReferences:\\ndense solver:\\nscipy.linalg.eigh documentation\\nrandomized solver:\\nAlgorithm 4.3 in\\n“Finding structure with randomness: Stochastic\\nalgorithms for constructing approximate matrix decompositions”\\nHalko, et al. (2009)\\n“An implementation of a randomized algorithm\\nfor principal component analysis”\\nA. Szlam et al. (2014)\\narpack solver:\\nscipy.sparse.linalg.eigsh documentation\\nR. B. Lehoucq, D. C. Sorensen, and C. Yang, (1998)\\n\\n2.5.3. Truncated singular value decomposition and latent semantic analysis¶\\nTruncatedSVD implements a variant of singular value decomposition\\n(SVD) that only computes the (k) largest singular values,\\nwhere (k) is a user-specified parameter.\\nTruncatedSVD is very similar to PCA, but differs\\nin that the matrix (X) does not need to be centered.\\nWhen the columnwise (per-feature) means of (X)\\nare subtracted from the feature values,\\ntruncated SVD on the resulting matrix is equivalent to PCA.\\nAbout truncated SVD and latent semantic analysis (LSA)\\nClick for more details\\n¶\\nWhen truncated SVD is applied to term-document matrices\\n(as returned by CountVectorizer or\\nTfidfVectorizer),\\nthis transformation is known as\\nlatent semantic analysis\\n(LSA), because it transforms such matrices\\nto a “semantic” space of low dimensionality.\\nIn particular, LSA is known to combat the effects of synonymy and polysemy\\n(both of which roughly mean there are multiple meanings per word),\\nwhich cause term-document matrices to be overly sparse\\nand exhibit poor similarity under measures such as cosine similarity.\\nNote\\nLSA is also known as latent semantic indexing, LSI,\\nthough strictly that refers to its use in persistent indexes\\nfor information retrieval purposes.\\nMathematically, truncated SVD applied to training samples (X)\\nproduces a low-rank approximation (X):\\n[X \\\\approx X_k = U_k \\\\Sigma_k V_k^\\\\top]\\nAfter this operation, (U_k \\\\Sigma_k)\\nis the transformed training set with (k) features\\n(called n_components in the API).\\nTo also transform a test set (X), we multiply it with (V_k):\\n[X\\' = X V_k]\\nNote\\nMost treatments of LSA in the natural language processing (NLP)\\nand information retrieval (IR) literature\\nswap the axes of the matrix (X) so that it has shape\\nn_features × n_samples.\\nWe present LSA in a different way that matches the scikit-learn API better,\\nbut the singular values found are the same.\\nWhile the TruncatedSVD transformer\\nworks with any feature matrix,\\nusing it on tf–idf matrices is recommended over raw frequency counts\\nin an LSA/document processing setting.\\nIn particular, sublinear scaling and inverse document frequency\\nshould be turned on (sublinear_tf=True, use_idf=True)\\nto bring the feature values closer to a Gaussian distribution,\\ncompensating for LSA’s erroneous assumptions about textual data.\\nExamples:\\nClustering text documents using k-means\\nReferences:\\nChristopher D. Manning, Prabhakar Raghavan and Hinrich Schütze (2008),\\nIntroduction to Information Retrieval, Cambridge University Press,\\nchapter 18: Matrix decompositions & latent semantic indexing\\n\\n2.5.4. Dictionary Learning¶\\n2.5.4.1. Sparse coding with a precomputed dictionary¶\\nThe SparseCoder object is an estimator that can be used to transform signals\\ninto sparse linear combination of atoms from a fixed, precomputed dictionary\\nsuch as a discrete wavelet basis. This object therefore does not\\nimplement a fit method. The transformation amounts\\nto a sparse coding problem: finding a representation of the data as a linear\\ncombination of as few dictionary atoms as possible. All variations of\\ndictionary learning implement the following transform methods, controllable via\\nthe transform_method initialization parameter:\\nOrthogonal matching pursuit (Orthogonal Matching Pursuit (OMP))\\nLeast-angle regression (Least Angle Regression)\\nLasso computed by least-angle regression\\nLasso using coordinate descent (Lasso)\\nThresholding\\nThresholding is very fast but it does not yield accurate reconstructions.\\nThey have been shown useful in literature for classification tasks. For image\\nreconstruction tasks, orthogonal matching pursuit yields the most accurate,\\nunbiased reconstruction.\\nThe dictionary learning objects offer, via the split_code parameter, the\\npossibility to separate the positive and negative values in the results of\\nsparse coding. This is useful when dictionary learning is used for extracting\\nfeatures that will be used for supervised learning, because it allows the\\nlearning algorithm to assign different weights to negative loadings of a\\nparticular atom, from to the corresponding positive loading.\\nThe split code for a single sample has length 2 * n_components\\nand is constructed using the following rule: First, the regular code of length\\nn_components is computed. Then, the first n_components entries of the\\nsplit_code are\\nfilled with the positive part of the regular code vector. The second half of\\nthe split code is filled with the negative part of the code vector, only with\\na positive sign. Therefore, the split_code is non-negative.\\nExamples:\\nSparse coding with a precomputed dictionary\\n2.5.4.2. Generic dictionary learning¶\\nDictionary learning (DictionaryLearning) is a matrix factorization\\nproblem that amounts to finding a (usually overcomplete) dictionary that will\\nperform well at sparsely encoding the fitted data.\\nRepresenting data as sparse combinations of atoms from an overcomplete\\ndictionary is suggested to be the way the mammalian primary visual cortex works.\\nConsequently, dictionary learning applied on image patches has been shown to\\ngive good results in image processing tasks such as image completion,\\ninpainting and denoising, as well as for supervised recognition tasks.\\nDictionary learning is an optimization problem solved by alternatively updating\\nthe sparse code, as a solution to multiple Lasso problems, considering the\\ndictionary fixed, and then updating the dictionary to best fit the sparse code.\\n[\\\\begin{split}(U^, V^) = \\\\underset{U, V}{\\\\operatorname{arg\\\\,min\\\\,}} & \\\\frac{1}{2}\\n||X-UV||{\\\\text{Fro}}^2+\\\\alpha||U||{1,1} \\\\\\n\\\\text{subject to } & ||V_k||2 <= 1 \\\\text{ for all }\\n0 \\\\leq k < n{\\\\mathrm{atoms}}\\\\end{split}]\\n(||.||{\\\\text{Fro}}) stands for the Frobenius norm and (||.||{1,1})\\nstands for the entry-wise matrix norm which is the sum of the absolute values\\nof all the entries in the matrix.\\nAfter using such a procedure to fit the dictionary, the transform is simply a\\nsparse coding step that shares the same implementation with all dictionary\\nlearning objects (see Sparse coding with a precomputed dictionary).\\nIt is also possible to constrain the dictionary and/or code to be positive to\\nmatch constraints that may be present in the data. Below are the faces with\\ndifferent positivity constraints applied. Red indicates negative values, blue\\nindicates positive values, and white represents zeros.\\nThe following image shows how a dictionary learned from 4x4 pixel image patches\\nextracted from part of the image of a raccoon face looks like.\\nExamples:\\nImage denoising using dictionary learning\\nReferences:\\n“Online dictionary learning for sparse coding”\\nJ. Mairal, F. Bach, J. Ponce, G. Sapiro, 2009\\n2.5.4.3. Mini-batch dictionary learning¶\\nMiniBatchDictionaryLearning implements a faster, but less accurate\\nversion of the dictionary learning algorithm that is better suited for large\\ndatasets.\\nBy default, MiniBatchDictionaryLearning divides the data into\\nmini-batches and optimizes in an online manner by cycling over the mini-batches\\nfor the specified number of iterations. However, at the moment it does not\\nimplement a stopping condition.\\nThe estimator also implements partial_fit, which updates the dictionary by\\niterating only once over a mini-batch. This can be used for online learning\\nwhen the data is not readily available from the start, or for when the data\\ndoes not fit into the memory.\\nClustering for dictionary learning\\nNote that when using dictionary learning to extract a representation\\n(e.g. for sparse coding) clustering can be a good proxy to learn the\\ndictionary. For instance the MiniBatchKMeans estimator is\\ncomputationally efficient and implements on-line learning with a\\npartial_fit method.\\nExample: Online learning of a dictionary of parts of faces\\n\\n2.5.5. Factor Analysis¶\\nIn unsupervised learning we only have a dataset (X = {x_1, x_2, \\\\dots, x_n\\n}). How can this dataset be described mathematically? A very simple\\ncontinuous latent variable model for (X) is\\n[x_i = W h_i + \\\\mu + \\\\epsilon]\\nThe vector (h_i) is called “latent” because it is unobserved. (\\\\epsilon) is\\nconsidered a noise term distributed according to a Gaussian with mean 0 and\\ncovariance (\\\\Psi) (i.e. (\\\\epsilon \\\\sim \\\\mathcal{N}(0, \\\\Psi))), (\\\\mu) is some\\narbitrary offset vector. Such a model is called “generative” as it describes\\nhow (x_i) is generated from (h_i). If we use all the (x_i)’s as columns to form\\na matrix (\\\\mathbf{X}) and all the (h_i)’s as columns of a matrix (\\\\mathbf{H})\\nthen we can write (with suitably defined (\\\\mathbf{M}) and (\\\\mathbf{E})):\\n[\\\\mathbf{X} = W \\\\mathbf{H} + \\\\mathbf{M} + \\\\mathbf{E}]\\nIn other words, we decomposed matrix (\\\\mathbf{X}).\\nIf (h_i) is given, the above equation automatically implies the following\\nprobabilistic interpretation:\\n[p(x_i|h_i) = \\\\mathcal{N}(Wh_i + \\\\mu, \\\\Psi)]\\nFor a complete probabilistic model we also need a prior distribution for the\\nlatent variable (h). The most straightforward assumption (based on the nice\\nproperties of the Gaussian distribution) is (h \\\\sim \\\\mathcal{N}(0,\\n\\\\mathbf{I})).  This yields a Gaussian as the marginal distribution of (x):\\n[p(x) = \\\\mathcal{N}(\\\\mu, WW^T + \\\\Psi)]\\nNow, without any further assumptions the idea of having a latent variable (h)\\nwould be superfluous – (x) can be completely modelled with a mean\\nand a covariance. We need to impose some more specific structure on one\\nof these two parameters. A simple additional assumption regards the\\nstructure of the error covariance (\\\\Psi):\\n(\\\\Psi = \\\\sigma^2 \\\\mathbf{I}): This assumption leads to\\nthe probabilistic model of PCA.\\n(\\\\Psi = \\\\mathrm{diag}(\\\\psi_1, \\\\psi_2, \\\\dots, \\\\psi_n)): This model is called\\nFactorAnalysis, a classical statistical model. The matrix W is\\nsometimes called the “factor loading matrix”.\\nBoth models essentially estimate a Gaussian with a low-rank covariance matrix.\\nBecause both models are probabilistic they can be integrated in more complex\\nmodels, e.g. Mixture of Factor Analysers. One gets very different models (e.g.\\nFastICA) if non-Gaussian priors on the latent variables are assumed.\\nFactor analysis can produce similar components (the columns of its loading\\nmatrix) to PCA. However, one can not make any general statements\\nabout these components (e.g. whether they are orthogonal):\\nThe main advantage for Factor Analysis over PCA is that\\nit can model the variance in every direction of the input space independently\\n(heteroscedastic noise):\\nThis allows better model selection than probabilistic PCA in the presence\\nof heteroscedastic noise:\\nFactor Analysis is often followed by a rotation of the factors (with the\\nparameter rotation), usually to improve interpretability. For example,\\nVarimax rotation maximizes the sum of the variances of the squared loadings,\\ni.e., it tends to produce sparser factors, which are influenced by only a few\\nfeatures each (the “simple structure”). See e.g., the first example below.\\nExamples:\\nFactor Analysis (with rotation) to visualize patterns\\nModel selection with Probabilistic PCA and Factor Analysis (FA)\\n\\n2.5.6. Independent component analysis (ICA)¶\\nIndependent component analysis separates a multivariate signal into\\nadditive subcomponents that are maximally independent. It is\\nimplemented in scikit-learn using the Fast ICA\\nalgorithm. Typically, ICA is not used for reducing dimensionality but\\nfor separating superimposed signals. Since the ICA model does not include\\na noise term, for the model to be correct, whitening must be applied.\\nThis can be done internally using the whiten argument or manually using one\\nof the PCA variants.\\nIt is classically used to separate mixed signals (a problem known as\\nblind source separation), as in the example below:\\nICA can also be used as yet another non linear decomposition that finds\\ncomponents with some sparsity:\\nExamples:\\nBlind source separation using FastICA\\nFastICA on 2D point clouds\\nFaces dataset decompositions\\n\\n2.5.7. Non-negative matrix factorization (NMF or NNMF)¶\\n2.5.7.1. NMF with the Frobenius norm¶\\nNMF [1] is an alternative approach to decomposition that assumes that the\\ndata and the components are non-negative. NMF can be plugged in\\ninstead of PCA or its variants, in the cases where the data matrix\\ndoes not contain negative values. It finds a decomposition of samples\\n(X) into two matrices (W) and (H) of non-negative elements,\\nby optimizing the distance (d) between (X) and the matrix product\\n(WH). The most widely used distance function is the squared Frobenius\\nnorm, which is an obvious extension of the Euclidean norm to matrices:\\n[d_{\\\\mathrm{Fro}}(X, Y) = \\\\frac{1}{2} ||X - Y||{\\\\mathrm{Fro}}^2 = \\\\frac{1}{2} \\\\sum{i,j} (X_{ij} - {Y}{ij})^2]\\nUnlike PCA, the representation of a vector is obtained in an additive\\nfashion, by superimposing the components, without subtracting. Such additive\\nmodels are efficient for representing images and text.\\nIt has been observed in [Hoyer, 2004] [2] that, when carefully constrained,\\nNMF can produce a parts-based representation of the dataset,\\nresulting in interpretable models. The following example displays 16\\nsparse components found by NMF from the images in the Olivetti\\nfaces dataset, in comparison with the PCA eigenfaces.\\nThe init attribute determines the initialization method applied, which\\nhas a great impact on the performance of the method. NMF implements the\\nmethod Nonnegative Double Singular Value Decomposition. NNDSVD [4] is based on\\ntwo SVD processes, one approximating the data matrix, the other approximating\\npositive sections of the resulting partial SVD factors utilizing an algebraic\\nproperty of unit rank matrices. The basic NNDSVD algorithm is better fit for\\nsparse factorization. Its variants NNDSVDa (in which all zeros are set equal to\\nthe mean of all elements of the data), and NNDSVDar (in which the zeros are set\\nto random perturbations less than the mean of the data divided by 100) are\\nrecommended in the dense case.\\nNote that the Multiplicative Update (‘mu’) solver cannot update zeros present in\\nthe initialization, so it leads to poorer results when used jointly with the\\nbasic NNDSVD algorithm which introduces a lot of zeros; in this case, NNDSVDa or\\nNNDSVDar should be preferred.\\nNMF can also be initialized with correctly scaled random non-negative\\nmatrices by setting init=\"random\". An integer seed or a\\nRandomState can also be passed to random_state to control\\nreproducibility.\\nIn NMF, L1 and L2 priors can be added to the loss function in order to\\nregularize the model. The L2 prior uses the Frobenius norm, while the L1 prior\\nuses an elementwise L1 norm. As in ElasticNet,\\nwe control the combination of L1 and L2 with the l1_ratio ((\\\\rho))\\nparameter, and the intensity of the regularization with the alpha_W and\\nalpha_H ((\\\\alpha_W) and (\\\\alpha_H)) parameters. The priors are\\nscaled by the number of samples ((n_samples)) for H and the number of\\nfeatures ((n_features)) for W to keep their impact balanced with\\nrespect to one another and to the data fit term as independent as possible of\\nthe size of the training set. Then the priors terms are:\\n[(\\\\alpha_W \\\\rho ||W||_1 + \\\\frac{\\\\alpha_W(1-\\\\rho)}{2} ||W||{\\\\mathrm{Fro}} ^ 2) * n_features\\n\\n(\\\\alpha_H \\\\rho ||H||1 + \\\\frac{\\\\alpha_H(1-\\\\rho)}{2} ||H||{\\\\mathrm{Fro}} ^ 2) * n_samples]\\nand the regularized objective function is:\\n[d_{\\\\mathrm{Fro}}(X, WH)\\n\\n(\\\\alpha_W \\\\rho ||W||1 + \\\\frac{\\\\alpha_W(1-\\\\rho)}{2} ||W||{\\\\mathrm{Fro}} ^ 2) * n_features\\n\\n(\\\\alpha_H \\\\rho ||H||1 + \\\\frac{\\\\alpha_H(1-\\\\rho)}{2} ||H||{\\\\mathrm{Fro}} ^ 2) * n_samples]\\n2.5.7.2. NMF with a beta-divergence¶\\nAs described previously, the most widely used distance function is the squared\\nFrobenius norm, which is an obvious extension of the Euclidean norm to\\nmatrices:\\n[d_{\\\\mathrm{Fro}}(X, Y) = \\\\frac{1}{2} ||X - Y||{Fro}^2 = \\\\frac{1}{2} \\\\sum{i,j} (X_{ij} - {Y}{ij})^2]\\nOther distance functions can be used in NMF as, for example, the (generalized)\\nKullback-Leibler (KL) divergence, also referred as I-divergence:\\n[d{KL}(X, Y) = \\\\sum_{i,j} (X_{ij} \\\\log(\\\\frac{X_{ij}}{Y_{ij}}) - X_{ij} + Y_{ij})]\\nOr, the Itakura-Saito (IS) divergence:\\n[d_{IS}(X, Y) = \\\\sum_{i,j} (\\\\frac{X_{ij}}{Y_{ij}} - \\\\log(\\\\frac{X_{ij}}{Y_{ij}}) - 1)]\\nThese three distances are special cases of the beta-divergence family, with\\n(\\\\beta = 2, 1, 0) respectively [6]. The beta-divergence are\\ndefined by :\\n[d_{\\\\beta}(X, Y) = \\\\sum_{i,j} \\\\frac{1}{\\\\beta(\\\\beta - 1)}(X_{ij}^\\\\beta + (\\\\beta-1)Y_{ij}^\\\\beta - \\\\beta X_{ij} Y_{ij}^{\\\\beta - 1})]\\nNote that this definition is not valid if (\\\\beta \\\\in (0; 1)), yet it can\\nbe continuously extended to the definitions of (d_{KL}) and (d_{IS})\\nrespectively.\\nNMF implemented solvers\\nClick for more details\\n¶\\nNMF implements two solvers, using Coordinate Descent (‘cd’) [5], and\\nMultiplicative Update (‘mu’) [6]. The ‘mu’ solver can optimize every\\nbeta-divergence, including of course the Frobenius norm ((\\\\beta=2)), the\\n(generalized) Kullback-Leibler divergence ((\\\\beta=1)) and the\\nItakura-Saito divergence ((\\\\beta=0)). Note that for\\n(\\\\beta \\\\in (1; 2)), the ‘mu’ solver is significantly faster than for other\\nvalues of (\\\\beta). Note also that with a negative (or 0, i.e.\\n‘itakura-saito’) (\\\\beta), the input matrix cannot contain zero values.\\nThe ‘cd’ solver can only optimize the Frobenius norm. Due to the\\nunderlying non-convexity of NMF, the different solvers may converge to\\ndifferent minima, even when optimizing the same distance function.\\nNMF is best used with the fit_transform method, which returns the matrix W.\\nThe matrix H is stored into the fitted model in the components_ attribute;\\nthe method transform will decompose a new matrix X_new based on these\\nstored components:\\n\\n\\nimport numpy as np\\nX = np.array([[1, 1], [2, 1], [3, 1.2], [4, 1], [5, 0.8], [6, 1]])\\nfrom sklearn.decomposition import NMF\\nmodel = NMF(n_components=2, init=\\'random\\', random_state=0)\\nW = model.fit_transform(X)\\nH = model.components_\\nX_new = np.array([[1, 0], [1, 6.1], [1, 0], [1, 4], [3.2, 1], [0, 4]])\\nW_new = model.transform(X_new)\\nExamples:\\nFaces dataset decompositions\\nTopic extraction with Non-negative Matrix Factorization and Latent Dirichlet Allocation\\n2.5.7.3. Mini-batch Non Negative Matrix Factorization¶\\nMiniBatchNMF [7] implements a faster, but less accurate version of the\\nnon negative matrix factorization (i.e. NMF),\\nbetter suited for large datasets.\\nBy default, MiniBatchNMF divides the data into mini-batches and\\noptimizes the NMF model in an online manner by cycling over the mini-batches\\nfor the specified number of iterations. The batch_size parameter controls\\nthe size of the batches.\\nIn order to speed up the mini-batch algorithm it is also possible to scale\\npast batches, giving them less importance than newer batches. This is done\\nintroducing a so-called forgetting factor controlled by the forget_factor\\nparameter.\\nThe estimator also implements partial_fit, which updates H by iterating\\nonly once over a mini-batch. This can be used for online learning when the data\\nis not readily available from the start, or when the data does not fit into memory.\\nReferences:\\n[1]\\n“Learning the parts of objects by non-negative matrix factorization”\\nD. Lee, S. Seung, 1999\\n[2]\\n“Non-negative Matrix Factorization with Sparseness Constraints”\\nP. Hoyer, 2004\\n[4]\\n“SVD based initialization: A head start for nonnegative\\nmatrix factorization”\\nC. Boutsidis, E. Gallopoulos, 2008\\n[5]\\n“Fast local algorithms for large scale nonnegative matrix and tensor\\nfactorizations.”\\nA. Cichocki, A. Phan, 2009\\n[6]\\n(1,2)\\n“Algorithms for nonnegative matrix factorization with\\nthe beta-divergence”\\nC. Fevotte, J. Idier, 2011\\n[7]\\n“Online algorithms for nonnegative matrix factorization with the\\nItakura-Saito divergence”\\nA. Lefevre, F. Bach, C. Fevotte, 2011\\n\\n2.5.8. Latent Dirichlet Allocation (LDA)¶\\nLatent Dirichlet Allocation is a generative probabilistic model for collections of\\ndiscrete dataset such as text corpora. It is also a topic model that is used for\\ndiscovering abstract topics from a collection of documents.\\nThe graphical model of LDA is a three-level generative model:\\nNote on notations presented in the graphical model above, which can be found in\\nHoffman et al. (2013):\\nThe corpus is a collection of (D) documents.\\nA document is a sequence of (N) words.\\nThere are (K) topics in the corpus.\\nThe boxes represent repeated sampling.\\nIn the graphical model, each node is a random variable and has a role in the\\ngenerative process. A shaded node indicates an observed variable and an unshaded\\nnode indicates a hidden (latent) variable. In this case, words in the corpus are\\nthe only data that we observe. The latent variables determine the random mixture\\nof topics in the corpus and the distribution of words in the documents.\\nThe goal of LDA is to use the observed words to infer the hidden topic\\nstructure.\\nDetails on modeling text corpora\\nClick for more details\\n¶\\nWhen modeling text corpora, the model assumes the following generative process\\nfor a corpus with (D) documents and (K) topics, with (K)\\ncorresponding to n_components in the API:\\nFor each topic (k \\\\in K), draw (\\\\beta_k \\\\sim\\n\\\\mathrm{Dirichlet}(\\\\eta)). This provides a distribution over the words,\\ni.e. the probability of a word appearing in topic (k).\\n(\\\\eta) corresponds to topic_word_prior.\\nFor each document (d \\\\in D), draw the topic proportions\\n(\\\\theta_d \\\\sim \\\\mathrm{Dirichlet}(\\\\alpha)). (\\\\alpha)\\ncorresponds to doc_topic_prior.\\nFor each word (i) in document (d):\\nDraw the topic assignment (z_{di} \\\\sim \\\\mathrm{Multinomial}\\n(\\\\theta_d))\\nDraw the observed word (w_{ij} \\\\sim \\\\mathrm{Multinomial}\\n(\\\\beta_{z_{di}}))\\nFor parameter estimation, the posterior distribution is:\\n[p(z, \\\\theta, \\\\beta |w, \\\\alpha, \\\\eta) =\\n\\\\frac{p(z, \\\\theta, \\\\beta|\\\\alpha, \\\\eta)}{p(w|\\\\alpha, \\\\eta)}]\\nSince the posterior is intractable, variational Bayesian method\\nuses a simpler distribution (q(z,\\\\theta,\\\\beta | \\\\lambda, \\\\phi, \\\\gamma))\\nto approximate it, and those variational parameters (\\\\lambda),\\n(\\\\phi), (\\\\gamma) are optimized to maximize the Evidence\\nLower Bound (ELBO):\\n[\\\\log\\\\: P(w | \\\\alpha, \\\\eta) \\\\geq L(w,\\\\phi,\\\\gamma,\\\\lambda) \\\\overset{\\\\triangle}{=}\\nE_{q}[\\\\log\\\\:p(w,z,\\\\theta,\\\\beta|\\\\alpha,\\\\eta)] - E_{q}[\\\\log\\\\:q(z, \\\\theta, \\\\beta)]]\\nMaximizing ELBO is equivalent to minimizing the Kullback-Leibler(KL) divergence\\nbetween (q(z,\\\\theta,\\\\beta)) and the true posterior\\n(p(z, \\\\theta, \\\\beta |w, \\\\alpha, \\\\eta)).\\nLatentDirichletAllocation implements the online variational Bayes\\nalgorithm and supports both online and batch update methods.\\nWhile the batch method updates variational variables after each full pass through\\nthe data, the online method updates variational variables from mini-batch data\\npoints.\\nNote\\nAlthough the online method is guaranteed to converge to a local optimum point, the quality of\\nthe optimum point and the speed of convergence may depend on mini-batch size and\\nattributes related to learning rate setting.\\nWhen LatentDirichletAllocation is applied on a “document-term” matrix, the matrix\\nwill be decomposed into a “topic-term” matrix and a “document-topic” matrix. While\\n“topic-term” matrix is stored as components_ in the model, “document-topic” matrix\\ncan be calculated from transform method.\\nLatentDirichletAllocation also implements partial_fit method. This is used\\nwhen data can be fetched sequentially.\\nExamples:\\nTopic extraction with Non-negative Matrix Factorization and Latent Dirichlet Allocation\\nReferences:\\n“Latent Dirichlet Allocation”\\nD. Blei, A. Ng, M. Jordan, 2003\\n“Online Learning for Latent Dirichlet Allocation”\\nM. Hoffman, D. Blei, F. Bach, 2010\\n“Stochastic Variational Inference”\\nM. Hoffman, D. Blei, C. Wang, J. Paisley, 2013\\n“The varimax criterion for analytic rotation in factor analysis”\\nH. F. Kaiser, 1958\\nSee also Dimensionality reduction for dimensionality reduction with\\nNeighborhood Components Analysis.\\n\\n2.6. Covariance estimation — scikit-learn 1.4.2 documentation\\n\\nInstall\\nUser Guide\\nAPI\\nExamples\\nCommunity\\nGetting Started\\nTutorial\\nWhat\\'s new\\nGlossary\\nDevelopment\\nFAQ\\nSupport\\nRelated packages\\nRoadmap\\nGovernance\\nAbout us\\nGitHub\\nOther Versions and Download\\nMore\\nGetting Started\\nTutorial\\nWhat\\'s new\\nGlossary\\nDevelopment\\nFAQ\\nSupport\\nRelated packages\\nRoadmap\\nGovernance\\nAbout us\\nGitHub\\nOther Versions and Download\\nToggle Menu\\nPrevUp\\nNext\\nscikit-learn 1.4.2\\nOther versions\\nPlease cite us if you use the software.\\n\\n2.6. Covariance estimation\\n\\n2.6.1. Empirical covariance\\n\\n2.6.2. Shrunk Covariance\\n2.6.2.1. Basic shrinkage\\n2.6.2.2. Ledoit-Wolf shrinkage\\n2.6.2.3. Oracle Approximating Shrinkage\\n\\n2.6.3. Sparse inverse covariance\\n\\n2.6.4. Robust Covariance Estimation\\n2.6.4.1. Minimum Covariance Determinant\\n\\n2.6. Covariance estimation¶\\n\\nMany statistical problems require the estimation of a\\npopulation’s covariance matrix, which can be seen as an estimation of\\ndata set scatter plot shape. Most of the time, such an estimation has\\nto be done on a sample whose properties (size, structure, homogeneity)\\nhave a large influence on the estimation’s quality. The\\nsklearn.covariance package provides tools for accurately estimating\\na population’s covariance matrix under various settings.\\nWe assume that the observations are independent and identically\\ndistributed (i.i.d.).\\n- 2.6.1. Empirical covariance¶\\nThe covariance matrix of a data set is known to be well approximated\\nby the classical maximum likelihood estimator (or “empirical\\ncovariance”), provided the number of observations is large enough\\ncompared to the number of features (the variables describing the\\nobservations). More precisely, the Maximum Likelihood Estimator of a\\nsample is an asymptotically unbiased estimator of the corresponding\\npopulation’s covariance matrix.\\nThe empirical covariance matrix of a sample can be computed using the\\nempirical_covariance function of the package, or by fitting an\\nEmpiricalCovariance object to the data sample with the\\nEmpiricalCovariance.fit method. Be careful that results depend\\non whether the data are centered, so one may want to use the\\nassume_centered parameter accurately. More precisely, if\\nassume_centered=False, then the test set is supposed to have the\\nsame mean vector as the training set. If not, both should be centered\\nby the user, and assume_centered=True should be used.\\nExamples:\\nSee Shrinkage covariance estimation: LedoitWolf vs OAS and max-likelihood for\\nan example on how to fit an EmpiricalCovariance object\\nto data.\\n- 2.6.2. Shrunk Covariance¶\\n2.6.2.1. Basic shrinkage¶\\nDespite being an asymptotically unbiased estimator of the covariance matrix,\\nthe Maximum Likelihood Estimator is not a good estimator of the\\neigenvalues of the covariance matrix, so the precision matrix obtained\\nfrom its inversion is not accurate. Sometimes, it even occurs that the\\nempirical covariance matrix cannot be inverted for numerical\\nreasons. To avoid such an inversion problem, a transformation of the\\nempirical covariance matrix has been introduced: the shrinkage.\\nIn scikit-learn, this transformation (with a user-defined shrinkage\\ncoefficient) can be directly applied to a pre-computed covariance with\\nthe shrunk_covariance method. Also, a shrunk estimator of the\\ncovariance can be fitted to data with a ShrunkCovariance object\\nand its ShrunkCovariance.fit method. Again, results depend on\\nwhether the data are centered, so one may want to use the\\nassume_centered parameter accurately.\\nMathematically, this shrinkage consists in reducing the ratio between the\\nsmallest and the largest eigenvalues of the empirical covariance matrix.\\nIt can be done by simply shifting every eigenvalue according to a given\\noffset, which is equivalent of finding the l2-penalized Maximum\\nLikelihood Estimator of the covariance matrix. In practice, shrinkage\\nboils down to a simple a convex transformation : (\\\\Sigma_{\\\\rm\\nshrunk} = (1-\\\\alpha)\\\\hat{\\\\Sigma} + \\\\alpha\\\\frac{{\\\\rm\\nTr}\\\\hat{\\\\Sigma}}{p}\\\\rm Id).\\nChoosing the amount of shrinkage, (\\\\alpha) amounts to setting a\\nbias/variance trade-off, and is discussed below.\\nExamples:\\nSee Shrinkage covariance estimation: LedoitWolf vs OAS and max-likelihood for\\nan example on how to fit a ShrunkCovariance object\\nto data.\\n2.6.2.2. Ledoit-Wolf shrinkage¶\\nIn their 2004 paper [1], O. Ledoit and M. Wolf propose a formula\\nto compute the optimal shrinkage coefficient (\\\\alpha) that\\nminimizes the Mean Squared Error between the estimated and the real\\ncovariance matrix.\\nThe Ledoit-Wolf estimator of the covariance matrix can be computed on\\na sample with the ledoit_wolf function of the\\nsklearn.covariance package, or it can be otherwise obtained by\\nfitting a LedoitWolf object to the same sample.\\nNote\\nCase when population covariance matrix is isotropic\\nIt is important to note that when the number of samples is much larger than\\nthe number of features, one would expect that no shrinkage would be\\nnecessary. The intuition behind this is that if the population covariance\\nis full rank, when the number of sample grows, the sample covariance will\\nalso become positive definite. As a result, no shrinkage would necessary\\nand the method should automatically do this.\\nThis, however, is not the case in the Ledoit-Wolf procedure when the\\npopulation covariance happens to be a multiple of the identity matrix. In\\nthis case, the Ledoit-Wolf shrinkage estimate approaches 1 as the number of\\nsamples increases. This indicates that the optimal estimate of the\\ncovariance matrix in the Ledoit-Wolf sense is multiple of the identity.\\nSince the population covariance is already a multiple of the identity\\nmatrix, the Ledoit-Wolf solution is indeed a reasonable estimate.\\nExamples:\\nSee Shrinkage covariance estimation: LedoitWolf vs OAS and max-likelihood for\\nan example on how to fit a LedoitWolf object to data and\\nfor visualizing the performances of the Ledoit-Wolf estimator in\\nterms of likelihood.\\nReferences:\\n[1]\\nO. Ledoit and M. Wolf, “A Well-Conditioned Estimator for Large-Dimensional\\nCovariance Matrices”, Journal of Multivariate Analysis, Volume 88, Issue 2,\\nFebruary 2004, pages 365-411.\\n2.6.2.3. Oracle Approximating Shrinkage¶\\nUnder the assumption that the data are Gaussian distributed, Chen et\\nal. [2] derived a formula aimed at choosing a shrinkage coefficient that\\nyields a smaller Mean Squared Error than the one given by Ledoit and\\nWolf’s formula. The resulting estimator is known as the Oracle\\nShrinkage Approximating estimator of the covariance.\\nThe OAS estimator of the covariance matrix can be computed on a sample\\nwith the oas function of the sklearn.covariance\\npackage, or it can be otherwise obtained by fitting an OAS\\nobject to the same sample.\\nBias-variance trade-off when setting the shrinkage: comparing the\\nchoices of Ledoit-Wolf and OAS estimators¶\\nReferences:\\n[2]\\n“Shrinkage algorithms for MMSE covariance estimation.”,\\nChen, Y., Wiesel, A., Eldar, Y. C., & Hero, A. O.\\nIEEE Transactions on Signal Processing, 58(10), 5016-5029, 2010.\\nExamples:\\nSee Shrinkage covariance estimation: LedoitWolf vs OAS and max-likelihood for\\nan example on how to fit an OAS object\\nto data.\\nSee Ledoit-Wolf vs OAS estimation to visualize the\\nMean Squared Error difference between a LedoitWolf and\\nan OAS estimator of the covariance.\\n- 2.6.3. Sparse inverse covariance¶\\nThe matrix inverse of the covariance matrix, often called the precision\\nmatrix, is proportional to the partial correlation matrix. It gives the\\npartial independence relationship. In other words, if two features are\\nindependent conditionally on the others, the corresponding coefficient in\\nthe precision matrix will be zero. This is why it makes sense to\\nestimate a sparse precision matrix: the estimation of the covariance\\nmatrix is better conditioned by learning independence relations from\\nthe data. This is known as covariance selection.\\nIn the small-samples situation, in which n_samples is on the order\\nof n_features or smaller, sparse inverse covariance estimators tend to work\\nbetter than shrunk covariance estimators. However, in the opposite\\nsituation, or for very correlated data, they can be numerically unstable.\\nIn addition, unlike shrinkage estimators, sparse estimators are able to\\nrecover off-diagonal structure.\\nThe GraphicalLasso estimator uses an l1 penalty to enforce sparsity on\\nthe precision matrix: the higher its alpha parameter, the more sparse\\nthe precision matrix. The corresponding GraphicalLassoCV object uses\\ncross-validation to automatically set the alpha parameter.\\nA comparison of maximum likelihood, shrinkage and sparse estimates of\\nthe covariance and precision matrix in the very small samples\\nsettings.¶\\nNote\\nStructure recovery\\nRecovering a graphical structure from correlations in the data is a\\nchallenging thing. If you are interested in such recovery keep in mind\\nthat:\\nRecovery is easier from a correlation matrix than a covariance\\nmatrix: standardize your observations before running GraphicalLasso\\nIf the underlying graph has nodes with much more connections than\\nthe average node, the algorithm will miss some of these connections.\\nIf your number of observations is not large compared to the number\\nof edges in your underlying graph, you will not recover it.\\nEven if you are in favorable recovery conditions, the alpha\\nparameter chosen by cross-validation (e.g. using the\\nGraphicalLassoCV object) will lead to selecting too many edges.\\nHowever, the relevant edges will have heavier weights than the\\nirrelevant ones.\\nThe mathematical formulation is the following:\\n[\\\\hat{K} = \\\\mathrm{argmin}K \\\\big(\\n\\\\mathrm{tr} S K - \\\\mathrm{log} \\\\mathrm{det} K\\n+ \\\\alpha |K|_1\\n\\\\big)]\\nWhere (K) is the precision matrix to be estimated, and (S) is the\\nsample covariance matrix. (|K|_1) is the sum of the absolute values of\\noff-diagonal coefficients of (K). The algorithm employed to solve this\\nproblem is the GLasso algorithm, from the Friedman 2008 Biostatistics\\npaper. It is the same algorithm as in the R glasso package.\\nExamples:\\nSparse inverse covariance estimation: example on synthetic\\ndata showing some recovery of a structure, and comparing to other\\ncovariance estimators.\\nVisualizing the stock market structure: example on real\\nstock market data, finding which symbols are most linked.\\nReferences:\\nFriedman et al, “Sparse inverse covariance estimation with the\\ngraphical lasso”,\\nBiostatistics 9, pp 432, 2008\\n- 2.6.4. Robust Covariance Estimation¶\\nReal data sets are often subject to measurement or recording\\nerrors. Regular but uncommon observations may also appear for a variety\\nof reasons. Observations which are very uncommon are called\\noutliers.\\nThe empirical covariance estimator and the shrunk covariance\\nestimators presented above are very sensitive to the presence of\\noutliers in the data. Therefore, one should use robust\\ncovariance estimators to estimate the covariance of its real data\\nsets. Alternatively, robust covariance estimators can be used to\\nperform outlier detection and discard/downweight some observations\\naccording to further processing of the data.\\nThe sklearn.covariance package implements a robust estimator of covariance,\\nthe Minimum Covariance Determinant [3].\\n2.6.4.1. Minimum Covariance Determinant¶\\nThe Minimum Covariance Determinant estimator is a robust estimator of\\na data set’s covariance introduced by P.J. Rousseeuw in [3].  The idea\\nis to find a given proportion (h) of “good” observations which are not\\noutliers and compute their empirical covariance matrix.  This\\nempirical covariance matrix is then rescaled to compensate the\\nperformed selection of observations (“consistency step”).  Having\\ncomputed the Minimum Covariance Determinant estimator, one can give\\nweights to observations according to their Mahalanobis distance,\\nleading to a reweighted estimate of the covariance matrix of the data\\nset (“reweighting step”).\\nRousseeuw and Van Driessen [4] developed the FastMCD algorithm in order\\nto compute the Minimum Covariance Determinant. This algorithm is used\\nin scikit-learn when fitting an MCD object to data. The FastMCD\\nalgorithm also computes a robust estimate of the data set location at\\nthe same time.\\nRaw estimates can be accessed as raw_location and raw_covariance_\\nattributes of a MinCovDet robust covariance estimator object.\\nReferences:\\n[3]\\n(1,2)\\nP. J. Rousseeuw. Least median of squares regression.\\nJ. Am Stat Ass, 79:871, 1984.\\n[4]\\nA Fast Algorithm for the Minimum Covariance Determinant Estimator,\\n1999, American Statistical Association and the American Society\\nfor Quality, TECHNOMETRICS.\\nExamples:\\nSee Robust vs Empirical covariance estimate for\\nan example on how to fit a MinCovDet object to data and see how\\nthe estimate remains accurate despite the presence of outliers.\\nSee Robust covariance estimation and Mahalanobis distances relevance to\\nvisualize the difference between EmpiricalCovariance and\\nMinCovDet covariance estimators in terms of Mahalanobis distance\\n(so we get a better estimate of the precision matrix too).\\nInfluence of outliers on location and covariance estimates\\nSeparating inliers from outliers using a Mahalanobis distance\\n\\n2.7. Novelty and Outlier Detection — scikit-learn 1.4.2 documentation\\n\\nInstall\\nUser Guide\\nAPI\\nExamples\\nCommunity\\nGetting Started\\nTutorial\\nWhat\\'s new\\nGlossary\\nDevelopment\\nFAQ\\nSupport\\nRelated packages\\nRoadmap\\nGovernance\\nAbout us\\nGitHub\\nOther Versions and Download\\nMore\\nGetting Started\\nTutorial\\nWhat\\'s new\\nGlossary\\nDevelopment\\nFAQ\\nSupport\\nRelated packages\\nRoadmap\\nGovernance\\nAbout us\\nGitHub\\nOther Versions and Download\\nToggle Menu\\nPrevUp\\nNext\\nscikit-learn 1.4.2\\nOther versions\\nPlease cite us if you use the software.\\n\\n2.7. Novelty and Outlier Detection\\n\\n2.7.1. Overview of outlier detection methods\\n\\n2.7.2. Novelty Detection\\n2.7.2.1. Scaling up the One-Class SVM\\n\\n2.7.3. Outlier Detection\\n2.7.3.1. Fitting an elliptic envelope\\n2.7.3.2. Isolation Forest\\n2.7.3.3. Local Outlier Factor\\n\\n2.7.4. Novelty detection with Local Outlier Factor\\n\\n2.7. Novelty and Outlier Detection¶\\n\\nMany applications require being able to decide whether a new observation\\nbelongs to the same distribution as existing observations (it is an\\ninlier), or should be considered as different (it is an outlier).\\nOften, this ability is used to clean real data sets. Two important\\ndistinctions must be made:\\noutlier detection:\\nThe training data contains outliers which are defined as observations that\\nare far from the others. Outlier detection estimators thus try to fit the\\nregions where the training data is the most concentrated, ignoring the\\ndeviant observations.\\nnovelty detection:\\nThe training data is not polluted by outliers and we are interested in\\ndetecting whether a new observation is an outlier. In this context an\\noutlier is also called a novelty.\\nOutlier detection and novelty detection are both used for anomaly\\ndetection, where one is interested in detecting abnormal or unusual\\nobservations. Outlier detection is then also known as unsupervised anomaly\\ndetection and novelty detection as semi-supervised anomaly detection. In the\\ncontext of outlier detection, the outliers/anomalies cannot form a\\ndense cluster as available estimators assume that the outliers/anomalies are\\nlocated in low density regions. On the contrary, in the context of novelty\\ndetection, novelties/anomalies can form a dense cluster as long as they are in\\na low density region of the training data, considered as normal in this\\ncontext.\\nThe scikit-learn project provides a set of machine learning tools that\\ncan be used both for novelty or outlier detection. This strategy is\\nimplemented with objects learning in an unsupervised way from the data:\\nestimator.fit(X_train)\\nnew observations can then be sorted as inliers or outliers with a\\npredict method:\\nestimator.predict(X_test)\\nInliers are labeled 1, while outliers are labeled -1. The predict method\\nmakes use of a threshold on the raw scoring function computed by the\\nestimator. This scoring function is accessible through the score_samples\\nmethod, while the threshold can be controlled by the contamination\\nparameter.\\nThe decision_function method is also defined from the scoring function,\\nin such a way that negative values are outliers and non-negative ones are\\ninliers:\\nestimator.decision_function(X_test)\\nNote that neighbors.LocalOutlierFactor does not support\\npredict, decision_function and score_samples methods by default\\nbut only a fit_predict method, as this estimator was originally meant to\\nbe applied for outlier detection. The scores of abnormality of the training\\nsamples are accessible through the negative_outlier_factor_ attribute.\\nIf you really want to use neighbors.LocalOutlierFactor for novelty\\ndetection, i.e. predict labels or compute the score of abnormality of new\\nunseen data, you can instantiate the estimator with the novelty parameter\\nset to True before fitting the estimator. In this case, fit_predict is\\nnot available.\\nWarning\\nNovelty detection with Local Outlier Factor\\nWhen novelty is set to True be aware that you must only use\\npredict, decision_function and score_samples on new unseen data\\nand not on the training samples as this would lead to wrong results.\\nI.e., the result of predict will not be the same as fit_predict.\\nThe scores of abnormality of the training samples are always accessible\\nthrough the negative_outlier_factor_ attribute.\\nThe behavior of neighbors.LocalOutlierFactor is summarized in the\\nfollowing table.\\nMethod\\nOutlier detection\\nNovelty detection\\nfit_predict\\nOK\\nNot available\\npredict\\nNot available\\nUse only on new data\\ndecision_function\\nNot available\\nUse only on new data\\nscore_samples\\nUse negative_outlier_factor_\\nUse only on new data\\nnegative_outlier_factor_\\nOK\\nOK\\n- 2.7.1. Overview of outlier detection methods¶\\nA comparison of the outlier detection algorithms in scikit-learn. Local\\nOutlier Factor (LOF) does not show a decision boundary in black as it\\nhas no predict method to be applied on new data when it is used for outlier\\ndetection.\\nensemble.IsolationForest and neighbors.LocalOutlierFactor\\nperform reasonably well on the data sets considered here.\\nThe svm.OneClassSVM is known to be sensitive to outliers and thus\\ndoes not perform very well for outlier detection. That being said, outlier\\ndetection in high-dimension, or without any assumptions on the distribution\\nof the inlying data is very challenging. svm.OneClassSVM may still\\nbe used with outlier detection but requires fine-tuning of its hyperparameter\\nnu to handle outliers and prevent overfitting.\\nlinear_model.SGDOneClassSVM provides an implementation of a\\nlinear One-Class SVM with a linear complexity in the number of samples. This\\nimplementation is here used with a kernel approximation technique to obtain\\nresults similar to svm.OneClassSVM which uses a Gaussian kernel\\nby default. Finally, covariance.EllipticEnvelope assumes the data is\\nGaussian and learns an ellipse. For more details on the different estimators\\nrefer to the example\\nComparing anomaly detection algorithms for outlier detection on toy datasets and the\\nsections hereunder.\\nExamples:\\nSee Comparing anomaly detection algorithms for outlier detection on toy datasets\\nfor a comparison of the svm.OneClassSVM, the\\nensemble.IsolationForest, the\\nneighbors.LocalOutlierFactor and\\ncovariance.EllipticEnvelope.\\nSee Evaluation of outlier detection estimators\\nfor an example showing how to evaluate outlier detection estimators,\\nthe neighbors.LocalOutlierFactor and the\\nensemble.IsolationForest, using ROC curves from\\nmetrics.RocCurveDisplay.\\n- 2.7.2. Novelty Detection¶\\nConsider a data set of (n) observations from the same\\ndistribution described by (p) features.  Consider now that we\\nadd one more observation to that data set. Is the new observation so\\ndifferent from the others that we can doubt it is regular? (i.e. does\\nit come from the same distribution?) Or on the contrary, is it so\\nsimilar to the other that we cannot distinguish it from the original\\nobservations? This is the question addressed by the novelty detection\\ntools and methods.\\nIn general, it is about to learn a rough, close frontier delimiting\\nthe contour of the initial observations distribution, plotted in\\nembedding (p)-dimensional space. Then, if further observations\\nlay within the frontier-delimited subspace, they are considered as\\ncoming from the same population than the initial\\nobservations. Otherwise, if they lay outside the frontier, we can say\\nthat they are abnormal with a given confidence in our assessment.\\nThe One-Class SVM has been introduced by Schölkopf et al. for that purpose\\nand implemented in the Support Vector Machines module in the\\nsvm.OneClassSVM object. It requires the choice of a\\nkernel and a scalar parameter to define a frontier.  The RBF kernel is\\nusually chosen although there exists no exact formula or algorithm to\\nset its bandwidth parameter. This is the default in the scikit-learn\\nimplementation. The nu parameter, also known as the margin of\\nthe One-Class SVM, corresponds to the probability of finding a new,\\nbut regular, observation outside the frontier.\\nReferences:\\nEstimating the support of a high-dimensional distribution\\nSchölkopf, Bernhard, et al. Neural computation 13.7 (2001): 1443-1471.\\nExamples:\\nSee One-class SVM with non-linear kernel (RBF) for visualizing the\\nfrontier learned around some data by a\\nsvm.OneClassSVM object.\\nSpecies distribution modeling\\n2.7.2.1. Scaling up the One-Class SVM¶\\nAn online linear version of the One-Class SVM is implemented in\\nlinear_model.SGDOneClassSVM. This implementation scales linearly with\\nthe number of samples and can be used with a kernel approximation to\\napproximate the solution of a kernelized svm.OneClassSVM whose\\ncomplexity is at best quadratic in the number of samples. See section\\nOnline One-Class SVM for more details.\\nExamples:\\nSee One-Class SVM versus One-Class SVM using Stochastic Gradient Descent\\nfor an illustration of the approximation of a kernelized One-Class SVM\\nwith the linear_model.SGDOneClassSVM combined with kernel approximation.\\n- 2.7.3. Outlier Detection¶\\nOutlier detection is similar to novelty detection in the sense that\\nthe goal is to separate a core of regular observations from some\\npolluting ones, called outliers. Yet, in the case of outlier\\ndetection, we don’t have a clean data set representing the population\\nof regular observations that can be used to train any tool.\\n2.7.3.1. Fitting an elliptic envelope¶\\nOne common way of performing outlier detection is to assume that the\\nregular data come from a known distribution (e.g. data are Gaussian\\ndistributed). From this assumption, we generally try to define the\\n“shape” of the data, and can define outlying observations as\\nobservations which stand far enough from the fit shape.\\nThe scikit-learn provides an object\\ncovariance.EllipticEnvelope that fits a robust covariance\\nestimate to the data, and thus fits an ellipse to the central data\\npoints, ignoring points outside the central mode.\\nFor instance, assuming that the inlier data are Gaussian distributed, it\\nwill estimate the inlier location and covariance in a robust way (i.e.\\nwithout being influenced by outliers). The Mahalanobis distances\\nobtained from this estimate is used to derive a measure of outlyingness.\\nThis strategy is illustrated below.\\nExamples:\\nSee Robust covariance estimation and Mahalanobis distances relevance for\\nan illustration of the difference between using a standard\\n(covariance.EmpiricalCovariance) or a robust estimate\\n(covariance.MinCovDet) of location and covariance to\\nassess the degree of outlyingness of an observation.\\nReferences:\\nRousseeuw, P.J., Van Driessen, K. “A fast algorithm for the minimum\\ncovariance determinant estimator” Technometrics 41(3), 212 (1999)\\n2.7.3.2. Isolation Forest¶\\nOne efficient way of performing outlier detection in high-dimensional datasets\\nis to use random forests.\\nThe ensemble.IsolationForest ‘isolates’ observations by randomly selecting\\na feature and then randomly selecting a split value between the maximum and\\nminimum values of the selected feature.\\nSince recursive partitioning can be represented by a tree structure, the\\nnumber of splittings required to isolate a sample is equivalent to the path\\nlength from the root node to the terminating node.\\nThis path length, averaged over a forest of such random trees, is a\\nmeasure of normality and our decision function.\\nRandom partitioning produces noticeably shorter paths for anomalies.\\nHence, when a forest of random trees collectively produce shorter path\\nlengths for particular samples, they are highly likely to be anomalies.\\nThe implementation of ensemble.IsolationForest is based on an ensemble\\nof tree.ExtraTreeRegressor. Following Isolation Forest original paper,\\nthe maximum depth of each tree is set to (\\\\lceil \\\\log_2(n) \\\\rceil) where\\n(n) is the number of samples used to build the tree (see (Liu et al.,\\n2008) for more details).\\nThis algorithm is illustrated below.\\nThe ensemble.IsolationForest supports warm_start=True which\\nallows you to add more trees to an already fitted model:\\n\\nfrom sklearn.ensemble import IsolationForest\\nimport numpy as np\\nX = np.array([[-1, -1], [-2, -1], [-3, -2], [0, 0], [-20, 50], [3, 5]])\\nclf = IsolationForest(n_estimators=10, warm_start=True)\\nclf.fit(X)  # fit 10 trees\\nclf.set_params(n_estimators=20)  # add 10 more trees\\nclf.fit(X)  # fit the added trees\\nExamples:\\nSee IsolationForest example for\\nan illustration of the use of IsolationForest.\\nSee Comparing anomaly detection algorithms for outlier detection on toy datasets\\nfor a comparison of ensemble.IsolationForest with\\nneighbors.LocalOutlierFactor,\\nsvm.OneClassSVM (tuned to perform like an outlier detection\\nmethod), linear_model.SGDOneClassSVM, and a covariance-based\\noutlier detection with covariance.EllipticEnvelope.\\nReferences:\\nLiu, Fei Tony, Ting, Kai Ming and Zhou, Zhi-Hua. “Isolation forest.”\\nData Mining, 2008. ICDM’08. Eighth IEEE International Conference on.\\n2.7.3.3. Local Outlier Factor¶\\nAnother efficient way to perform outlier detection on moderately high dimensional\\ndatasets is to use the Local Outlier Factor (LOF) algorithm.\\nThe neighbors.LocalOutlierFactor (LOF) algorithm computes a score\\n(called local outlier factor) reflecting the degree of abnormality of the\\nobservations.\\nIt measures the local density deviation of a given data point with respect to\\nits neighbors. The idea is to detect the samples that have a substantially\\nlower density than their neighbors.\\nIn practice the local density is obtained from the k-nearest neighbors.\\nThe LOF score of an observation is equal to the ratio of the\\naverage local density of its k-nearest neighbors, and its own local density:\\na normal instance is expected to have a local density similar to that of its\\nneighbors, while abnormal data are expected to have much smaller local density.\\nThe number k of neighbors considered, (alias parameter n_neighbors) is typically\\nchosen 1) greater than the minimum number of objects a cluster has to contain,\\nso that other objects can be local outliers relative to this cluster, and 2)\\nsmaller than the maximum number of close by objects that can potentially be\\nlocal outliers.\\nIn practice, such information is generally not available, and taking\\nn_neighbors=20 appears to work well in general.\\nWhen the proportion of outliers is high (i.e. greater than 10 %, as in the\\nexample below), n_neighbors should be greater (n_neighbors=35 in the example\\nbelow).\\nThe strength of the LOF algorithm is that it takes both local and global\\nproperties of datasets into consideration: it can perform well even in datasets\\nwhere abnormal samples have different underlying densities.\\nThe question is not, how isolated the sample is, but how isolated it is\\nwith respect to the surrounding neighborhood.\\nWhen applying LOF for outlier detection, there are no predict,\\ndecision_function and score_samples methods but only a fit_predict\\nmethod. The scores of abnormality of the training samples are accessible\\nthrough the negative_outlier_factor_ attribute.\\nNote that predict, decision_function and score_samples can be used\\non new unseen data when LOF is applied for novelty detection, i.e. when the\\nnovelty parameter is set to True, but the result of predict may\\ndiffer from that of fit_predict. See Novelty detection with Local Outlier Factor.\\nThis strategy is illustrated below.\\nExamples:\\nSee Outlier detection with Local Outlier Factor (LOF)\\nfor an illustration of the use of neighbors.LocalOutlierFactor.\\nSee Comparing anomaly detection algorithms for outlier detection on toy datasets\\nfor a comparison with other anomaly detection methods.\\nReferences:\\nBreunig, Kriegel, Ng, and Sander (2000)\\nLOF: identifying density-based local outliers.\\nProc. ACM SIGMOD\\n- 2.7.4. Novelty detection with Local Outlier Factor¶\\nTo use neighbors.LocalOutlierFactor for novelty detection, i.e.\\npredict labels or compute the score of abnormality of new unseen data, you\\nneed to instantiate the estimator with the novelty parameter\\nset to True before fitting the estimator:\\nlof = LocalOutlierFactor(novelty=True)\\nlof.fit(X_train)\\nNote that fit_predict is not available in this case to avoid inconsistencies.\\nWarning\\nNovelty detection with Local Outlier Factor`\\nWhen novelty is set to True be aware that you must only use\\npredict, decision_function and score_samples on new unseen data\\nand not on the training samples as this would lead to wrong results.\\nI.e., the result of predict will not be the same as fit_predict.\\nThe scores of abnormality of the training samples are always accessible\\nthrough the negative_outlier_factor_ attribute.\\nNovelty detection with Local Outlier Factor is illustrated below.\\n\\n2.8. Density Estimation — scikit-learn 1.4.2 documentation\\n\\nInstall\\nUser Guide\\nAPI\\nExamples\\nCommunity\\nGetting Started\\nTutorial\\nWhat\\'s new\\nGlossary\\nDevelopment\\nFAQ\\nSupport\\nRelated packages\\nRoadmap\\nGovernance\\nAbout us\\nGitHub\\nOther Versions and Download\\nMore\\nGetting Started\\nTutorial\\nWhat\\'s new\\nGlossary\\nDevelopment\\nFAQ\\nSupport\\nRelated packages\\nRoadmap\\nGovernance\\nAbout us\\nGitHub\\nOther Versions and Download\\nToggle Menu\\nPrevUp\\nNext\\nscikit-learn 1.4.2\\nOther versions\\nPlease cite us if you use the software.\\n\\n2.8. Density Estimation\\n\\n2.8.1. Density Estimation: Histograms\\n\\n2.8.2. Kernel Density Estimation\\n\\n2.8. Density Estimation¶\\n\\nDensity estimation walks the line between unsupervised learning, feature\\nengineering, and data modeling.  Some of the most popular and useful\\ndensity estimation techniques are mixture models such as\\nGaussian Mixtures (GaussianMixture), and\\nneighbor-based approaches such as the kernel density estimate\\n(KernelDensity).\\nGaussian Mixtures are discussed more fully in the context of\\nclustering, because the technique is also useful as\\nan unsupervised clustering scheme.\\nDensity estimation is a very simple concept, and most people are already\\nfamiliar with one common density estimation technique: the histogram.\\n- 2.8.1. Density Estimation: Histograms¶\\nA histogram is a simple visualization of data where bins are defined, and the\\nnumber of data points within each bin is tallied.  An example of a histogram\\ncan be seen in the upper-left panel of the following figure:\\nA major problem with histograms, however, is that the choice of binning can\\nhave a disproportionate effect on the resulting visualization.  Consider the\\nupper-right panel of the above figure.  It shows a histogram over the same\\ndata, with the bins shifted right.  The results of the two visualizations look\\nentirely different, and might lead to different interpretations of the data.\\nIntuitively, one can also think of a histogram as a stack of blocks, one block\\nper point.  By stacking the blocks in the appropriate grid space, we recover\\nthe histogram.  But what if, instead of stacking the blocks on a regular grid,\\nwe center each block on the point it represents, and sum the total height at\\neach location?  This idea leads to the lower-left visualization.  It is perhaps\\nnot as clean as a histogram, but the fact that the data drive the block\\nlocations mean that it is a much better representation of the underlying\\ndata.\\nThis visualization is an example of a kernel density estimation, in this case\\nwith a top-hat kernel (i.e. a square block at each point).  We can recover a\\nsmoother distribution by using a smoother kernel.  The bottom-right plot shows\\na Gaussian kernel density estimate, in which each point contributes a Gaussian\\ncurve to the total.  The result is a smooth density estimate which is derived\\nfrom the data, and functions as a powerful non-parametric model of the\\ndistribution of points.\\n- 2.8.2. Kernel Density Estimation¶\\nKernel density estimation in scikit-learn is implemented in the\\nKernelDensity estimator, which uses the\\nBall Tree or KD Tree for efficient queries (see Nearest Neighbors for\\na discussion of these).  Though the above example\\nuses a 1D data set for simplicity, kernel density estimation can be\\nperformed in any number of dimensions, though in practice the curse of\\ndimensionality causes its performance to degrade in high dimensions.\\nIn the following figure, 100 points are drawn from a bimodal distribution,\\nand the kernel density estimates are shown for three choices of kernels:\\nIt’s clear how the kernel shape affects the smoothness of the resulting\\ndistribution.  The scikit-learn kernel density estimator can be used as\\nfollows:\\n\\nfrom sklearn.neighbors import KernelDensity\\nimport numpy as np\\nX = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\\nkde = KernelDensity(kernel=\\'gaussian\\', bandwidth=0.2).fit(X)\\nkde.score_samples(X)\\narray([-0.41075698, -0.41075698, -0.41076071, -0.41075698, -0.41075698,\\n-0.41076071])\\nHere we have used kernel=\\'gaussian\\', as seen above.\\nMathematically, a kernel is a positive function (K(x;h))\\nwhich is controlled by the bandwidth parameter (h).\\nGiven this kernel form, the density estimate at a point (y) within\\na group of points (x_i; i=1\\\\cdots N) is given by:\\n[\\\\rho_K(y) = \\\\sum_{i=1}^{N} K(y - x_i; h)]\\nThe bandwidth here acts as a smoothing parameter, controlling the tradeoff\\nbetween bias and variance in the result.  A large bandwidth leads to a very\\nsmooth (i.e. high-bias) density distribution.  A small bandwidth leads\\nto an unsmooth (i.e. high-variance) density distribution.\\nThe parameter bandwidth controls this smoothing. One can either set\\nmanually this parameter or use Scott’s and Silvermann’s estimation\\nmethods.\\nKernelDensity implements several common kernel\\nforms, which are shown in the following figure:\\nkernels’ mathematical expressions\\nClick for more details\\n¶\\nThe form of these kernels is as follows:\\nGaussian kernel (kernel = \\'gaussian\\')\\n(K(x; h) \\\\propto \\\\exp(- \\\\frac{x^2}{2h^2} ))\\nTophat kernel (kernel = \\'tophat\\')\\n(K(x; h) \\\\propto 1) if (x < h)\\nEpanechnikov kernel (kernel = \\'epanechnikov\\')\\n(K(x; h) \\\\propto 1 - \\\\frac{x^2}{h^2})\\nExponential kernel (kernel = \\'exponential\\')\\n(K(x; h) \\\\propto \\\\exp(-x/h))\\nLinear kernel (kernel = \\'linear\\')\\n(K(x; h) \\\\propto 1 - x/h) if (x < h)\\nCosine kernel (kernel = \\'cosine\\')\\n(K(x; h) \\\\propto \\\\cos(\\\\frac{\\\\pi x}{2h})) if (x < h)\\nThe kernel density estimator can be used with any of the valid distance\\nmetrics (see DistanceMetric for a list of\\navailable metrics), though the results are properly normalized only\\nfor the Euclidean metric.  One particularly useful metric is the\\nHaversine distance\\nwhich measures the angular distance between points on a sphere.  Here\\nis an example of using a kernel density estimate for a visualization\\nof geospatial data, in this case the distribution of observations of two\\ndifferent species on the South American continent:\\nOne other useful application of kernel density estimation is to learn a\\nnon-parametric generative model of a dataset in order to efficiently\\ndraw new samples from this generative model.\\nHere is an example of using this process to\\ncreate a new set of hand-written digits, using a Gaussian kernel learned\\non a PCA projection of the data:\\nThe “new” data consists of linear combinations of the input data, with weights\\nprobabilistically drawn given the KDE model.\\nExamples:\\nSimple 1D Kernel Density Estimation: computation of simple kernel\\ndensity estimates in one dimension.\\nKernel Density Estimation: an example of using\\nKernel Density estimation to learn a generative model of the hand-written\\ndigits data, and drawing new samples from this model.\\nKernel Density Estimate of Species Distributions: an example of Kernel Density\\nestimation using the Haversine distance metric to visualize geospatial data\\n\\n2.9. Neural network models (unsupervised) — scikit-learn 1.4.2 documentation\\n\\nInstall\\nUser Guide\\nAPI\\nExamples\\nCommunity\\nGetting Started\\nTutorial\\nWhat\\'s new\\nGlossary\\nDevelopment\\nFAQ\\nSupport\\nRelated packages\\nRoadmap\\nGovernance\\nAbout us\\nGitHub\\nOther Versions and Download\\nMore\\nGetting Started\\nTutorial\\nWhat\\'s new\\nGlossary\\nDevelopment\\nFAQ\\nSupport\\nRelated packages\\nRoadmap\\nGovernance\\nAbout us\\nGitHub\\nOther Versions and Download\\nToggle Menu\\nPrevUp\\nNext\\nscikit-learn 1.4.2\\nOther versions\\nPlease cite us if you use the software.\\n\\n2.9. Neural network models (unsupervised)\\n\\n2.9.1. Restricted Boltzmann machines\\n2.9.1.1. Graphical model and parametrization\\n2.9.1.2. Bernoulli Restricted Boltzmann machines\\n2.9.1.3. Stochastic Maximum Likelihood learning\\n\\n2.9. Neural network models (unsupervised)¶\\n\\n2.9.1. Restricted Boltzmann machines¶\\nRestricted Boltzmann machines (RBM) are unsupervised nonlinear feature learners\\nbased on a probabilistic model. The features extracted by an RBM or a hierarchy\\nof RBMs often give good results when fed into a linear classifier such as a\\nlinear SVM or a perceptron.\\nThe model makes assumptions regarding the distribution of inputs. At the moment,\\nscikit-learn only provides BernoulliRBM, which assumes the inputs are\\neither binary values or values between 0 and 1, each encoding the probability\\nthat the specific feature would be turned on.\\nThe RBM tries to maximize the likelihood of the data using a particular\\ngraphical model. The parameter learning algorithm used (Stochastic\\nMaximum Likelihood) prevents the representations from straying far\\nfrom the input data, which makes them capture interesting regularities, but\\nmakes the model less useful for small datasets, and usually not useful for\\ndensity estimation.\\nThe method gained popularity for initializing deep neural networks with the\\nweights of independent RBMs. This method is known as unsupervised pre-training.\\nExamples:\\nRestricted Boltzmann Machine features for digit classification\\n2.9.1.1. Graphical model and parametrization¶\\nThe graphical model of an RBM is a fully-connected bipartite graph.\\nThe nodes are random variables whose states depend on the state of the other\\nnodes they are connected to. The model is therefore parameterized by the\\nweights of the connections, as well as one intercept (bias) term for each\\nvisible and hidden unit, omitted from the image for simplicity.\\nThe energy function measures the quality of a joint assignment:\\n[E(\\\\mathbf{v}, \\\\mathbf{h}) = -\\\\sum_i \\\\sum_j w_{ij}v_ih_j - \\\\sum_i b_iv_i\\n\\n\\\\sum_j c_jh_j]\\nIn the formula above, (\\\\mathbf{b}) and (\\\\mathbf{c}) are the\\nintercept vectors for the visible and hidden layers, respectively. The\\njoint probability of the model is defined in terms of the energy:\\n[P(\\\\mathbf{v}, \\\\mathbf{h}) = \\\\frac{e^{-E(\\\\mathbf{v}, \\\\mathbf{h})}}{Z}]\\nThe word restricted refers to the bipartite structure of the model, which\\nprohibits direct interaction between hidden units, or between visible units.\\nThis means that the following conditional independencies are assumed:\\n[\\\\begin{split}h_i \\\\bot h_j | \\\\mathbf{v} \\\\\\nv_i \\\\bot v_j | \\\\mathbf{h}\\\\end{split}]\\nThe bipartite structure allows for the use of efficient block Gibbs sampling for\\ninference.\\n2.9.1.2. Bernoulli Restricted Boltzmann machines¶\\nIn the BernoulliRBM, all units are binary stochastic units. This\\nmeans that the input data should either be binary, or real-valued between 0 and\\n1 signifying the probability that the visible unit would turn on or off. This\\nis a good model for character recognition, where the interest is on which\\npixels are active and which aren’t. For images of natural scenes it no longer\\nfits because of background, depth and the tendency of neighbouring pixels to\\ntake the same values.\\nThe conditional probability distribution of each unit is given by the\\nlogistic sigmoid activation function of the input it receives:\\n[\\\\begin{split}P(v_i=1|\\\\mathbf{h}) = \\\\sigma(\\\\sum_j w_{ij}h_j + b_i) \\\\\\nP(h_i=1|\\\\mathbf{v}) = \\\\sigma(\\\\sum_i w_{ij}v_i + c_j)\\\\end{split}]\\nwhere (\\\\sigma) is the logistic sigmoid function:\\n[\\\\sigma(x) = \\\\frac{1}{1 + e^{-x}}]\\n2.9.1.3. Stochastic Maximum Likelihood learning¶\\nThe training algorithm implemented in BernoulliRBM is known as\\nStochastic Maximum Likelihood (SML) or Persistent Contrastive Divergence\\n(PCD). Optimizing maximum likelihood directly is infeasible because of\\nthe form of the data likelihood:\\n[\\\\log P(v) = \\\\log \\\\sum_h e^{-E(v, h)} - \\\\log \\\\sum_{x, y} e^{-E(x, y)}]\\nFor simplicity the equation above is written for a single training example.\\nThe gradient with respect to the weights is formed of two terms corresponding to\\nthe ones above. They are usually known as the positive gradient and the negative\\ngradient, because of their respective signs.  In this implementation, the\\ngradients are estimated over mini-batches of samples.\\nIn maximizing the log-likelihood, the positive gradient makes the model prefer\\nhidden states that are compatible with the observed training data. Because of\\nthe bipartite structure of RBMs, it can be computed efficiently. The\\nnegative gradient, however, is intractable. Its goal is to lower the energy of\\njoint states that the model prefers, therefore making it stay true to the data.\\nIt can be approximated by Markov chain Monte Carlo using block Gibbs sampling by\\niteratively sampling each of (v) and (h) given the other, until the\\nchain mixes. Samples generated in this way are sometimes referred as fantasy\\nparticles. This is inefficient and it is difficult to determine whether the\\nMarkov chain mixes.\\nThe Contrastive Divergence method suggests to stop the chain after a small\\nnumber of iterations, (k), usually even 1. This method is fast and has\\nlow variance, but the samples are far from the model distribution.\\nPersistent Contrastive Divergence addresses this. Instead of starting a new\\nchain each time the gradient is needed, and performing only one Gibbs sampling\\nstep, in PCD we keep a number of chains (fantasy particles) that are updated\\n(k) Gibbs steps after each weight update. This allows the particles to\\nexplore the space more thoroughly.\\nReferences:\\n“A fast learning algorithm for deep belief nets”\\nG. Hinton, S. Osindero, Y.-W. Teh, 2006\\n“Training Restricted Boltzmann Machines using Approximations to\\nthe Likelihood Gradient”\\nT. Tieleman, 2008\\n\\n3.1. Cross-validation: evaluating estimator performance — scikit-learn 1.4.2 documentation\\n\\nInstall\\nUser Guide\\nAPI\\nExamples\\nCommunity\\nGetting Started\\nTutorial\\nWhat\\'s new\\nGlossary\\nDevelopment\\nFAQ\\nSupport\\nRelated packages\\nRoadmap\\nGovernance\\nAbout us\\nGitHub\\nOther Versions and Download\\nMore\\nGetting Started\\nTutorial\\nWhat\\'s new\\nGlossary\\nDevelopment\\nFAQ\\nSupport\\nRelated packages\\nRoadmap\\nGovernance\\nAbout us\\nGitHub\\nOther Versions and Download\\nToggle Menu\\nPrevUp\\nNext\\nscikit-learn 1.4.2\\nOther versions\\nPlease cite us if you use the software.\\n\\n3.1. Cross-validation: evaluating estimator performance\\n\\n3.1.1. Computing cross-validated metrics\\n3.1.1.1. The cross_validate function and multiple metric evaluation\\n3.1.1.2. Obtaining predictions by cross-validation\\n\\n3.1.2. Cross validation iterators\\n3.1.2.1. Cross-validation iterators for i.i.d. data\\n3.1.2.1.1. K-fold\\n3.1.2.1.2. Repeated K-Fold\\n3.1.2.1.3. Leave One Out (LOO)\\n3.1.2.1.4. Leave P Out (LPO)\\n3.1.2.1.5. Random permutations cross-validation a.k.a. Shuffle & Split\\n3.1.2.2. Cross-validation iterators with stratification based on class labels\\n3.1.2.2.1. Stratified k-fold\\n3.1.2.2.2. Stratified Shuffle Split\\n3.1.2.3. Cross-validation iterators for grouped data\\n3.1.2.3.1. Group k-fold\\n3.1.2.3.2. StratifiedGroupKFold\\n3.1.2.3.3. Leave One Group Out\\n3.1.2.3.4. Leave P Groups Out\\n3.1.2.3.5. Group Shuffle Split\\n3.1.2.4. Predefined fold-splits / Validation-sets\\n3.1.2.5. Using cross-validation iterators to split train and test\\n3.1.2.6. Cross validation of time series data\\n3.1.2.6.1. Time Series Split\\n\\n3.1.3. A note on shuffling\\n\\n3.1.4. Cross validation and model selection\\n\\n3.1.5. Permutation test score\\n\\n3.1. Cross-validation: evaluating estimator performance¶\\n\\nLearning the parameters of a prediction function and testing it on the\\nsame data is a methodological mistake: a model that would just repeat\\nthe labels of the samples that it has just seen would have a perfect\\nscore but would fail to predict anything useful on yet-unseen data.\\nThis situation is called overfitting.\\nTo avoid it, it is common practice when performing\\na (supervised) machine learning experiment\\nto hold out part of the available data as a test set X_test, y_test.\\nNote that the word “experiment” is not intended\\nto denote academic use only,\\nbecause even in commercial settings\\nmachine learning usually starts out experimentally.\\nHere is a flowchart of typical cross validation workflow in model training.\\nThe best parameters can be determined by\\ngrid search techniques.\\nIn scikit-learn a random split into training and test sets\\ncan be quickly computed with the train_test_split helper function.\\nLet’s load the iris data set to fit a linear support vector machine on it:\\n\\nimport numpy as np\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn import datasets\\nfrom sklearn import svm\\nX, y = datasets.load_iris(return_X_y=True)\\nX.shape, y.shape\\n((150, 4), (150,))\\nWe can now quickly sample a training set while holding out 40% of the\\ndata for testing (evaluating) our classifier:\\nX_train, X_test, y_train, y_test = train_test_split(\\n...     X, y, test_size=0.4, random_state=0)\\nX_train.shape, y_train.shape\\n((90, 4), (90,))\\nX_test.shape, y_test.shape\\n((60, 4), (60,))\\nclf = svm.SVC(kernel=\\'linear\\', C=1).fit(X_train, y_train)\\nclf.score(X_test, y_test)\\n0.96...\\nWhen evaluating different settings (“hyperparameters”) for estimators,\\nsuch as the C setting that must be manually set for an SVM,\\nthere is still a risk of overfitting on the test set\\nbecause the parameters can be tweaked until the estimator performs optimally.\\nThis way, knowledge about the test set can “leak” into the model\\nand evaluation metrics no longer report on generalization performance.\\nTo solve this problem, yet another part of the dataset can be held out\\nas a so-called “validation set”: training proceeds on the training set,\\nafter which evaluation is done on the validation set,\\nand when the experiment seems to be successful,\\nfinal evaluation can be done on the test set.\\nHowever, by partitioning the available data into three sets,\\nwe drastically reduce the number of samples\\nwhich can be used for learning the model,\\nand the results can depend on a particular random choice for the pair of\\n(train, validation) sets.\\nA solution to this problem is a procedure called\\ncross-validation\\n(CV for short).\\nA test set should still be held out for final evaluation,\\nbut the validation set is no longer needed when doing CV.\\nIn the basic approach, called k-fold CV,\\nthe training set is split into k smaller sets\\n(other approaches are described below,\\nbut generally follow the same principles).\\nThe following procedure is followed for each of the k “folds”:\\nA model is trained using (k-1) of the folds as training data;\\nthe resulting model is validated on the remaining part of the data\\n(i.e., it is used as a test set to compute a performance measure\\nsuch as accuracy).\\nThe performance measure reported by k-fold cross-validation\\nis then the average of the values computed in the loop.\\nThis approach can be computationally expensive,\\nbut does not waste too much data\\n(as is the case when fixing an arbitrary validation set),\\nwhich is a major advantage in problems such as inverse inference\\nwhere the number of samples is very small.\\n- 3.1.1. Computing cross-validated metrics¶\\nThe simplest way to use cross-validation is to call the\\ncross_val_score helper function on the estimator and the dataset.\\nThe following example demonstrates how to estimate the accuracy of a linear\\nkernel support vector machine on the iris dataset by splitting the data, fitting\\na model and computing the score 5 consecutive times (with different splits each\\ntime):\\nfrom sklearn.model_selection import cross_val_score\\nclf = svm.SVC(kernel=\\'linear\\', C=1, random_state=42)\\nscores = cross_val_score(clf, X, y, cv=5)\\nscores\\narray([0.96..., 1. , 0.96..., 0.96..., 1. ])\\nThe mean score and the standard deviation are hence given by:\\nprint(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\\n0.98 accuracy with a standard deviation of 0.02\\nBy default, the score computed at each CV iteration is the score\\nmethod of the estimator. It is possible to change this by using the\\nscoring parameter:\\nfrom sklearn import metrics\\nscores = cross_val_score(\\n...     clf, X, y, cv=5, scoring=\\'f1_macro\\')\\nscores\\narray([0.96..., 1.  ..., 0.96..., 0.96..., 1.        ])\\nSee The scoring parameter: defining model evaluation rules for details.\\nIn the case of the Iris dataset, the samples are balanced across target\\nclasses hence the accuracy and the F1-score are almost equal.\\nWhen the cv argument is an integer, cross_val_score uses the\\nKFold or StratifiedKFold strategies by default, the latter\\nbeing used if the estimator derives from ClassifierMixin.\\nIt is also possible to use other cross validation strategies by passing a cross\\nvalidation iterator instead, for instance:\\nfrom sklearn.model_selection import ShuffleSplit\\nn_samples = X.shape[0]\\ncv = ShuffleSplit(n_splits=5, test_size=0.3, random_state=0)\\ncross_val_score(clf, X, y, cv=cv)\\narray([0.977..., 0.977..., 1.  ..., 0.955..., 1.        ])\\nAnother option is to use an iterable yielding (train, test) splits as arrays of\\nindices, for example:\\ndef custom_cv_2folds(X):\\n...     n = X.shape[0]\\n...     i = 1\\n...     while i <= 2:\\n...         idx = np.arange(n * (i - 1) / 2, n * i / 2, dtype=int)\\n...         yield idx, idx\\n...         i += 1\\n...\\ncustom_cv = custom_cv_2folds(X)\\ncross_val_score(clf, X, y, cv=custom_cv)\\narray([1.        , 0.973...])\\nData transformation with held out data\\nJust as it is important to test a predictor on data held-out from\\ntraining, preprocessing (such as standardization, feature selection, etc.)\\nand similar data transformations similarly should\\nbe learnt from a training set and applied to held-out data for prediction:\\nfrom sklearn import preprocessing\\nX_train, X_test, y_train, y_test = train_test_split(\\n...     X, y, test_size=0.4, random_state=0)\\nscaler = preprocessing.StandardScaler().fit(X_train)\\nX_train_transformed = scaler.transform(X_train)\\nclf = svm.SVC(C=1).fit(X_train_transformed, y_train)\\nX_test_transformed = scaler.transform(X_test)\\nclf.score(X_test_transformed, y_test)\\n0.9333...\\nA Pipeline makes it easier to compose\\nestimators, providing this behavior under cross-validation:\\nfrom sklearn.pipeline import make_pipeline\\nclf = make_pipeline(preprocessing.StandardScaler(), svm.SVC(C=1))\\ncross_val_score(clf, X, y, cv=cv)\\narray([0.977..., 0.933..., 0.955..., 0.933..., 0.977...])\\nSee Pipelines and composite estimators.\\n3.1.1.1. The cross_validate function and multiple metric evaluation¶\\nThe cross_validate function differs from cross_val_score in\\ntwo ways:\\nIt allows specifying multiple metrics for evaluation.\\nIt returns a dict containing fit-times, score-times\\n(and optionally training scores, fitted estimators, train-test split indices)\\nin addition to the test score.\\nFor single metric evaluation, where the scoring parameter is a string,\\ncallable or None, the keys will be - [\\'test_score\\', \\'fit_time\\', \\'score_time\\']\\nAnd for multiple metric evaluation, the return value is a dict with the\\nfollowing keys -\\n[\\'test_\\', \\'test_\\', \\'test_\\', \\'fit_time\\', \\'score_time\\']\\nreturn_train_score is set to False by default to save computation time.\\nTo evaluate the scores on the training set as well you need to set it to\\nTrue. You may also retain the estimator fitted on each training set by\\nsetting return_estimator=True. Similarly, you may set\\nreturn_indices=True to retain the training and testing indices used to split\\nthe dataset into train and test sets for each cv split.\\nThe multiple metrics can be specified either as a list, tuple or set of\\npredefined scorer names:\\nfrom sklearn.model_selection import cross_validate\\nfrom sklearn.metrics import recall_score\\nscoring = [\\'precision_macro\\', \\'recall_macro\\']\\nclf = svm.SVC(kernel=\\'linear\\', C=1, random_state=0)\\nscores = cross_validate(clf, X, y, scoring=scoring)\\nsorted(scores.keys())\\n[\\'fit_time\\', \\'score_time\\', \\'test_precision_macro\\', \\'test_recall_macro\\']\\nscores[\\'test_recall_macro\\']\\narray([0.96..., 1.  ..., 0.96..., 0.96..., 1.        ])\\nOr as a dict mapping scorer name to a predefined or custom scoring function:\\nfrom sklearn.metrics import make_scorer\\nscoring = {\\'prec_macro\\': \\'precision_macro\\',\\n...            \\'rec_macro\\': make_scorer(recall_score, average=\\'macro\\')}\\nscores = cross_validate(clf, X, y, scoring=scoring,\\n...                         cv=5, return_train_score=True)\\nsorted(scores.keys())\\n[\\'fit_time\\', \\'score_time\\', \\'test_prec_macro\\', \\'test_rec_macro\\',\\n\\'train_prec_macro\\', \\'train_rec_macro\\']\\nscores[\\'train_rec_macro\\']\\narray([0.97..., 0.97..., 0.99..., 0.98..., 0.98...])\\nHere is an example of cross_validate using a single metric:\\nscores = cross_validate(clf, X, y,\\n...                         scoring=\\'precision_macro\\', cv=5,\\n...                         return_estimator=True)\\nsorted(scores.keys())\\n[\\'estimator\\', \\'fit_time\\', \\'score_time\\', \\'test_score\\']\\n3.1.1.2. Obtaining predictions by cross-validation¶\\nThe function cross_val_predict has a similar interface to\\ncross_val_score, but returns, for each element in the input, the\\nprediction that was obtained for that element when it was in the test set. Only\\ncross-validation strategies that assign all elements to a test set exactly once\\ncan be used (otherwise, an exception is raised).\\nWarning\\nNote on inappropriate usage of cross_val_predict\\nThe result of cross_val_predict may be different from those\\nobtained using cross_val_score as the elements are grouped in\\ndifferent ways. The function cross_val_score takes an average\\nover cross-validation folds, whereas cross_val_predict simply\\nreturns the labels (or probabilities) from several distinct models\\nundistinguished. Thus, cross_val_predict is not an appropriate\\nmeasure of generalization error.\\nThe function cross_val_predict is appropriate for:\\nVisualization of predictions obtained from different models.\\nModel blending: When predictions of one supervised estimator are used to\\ntrain another estimator in ensemble methods.\\nThe available cross validation iterators are introduced in the following\\nsection.\\nExamples\\nReceiver Operating Characteristic (ROC) with cross validation,\\nRecursive feature elimination with cross-validation,\\nCustom refit strategy of a grid search with cross-validation,\\nSample pipeline for text feature extraction and evaluation,\\nPlotting Cross-Validated Predictions,\\nNested versus non-nested cross-validation.\\n- 3.1.2. Cross validation iterators¶\\nThe following sections list utilities to generate indices\\nthat can be used to generate dataset splits according to different cross\\nvalidation strategies.\\n3.1.2.1. Cross-validation iterators for i.i.d. data¶\\nAssuming that some data is Independent and Identically Distributed (i.i.d.) is\\nmaking the assumption that all samples stem from the same generative process\\nand that the generative process is assumed to have no memory of past generated\\nsamples.\\nThe following cross-validators can be used in such cases.\\nNote\\nWhile i.i.d. data is a common assumption in machine learning theory, it rarely\\nholds in practice. If one knows that the samples have been generated using a\\ntime-dependent process, it is safer to\\nuse a time-series aware cross-validation scheme.\\nSimilarly, if we know that the generative process has a group structure\\n(samples collected from different subjects, experiments, measurement\\ndevices), it is safer to use group-wise cross-validation.\\n3.1.2.1.1. K-fold¶\\nKFold divides all the samples in (k) groups of samples,\\ncalled folds (if (k = n), this is equivalent to the Leave One\\nOut strategy), of equal sizes (if possible). The prediction function is\\nlearned using (k - 1) folds, and the fold left out is used for test.\\nExample of 2-fold cross-validation on a dataset with 4 samples:\\nimport numpy as np\\nfrom sklearn.model_selection import KFold\\nX = [\"a\", \"b\", \"c\", \"d\"]\\nkf = KFold(n_splits=2)\\nfor train, test in kf.split(X):\\n...     print(\"%s %s\" % (train, test))\\n[2 3] [0 1]\\n[0 1] [2 3]\\nHere is a visualization of the cross-validation behavior. Note that\\nKFold is not affected by classes or groups.\\nEach fold is constituted by two arrays: the first one is related to the\\ntraining set, and the second one to the test set.\\nThus, one can create the training/test sets using numpy indexing:\\nX = np.array([[0., 0.], [1., 1.], [-1., -1.], [2., 2.]])\\ny = np.array([0, 1, 0, 1])\\nX_train, X_test, y_train, y_test = X[train], X[test], y[train], y[test]\\n3.1.2.1.2. Repeated K-Fold¶\\nRepeatedKFold repeats K-Fold n times. It can be used when one\\nrequires to run KFold n times, producing different splits in\\neach repetition.\\nExample of 2-fold K-Fold repeated 2 times:\\nimport numpy as np\\nfrom sklearn.model_selection import RepeatedKFold\\nX = np.array([[1, 2], [3, 4], [1, 2], [3, 4]])\\nrandom_state = 12883823\\nrkf = RepeatedKFold(n_splits=2, n_repeats=2, random_state=random_state)\\nfor train, test in rkf.split(X):\\n...     print(\"%s %s\" % (train, test))\\n...\\n[2 3] [0 1]\\n[0 1] [2 3]\\n[0 2] [1 3]\\n[1 3] [0 2]\\nSimilarly, RepeatedStratifiedKFold repeats Stratified K-Fold n times\\nwith different randomization in each repetition.\\n3.1.2.1.3. Leave One Out (LOO)¶\\nLeaveOneOut (or LOO) is a simple cross-validation. Each learning\\nset is created by taking all the samples except one, the test set being\\nthe sample left out. Thus, for (n) samples, we have (n) different\\ntraining sets and (n) different tests set. This cross-validation\\nprocedure does not waste much data as only one sample is removed from the\\ntraining set:\\nfrom sklearn.model_selection import LeaveOneOut\\nX = [1, 2, 3, 4]\\nloo = LeaveOneOut()\\nfor train, test in loo.split(X):\\n...     print(\"%s %s\" % (train, test))\\n[1 2 3] [0]\\n[0 2 3] [1]\\n[0 1 3] [2]\\n[0 1 2] [3]\\nPotential users of LOO for model selection should weigh a few known caveats.\\nWhen compared with (k)-fold cross validation, one builds (n) models\\nfrom (n) samples instead of (k) models, where (n > k).\\nMoreover, each is trained on (n - 1) samples rather than\\n((k-1) n / k). In both ways, assuming (k) is not too large\\nand (k < n), LOO is more computationally expensive than (k)-fold\\ncross validation.\\nIn terms of accuracy, LOO often results in high variance as an estimator for the\\ntest error. Intuitively, since (n - 1) of\\nthe (n) samples are used to build each model, models constructed from\\nfolds are virtually identical to each other and to the model built from the\\nentire training set.\\nHowever, if the learning curve is steep for the training size in question,\\nthen 5- or 10- fold cross validation can overestimate the generalization error.\\nAs a general rule, most authors, and empirical evidence, suggest that 5- or 10-\\nfold cross validation should be preferred to LOO.\\nReferences:\\nhttp://www.faqs.org/faqs/ai-faq/neural-nets/part3/section-12.html;\\nT. Hastie, R. Tibshirani, J. Friedman,  The Elements of Statistical Learning, Springer 2009\\nL. Breiman, P. Spector Submodel selection and evaluation in regression: The X-random case, International Statistical Review 1992;\\nR. Kohavi, A Study of Cross-Validation and Bootstrap for Accuracy Estimation and Model Selection, Intl. Jnt. Conf. AI\\nR. Bharat Rao, G. Fung, R. Rosales, On the Dangers of Cross-Validation. An Experimental Evaluation, SIAM 2008;\\nG. James, D. Witten, T. Hastie, R Tibshirani, An Introduction to\\nStatistical Learning, Springer 2013.\\n3.1.2.1.4. Leave P Out (LPO)¶\\nLeavePOut is very similar to LeaveOneOut as it creates all\\nthe possible training/test sets by removing (p) samples from the complete\\nset. For (n) samples, this produces ({n \\\\choose p}) train-test\\npairs. Unlike LeaveOneOut and KFold, the test sets will\\noverlap for (p > 1).\\nExample of Leave-2-Out on a dataset with 4 samples:\\nfrom sklearn.model_selection import LeavePOut\\nX = np.ones(4)\\nlpo = LeavePOut(p=2)\\nfor train, test in lpo.split(X):\\n...     print(\"%s %s\" % (train, test))\\n[2 3] [0 1]\\n[1 3] [0 2]\\n[1 2] [0 3]\\n[0 3] [1 2]\\n[0 2] [1 3]\\n[0 1] [2 3]\\n3.1.2.1.5. Random permutations cross-validation a.k.a. Shuffle & Split¶\\nThe ShuffleSplit iterator will generate a user defined number of\\nindependent train / test dataset splits. Samples are first shuffled and\\nthen split into a pair of train and test sets.\\nIt is possible to control the randomness for reproducibility of the\\nresults by explicitly seeding the random_state pseudo random number\\ngenerator.\\nHere is a usage example:\\nfrom sklearn.model_selection import ShuffleSplit\\nX = np.arange(10)\\nss = ShuffleSplit(n_splits=5, test_size=0.25, random_state=0)\\nfor train_index, test_index in ss.split(X):\\n...     print(\"%s %s\" % (train_index, test_index))\\n[9 1 6 7 3 0 5] [2 8 4]\\n[2 9 8 0 6 7 4] [3 5 1]\\n[4 5 1 0 6 9 7] [2 3 8]\\n[2 7 5 8 0 3 4] [6 1 9]\\n[4 1 0 6 8 9 3] [5 2 7]\\nHere is a visualization of the cross-validation behavior. Note that\\nShuffleSplit is not affected by classes or groups.\\nShuffleSplit is thus a good alternative to KFold cross\\nvalidation that allows a finer control on the number of iterations and\\nthe proportion of samples on each side of the train / test split.\\n3.1.2.2. Cross-validation iterators with stratification based on class labels¶\\nSome classification problems can exhibit a large imbalance in the distribution\\nof the target classes: for instance there could be several times more negative\\nsamples than positive samples. In such cases it is recommended to use\\nstratified sampling as implemented in StratifiedKFold and\\nStratifiedShuffleSplit to ensure that relative class frequencies is\\napproximately preserved in each train and validation fold.\\n3.1.2.2.1. Stratified k-fold¶\\nStratifiedKFold is a variation of k-fold which returns stratified\\nfolds: each set contains approximately the same percentage of samples of each\\ntarget class as the complete set.\\nHere is an example of stratified 3-fold cross-validation on a dataset with 50 samples from\\ntwo unbalanced classes.  We show the number of samples in each class and compare with\\nKFold.\\nfrom sklearn.model_selection import StratifiedKFold, KFold\\nimport numpy as np\\nX, y = np.ones((50, 1)), np.hstack(([0] * 45, [1] * 5))\\nskf = StratifiedKFold(n_splits=3)\\nfor train, test in skf.split(X, y):\\n...     print(\\'train -  {}   |   test -  {}\\'.format(\\n...         np.bincount(y[train]), np.bincount(y[test])))\\ntrain -  [30  3]   |   test -  [15  2]\\ntrain -  [30  3]   |   test -  [15  2]\\ntrain -  [30  4]   |   test -  [15  1]\\nkf = KFold(n_splits=3)\\nfor train, test in kf.split(X, y):\\n...     print(\\'train -  {}   |   test -  {}\\'.format(\\n...         np.bincount(y[train]), np.bincount(y[test])))\\ntrain -  [28  5]   |   test -  [17]\\ntrain -  [28  5]   |   test -  [17]\\ntrain -  [34]   |   test -  [11  5]\\nWe can see that StratifiedKFold preserves the class ratios\\n(approximately 1 / 10) in both train and test dataset.\\nHere is a visualization of the cross-validation behavior.\\nRepeatedStratifiedKFold can be used to repeat Stratified K-Fold n times\\nwith different randomization in each repetition.\\n3.1.2.2.2. Stratified Shuffle Split¶\\nStratifiedShuffleSplit is a variation of ShuffleSplit, which returns\\nstratified splits, i.e which creates splits by preserving the same\\npercentage for each target class as in the complete set.\\nHere is a visualization of the cross-validation behavior.\\n3.1.2.3. Cross-validation iterators for grouped data¶\\nThe i.i.d. assumption is broken if the underlying generative process yield\\ngroups of dependent samples.\\nSuch a grouping of data is domain specific. An example would be when there is\\nmedical data collected from multiple patients, with multiple samples taken from\\neach patient. And such data is likely to be dependent on the individual group.\\nIn our example, the patient id for each sample will be its group identifier.\\nIn this case we would like to know if a model trained on a particular set of\\ngroups generalizes well to the unseen groups. To measure this, we need to\\nensure that all the samples in the validation fold come from groups that are\\nnot represented at all in the paired training fold.\\nThe following cross-validation splitters can be used to do that.\\nThe grouping identifier for the samples is specified via the groups\\nparameter.\\n3.1.2.3.1. Group k-fold¶\\nGroupKFold is a variation of k-fold which ensures that the same group is\\nnot represented in both testing and training sets. For example if the data is\\nobtained from different subjects with several samples per-subject and if the\\nmodel is flexible enough to learn from highly person specific features it\\ncould fail to generalize to new subjects. GroupKFold makes it possible\\nto detect this kind of overfitting situations.\\nImagine you have three subjects, each with an associated number from 1 to 3:\\nfrom sklearn.model_selection import GroupKFold\\nX = [0.1, 0.2, 2.2, 2.4, 2.3, 4.55, 5.8, 8.8, 9, 10]\\ny = [\"a\", \"b\", \"b\", \"b\", \"c\", \"c\", \"c\", \"d\", \"d\", \"d\"]\\ngroups = [1, 1, 1, 2, 2, 2, 3, 3, 3, 3]\\ngkf = GroupKFold(n_splits=3)\\nfor train, test in gkf.split(X, y, groups=groups):\\n...     print(\"%s %s\" % (train, test))\\n[0 1 2 3 4 5] [6 7 8 9]\\n[0 1 2 6 7 8 9] [3 4 5]\\n[3 4 5 6 7 8 9] [0 1 2]\\nEach subject is in a different testing fold, and the same subject is never in\\nboth testing and training. Notice that the folds do not have exactly the same\\nsize due to the imbalance in the data. If class proportions must be balanced\\nacross folds, StratifiedGroupKFold is a better option.\\nHere is a visualization of the cross-validation behavior.\\nSimilar to KFold, the test sets from GroupKFold will form a\\ncomplete partition of all the data. Unlike KFold, GroupKFold\\nis not randomized at all, whereas KFold is randomized when\\nshuffle=True.\\n3.1.2.3.2. StratifiedGroupKFold¶\\nStratifiedGroupKFold is a cross-validation scheme that combines both\\nStratifiedKFold and GroupKFold. The idea is to try to\\npreserve the distribution of classes in each split while keeping each group\\nwithin a single split. That might be useful when you have an unbalanced\\ndataset so that using just GroupKFold might produce skewed splits.\\nExample:\\nfrom sklearn.model_selection import StratifiedGroupKFold\\nX = list(range(18))\\ny = [1] * 6 + [0] * 12\\ngroups = [1, 2, 3, 3, 4, 4, 1, 1, 2, 2, 3, 4, 5, 5, 5, 6, 6, 6]\\nsgkf = StratifiedGroupKFold(n_splits=3)\\nfor train, test in sgkf.split(X, y, groups=groups):\\n...     print(\"%s %s\" % (train, test))\\n[ 0  2  3  4  5  6  7 10 11 15 16 17] [ 1  8  9 12 13 14]\\n[ 0  1  4  5  6  7  8  9 11 12 13 14] [ 2  3 10 15 16 17]\\n[ 1  2  3  8  9 10 12 13 14 15 16 17] [ 0  4  5  6  7 11]\\nImplementation notes:\\nWith the current implementation full shuffle is not possible in most\\nscenarios. When shuffle=True, the following happens:\\nAll groups are shuffled.\\nGroups are sorted by standard deviation of classes using stable sort.\\nSorted groups are iterated over and assigned to folds.\\nThat means that only groups with the same standard deviation of class\\ndistribution will be shuffled, which might be useful when each group has only\\na single class.\\nThe algorithm greedily assigns each group to one of n_splits test sets,\\nchoosing the test set that minimises the variance in class distribution\\nacross test sets. Group assignment proceeds from groups with highest to\\nlowest variance in class frequency, i.e. large groups peaked on one or few\\nclasses are assigned first.\\nThis split is suboptimal in a sense that it might produce imbalanced splits\\neven if perfect stratification is possible. If you have relatively close\\ndistribution of classes in each group, using GroupKFold is better.\\nHere is a visualization of cross-validation behavior for uneven groups:\\n3.1.2.3.3. Leave One Group Out¶\\nLeaveOneGroupOut is a cross-validation scheme where each split holds\\nout samples belonging to one specific group. Group information is\\nprovided via an array that encodes the group of each sample.\\nEach training set is thus constituted by all the samples except the ones\\nrelated to a specific group. This is the same as LeavePGroupsOut with\\nn_groups=1 and the same as GroupKFold with n_splits equal to the\\nnumber of unique labels passed to the groups parameter.\\nFor example, in the cases of multiple experiments, LeaveOneGroupOut\\ncan be used to create a cross-validation based on the different experiments:\\nwe create a training set using the samples of all the experiments except one:\\nfrom sklearn.model_selection import LeaveOneGroupOut\\nX = [1, 5, 10, 50, 60, 70, 80]\\ny = [0, 1, 1, 2, 2, 2, 2]\\ngroups = [1, 1, 2, 2, 3, 3, 3]\\nlogo = LeaveOneGroupOut()\\nfor train, test in logo.split(X, y, groups=groups):\\n...     print(\"%s %s\" % (train, test))\\n[2 3 4 5 6] [0 1]\\n[0 1 4 5 6] [2 3]\\n[0 1 2 3] [4 5 6]\\nAnother common application is to use time information: for instance the\\ngroups could be the year of collection of the samples and thus allow\\nfor cross-validation against time-based splits.\\n3.1.2.3.4. Leave P Groups Out¶\\nLeavePGroupsOut is similar as LeaveOneGroupOut, but removes\\nsamples related to (P) groups for each training/test set. All possible\\ncombinations of (P) groups are left out, meaning test sets will overlap\\nfor (P>1).\\nExample of Leave-2-Group Out:\\nfrom sklearn.model_selection import LeavePGroupsOut\\nX = np.arange(6)\\ny = [1, 1, 1, 2, 2, 2]\\ngroups = [1, 1, 2, 2, 3, 3]\\nlpgo = LeavePGroupsOut(n_groups=2)\\nfor train, test in lpgo.split(X, y, groups=groups):\\n...     print(\"%s %s\" % (train, test))\\n[4 5] [0 1 2 3]\\n[2 3] [0 1 4 5]\\n[0 1] [2 3 4 5]\\n3.1.2.3.5. Group Shuffle Split¶\\nThe GroupShuffleSplit iterator behaves as a combination of\\nShuffleSplit and LeavePGroupsOut, and generates a\\nsequence of randomized partitions in which a subset of groups are held\\nout for each split. Each train/test split is performed independently meaning\\nthere is no guaranteed relationship between successive test sets.\\nHere is a usage example:\\nfrom sklearn.model_selection import GroupShuffleSplit\\nX = [0.1, 0.2, 2.2, 2.4, 2.3, 4.55, 5.8, 0.001]\\ny = [\"a\", \"b\", \"b\", \"b\", \"c\", \"c\", \"c\", \"a\"]\\ngroups = [1, 1, 2, 2, 3, 3, 4, 4]\\ngss = GroupShuffleSplit(n_splits=4, test_size=0.5, random_state=0)\\nfor train, test in gss.split(X, y, groups=groups):\\n...     print(\"%s %s\" % (train, test))\\n...\\n[0 1 2 3] [4 5 6 7]\\n[2 3 6 7] [0 1 4 5]\\n[2 3 4 5] [0 1 6 7]\\n[4 5 6 7] [0 1 2 3]\\nHere is a visualization of the cross-validation behavior.\\nThis class is useful when the behavior of LeavePGroupsOut is\\ndesired, but the number of groups is large enough that generating all\\npossible partitions with (P) groups withheld would be prohibitively\\nexpensive. In such a scenario, GroupShuffleSplit provides\\na random sample (with replacement) of the train / test splits\\ngenerated by LeavePGroupsOut.\\n3.1.2.4. Predefined fold-splits / Validation-sets¶\\nFor some datasets, a pre-defined split of the data into training- and\\nvalidation fold or into several cross-validation folds already\\nexists. Using PredefinedSplit it is possible to use these folds\\ne.g. when searching for hyperparameters.\\nFor example, when using a validation set, set the test_fold to 0 for all\\nsamples that are part of the validation set, and to -1 for all other samples.\\n3.1.2.5. Using cross-validation iterators to split train and test¶\\nThe above group cross-validation functions may also be useful for splitting a\\ndataset into training and testing subsets. Note that the convenience\\nfunction train_test_split is a wrapper around ShuffleSplit\\nand thus only allows for stratified splitting (using the class labels)\\nand cannot account for groups.\\nTo perform the train and test split, use the indices for the train and test\\nsubsets yielded by the generator output by the split() method of the\\ncross-validation splitter. For example:\\nimport numpy as np\\nfrom sklearn.model_selection import GroupShuffleSplit\\nX = np.array([0.1, 0.2, 2.2, 2.4, 2.3, 4.55, 5.8, 0.001])\\ny = np.array([\"a\", \"b\", \"b\", \"b\", \"c\", \"c\", \"c\", \"a\"])\\ngroups = np.array([1, 1, 2, 2, 3, 3, 4, 4])\\ntrain_indx, test_indx = next(\\n...     GroupShuffleSplit(random_state=7).split(X, y, groups)\\n... )\\nX_train, X_test, y_train, y_test = \\\\\\n...     X[train_indx], X[test_indx], y[train_indx], y[test_indx]\\nX_train.shape, X_test.shape\\n((6,), (2,))\\nnp.unique(groups[train_indx]), np.unique(groups[test_indx])\\n(array([1, 2, 4]), array([3]))\\n3.1.2.6. Cross validation of time series data¶\\nTime series data is characterized by the correlation between observations\\nthat are near in time (autocorrelation). However, classical\\ncross-validation techniques such as KFold and\\nShuffleSplit assume the samples are independent and\\nidentically distributed, and would result in unreasonable correlation\\nbetween training and testing instances (yielding poor estimates of\\ngeneralization error) on time series data. Therefore, it is very important\\nto evaluate our model for time series data on the “future” observations\\nleast like those that are used to train the model. To achieve this, one\\nsolution is provided by TimeSeriesSplit.\\n3.1.2.6.1. Time Series Split¶\\nTimeSeriesSplit is a variation of k-fold which\\nreturns first (k) folds as train set and the ((k+1)) th\\nfold as test set. Note that unlike standard cross-validation methods,\\nsuccessive training sets are supersets of those that come before them.\\nAlso, it adds all surplus data to the first training partition, which\\nis always used to train the model.\\nThis class can be used to cross-validate time series data samples\\nthat are observed at fixed time intervals.\\nExample of 3-split time series cross-validation on a dataset with 6 samples:\\nfrom sklearn.model_selection import TimeSeriesSplit\\nX = np.array([[1, 2], [3, 4], [1, 2], [3, 4], [1, 2], [3, 4]])\\ny = np.array([1, 2, 3, 4, 5, 6])\\ntscv = TimeSeriesSplit(n_splits=3)\\nprint(tscv)\\nTimeSeriesSplit(gap=0, max_train_size=None, n_splits=3, test_size=None)\\nfor train, test in tscv.split(X):\\n...     print(\"%s %s\" % (train, test))\\n[0 1 2] [3]\\n[0 1 2 3] [4]\\n[0 1 2 3 4] [5]\\nHere is a visualization of the cross-validation behavior.\\n- 3.1.3. A note on shuffling¶\\nIf the data ordering is not arbitrary (e.g. samples with the same class label\\nare contiguous), shuffling it first may be essential to get a meaningful cross-\\nvalidation result. However, the opposite may be true if the samples are not\\nindependently and identically distributed. For example, if samples correspond\\nto news articles, and are ordered by their time of publication, then shuffling\\nthe data will likely lead to a model that is overfit and an inflated validation\\nscore: it will be tested on samples that are artificially similar (close in\\ntime) to training samples.\\nSome cross validation iterators, such as KFold, have an inbuilt option\\nto shuffle the data indices before splitting them. Note that:\\nThis consumes less memory than shuffling the data directly.\\nBy default no shuffling occurs, including for the (stratified) K fold cross-\\nvalidation performed by specifying cv=some_integer to\\ncross_val_score, grid search, etc. Keep in mind that\\ntrain_test_split still returns a random split.\\nThe random_state parameter defaults to None, meaning that the\\nshuffling will be different every time KFold(..., shuffle=True) is\\niterated. However, GridSearchCV will use the same shuffling for each set\\nof parameters validated by a single call to its fit method.\\nTo get identical results for each split, set random_state to an integer.\\nFor more details on how to control the randomness of cv splitters and avoid\\ncommon pitfalls, see Controlling randomness.\\n- 3.1.4. Cross validation and model selection¶\\nCross validation iterators can also be used to directly perform model\\nselection using Grid Search for the optimal hyperparameters of the\\nmodel. This is the topic of the next section: Tuning the hyper-parameters of an estimator.\\n- 3.1.5. Permutation test score¶\\npermutation_test_score offers another way\\nto evaluate the performance of classifiers. It provides a permutation-based\\np-value, which represents how likely an observed performance of the\\nclassifier would be obtained by chance. The null hypothesis in this test is\\nthat the classifier fails to leverage any statistical dependency between the\\nfeatures and the labels to make correct predictions on left out data.\\npermutation_test_score generates a null\\ndistribution by calculating n_permutations different permutations of the\\ndata. In each permutation the labels are randomly shuffled, thereby removing\\nany dependency between the features and the labels. The p-value output\\nis the fraction of permutations for which the average cross-validation score\\nobtained by the model is better than the cross-validation score obtained by\\nthe model using the original data. For reliable results n_permutations\\nshould typically be larger than 100 and cv between 3-10 folds.\\nA low p-value provides evidence that the dataset contains real dependency\\nbetween features and labels and the classifier was able to utilize this\\nto obtain good results. A high p-value could be due to a lack of dependency\\nbetween features and labels (there is no difference in feature values between\\nthe classes) or because the classifier was not able to use the dependency in\\nthe data. In the latter case, using a more appropriate classifier that\\nis able to utilize the structure in the data, would result in a lower\\np-value.\\nCross-validation provides information about how well a classifier generalizes,\\nspecifically the range of expected errors of the classifier. However, a\\nclassifier trained on a high dimensional dataset with no structure may still\\nperform better than expected on cross-validation, just by chance.\\nThis can typically happen with small datasets with less than a few hundred\\nsamples.\\npermutation_test_score provides information\\non whether the classifier has found a real class structure and can help in\\nevaluating the performance of the classifier.\\nIt is important to note that this test has been shown to produce low\\np-values even if there is only weak structure in the data because in the\\ncorresponding permutated datasets there is absolutely no structure. This\\ntest is therefore only able to show when the model reliably outperforms\\nrandom guessing.\\nFinally, permutation_test_score is computed\\nusing brute force and internally fits (n_permutations + 1) * n_cv models.\\nIt is therefore only tractable with small datasets for which fitting an\\nindividual model is very fast.\\nExamples\\nTest with permutations the significance of a classification score\\nReferences:\\nOjala and Garriga. Permutation Tests for Studying Classifier Performance.\\nJ. Mach. Learn. Res. 2010.\\n\\n3.2. Tuning the hyper-parameters of an estimator — scikit-learn 1.4.2 documentation\\n\\nInstall\\nUser Guide\\nAPI\\nExamples\\nCommunity\\nGetting Started\\nTutorial\\nWhat\\'s new\\nGlossary\\nDevelopment\\nFAQ\\nSupport\\nRelated packages\\nRoadmap\\nGovernance\\nAbout us\\nGitHub\\nOther Versions and Download\\nMore\\nGetting Started\\nTutorial\\nWhat\\'s new\\nGlossary\\nDevelopment\\nFAQ\\nSupport\\nRelated packages\\nRoadmap\\nGovernance\\nAbout us\\nGitHub\\nOther Versions and Download\\nToggle Menu\\nPrevUp\\nNext\\nscikit-learn 1.4.2\\nOther versions\\nPlease cite us if you use the software.\\n\\n3.2. Tuning the hyper-parameters of an estimator\\n\\n3.2.1. Exhaustive Grid Search\\n\\n3.2.2. Randomized Parameter Optimization\\n\\n3.2.3. Searching for optimal parameters with successive halving\\n3.2.3.1. Choosing min_resources and the number of candidates\\n3.2.3.2. Amount of resource and number of candidates at each iteration\\n3.2.3.3. Choosing a resource\\n3.2.3.4. Exhausting the available resources\\n3.2.3.5. Aggressive elimination of candidates\\n3.2.3.6. Analyzing results with the cv_results_ attribute\\n\\n3.2.4. Tips for parameter search\\n3.2.4.1. Specifying an objective metric\\n3.2.4.2. Specifying multiple metrics for evaluation\\n3.2.4.3. Composite estimators and parameter spaces\\n3.2.4.4. Model selection: development and evaluation\\n3.2.4.5. Parallelism\\n3.2.4.6. Robustness to failure\\n\\n3.2.5. Alternatives to brute force parameter search\\n3.2.5.1. Model specific cross-validation\\n3.2.5.2. Information Criterion\\n3.2.5.3. Out of Bag Estimates\\n\\n3.2. Tuning the hyper-parameters of an estimator¶\\n\\nHyper-parameters are parameters that are not directly learnt within estimators.\\nIn scikit-learn they are passed as arguments to the constructor of the\\nestimator classes. Typical examples include C, kernel and gamma\\nfor Support Vector Classifier, alpha for Lasso, etc.\\nIt is possible and recommended to search the hyper-parameter space for the\\nbest cross validation score.\\nAny parameter provided when constructing an estimator may be optimized in this\\nmanner. Specifically, to find the names and current values for all parameters\\nfor a given estimator, use:\\nestimator.get_params()\\nA search consists of:\\nan estimator (regressor or classifier such as sklearn.svm.SVC());\\na parameter space;\\na method for searching or sampling candidates;\\na cross-validation scheme; and\\na score function.\\nTwo generic approaches to parameter search are provided in\\nscikit-learn: for given values, GridSearchCV exhaustively considers\\nall parameter combinations, while RandomizedSearchCV can sample a\\ngiven number of candidates from a parameter space with a specified\\ndistribution. Both these tools have successive halving counterparts\\nHalvingGridSearchCV and HalvingRandomSearchCV, which can be\\nmuch faster at finding a good parameter combination.\\nAfter describing these tools we detail best practices applicable to these approaches. Some models allow for\\nspecialized, efficient parameter search strategies, outlined in\\nAlternatives to brute force parameter search.\\nNote that it is common that a small subset of those parameters can have a large\\nimpact on the predictive or computation performance of the model while others\\ncan be left to their default values. It is recommended to read the docstring of\\nthe estimator class to get a finer understanding of their expected behavior,\\npossibly by reading the enclosed reference to the literature.\\n- 3.2.1. Exhaustive Grid Search¶\\nThe grid search provided by GridSearchCV exhaustively generates\\ncandidates from a grid of parameter values specified with the param_grid\\nparameter. For instance, the following param_grid:\\nparam_grid = [\\n{\\'C\\': [1, 10, 100, 1000], \\'kernel\\': [\\'linear\\']},\\n{\\'C\\': [1, 10, 100, 1000], \\'gamma\\': [0.001, 0.0001], \\'kernel\\': [\\'rbf\\']},\\n]\\nspecifies that two grids should be explored: one with a linear kernel and\\nC values in [1, 10, 100, 1000], and the second one with an RBF kernel,\\nand the cross-product of C values ranging in [1, 10, 100, 1000] and gamma\\nvalues in [0.001, 0.0001].\\nThe GridSearchCV instance implements the usual estimator API: when\\n“fitting” it on a dataset all the possible combinations of parameter values are\\nevaluated and the best combination is retained.\\nExamples:\\nSee Custom refit strategy of a grid search with cross-validation for an example of\\nGrid Search computation on the digits dataset.\\nSee Sample pipeline for text feature extraction and evaluation for an example\\nof Grid Search coupling parameters from a text documents feature\\nextractor (n-gram count vectorizer and TF-IDF transformer) with a\\nclassifier (here a linear SVM trained with SGD with either elastic\\nnet or L2 penalty) using a Pipeline instance.\\nSee Nested versus non-nested cross-validation\\nfor an example of Grid Search within a cross validation loop on the iris\\ndataset. This is the best practice for evaluating the performance of a\\nmodel with grid search.\\nSee Demonstration of multi-metric evaluation on cross_val_score and GridSearchCV\\nfor an example of GridSearchCV being used to evaluate multiple\\nmetrics simultaneously.\\nSee Balance model complexity and cross-validated score\\nfor an example of using refit=callable interface in\\nGridSearchCV. The example shows how this interface adds certain\\namount of flexibility in identifying the “best” estimator. This interface\\ncan also be used in multiple metrics evaluation.\\nSee Statistical comparison of models using grid search\\nfor an example of how to do a statistical comparison on the outputs of\\nGridSearchCV.\\n- 3.2.2. Randomized Parameter Optimization¶\\nWhile using a grid of parameter settings is currently the most widely used\\nmethod for parameter optimization, other search methods have more\\nfavorable properties.\\nRandomizedSearchCV implements a randomized search over parameters,\\nwhere each setting is sampled from a distribution over possible parameter values.\\nThis has two main benefits over an exhaustive search:\\nA budget can be chosen independent of the number of parameters and possible values.\\nAdding parameters that do not influence the performance does not decrease efficiency.\\nSpecifying how parameters should be sampled is done using a dictionary, very\\nsimilar to specifying parameters for GridSearchCV. Additionally,\\na computation budget, being the number of sampled candidates or sampling\\niterations, is specified using the n_iter parameter.\\nFor each parameter, either a distribution over possible values or a list of\\ndiscrete choices (which will be sampled uniformly) can be specified:\\n{\\'C\\': scipy.stats.expon(scale=100), \\'gamma\\': scipy.stats.expon(scale=.1),\\n\\'kernel\\': [\\'rbf\\'], \\'class_weight\\':[\\'balanced\\', None]}\\nThis example uses the scipy.stats module, which contains many useful\\ndistributions for sampling parameters, such as expon, gamma,\\nuniform, loguniform or randint.\\nIn principle, any function can be passed that provides a rvs (random\\nvariate sample) method to sample a value. A call to the rvs function should\\nprovide independent random samples from possible parameter values on\\nconsecutive calls.\\nWarning\\nThe distributions in scipy.stats prior to version scipy 0.16\\ndo not allow specifying a random state. Instead, they use the global\\nnumpy random state, that can be seeded via np.random.seed or set\\nusing np.random.set_state. However, beginning scikit-learn 0.18,\\nthe sklearn.model_selection module sets the random state provided\\nby the user if scipy >= 0.16 is also available.\\nFor continuous parameters, such as C above, it is important to specify\\na continuous distribution to take full advantage of the randomization. This way,\\nincreasing n_iter will always lead to a finer search.\\nA continuous log-uniform random variable is the continuous version of\\na log-spaced parameter. For example to specify the equivalent of C from above,\\nloguniform(1, 100) can be used instead of [1, 10, 100].\\nMirroring the example above in grid search, we can specify a continuous random\\nvariable that is log-uniformly distributed between 1e0 and 1e3:\\nfrom sklearn.utils.fixes import loguniform\\n{\\'C\\': loguniform(1e0, 1e3),\\n\\'gamma\\': loguniform(1e-4, 1e-3),\\n\\'kernel\\': [\\'rbf\\'],\\n\\'class_weight\\':[\\'balanced\\', None]}\\nExamples:\\nComparing randomized search and grid search for hyperparameter estimation compares the usage and efficiency\\nof randomized search and grid search.\\nReferences:\\nBergstra, J. and Bengio, Y.,\\nRandom search for hyper-parameter optimization,\\nThe Journal of Machine Learning Research (2012)\\n- 3.2.3. Searching for optimal parameters with successive halving¶\\nScikit-learn also provides the HalvingGridSearchCV and\\nHalvingRandomSearchCV estimators that can be used to\\nsearch a parameter space using successive halving [1] [2]. Successive\\nhalving (SH) is like a tournament among candidate parameter combinations.\\nSH is an iterative selection process where all candidates (the\\nparameter combinations) are evaluated with a small amount of resources at\\nthe first iteration. Only some of these candidates are selected for the next\\niteration, which will be allocated more resources. For parameter tuning, the\\nresource is typically the number of training samples, but it can also be an\\narbitrary numeric parameter such as n_estimators in a random forest.\\nAs illustrated in the figure below, only a subset of candidates\\n‘survive’ until the last iteration. These are the candidates that have\\nconsistently ranked among the top-scoring candidates across all iterations.\\nEach iteration is allocated an increasing amount of resources per candidate,\\nhere the number of samples.\\nWe here briefly describe the main parameters, but each parameter and their\\ninteractions are described in more details in the sections below. The\\nfactor (> 1) parameter controls the rate at which the resources grow, and\\nthe rate at which the number of candidates decreases. In each iteration, the\\nnumber of resources per candidate is multiplied by factor and the number\\nof candidates is divided by the same factor. Along with resource and\\nmin_resources, factor is the most important parameter to control the\\nsearch in our implementation, though a value of 3 usually works well.\\nfactor effectively controls the number of iterations in\\nHalvingGridSearchCV and the number of candidates (by default) and\\niterations in HalvingRandomSearchCV. aggressive_elimination=True\\ncan also be used if the number of available resources is small. More control\\nis available through tuning the min_resources parameter.\\nThese estimators are still experimental: their predictions\\nand their API might change without any deprecation cycle. To use them, you\\nneed to explicitly import enable_halving_search_cv:\\n\\nexplicitly require this experimental feature\\n\\nfrom sklearn.experimental import enable_halving_search_cv  # noqa\\n\\nnow you can import normally from model_selection\\n\\nfrom sklearn.model_selection import HalvingGridSearchCV\\nfrom sklearn.model_selection import HalvingRandomSearchCV\\nExamples:\\nComparison between grid search and successive halving\\nSuccessive Halving Iterations\\n3.2.3.1. Choosing min_resources and the number of candidates¶\\nBeside factor, the two main parameters that influence the behaviour of a\\nsuccessive halving search are the min_resources parameter, and the\\nnumber of candidates (or parameter combinations) that are evaluated.\\nmin_resources is the amount of resources allocated at the first\\niteration for each candidate. The number of candidates is specified directly\\nin HalvingRandomSearchCV, and is determined from the param_grid\\nparameter of HalvingGridSearchCV.\\nConsider a case where the resource is the number of samples, and where we\\nhave 1000 samples. In theory, with min_resources=10 and factor=2, we\\nare able to run at most 7 iterations with the following number of\\nsamples: [10, 20, 40, 80, 160, 320, 640].\\nBut depending on the number of candidates, we might run less than 7\\niterations: if we start with a small number of candidates, the last\\niteration might use less than 640 samples, which means not using all the\\navailable resources (samples). For example if we start with 5 candidates, we\\nonly need 2 iterations: 5 candidates for the first iteration, then\\n5 // 2 = 2 candidates at the second iteration, after which we know which\\ncandidate performs the best (so we don’t need a third one). We would only be\\nusing at most 20 samples which is a waste since we have 1000 samples at our\\ndisposal. On the other hand, if we start with a high number of\\ncandidates, we might end up with a lot of candidates at the last iteration,\\nwhich may not always be ideal: it means that many candidates will run with\\nthe full resources, basically reducing the procedure to standard search.\\nIn the case of HalvingRandomSearchCV, the number of candidates is set\\nby default such that the last iteration uses as much of the available\\nresources as possible. For HalvingGridSearchCV, the number of\\ncandidates is determined by the param_grid parameter. Changing the value of\\nmin_resources will impact the number of possible iterations, and as a\\nresult will also have an effect on the ideal number of candidates.\\nAnother consideration when choosing min_resources is whether or not it\\nis easy to discriminate between good and bad candidates with a small amount\\nof resources. For example, if you need a lot of samples to distinguish\\nbetween good and bad parameters, a high min_resources is recommended. On\\nthe other hand if the distinction is clear even with a small amount of\\nsamples, then a small min_resources may be preferable since it would\\nspeed up the computation.\\nNotice in the example above that the last iteration does not use the maximum\\namount of resources available: 1000 samples are available, yet only 640 are\\nused, at most. By default, both HalvingRandomSearchCV and\\nHalvingGridSearchCV try to use as many resources as possible in the\\nlast iteration, with the constraint that this amount of resources must be a\\nmultiple of both min_resources and factor (this constraint will be clear\\nin the next section). HalvingRandomSearchCV achieves this by\\nsampling the right amount of candidates, while HalvingGridSearchCV\\nachieves this by properly setting min_resources. Please see\\nExhausting the available resources for details.\\n3.2.3.2. Amount of resource and number of candidates at each iteration¶\\nAt any iteration i, each candidate is allocated a given amount of resources\\nwhich we denote n_resources_i. This quantity is controlled by the\\nparameters factor and min_resources as follows (factor is strictly\\ngreater than 1):\\nn_resources_i = factor**i * min_resources,\\nor equivalently:\\nn_resources_{i+1} = n_resources_i * factor\\nwhere min_resources == n_resources_0 is the amount of resources used at\\nthe first iteration. factor also defines the proportions of candidates\\nthat will be selected for the next iteration:\\nn_candidates_i = n_candidates // (factor ** i)\\nor equivalently:\\nn_candidates_0 = n_candidates\\nn_candidates_{i+1} = n_candidates_i // factor\\nSo in the first iteration, we use min_resources resources\\nn_candidates times. In the second iteration, we use min_resources *\\nfactor resources n_candidates // factor times. The third again\\nmultiplies the resources per candidate and divides the number of candidates.\\nThis process stops when the maximum amount of resource per candidate is\\nreached, or when we have identified the best candidate. The best candidate\\nis identified at the iteration that is evaluating factor or less candidates\\n(see just below for an explanation).\\nHere is an example with min_resources=3 and factor=2, starting with\\n70 candidates:\\nn_resources_i\\nn_candidates_i\\n3 (=min_resources)\\n70 (=n_candidates)\\n3 * 2 = 6\\n70 // 2 = 35\\n6 * 2 = 12\\n35 // 2 = 17\\n12 * 2 = 24\\n17 // 2 = 8\\n24 * 2 = 48\\n8 // 2 = 4\\n48 * 2 = 96\\n4 // 2 = 2\\nWe can note that:\\nthe process stops at the first iteration which evaluates factor=2\\ncandidates: the best candidate is the best out of these 2 candidates. It\\nis not necessary to run an additional iteration, since it would only\\nevaluate one candidate (namely the best one, which we have already\\nidentified). For this reason, in general, we want the last iteration to\\nrun at most factor candidates. If the last iteration evaluates more\\nthan factor candidates, then this last iteration reduces to a regular\\nsearch (as in RandomizedSearchCV or GridSearchCV).\\neach n_resources_i is a multiple of both factor and\\nmin_resources (which is confirmed by its definition above).\\nThe amount of resources that is used at each iteration can be found in the\\nn_resources_ attribute.\\n3.2.3.3. Choosing a resource¶\\nBy default, the resource is defined in terms of number of samples. That is,\\neach iteration will use an increasing amount of samples to train on. You can\\nhowever manually specify a parameter to use as the resource with the\\nresource parameter. Here is an example where the resource is defined in\\nterms of the number of estimators of a random forest:\\nfrom sklearn.datasets import make_classification\\nfrom sklearn.ensemble import RandomForestClassifier\\nfrom sklearn.experimental import enable_halving_search_cv  # noqa\\nfrom sklearn.model_selection import HalvingGridSearchCV\\nimport pandas as pd\\n\\nparam_grid = {\\'max_depth\\': [3, 5, 10],\\n...               \\'min_samples_split\\': [2, 5, 10]}\\nbase_estimator = RandomForestClassifier(random_state=0)\\nX, y = make_classification(n_samples=1000, random_state=0)\\nsh = HalvingGridSearchCV(base_estimator, param_grid, cv=5,\\n...                          factor=2, resource=\\'n_estimators\\',\\n...                          max_resources=30).fit(X, y)\\nsh.best_estimator_\\nRandomForestClassifier(max_depth=5, n_estimators=24, random_state=0)\\nNote that it is not possible to budget on a parameter that is part of the\\nparameter grid.\\n3.2.3.4. Exhausting the available resources¶\\nAs mentioned above, the number of resources that is used at each iteration\\ndepends on the min_resources parameter.\\nIf you have a lot of resources available but start with a low number of\\nresources, some of them might be wasted (i.e. not used):\\nfrom sklearn.datasets import make_classification\\nfrom sklearn.svm import SVC\\nfrom sklearn.experimental import enable_halving_search_cv  # noqa\\nfrom sklearn.model_selection import HalvingGridSearchCV\\nimport pandas as pd\\nparam_grid= {\\'kernel\\': (\\'linear\\', \\'rbf\\'),\\n...              \\'C\\': [1, 10, 100]}\\nbase_estimator = SVC(gamma=\\'scale\\')\\nX, y = make_classification(n_samples=1000)\\nsh = HalvingGridSearchCV(base_estimator, param_grid, cv=5,\\n...                          factor=2, min_resources=20).fit(X, y)\\nsh.n_resources_\\n[20, 40, 80]\\nThe search process will only use 80 resources at most, while our maximum\\namount of available resources is n_samples=1000. Here, we have\\nmin_resources = r_0 = 20.\\nFor HalvingGridSearchCV, by default, the min_resources parameter\\nis set to ‘exhaust’. This means that min_resources is automatically set\\nsuch that the last iteration can use as many resources as possible, within\\nthe max_resources limit:\\nsh = HalvingGridSearchCV(base_estimator, param_grid, cv=5,\\n...                          factor=2, min_resources=\\'exhaust\\').fit(X, y)\\nsh.n_resources_\\n[250, 500, 1000]\\nmin_resources was here automatically set to 250, which results in the last\\niteration using all the resources. The exact value that is used depends on\\nthe number of candidate parameter, on max_resources and on factor.\\nFor HalvingRandomSearchCV, exhausting the resources can be done in 2\\nways:\\nby setting min_resources=\\'exhaust\\', just like for\\nHalvingGridSearchCV;\\nby setting n_candidates=\\'exhaust\\'.\\nBoth options are mutually exclusive: using min_resources=\\'exhaust\\' requires\\nknowing the number of candidates, and symmetrically n_candidates=\\'exhaust\\'\\nrequires knowing min_resources.\\nIn general, exhausting the total number of resources leads to a better final\\ncandidate parameter, and is slightly more time-intensive.\\n3.2.3.5. Aggressive elimination of candidates¶\\nIdeally, we want the last iteration to evaluate factor candidates (see\\nAmount of resource and number of candidates at each iteration). We then just have to\\npick the best one. When the number of available resources is small with\\nrespect to the number of candidates, the last iteration may have to evaluate\\nmore than factor candidates:\\nfrom sklearn.datasets import make_classification\\nfrom sklearn.svm import SVC\\nfrom sklearn.experimental import enable_halving_search_cv  # noqa\\nfrom sklearn.model_selection import HalvingGridSearchCV\\nimport pandas as pd\\n\\nparam_grid = {\\'kernel\\': (\\'linear\\', \\'rbf\\'),\\n...               \\'C\\': [1, 10, 100]}\\nbase_estimator = SVC(gamma=\\'scale\\')\\nX, y = make_classification(n_samples=1000)\\nsh = HalvingGridSearchCV(base_estimator, param_grid, cv=5,\\n...                          factor=2, max_resources=40,\\n...                          aggressive_elimination=False).fit(X, y)\\nsh.n_resources_\\n[20, 40]\\nsh.n_candidates_\\n[6, 3]\\nSince we cannot use more than max_resources=40 resources, the process\\nhas to stop at the second iteration which evaluates more than factor=2\\ncandidates.\\nUsing the aggressive_elimination parameter, you can force the search\\nprocess to end up with less than factor candidates at the last\\niteration. To do this, the process will eliminate as many candidates as\\nnecessary using min_resources resources:\\nsh = HalvingGridSearchCV(base_estimator, param_grid, cv=5,\\n...                            factor=2,\\n...                            max_resources=40,\\n...                            aggressive_elimination=True,\\n...                            ).fit(X, y)\\nsh.n_resources_\\n[20, 20,  40]\\nsh.n_candidates_\\n[6, 3, 2]\\nNotice that we end with 2 candidates at the last iteration since we have\\neliminated enough candidates during the first iterations, using n_resources =\\nmin_resources = 20.\\n3.2.3.6. Analyzing results with the cv_results_ attribute¶\\nThe cv_results_ attribute contains useful information for analyzing the\\nresults of a search. It can be converted to a pandas dataframe with df =\\npd.DataFrame(est.cv_results_). The cv_results_ attribute of\\nHalvingGridSearchCV and HalvingRandomSearchCV is similar\\nto that of GridSearchCV and RandomizedSearchCV, with\\nadditional information related to the successive halving process.\\nHere is an example with some of the columns of a (truncated) dataframe:\\niter\\nn_resources\\nmean_test_score\\nparams\\n0\\n0\\n125\\n0.983667\\n{‘criterion’: ‘log_loss’, ‘max_depth’: None, ‘max_features’: 9, ‘min_samples_split’: 5}\\n1\\n0\\n125\\n0.983667\\n{‘criterion’: ‘gini’, ‘max_depth’: None, ‘max_features’: 8, ‘min_samples_split’: 7}\\n2\\n0\\n125\\n0.983667\\n{‘criterion’: ‘gini’, ‘max_depth’: None, ‘max_features’: 10, ‘min_samples_split’: 10}\\n3\\n0\\n125\\n0.983667\\n{‘criterion’: ‘log_loss’, ‘max_depth’: None, ‘max_features’: 6, ‘min_samples_split’: 6}\\n…\\n…\\n…\\n…\\n…\\n15\\n2\\n500\\n0.951958\\n{‘criterion’: ‘log_loss’, ‘max_depth’: None, ‘max_features’: 9, ‘min_samples_split’: 10}\\n16\\n2\\n500\\n0.947958\\n{‘criterion’: ‘gini’, ‘max_depth’: None, ‘max_features’: 10, ‘min_samples_split’: 10}\\n17\\n2\\n500\\n0.951958\\n{‘criterion’: ‘gini’, ‘max_depth’: None, ‘max_features’: 10, ‘min_samples_split’: 4}\\n18\\n3\\n1000\\n0.961009\\n{‘criterion’: ‘log_loss’, ‘max_depth’: None, ‘max_features’: 9, ‘min_samples_split’: 10}\\n19\\n3\\n1000\\n0.955989\\n{‘criterion’: ‘gini’, ‘max_depth’: None, ‘max_features’: 10, ‘min_samples_split’: 4}\\nEach row corresponds to a given parameter combination (a candidate) and a given\\niteration. The iteration is given by the iter column. The n_resources\\ncolumn tells you how many resources were used.\\nIn the example above, the best parameter combination is {\\'criterion\\':\\n\\'log_loss\\', \\'max_depth\\': None, \\'max_features\\': 9, \\'min_samples_split\\': 10}\\nsince it has reached the last iteration (3) with the highest score:\\n0.96.\\nReferences:\\n[1]\\nK. Jamieson, A. Talwalkar,\\nNon-stochastic Best Arm Identification and Hyperparameter\\nOptimization, in\\nproc. of Machine Learning Research, 2016.\\n[2]\\nL. Li, K. Jamieson, G. DeSalvo, A. Rostamizadeh, A. Talwalkar,\\nHyperband: A Novel Bandit-Based Approach to Hyperparameter Optimization, in Machine Learning Research 18, 2018.\\n- 3.2.4. Tips for parameter search¶\\n3.2.4.1. Specifying an objective metric¶\\nBy default, parameter search uses the score function of the estimator\\nto evaluate a parameter setting. These are the\\nsklearn.metrics.accuracy_score for classification and\\nsklearn.metrics.r2_score for regression.  For some applications,\\nother scoring functions are better suited (for example in unbalanced\\nclassification, the accuracy score is often uninformative). An alternative\\nscoring function can be specified via the scoring parameter of most\\nparameter search tools. See The scoring parameter: defining model evaluation rules for more details.\\n3.2.4.2. Specifying multiple metrics for evaluation¶\\nGridSearchCV and RandomizedSearchCV allow specifying\\nmultiple metrics for the scoring parameter.\\nMultimetric scoring can either be specified as a list of strings of predefined\\nscores names or a dict mapping the scorer name to the scorer function and/or\\nthe predefined scorer name(s). See Using multiple metric evaluation for more details.\\nWhen specifying multiple metrics, the refit parameter must be set to the\\nmetric (string) for which the best_params_ will be found and used to build\\nthe best_estimator_ on the whole dataset. If the search should not be\\nrefit, set refit=False. Leaving refit to the default value None will\\nresult in an error when using multiple metrics.\\nSee Demonstration of multi-metric evaluation on cross_val_score and GridSearchCV\\nfor an example usage.\\nHalvingRandomSearchCV and HalvingGridSearchCV do not support\\nmultimetric scoring.\\n3.2.4.3. Composite estimators and parameter spaces¶\\nGridSearchCV and RandomizedSearchCV allow searching over\\nparameters of composite or nested estimators such as\\nPipeline,\\nColumnTransformer,\\nVotingClassifier or\\nCalibratedClassifierCV using a dedicated\\n__ syntax:\\nfrom sklearn.model_selection import GridSearchCV\\nfrom sklearn.calibration import CalibratedClassifierCV\\nfrom sklearn.ensemble import RandomForestClassifier\\nfrom sklearn.datasets import make_moons\\nX, y = make_moons()\\ncalibrated_forest = CalibratedClassifierCV(\\n...    estimator=RandomForestClassifier(n_estimators=10))\\nparam_grid = {\\n...    \\'estimator__max_depth\\': [2, 4, 6, 8]}\\nsearch = GridSearchCV(calibrated_forest, param_grid, cv=5)\\nsearch.fit(X, y)\\nGridSearchCV(cv=5,\\nestimator=CalibratedClassifierCV(...),\\nparam_grid={\\'estimator__max_depth\\': [2, 4, 6, 8]})\\nHere,  is the parameter name of the nested estimator,\\nin this case estimator.\\nIf the meta-estimator is constructed as a collection of estimators as in\\npipeline.Pipeline, then  refers to the name of the estimator,\\nsee Access to nested parameters. In practice, there can be several\\nlevels of nesting:\\nfrom sklearn.pipeline import Pipeline\\nfrom sklearn.feature_selection import SelectKBest\\npipe = Pipeline([\\n...    (\\'select\\', SelectKBest()),\\n...    (\\'model\\', calibrated_forest)])\\nparam_grid = {\\n...    \\'select__k\\': [1, 2],\\n...    \\'model__estimator__max_depth\\': [2, 4, 6, 8]}\\nsearch = GridSearchCV(pipe, param_grid, cv=5).fit(X, y)\\nPlease refer to Pipeline: chaining estimators for performing parameter searches over\\npipelines.\\n3.2.4.4. Model selection: development and evaluation¶\\nModel selection by evaluating various parameter settings can be seen as a way\\nto use the labeled data to “train” the parameters of the grid.\\nWhen evaluating the resulting model it is important to do it on\\nheld-out samples that were not seen during the grid search process:\\nit is recommended to split the data into a development set (to\\nbe fed to the GridSearchCV instance) and an evaluation set\\nto compute performance metrics.\\nThis can be done by using the train_test_split\\nutility function.\\n3.2.4.5. Parallelism¶\\nThe parameter search tools evaluate each parameter combination on each data\\nfold independently. Computations can be run in parallel by using the keyword\\nn_jobs=-1. See function signature for more details, and also the Glossary\\nentry for n_jobs.\\n3.2.4.6. Robustness to failure¶\\nSome parameter settings may result in a failure to fit one or more folds\\nof the data.  By default, this will cause the entire search to fail, even if\\nsome parameter settings could be fully evaluated. Setting error_score=0\\n(or =np.nan) will make the procedure robust to such failure, issuing a\\nwarning and setting the score for that fold to 0 (or nan), but completing\\nthe search.\\n- 3.2.5. Alternatives to brute force parameter search¶\\n3.2.5.1. Model specific cross-validation¶\\nSome models can fit data for a range of values of some parameter almost\\nas efficiently as fitting the estimator for a single value of the\\nparameter. This feature can be leveraged to perform a more efficient\\ncross-validation used for model selection of this parameter.\\nThe most common parameter amenable to this strategy is the parameter\\nencoding the strength of the regularizer. In this case we say that we\\ncompute the regularization path of the estimator.\\nHere is the list of such models:\\nlinear_model.ElasticNetCV([, l1_ratio, ...])\\nElastic Net model with iterative fitting along a regularization path.\\nlinear_model.LarsCV([, fit_intercept, ...])\\nCross-validated Least Angle Regression model.\\nlinear_model.LassoCV([, eps, n_alphas, ...])\\nLasso linear model with iterative fitting along a regularization path.\\nlinear_model.LassoLarsCV([, fit_intercept, ...])\\nCross-validated Lasso, using the LARS algorithm.\\nlinear_model.LogisticRegressionCV([, Cs, ...])\\nLogistic Regression CV (aka logit, MaxEnt) classifier.\\nlinear_model.MultiTaskElasticNetCV([, ...])\\nMulti-task L1/L2 ElasticNet with built-in cross-validation.\\nlinear_model.MultiTaskLassoCV([, eps, ...])\\nMulti-task Lasso model trained with L1/L2 mixed-norm as regularizer.\\nlinear_model.OrthogonalMatchingPursuitCV()\\nCross-validated Orthogonal Matching Pursuit model (OMP).\\nlinear_model.RidgeCV([alphas, ...])\\nRidge regression with built-in cross-validation.\\nlinear_model.RidgeClassifierCV([alphas, ...])\\nRidge classifier with built-in cross-validation.\\n3.2.5.2. Information Criterion¶\\nSome models can offer an information-theoretic closed-form formula of the\\noptimal estimate of the regularization parameter by computing a single\\nregularization path (instead of several when using cross-validation).\\nHere is the list of models benefiting from the Akaike Information\\nCriterion (AIC) or the Bayesian Information Criterion (BIC) for automated\\nmodel selection:\\nlinear_model.LassoLarsIC([criterion, ...])\\nLasso model fit with Lars using BIC or AIC for model selection.\\n3.2.5.3. Out of Bag Estimates¶\\nWhen using ensemble methods base upon bagging, i.e. generating new\\ntraining sets using sampling with replacement, part of the training set\\nremains unused.  For each classifier in the ensemble, a different part\\nof the training set is left out.\\nThis left out portion can be used to estimate the generalization error\\nwithout having to rely on a separate validation set.  This estimate\\ncomes “for free” as no additional data is needed and can be used for\\nmodel selection.\\nThis is currently implemented in the following classes:\\nensemble.RandomForestClassifier([...])\\nA random forest classifier.\\nensemble.RandomForestRegressor([...])\\nA random forest regressor.\\nensemble.ExtraTreesClassifier([...])\\nAn extra-trees classifier.\\nensemble.ExtraTreesRegressor([n_estimators, ...])\\nAn extra-trees regressor.\\nensemble.GradientBoostingClassifier([, ...])\\nGradient Boosting for classification.\\nensemble.GradientBoostingRegressor([, ...])\\nGradient Boosting for regression.\\n\\n3.3. Metrics and scoring: quantifying the quality of predictions — scikit-learn 1.4.2 documentation\\n\\nInstall\\nUser Guide\\nAPI\\nExamples\\nCommunity\\nGetting Started\\nTutorial\\nWhat\\'s new\\nGlossary\\nDevelopment\\nFAQ\\nSupport\\nRelated packages\\nRoadmap\\nGovernance\\nAbout us\\nGitHub\\nOther Versions and Download\\nMore\\nGetting Started\\nTutorial\\nWhat\\'s new\\nGlossary\\nDevelopment\\nFAQ\\nSupport\\nRelated packages\\nRoadmap\\nGovernance\\nAbout us\\nGitHub\\nOther Versions and Download\\nToggle Menu\\nPrevUp\\nNext\\nscikit-learn 1.4.2\\nOther versions\\nPlease cite us if you use the software.\\n\\n3.3. Metrics and scoring: quantifying the quality of predictions\\n\\n3.3.1. The scoring parameter: defining model evaluation rules\\n3.3.1.1. Common cases: predefined values\\n3.3.1.2. Defining your scoring strategy from metric functions\\n3.3.1.3. Implementing your own scoring object\\n3.3.1.4. Using multiple metric evaluation\\n\\n3.3.2. Classification metrics\\n3.3.2.1. From binary to multiclass and multilabel\\n3.3.2.2. Accuracy score\\n3.3.2.3. Top-k accuracy score\\n3.3.2.4. Balanced accuracy score\\n3.3.2.5. Cohen’s kappa\\n3.3.2.6. Confusion matrix\\n3.3.2.7. Classification report\\n3.3.2.8. Hamming loss\\n3.3.2.9. Precision, recall and F-measures\\n3.3.2.9.1. Binary classification\\n3.3.2.9.2. Multiclass and multilabel classification\\n3.3.2.10. Jaccard similarity coefficient score\\n3.3.2.11. Hinge loss\\n3.3.2.12. Log loss\\n3.3.2.13. Matthews correlation coefficient\\n3.3.2.14. Multi-label confusion matrix\\n3.3.2.15. Receiver operating characteristic (ROC)\\n3.3.2.15.1. Binary case\\n3.3.2.15.2. Multi-class case\\n3.3.2.15.3. Multi-label case\\n3.3.2.16. Detection error tradeoff (DET)\\n3.3.2.17. Zero one loss\\n3.3.2.18. Brier score loss\\n3.3.2.19. Class likelihood ratios\\n\\n3.3.3. Multilabel ranking metrics\\n3.3.3.1. Coverage error\\n3.3.3.2. Label ranking average precision\\n3.3.3.3. Ranking loss\\n3.3.3.4. Normalized Discounted Cumulative Gain\\n\\n3.3.4. Regression metrics\\n3.3.4.1. R² score, the coefficient of determination\\n3.3.4.2. Mean absolute error\\n3.3.4.3. Mean squared error\\n3.3.4.4. Mean squared logarithmic error\\n3.3.4.5. Mean absolute percentage error\\n3.3.4.6. Median absolute error\\n3.3.4.7. Max error\\n3.3.4.8. Explained variance score\\n3.3.4.9. Mean Poisson, Gamma, and Tweedie deviances\\n3.3.4.10. Pinball loss\\n3.3.4.11. D² score\\n3.3.4.11.1. D² Tweedie score\\n3.3.4.11.2. D² pinball score\\n3.3.4.11.3. D² absolute error score\\n3.3.4.12. Visual evaluation of regression models\\n\\n3.3.5. Clustering metrics\\n\\n3.3.6. Dummy estimators\\n\\n3.3. Metrics and scoring: quantifying the quality of predictions¶\\n\\nThere are 3 different APIs for evaluating the quality of a model’s\\npredictions:\\nEstimator score method: Estimators have a score method providing a\\ndefault evaluation criterion for the problem they are designed to solve.\\nThis is not discussed on this page, but in each estimator’s documentation.\\nScoring parameter: Model-evaluation tools using\\ncross-validation (such as\\nmodel_selection.cross_val_score and\\nmodel_selection.GridSearchCV) rely on an internal scoring strategy.\\nThis is discussed in the section The scoring parameter: defining model evaluation rules.\\nMetric functions: The sklearn.metrics module implements functions\\nassessing prediction error for specific purposes. These metrics are detailed\\nin sections on Classification metrics,\\nMultilabel ranking metrics, Regression metrics and\\nClustering metrics.\\nFinally, Dummy estimators are useful to get a baseline\\nvalue of those metrics for random predictions.\\nSee also\\nFor “pairwise” metrics, between samples and not estimators or\\npredictions, see the Pairwise metrics, Affinities and Kernels section.\\n- 3.3.1. The scoring parameter: defining model evaluation rules¶\\nModel selection and evaluation using tools, such as\\nmodel_selection.GridSearchCV and\\nmodel_selection.cross_val_score, take a scoring parameter that\\ncontrols what metric they apply to the estimators evaluated.\\n3.3.1.1. Common cases: predefined values¶\\nFor the most common use cases, you can designate a scorer object with the\\nscoring parameter; the table below shows all possible values.\\nAll scorer objects follow the convention that higher return values are better\\nthan lower return values.  Thus metrics which measure the distance between\\nthe model and the data, like metrics.mean_squared_error, are\\navailable as neg_mean_squared_error which return the negated value\\nof the metric.\\nScoring\\nFunction\\nComment\\nClassification\\n‘accuracy’\\nmetrics.accuracy_score\\n‘balanced_accuracy’\\nmetrics.balanced_accuracy_score\\n‘top_k_accuracy’\\nmetrics.top_k_accuracy_score\\n‘average_precision’\\nmetrics.average_precision_score\\n‘neg_brier_score’\\nmetrics.brier_score_loss\\n‘f1’\\nmetrics.f1_score\\nfor binary targets\\n‘f1_micro’\\nmetrics.f1_score\\nmicro-averaged\\n‘f1_macro’\\nmetrics.f1_score\\nmacro-averaged\\n‘f1_weighted’\\nmetrics.f1_score\\nweighted average\\n‘f1_samples’\\nmetrics.f1_score\\nby multilabel sample\\n‘neg_log_loss’\\nmetrics.log_loss\\nrequires predict_proba support\\n‘precision’ etc.\\nmetrics.precision_score\\nsuffixes apply as with ‘f1’\\n‘recall’ etc.\\nmetrics.recall_score\\nsuffixes apply as with ‘f1’\\n‘jaccard’ etc.\\nmetrics.jaccard_score\\nsuffixes apply as with ‘f1’\\n‘roc_auc’\\nmetrics.roc_auc_score\\n‘roc_auc_ovr’\\nmetrics.roc_auc_score\\n‘roc_auc_ovo’\\nmetrics.roc_auc_score\\n‘roc_auc_ovr_weighted’\\nmetrics.roc_auc_score\\n‘roc_auc_ovo_weighted’\\nmetrics.roc_auc_score\\nClustering\\n‘adjusted_mutual_info_score’\\nmetrics.adjusted_mutual_info_score\\n‘adjusted_rand_score’\\nmetrics.adjusted_rand_score\\n‘completeness_score’\\nmetrics.completeness_score\\n‘fowlkes_mallows_score’\\nmetrics.fowlkes_mallows_score\\n‘homogeneity_score’\\nmetrics.homogeneity_score\\n‘mutual_info_score’\\nmetrics.mutual_info_score\\n‘normalized_mutual_info_score’\\nmetrics.normalized_mutual_info_score\\n‘rand_score’\\nmetrics.rand_score\\n‘v_measure_score’\\nmetrics.v_measure_score\\nRegression\\n‘explained_variance’\\nmetrics.explained_variance_score\\n‘max_error’\\nmetrics.max_error\\n‘neg_mean_absolute_error’\\nmetrics.mean_absolute_error\\n‘neg_mean_squared_error’\\nmetrics.mean_squared_error\\n‘neg_root_mean_squared_error’\\nmetrics.root_mean_squared_error\\n‘neg_mean_squared_log_error’\\nmetrics.mean_squared_log_error\\n‘neg_root_mean_squared_log_error’\\nmetrics.root_mean_squared_log_error\\n‘neg_median_absolute_error’\\nmetrics.median_absolute_error\\n‘r2’\\nmetrics.r2_score\\n‘neg_mean_poisson_deviance’\\nmetrics.mean_poisson_deviance\\n‘neg_mean_gamma_deviance’\\nmetrics.mean_gamma_deviance\\n‘neg_mean_absolute_percentage_error’\\nmetrics.mean_absolute_percentage_error\\n‘d2_absolute_error_score’\\nmetrics.d2_absolute_error_score\\n‘d2_pinball_score’\\nmetrics.d2_pinball_score\\n‘d2_tweedie_score’\\nmetrics.d2_tweedie_score\\nUsage examples:\\n\\nfrom sklearn import svm, datasets\\nfrom sklearn.model_selection import cross_val_score\\nX, y = datasets.load_iris(return_X_y=True)\\nclf = svm.SVC(random_state=0)\\ncross_val_score(clf, X, y, cv=5, scoring=\\'recall_macro\\')\\narray([0.96..., 0.96..., 0.96..., 0.93..., 1.        ])\\nNote\\nIf a wrong scoring name is passed, an InvalidParameterError is raised.\\nYou can retrieve the names of all available scorers by calling\\nget_scorer_names.\\n3.3.1.2. Defining your scoring strategy from metric functions¶\\nThe module sklearn.metrics also exposes a set of simple functions\\nmeasuring a prediction error given ground truth and prediction:\\nfunctions ending with _score return a value to\\nmaximize, the higher the better.\\nfunctions ending with _error or _loss return a\\nvalue to minimize, the lower the better.  When converting\\ninto a scorer object using make_scorer, set\\nthe greater_is_better parameter to False (True by default; see the\\nparameter description below).\\nMetrics available for various machine learning tasks are detailed in sections\\nbelow.\\nMany metrics are not given names to be used as scoring values,\\nsometimes because they require additional parameters, such as\\nfbeta_score. In such cases, you need to generate an appropriate\\nscoring object.  The simplest way to generate a callable object for scoring\\nis by using make_scorer. That function converts metrics\\ninto callables that can be used for model evaluation.\\nOne typical use case is to wrap an existing metric function from the library\\nwith non-default values for its parameters, such as the beta parameter for\\nthe fbeta_score function:\\nfrom sklearn.metrics import fbeta_score, make_scorer\\nftwo_scorer = make_scorer(fbeta_score, beta=2)\\nfrom sklearn.model_selection import GridSearchCV\\nfrom sklearn.svm import LinearSVC\\ngrid = GridSearchCV(LinearSVC(dual=\"auto\"), param_grid={\\'C\\': [1, 10]},\\n...                     scoring=ftwo_scorer, cv=5)\\nCustom scorer objects\\nClick for more details\\n¶\\nThe second use case is to build a completely custom scorer object\\nfrom a simple python function using make_scorer, which can\\ntake several parameters:\\nthe python function you want to use (my_custom_loss_func\\nin the example below)\\nwhether the python function returns a score (greater_is_better=True,\\nthe default) or a loss (greater_is_better=False).  If a loss, the output\\nof the python function is negated by the scorer object, conforming to\\nthe cross validation convention that scorers return higher values for better models.\\nfor classification metrics only: whether the python function you provided requires\\ncontinuous decision certainties. If the scoring function only accepts probability\\nestimates (e.g. metrics.log_loss) then one needs to set the parameter\\nresponse_method, thus in this case response_method=\"predict_proba\". Some scoring\\nfunction do not necessarily require probability estimates but rather non-thresholded\\ndecision values (e.g. metrics.roc_auc_score). In this case, one provides a\\nlist such as response_method=[\"decision_function\", \"predict_proba\"]. In this case,\\nthe scorer will use the first available method, in the order given in the list,\\nto compute the scores.\\nany additional parameters, such as beta or labels in f1_score.\\nHere is an example of building custom scorers, and of using the\\ngreater_is_better parameter:\\nimport numpy as np\\ndef my_custom_loss_func(y_true, y_pred):\\n...     diff = np.abs(y_true - y_pred).max()\\n...     return np.log1p(diff)\\n...\\n\\nscore will negate the return value of my_custom_loss_func,\\n\\nwhich will be np.log(2), 0.693, given the values for X\\n\\nand y defined below.\\n\\nscore = make_scorer(my_custom_loss_func, greater_is_better=False)\\nX = [[1], [1]]\\ny = [0, 1]\\nfrom sklearn.dummy import DummyClassifier\\nclf = DummyClassifier(strategy=\\'most_frequent\\', random_state=0)\\nclf = clf.fit(X, y)\\nmy_custom_loss_func(y, clf.predict(X))\\n0.69...\\nscore(clf, X, y)\\n-0.69...\\n3.3.1.3. Implementing your own scoring object¶\\nYou can generate even more flexible model scorers by constructing your own\\nscoring object from scratch, without using the make_scorer factory.\\nHow to build a scorer from scratch\\nClick for more details\\n¶\\nFor a callable to be a scorer, it needs to meet the protocol specified by\\nthe following two rules:\\nIt can be called with parameters (estimator, X, y), where estimator\\nis the model that should be evaluated, X is validation data, and y is\\nthe ground truth target for X (in the supervised case) or None (in the\\nunsupervised case).\\nIt returns a floating point number that quantifies the\\nestimator prediction quality on X, with reference to y.\\nAgain, by convention higher numbers are better, so if your scorer\\nreturns loss, that value should be negated.\\nAdvanced: If it requires extra metadata to be passed to it, it should expose\\na get_metadata_routing method returning the requested metadata. The user\\nshould be able to set the requested metadata via a set_score_request\\nmethod. Please see User Guide and Developer\\nGuide for\\nmore details.\\nNote\\nUsing custom scorers in functions where n_jobs > 1\\nWhile defining the custom scoring function alongside the calling function\\nshould work out of the box with the default joblib backend (loky),\\nimporting it from another module will be a more robust approach and work\\nindependently of the joblib backend.\\nFor example, to use n_jobs greater than 1 in the example below,\\ncustom_scoring_function function is saved in a user-created module\\n(custom_scorer_module.py) and imported:\\nfrom custom_scorer_module import custom_scoring_function\\ncross_val_score(model,\\n...  X_train,\\n...  y_train,\\n...  scoring=make_scorer(custom_scoring_function, greater_is_better=False),\\n...  cv=5,\\n...  n_jobs=-1)\\n3.3.1.4. Using multiple metric evaluation¶\\nScikit-learn also permits evaluation of multiple metrics in GridSearchCV,\\nRandomizedSearchCV and cross_validate.\\nThere are three ways to specify multiple scoring metrics for the scoring\\nparameter:\\nAs an iterable of string metrics::>>> scoring = [\\'accuracy\\', \\'precision\\']\\nAs a dict mapping the scorer name to the scoring function::>>> from sklearn.metrics import accuracy_score\\nfrom sklearn.metrics import make_scorer\\nscoring = {\\'accuracy\\': make_scorer(accuracy_score),\\n...            \\'prec\\': \\'precision\\'}\\nNote that the dict values can either be scorer functions or one of the\\npredefined metric strings.\\nAs a callable that returns a dictionary of scores:\\nfrom sklearn.model_selection import cross_validate\\nfrom sklearn.metrics import confusion_matrix\\n\\nA sample toy binary classification dataset\\n\\nX, y = datasets.make_classification(n_classes=2, random_state=0)\\nsvm = LinearSVC(dual=\"auto\", random_state=0)\\ndef confusion_matrix_scorer(clf, X, y):\\n...      y_pred = clf.predict(X)\\n...      cm = confusion_matrix(y, y_pred)\\n...      return {\\'tn\\': cm[0, 0], \\'fp\\': cm[0, 1],\\n...              \\'fn\\': cm[1, 0], \\'tp\\': cm[1, 1]}\\ncv_results = cross_validate(svm, X, y, cv=5,\\n...                             scoring=confusion_matrix_scorer)\\n\\nGetting the test set true positive scores\\n\\nprint(cv_results[\\'test_tp\\'])\\n[10  9  8  7  8]\\n\\nGetting the test set false negative scores\\n\\nprint(cv_results[\\'test_fn\\'])\\n[0 1 2 3 2]\\n- 3.3.2. Classification metrics¶\\nThe sklearn.metrics module implements several loss, score, and utility\\nfunctions to measure classification performance.\\nSome metrics might require probability estimates of the positive class,\\nconfidence values, or binary decisions values.\\nMost implementations allow each sample to provide a weighted contribution\\nto the overall score, through the sample_weight parameter.\\nSome of these are restricted to the binary classification case:\\nprecision_recall_curve(y_true, probas_pred, )\\nCompute precision-recall pairs for different probability thresholds.\\nroc_curve(y_true, y_score, [, pos_label, ...])\\nCompute Receiver operating characteristic (ROC).\\nclass_likelihood_ratios(y_true, y_pred, [, ...])\\nCompute binary classification positive and negative likelihood ratios.\\ndet_curve(y_true, y_score[, pos_label, ...])\\nCompute error rates for different probability thresholds.\\nOthers also work in the multiclass case:\\nbalanced_accuracy_score(y_true, y_pred, [, ...])\\nCompute the balanced accuracy.\\ncohen_kappa_score(y1, y2, [, labels, ...])\\nCompute Cohen\\'s kappa: a statistic that measures inter-annotator agreement.\\nconfusion_matrix(y_true, y_pred, [, ...])\\nCompute confusion matrix to evaluate the accuracy of a classification.\\nhinge_loss(y_true, pred_decision, [, ...])\\nAverage hinge loss (non-regularized).\\nmatthews_corrcoef(y_true, y_pred, [, ...])\\nCompute the Matthews correlation coefficient (MCC).\\nroc_auc_score(y_true, y_score, [, average, ...])\\nCompute Area Under the Receiver Operating Characteristic Curve (ROC AUC)     from prediction scores.\\ntop_k_accuracy_score(y_true, y_score, [, ...])\\nTop-k Accuracy classification score.\\nSome also work in the multilabel case:\\naccuracy_score(y_true, y_pred, [, ...])\\nAccuracy classification score.\\nclassification_report(y_true, y_pred, [, ...])\\nBuild a text report showing the main classification metrics.\\nf1_score(y_true, y_pred, [, labels, ...])\\nCompute the F1 score, also known as balanced F-score or F-measure.\\nfbeta_score(y_true, y_pred, , beta[, ...])\\nCompute the F-beta score.\\nhamming_loss(y_true, y_pred, [, sample_weight])\\nCompute the average Hamming loss.\\njaccard_score(y_true, y_pred, [, labels, ...])\\nJaccard similarity coefficient score.\\nlog_loss(y_true, y_pred, [, eps, ...])\\nLog loss, aka logistic loss or cross-entropy loss.\\nmultilabel_confusion_matrix(y_true, y_pred, )\\nCompute a confusion matrix for each class or sample.\\nprecision_recall_fscore_support(y_true, ...)\\nCompute precision, recall, F-measure and support for each class.\\nprecision_score(y_true, y_pred, [, labels, ...])\\nCompute the precision.\\nrecall_score(y_true, y_pred, [, labels, ...])\\nCompute the recall.\\nroc_auc_score(y_true, y_score, [, average, ...])\\nCompute Area Under the Receiver Operating Characteristic Curve (ROC AUC)     from prediction scores.\\nzero_one_loss(y_true, y_pred, [, ...])\\nZero-one classification loss.\\nAnd some work with binary and multilabel (but not multiclass) problems:\\naverage_precision_score(y_true, y_score, *)\\nCompute average precision (AP) from prediction scores.\\nIn the following sub-sections, we will describe each of those functions,\\npreceded by some notes on common API and metric definition.\\n3.3.2.1. From binary to multiclass and multilabel¶\\nSome metrics are essentially defined for binary classification tasks (e.g.\\nf1_score, roc_auc_score). In these cases, by default\\nonly the positive label is evaluated, assuming by default that the positive\\nclass is labelled 1 (though this may be configurable through the\\npos_label parameter).\\nIn extending a binary metric to multiclass or multilabel problems, the data\\nis treated as a collection of binary problems, one for each class.\\nThere are then a number of ways to average binary metric calculations across\\nthe set of classes, each of which may be useful in some scenario.\\nWhere available, you should select among these using the average parameter.\\n\"macro\" simply calculates the mean of the binary metrics,\\ngiving equal weight to each class.  In problems where infrequent classes\\nare nonetheless important, macro-averaging may be a means of highlighting\\ntheir performance. On the other hand, the assumption that all classes are\\nequally important is often untrue, such that macro-averaging will\\nover-emphasize the typically low performance on an infrequent class.\\n\"weighted\" accounts for class imbalance by computing the average of\\nbinary metrics in which each class’s score is weighted by its presence in the\\ntrue data sample.\\n\"micro\" gives each sample-class pair an equal contribution to the overall\\nmetric (except as a result of sample-weight). Rather than summing the\\nmetric per class, this sums the dividends and divisors that make up the\\nper-class metrics to calculate an overall quotient.\\nMicro-averaging may be preferred in multilabel settings, including\\nmulticlass classification where a majority class is to be ignored.\\n\"samples\" applies only to multilabel problems. It does not calculate a\\nper-class measure, instead calculating the metric over the true and predicted\\nclasses for each sample in the evaluation data, and returning their\\n(sample_weight-weighted) average.\\nSelecting average=None will return an array with the score for each\\nclass.\\nWhile multiclass data is provided to the metric, like binary targets, as an\\narray of class labels, multilabel data is specified as an indicator matrix,\\nin which cell [i, j] has value 1 if sample i has label j and value\\n0 otherwise.\\n3.3.2.2. Accuracy score¶\\nThe accuracy_score function computes the\\naccuracy, either the fraction\\n(default) or the count (normalize=False) of correct predictions.\\nIn multilabel classification, the function returns the subset accuracy. If\\nthe entire set of predicted labels for a sample strictly match with the true\\nset of labels, then the subset accuracy is 1.0; otherwise it is 0.0.\\nIf (\\\\hat{y}i) is the predicted value of\\nthe (i)-th sample and (y_i) is the corresponding true value,\\nthen the fraction of correct predictions over (n\\\\text{samples}) is\\ndefined as\\n[\\\\texttt{accuracy}(y, \\\\hat{y}) = \\\\frac{1}{n_\\\\text{samples}} \\\\sum_{i=0}^{n_\\\\text{samples}-1} 1(\\\\hat{y}i = y_i)]\\nwhere (1(x)) is the indicator function.\\nimport numpy as np\\nfrom sklearn.metrics import accuracy_score\\ny_pred = [0, 2, 1, 3]\\ny_true = [0, 1, 2, 3]\\naccuracy_score(y_true, y_pred)\\n0.5\\naccuracy_score(y_true, y_pred, normalize=False)\\n2.0\\nIn the multilabel case with binary label indicators:\\naccuracy_score(np.array([[0, 1], [1, 1]]), np.ones((2, 2)))\\n0.5\\nExample:\\nSee Test with permutations the significance of a classification score\\nfor an example of accuracy score usage using permutations of\\nthe dataset.\\n3.3.2.3. Top-k accuracy score¶\\nThe top_k_accuracy_score function is a generalization of\\naccuracy_score. The difference is that a prediction is considered\\ncorrect as long as the true label is associated with one of the k highest\\npredicted scores. accuracy_score is the special case of k = 1.\\nThe function covers the binary and multiclass classification cases but not the\\nmultilabel case.\\nIf (\\\\hat{f}{i,j}) is the predicted class for the (i)-th sample\\ncorresponding to the (j)-th largest predicted score and (y_i) is the\\ncorresponding true value, then the fraction of correct predictions over\\n(n_\\\\text{samples}) is defined as\\n[\\\\texttt{top-k accuracy}(y, \\\\hat{f}) = \\\\frac{1}{n_\\\\text{samples}} \\\\sum_{i=0}^{n_\\\\text{samples}-1} \\\\sum_{j=1}^{k} 1(\\\\hat{f}_{i,j} = y_i)]\\nwhere (k) is the number of guesses allowed and (1(x)) is the\\nindicator function.\\nimport numpy as np\\nfrom sklearn.metrics import top_k_accuracy_score\\ny_true = np.array([0, 1, 2, 2])\\ny_score = np.array([[0.5, 0.2, 0.2],\\n...                     [0.3, 0.4, 0.2],\\n...                     [0.2, 0.4, 0.3],\\n...                     [0.7, 0.2, 0.1]])\\ntop_k_accuracy_score(y_true, y_score, k=2)\\n0.75\\n\\nNot normalizing gives the number of \"correctly\" classified samples\\n\\nWith the following prediction, we have perfect and minimal loss\\n\\ny_score = np.array([[1.0, 0.1, 0.2], [0.1, 0.2, 0.9]])\\nlabel_ranking_loss(y_true, y_score)\\n0.0\\nReferences:\\nTsoumakas, G., Katakis, I., & Vlahavas, I. (2010). Mining multi-label data. In\\nData mining and knowledge discovery handbook (pp. 667-685). Springer US.\\n3.3.3.4. Normalized Discounted Cumulative Gain¶\\nDiscounted Cumulative Gain (DCG) and Normalized Discounted Cumulative Gain\\n(NDCG) are ranking metrics implemented in dcg_score\\nand ndcg_score ; they compare a predicted order to\\nground-truth scores, such as the relevance of answers to a query.\\nFrom the Wikipedia page for Discounted Cumulative Gain:\\n“Discounted cumulative gain (DCG) is a measure of ranking quality. In\\ninformation retrieval, it is often used to measure effectiveness of web search\\nengine algorithms or related applications. Using a graded relevance scale of\\ndocuments in a search-engine result set, DCG measures the usefulness, or gain,\\nof a document based on its position in the result list. The gain is accumulated\\nfrom the top of the result list to the bottom, with the gain of each result\\ndiscounted at lower ranks”\\nDCG orders the true targets (e.g. relevance of query answers) in the predicted\\norder, then multiplies them by a logarithmic decay and sums the result. The sum\\ncan be truncated after the first (K) results, in which case we call it\\nDCG@K.\\nNDCG, or NDCG@K is DCG divided by the DCG obtained by a perfect prediction, so\\nthat it is always between 0 and 1. Usually, NDCG is preferred to DCG.\\nCompared with the ranking loss, NDCG can take into account relevance scores,\\nrather than a ground-truth ranking. So if the ground-truth consists only of an\\nordering, the ranking loss should be preferred; if the ground-truth consists of\\nactual usefulness scores (e.g. 0 for irrelevant, 1 for relevant, 2 for very\\nrelevant), NDCG can be used.\\nFor one sample, given the vector of continuous ground-truth values for each\\ntarget (y \\\\in \\\\mathbb{R}^{M}), where (M) is the number of outputs, and\\nthe prediction (\\\\hat{y}), which induces the ranking function (f), the\\nDCG score is\\n[\\\\sum_{r=1}^{\\\\min(K, M)}\\\\frac{y_{f(r)}}{\\\\log(1 + r)}]\\nand the NDCG score is the DCG score divided by the DCG score obtained for\\n(y).\\nReferences:\\nWikipedia entry for Discounted Cumulative Gain\\nJarvelin, K., & Kekalainen, J. (2002).\\nCumulated gain-based evaluation of IR techniques. ACM Transactions on\\nInformation Systems (TOIS), 20(4), 422-446.\\nWang, Y., Wang, L., Li, Y., He, D., Chen, W., & Liu, T. Y. (2013, May).\\nA theoretical analysis of NDCG ranking measures. In Proceedings of the 26th\\nAnnual Conference on Learning Theory (COLT 2013)\\nMcSherry, F., & Najork, M. (2008, March). Computing information retrieval\\nperformance measures efficiently in the presence of tied scores. In\\nEuropean conference on information retrieval (pp. 414-421). Springer,\\nBerlin, Heidelberg.\\n- 3.3.4. Regression metrics¶\\nThe sklearn.metrics module implements several loss, score, and utility\\nfunctions to measure regression performance. Some of those have been enhanced\\nto handle the multioutput case: mean_squared_error,\\nmean_absolute_error, r2_score,\\nexplained_variance_score, mean_pinball_loss, d2_pinball_score\\nand d2_absolute_error_score.\\nThese functions have a multioutput keyword argument which specifies the\\nway the scores or losses for each individual target should be averaged. The\\ndefault is \\'uniform_average\\', which specifies a uniformly weighted mean\\nover outputs. If an ndarray of shape (n_outputs,) is passed, then its\\nentries are interpreted as weights and an according weighted average is\\nreturned. If multioutput is \\'raw_values\\', then all unaltered\\nindividual scores or losses will be returned in an array of shape\\n(n_outputs,).\\nThe r2_score and explained_variance_score accept an additional\\nvalue \\'variance_weighted\\' for the multioutput parameter. This option\\nleads to a weighting of each individual score by the variance of the\\ncorresponding target variable. This setting quantifies the globally captured\\nunscaled variance. If the target variables are of different scale, then this\\nscore puts more importance on explaining the higher variance variables.\\nmultioutput=\\'variance_weighted\\' is the default value for r2_score\\nfor backward compatibility. This will be changed to uniform_average in the\\nfuture.\\n3.3.4.1. R² score, the coefficient of determination¶\\nThe r2_score function computes the coefficient of\\ndetermination,\\nusually denoted as (R^2).\\nIt represents the proportion of variance (of y) that has been explained by the\\nindependent variables in the model. It provides an indication of goodness of\\nfit and therefore a measure of how well unseen samples are likely to be\\npredicted by the model, through the proportion of explained variance.\\nAs such variance is dataset dependent, (R^2) may not be meaningfully comparable\\nacross different datasets. Best possible score is 1.0 and it can be negative\\n(because the model can be arbitrarily worse). A constant model that always\\npredicts the expected (average) value of y, disregarding the input features,\\nwould get an (R^2) score of 0.0.\\nNote: when the prediction residuals have zero mean, the (R^2) score and\\nthe Explained variance score are identical.\\nIf (\\\\hat{y}i) is the predicted value of the (i)-th sample\\nand (y_i) is the corresponding true value for total (n) samples,\\nthe estimated (R^2) is defined as:\\n[R^2(y, \\\\hat{y}) = 1 - \\\\frac{\\\\sum{i=1}^{n} (y_i - \\\\hat{y}i)^2}{\\\\sum{i=1}^{n} (y_i - \\\\bar{y})^2}]\\nwhere (\\\\bar{y} = \\\\frac{1}{n} \\\\sum_{i=1}^{n} y_i) and (\\\\sum_{i=1}^{n} (y_i - \\\\hat{y}i)^2 = \\\\sum{i=1}^{n} \\\\epsilon_i^2).\\nNote that r2_score calculates unadjusted (R^2) without correcting for\\nbias in sample variance of y.\\nIn the particular case where the true target is constant, the (R^2) score is\\nnot finite: it is either NaN (perfect predictions) or -Inf (imperfect\\npredictions). Such non-finite scores may prevent correct model optimization\\nsuch as grid-search cross-validation to be performed correctly. For this reason\\nthe default behaviour of r2_score is to replace them with 1.0 (perfect\\npredictions) or 0.0 (imperfect predictions). If force_finite\\nis set to False, this score falls back on the original (R^2) definition.\\nHere is a small example of usage of the r2_score function:\\nfrom sklearn.metrics import r2_score\\ny_true = [3, -0.5, 2, 7]\\ny_pred = [2.5, 0.0, 2, 8]\\nr2_score(y_true, y_pred)\\n0.948...\\ny_true = [[0.5, 1], [-1, 1], [7, -6]]\\ny_pred = [[0, 2], [-1, 2], [8, -5]]\\nr2_score(y_true, y_pred, multioutput=\\'variance_weighted\\')\\n0.938...\\ny_true = [[0.5, 1], [-1, 1], [7, -6]]\\ny_pred = [[0, 2], [-1, 2], [8, -5]]\\nr2_score(y_true, y_pred, multioutput=\\'uniform_average\\')\\n0.936...\\nr2_score(y_true, y_pred, multioutput=\\'raw_values\\')\\narray([0.965..., 0.908...])\\nr2_score(y_true, y_pred, multioutput=[0.3, 0.7])\\n0.925...\\ny_true = [-2, -2, -2]\\ny_pred = [-2, -2, -2]\\nr2_score(y_true, y_pred)\\n1.0\\nr2_score(y_true, y_pred, force_finite=False)\\nnan\\ny_true = [-2, -2, -2]\\ny_pred = [-2, -2, -2 + 1e-8]\\nr2_score(y_true, y_pred)\\n0.0\\nr2_score(y_true, y_pred, force_finite=False)\\n-inf\\nExample:\\nSee L1-based models for Sparse Signals\\nfor an example of R² score usage to\\nevaluate Lasso and Elastic Net on sparse signals.\\n3.3.4.2. Mean absolute error¶\\nThe mean_absolute_error function computes mean absolute\\nerror, a risk\\nmetric corresponding to the expected value of the absolute error loss or\\n(l1)-norm loss.\\nIf (\\\\hat{y}i) is the predicted value of the (i)-th sample,\\nand (y_i) is the corresponding true value, then the mean absolute error\\n(MAE) estimated over (n{\\\\text{samples}}) is defined as\\n[\\\\text{MAE}(y, \\\\hat{y}) = \\\\frac{1}{n_{\\\\text{samples}}} \\\\sum_{i=0}^{n_{\\\\text{samples}}-1} \\\\left| y_i - \\\\hat{y}i \\\\right|.]\\nHere is a small example of usage of the mean_absolute_error function:\\nfrom sklearn.metrics import mean_absolute_error\\ny_true = [3, -0.5, 2, 7]\\ny_pred = [2.5, 0.0, 2, 8]\\nmean_absolute_error(y_true, y_pred)\\n0.5\\ny_true = [[0.5, 1], [-1, 1], [7, -6]]\\ny_pred = [[0, 2], [-1, 2], [8, -5]]\\nmean_absolute_error(y_true, y_pred)\\n0.75\\nmean_absolute_error(y_true, y_pred, multioutput=\\'raw_values\\')\\narray([0.5, 1. ])\\nmean_absolute_error(y_true, y_pred, multioutput=[0.3, 0.7])\\n0.85...\\n3.3.4.3. Mean squared error¶\\nThe mean_squared_error function computes mean square\\nerror, a risk\\nmetric corresponding to the expected value of the squared (quadratic) error or\\nloss.\\nIf (\\\\hat{y}_i) is the predicted value of the (i)-th sample,\\nand (y_i) is the corresponding true value, then the mean squared error\\n(MSE) estimated over (n{\\\\text{samples}}) is defined as\\n[\\\\text{MSE}(y, \\\\hat{y}) = \\\\frac{1}{n_\\\\text{samples}} \\\\sum_{i=0}^{n_\\\\text{samples} - 1} (y_i - \\\\hat{y}i)^2.]\\nHere is a small example of usage of the mean_squared_error\\nfunction:\\nfrom sklearn.metrics import mean_squared_error\\ny_true = [3, -0.5, 2, 7]\\ny_pred = [2.5, 0.0, 2, 8]\\nmean_squared_error(y_true, y_pred)\\n0.375\\ny_true = [[0.5, 1], [-1, 1], [7, -6]]\\ny_pred = [[0, 2], [-1, 2], [8, -5]]\\nmean_squared_error(y_true, y_pred)\\n0.7083...\\nExamples:\\nSee Gradient Boosting regression\\nfor an example of mean squared error usage to\\nevaluate gradient boosting regression.\\nTaking the square root of the MSE, called the root mean squared error (RMSE), is another\\ncommon metric that provides a measure in the same units as the target variable. RSME is\\navailable through the root_mean_squared_error function.\\n3.3.4.4. Mean squared logarithmic error¶\\nThe mean_squared_log_error function computes a risk metric\\ncorresponding to the expected value of the squared logarithmic (quadratic)\\nerror or loss.\\nIf (\\\\hat{y}_i) is the predicted value of the (i)-th sample,\\nand (y_i) is the corresponding true value, then the mean squared\\nlogarithmic error (MSLE) estimated over (n{\\\\text{samples}}) is\\ndefined as\\n[\\\\text{MSLE}(y, \\\\hat{y}) = \\\\frac{1}{n_\\\\text{samples}} \\\\sum_{i=0}^{n_\\\\text{samples} - 1} (\\\\log_e (1 + y_i) - \\\\log_e (1 + \\\\hat{y}i) )^2.]\\nWhere (\\\\log_e (x)) means the natural logarithm of (x). This metric\\nis best to use when targets having exponential growth, such as population\\ncounts, average sales of a commodity over a span of years etc. Note that this\\nmetric penalizes an under-predicted estimate greater than an over-predicted\\nestimate.\\nHere is a small example of usage of the mean_squared_log_error\\nfunction:\\nfrom sklearn.metrics import mean_squared_log_error\\ny_true = [3, 5, 2.5, 7]\\ny_pred = [2.5, 5, 4, 8]\\nmean_squared_log_error(y_true, y_pred)\\n0.039...\\ny_true = [[0.5, 1], [1, 2], [7, 6]]\\ny_pred = [[0.5, 2], [1, 2.5], [8, 8]]\\nmean_squared_log_error(y_true, y_pred)\\n0.044...\\nThe root mean squared logarithmic error (RMSLE) is available through the\\nroot_mean_squared_log_error function.\\n3.3.4.5. Mean absolute percentage error¶\\nThe mean_absolute_percentage_error (MAPE), also known as mean absolute\\npercentage deviation (MAPD), is an evaluation metric for regression problems.\\nThe idea of this metric is to be sensitive to relative errors. It is for example\\nnot changed by a global scaling of the target variable.\\nIf (\\\\hat{y}_i) is the predicted value of the (i)-th sample\\nand (y_i) is the corresponding true value, then the mean absolute percentage\\nerror (MAPE) estimated over (n{\\\\text{samples}}) is defined as\\n[\\\\text{MAPE}(y, \\\\hat{y}) = \\\\frac{1}{n_{\\\\text{samples}}} \\\\sum_{i=0}^{n_{\\\\text{samples}}-1} \\\\frac{{}\\\\left| y_i - \\\\hat{y}i \\\\right|}{\\\\max(\\\\epsilon, \\\\left| y_i \\\\right|)}]\\nwhere (\\\\epsilon) is an arbitrary small yet strictly positive number to\\navoid undefined results when y is zero.\\nThe mean_absolute_percentage_error function supports multioutput.\\nHere is a small example of usage of the mean_absolute_percentage_error\\nfunction:\\nfrom sklearn.metrics import mean_absolute_percentage_error\\ny_true = [1, 10, 1e6]\\ny_pred = [0.9, 15, 1.2e6]\\nmean_absolute_percentage_error(y_true, y_pred)\\n0.2666...\\nIn above example, if we had used mean_absolute_error, it would have ignored\\nthe small magnitude values and only reflected the error in prediction of highest\\nmagnitude value. But that problem is resolved in case of MAPE because it calculates\\nrelative percentage error with respect to actual output.\\n3.3.4.6. Median absolute error¶\\nThe median_absolute_error is particularly interesting because it is\\nrobust to outliers. The loss is calculated by taking the median of all absolute\\ndifferences between the target and the prediction.\\nIf (\\\\hat{y}_i) is the predicted value of the (i)-th sample\\nand (y_i) is the corresponding true value, then the median absolute error\\n(MedAE) estimated over (n{\\\\text{samples}}) is defined as\\n[\\\\text{MedAE}(y, \\\\hat{y}) = \\\\text{median}(\\\\mid y_1 - \\\\hat{y}1 \\\\mid, \\\\ldots, \\\\mid y_n - \\\\hat{y}_n \\\\mid).]\\nThe median_absolute_error does not support multioutput.\\nHere is a small example of usage of the median_absolute_error\\nfunction:\\nfrom sklearn.metrics import median_absolute_error\\ny_true = [3, -0.5, 2, 7]\\ny_pred = [2.5, 0.0, 2, 8]\\nmedian_absolute_error(y_true, y_pred)\\n0.5\\n3.3.4.7. Max error¶\\nThe max_error function computes the maximum residual error , a metric\\nthat captures the worst case error between the predicted value and\\nthe true value. In a perfectly fitted single output regression\\nmodel, max_error would be 0 on the training set and though this\\nwould be highly unlikely in the real world, this metric shows the\\nextent of error that the model had when it was fitted.\\nIf (\\\\hat{y}_i) is the predicted value of the (i)-th sample,\\nand (y_i) is the corresponding true value, then the max error is\\ndefined as\\n[\\\\text{Max Error}(y, \\\\hat{y}) = \\\\max(| y_i - \\\\hat{y}_i |)]\\nHere is a small example of usage of the max_error function:\\nfrom sklearn.metrics import max_error\\ny_true = [3, 2, 7, 1]\\ny_pred = [9, 2, 7, 1]\\nmax_error(y_true, y_pred)\\n6\\nThe max_error does not support multioutput.\\n3.3.4.8. Explained variance score¶\\nThe explained_variance_score computes the explained variance\\nregression score.\\nIf (\\\\hat{y}) is the estimated target output, (y) the corresponding\\n(correct) target output, and (Var) is Variance, the square of the standard deviation,\\nthen the explained variance is estimated as follow:\\n[explained_{}variance(y, \\\\hat{y}) = 1 - \\\\frac{Var{ y - \\\\hat{y}}}{Var{y}}]\\nThe best possible score is 1.0, lower values are worse.\\nLink to R² score, the coefficient of determination\\nThe difference between the explained variance score and the R² score, the coefficient of determination\\nis that when the explained variance score does not account for\\nsystematic offset in the prediction. For this reason, the\\nR² score, the coefficient of determination should be preferred in general.\\nIn the particular case where the true target is constant, the Explained\\nVariance score is not finite: it is either NaN (perfect predictions) or\\n-Inf (imperfect predictions). Such non-finite scores may prevent correct\\nmodel optimization such as grid-search cross-validation to be performed\\ncorrectly. For this reason the default behaviour of\\nexplained_variance_score is to replace them with 1.0 (perfect\\npredictions) or 0.0 (imperfect predictions). You can set the force_finite\\nparameter to False to prevent this fix from happening and fallback on the\\noriginal Explained Variance score.\\nHere is a small example of usage of the explained_variance_score\\nfunction:\\nfrom sklearn.metrics import explained_variance_score\\ny_true = [3, -0.5, 2, 7]\\ny_pred = [2.5, 0.0, 2, 8]\\nexplained_variance_score(y_true, y_pred)\\n0.957...\\ny_true = [[0.5, 1], [-1, 1], [7, -6]]\\ny_pred = [[0, 2], [-1, 2], [8, -5]]\\nexplained_variance_score(y_true, y_pred, multioutput=\\'raw_values\\')\\narray([0.967..., 1.        ])\\nexplained_variance_score(y_true, y_pred, multioutput=[0.3, 0.7])\\n0.990...\\ny_true = [-2, -2, -2]\\ny_pred = [-2, -2, -2]\\nexplained_variance_score(y_true, y_pred)\\n1.0\\nexplained_variance_score(y_true, y_pred, force_finite=False)\\nnan\\ny_true = [-2, -2, -2]\\ny_pred = [-2, -2, -2 + 1e-8]\\nexplained_variance_score(y_true, y_pred)\\n0.0\\nexplained_variance_score(y_true, y_pred, force_finite=False)\\n-inf\\n3.3.4.9. Mean Poisson, Gamma, and Tweedie deviances¶\\nThe mean_tweedie_deviance function computes the mean Tweedie\\ndeviance error\\nwith a power parameter ((p)). This is a metric that elicits\\npredicted expectation values of regression targets.\\nFollowing special cases exist,\\nwhen power=0 it is equivalent to mean_squared_error.\\nwhen power=1 it is equivalent to mean_poisson_deviance.\\nwhen power=2 it is equivalent to mean_gamma_deviance.\\nIf (\\\\hat{y}_i) is the predicted value of the (i)-th sample,\\nand (y_i) is the corresponding true value, then the mean Tweedie\\ndeviance error (D) for power (p), estimated over (n{\\\\text{samples}})\\nis defined as\\n[\\\\begin{split}\\\\text{D}(y, \\\\hat{y}) = \\\\frac{1}{n_\\\\text{samples}}\\n\\\\sum_{i=0}^{n_\\\\text{samples} - 1}\\n\\\\begin{cases}\\n(y_i-\\\\hat{y}i)^2, & \\\\text{for }p=0\\\\text{ (Normal)}\\\\\\n2(y_i \\\\log(y_i/\\\\hat{y}_i) + \\\\hat{y}_i - y_i),  & \\\\text{for }p=1\\\\text{ (Poisson)}\\\\\\n2(\\\\log(\\\\hat{y}_i/y_i) + y_i/\\\\hat{y}_i - 1),  & \\\\text{for }p=2\\\\text{ (Gamma)}\\\\\\n2\\\\left(\\\\frac{\\\\max(y_i,0)^{2-p}}{(1-p)(2-p)}-\\n\\\\frac{y_i\\\\,\\\\hat{y}_i^{1-p}}{1-p}+\\\\frac{\\\\hat{y}_i^{2-p}}{2-p}\\\\right),\\n& \\\\text{otherwise}\\n\\\\end{cases}\\\\end{split}]\\nTweedie deviance is a homogeneous function of degree 2-power.\\nThus, Gamma distribution with power=2 means that simultaneously scaling\\ny_true and y_pred has no effect on the deviance. For Poisson\\ndistribution power=1 the deviance scales linearly, and for Normal\\ndistribution (power=0), quadratically.  In general, the higher\\npower the less weight is given to extreme deviations between true\\nand predicted targets.\\nFor instance, let’s compare the two predictions 1.5 and 150 that are both\\n50% larger than their corresponding true value.\\nThe mean squared error (power=0) is very sensitive to the\\nprediction difference of the second point,:\\nfrom sklearn.metrics import mean_tweedie_deviance\\nmean_tweedie_deviance([1.0], [1.5], power=0)\\n0.25\\nmean_tweedie_deviance([100.], [150.], power=0)\\n2500.0\\nIf we increase power to 1,:\\nmean_tweedie_deviance([1.0], [1.5], power=1)\\n0.18...\\nmean_tweedie_deviance([100.], [150.], power=1)\\n18.9...\\nthe difference in errors decreases. Finally, by setting, power=2:\\nmean_tweedie_deviance([1.0], [1.5], power=2)\\n0.14...\\nmean_tweedie_deviance([100.], [150.], power=2)\\n0.14...\\nwe would get identical errors. The deviance when power=2 is thus only\\nsensitive to relative errors.\\n3.3.4.10. Pinball loss¶\\nThe mean_pinball_loss function is used to evaluate the predictive\\nperformance of quantile regression models.\\n[\\\\text{pinball}(y, \\\\hat{y}) = \\\\frac{1}{n{\\\\text{samples}}} \\\\sum_{i=0}^{n_{\\\\text{samples}}-1}  \\\\alpha \\\\max(y_i - \\\\hat{y}_i, 0) + (1 - \\\\alpha) \\\\max(\\\\hat{y}_i - y_i, 0)]\\nThe value of pinball loss is equivalent to half of mean_absolute_error when the quantile\\nparameter alpha is set to 0.5.\\nHere is a small example of usage of the mean_pinball_loss function:\\nfrom sklearn.metrics import mean_pinball_loss\\ny_true = [1, 2, 3]\\nmean_pinball_loss(y_true, [0, 2, 3], alpha=0.1)\\n0.03...\\nmean_pinball_loss(y_true, [1, 2, 4], alpha=0.1)\\n0.3...\\nmean_pinball_loss(y_true, [0, 2, 3], alpha=0.9)\\n0.3...\\nmean_pinball_loss(y_true, [1, 2, 4], alpha=0.9)\\n0.03...\\nmean_pinball_loss(y_true, y_true, alpha=0.1)\\n0.0\\nmean_pinball_loss(y_true, y_true, alpha=0.9)\\n0.0\\nIt is possible to build a scorer object with a specific choice of alpha:\\nfrom sklearn.metrics import make_scorer\\nmean_pinball_loss_95p = make_scorer(mean_pinball_loss, alpha=0.95)\\nSuch a scorer can be used to evaluate the generalization performance of a\\nquantile regressor via cross-validation:\\nfrom sklearn.datasets import make_regression\\nfrom sklearn.model_selection import cross_val_score\\nfrom sklearn.ensemble import GradientBoostingRegressor\\n\\nX, y = make_regression(n_samples=100, random_state=0)\\nestimator = GradientBoostingRegressor(\\n...     loss=\"quantile\",\\n...     alpha=0.95,\\n...     random_state=0,\\n... )\\ncross_val_score(estimator, X, y, cv=5, scoring=mean_pinball_loss_95p)\\narray([13.6..., 9.7..., 23.3..., 9.5..., 10.4...])\\nIt is also possible to build scorer objects for hyper-parameter tuning. The\\nsign of the loss must be switched to ensure that greater means better as\\nexplained in the example linked below.\\nExample:\\nSee Prediction Intervals for Gradient Boosting Regression\\nfor an example of using the pinball loss to evaluate and tune the\\nhyper-parameters of quantile regression models on data with non-symmetric\\nnoise and outliers.\\n3.3.4.11. D² score¶\\nThe D² score computes the fraction of deviance explained.\\nIt is a generalization of R², where the squared error is generalized and replaced\\nby a deviance of choice (\\\\text{dev}(y, \\\\hat{y}))\\n(e.g., Tweedie, pinball or mean absolute error). D² is a form of a skill score.\\nIt is calculated as\\n[D^2(y, \\\\hat{y}) = 1 - \\\\frac{\\\\text{dev}(y, \\\\hat{y})}{\\\\text{dev}(y, y_{\\\\text{null}})} \\\\,.]\\nWhere (y_{\\\\text{null}}) is the optimal prediction of an intercept-only model\\n(e.g., the mean of y_true for the Tweedie case, the median for absolute\\nerror and the alpha-quantile for pinball loss).\\nLike R², the best possible score is 1.0 and it can be negative (because the\\nmodel can be arbitrarily worse). A constant model that always predicts\\n(y_{\\\\text{null}}), disregarding the input features, would get a D² score\\nof 0.0.\\n3.3.4.11.1. D² Tweedie score¶\\nThe d2_tweedie_score function implements the special case of D²\\nwhere (\\\\text{dev}(y, \\\\hat{y})) is the Tweedie deviance, see Mean Poisson, Gamma, and Tweedie deviances.\\nIt is also known as D² Tweedie and is related to McFadden’s likelihood ratio index.\\nThe argument power defines the Tweedie power as for\\nmean_tweedie_deviance. Note that for power=0,\\nd2_tweedie_score equals r2_score (for single targets).\\nA scorer object with a specific choice of power can be built by:\\nfrom sklearn.metrics import d2_tweedie_score, make_scorer\\nd2_tweedie_score_15 = make_scorer(d2_tweedie_score, power=1.5)\\n3.3.4.11.2. D² pinball score¶\\nThe d2_pinball_score function implements the special case\\nof D² with the pinball loss, see Pinball loss, i.e.:\\n[\\\\text{dev}(y, \\\\hat{y}) = \\\\text{pinball}(y, \\\\hat{y}).]\\nThe argument alpha defines the slope of the pinball loss as for\\nmean_pinball_loss (Pinball loss). It determines the\\nquantile level alpha for which the pinball loss and also D²\\nare optimal. Note that for alpha=0.5 (the default) d2_pinball_score\\nequals d2_absolute_error_score.\\nA scorer object with a specific choice of alpha can be built by:\\nfrom sklearn.metrics import d2_pinball_score, make_scorer\\nd2_pinball_score_08 = make_scorer(d2_pinball_score, alpha=0.8)\\n3.3.4.11.3. D² absolute error score¶\\nThe d2_absolute_error_score function implements the special case of\\nthe Mean absolute error:\\n[\\\\text{dev}(y, \\\\hat{y}) = \\\\text{MAE}(y, \\\\hat{y}).]\\nHere are some usage examples of the d2_absolute_error_score function:\\nfrom sklearn.metrics import d2_absolute_error_score\\ny_true = [3, -0.5, 2, 7]\\ny_pred = [2.5, 0.0, 2, 8]\\nd2_absolute_error_score(y_true, y_pred)\\n0.764...\\ny_true = [1, 2, 3]\\ny_pred = [1, 2, 3]\\nd2_absolute_error_score(y_true, y_pred)\\n1.0\\ny_true = [1, 2, 3]\\ny_pred = [2, 2, 2]\\nd2_absolute_error_score(y_true, y_pred)\\n0.0\\n3.3.4.12. Visual evaluation of regression models¶\\nAmong methods to assess the quality of regression models, scikit-learn provides\\nthe PredictionErrorDisplay class. It allows to\\nvisually inspect the prediction errors of a model in two different manners.\\nThe plot on the left shows the actual values vs predicted values. For a\\nnoise-free regression task aiming to predict the (conditional) expectation of\\ny, a perfect regression model would display data points on the diagonal\\ndefined by predicted equal to actual values. The further away from this optimal\\nline, the larger the error of the model. In a more realistic setting with\\nirreducible noise, that is, when not all the variations of y can be explained\\nby features in X, then the best model would lead to a cloud of points densely\\narranged around the diagonal.\\nNote that the above only holds when the predicted values is the expected value\\nof y given X. This is typically the case for regression models that\\nminimize the mean squared error objective function or more generally the\\nmean Tweedie deviance for any value of its\\n“power” parameter.\\nWhen plotting the predictions of an estimator that predicts a quantile\\nof y given X, e.g. QuantileRegressor\\nor any other model minimizing the pinball loss, a\\nfraction of the points are either expected to lie above or below the diagonal\\ndepending on the estimated quantile level.\\nAll in all, while intuitive to read, this plot does not really inform us on\\nwhat to do to obtain a better model.\\nThe right-hand side plot shows the residuals (i.e. the difference between the\\nactual and the predicted values) vs. the predicted values.\\nThis plot makes it easier to visualize if the residuals follow and\\nhomoscedastic or heteroschedastic\\ndistribution.\\nIn particular, if the true distribution of y|X is Poisson or Gamma\\ndistributed, it is expected that the variance of the residuals of the optimal\\nmodel would grow with the predicted value of E[y|X] (either linearly for\\nPoisson or quadratically for Gamma).\\nWhen fitting a linear least squares regression model (see\\nLinearRegression and\\nRidge), we can use this plot to check\\nif some of the model assumptions\\nare met, in particular that the residuals should be uncorrelated, their\\nexpected value should be null and that their variance should be constant\\n(homoschedasticity).\\nIf this is not the case, and in particular if the residuals plot show some\\nbanana-shaped structure, this is a hint that the model is likely mis-specified\\nand that non-linear feature engineering or switching to a non-linear regression\\nmodel might be useful.\\nRefer to the example below to see a model evaluation that makes use of this\\ndisplay.\\nExample:\\nSee Effect of transforming the targets in regression model for\\nan example on how to use PredictionErrorDisplay\\nto visualize the prediction quality improvement of a regression model\\nobtained by transforming the target before learning.\\n- 3.3.5. Clustering metrics¶\\nThe sklearn.metrics module implements several loss, score, and utility\\nfunctions. For more information see the Clustering performance evaluation\\nsection for instance clustering, and Biclustering evaluation for\\nbiclustering.\\n- 3.3.6. Dummy estimators¶\\nWhen doing supervised learning, a simple sanity check consists of comparing\\none’s estimator against simple rules of thumb. DummyClassifier\\nimplements several such simple strategies for classification:\\nstratified generates random predictions by respecting the training\\nset class distribution.\\nmost_frequent always predicts the most frequent label in the training set.\\nprior always predicts the class that maximizes the class prior\\n(like most_frequent) and predict_proba returns the class prior.\\nuniform generates predictions uniformly at random.\\nconstant always predicts a constant label that is provided by the user.A major motivation of this method is F1-scoring, when the positive class\\nis in the minority.\\nNote that with all these strategies, the predict method completely ignores\\nthe input data!\\nTo illustrate DummyClassifier, first let’s create an imbalanced\\ndataset:\\nfrom sklearn.datasets import load_iris\\nfrom sklearn.model_selection import train_test_split\\nX, y = load_iris(return_X_y=True)\\ny[y != 1] = -1\\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\\nNext, let’s compare the accuracy of SVC and most_frequent:\\nfrom sklearn.dummy import DummyClassifier\\nfrom sklearn.svm import SVC\\nclf = SVC(kernel=\\'linear\\', C=1).fit(X_train, y_train)\\nclf.score(X_test, y_test)\\n0.63...\\nclf = DummyClassifier(strategy=\\'most_frequent\\', random_state=0)\\nclf.fit(X_train, y_train)\\nDummyClassifier(random_state=0, strategy=\\'most_frequent\\')\\nclf.score(X_test, y_test)\\n0.57...\\nWe see that SVC doesn’t do much better than a dummy classifier. Now, let’s\\nchange the kernel:\\nclf = SVC(kernel=\\'rbf\\', C=1).fit(X_train, y_train)\\nclf.score(X_test, y_test)\\n0.94...\\nWe see that the accuracy was boosted to almost 100%.  A cross validation\\nstrategy is recommended for a better estimate of the accuracy, if it\\nis not too CPU costly. For more information see the Cross-validation: evaluating estimator performance\\nsection. Moreover if you want to optimize over the parameter space, it is highly\\nrecommended to use an appropriate methodology; see the Tuning the hyper-parameters of an estimator\\nsection for details.\\nMore generally, when the accuracy of a classifier is too close to random, it\\nprobably means that something went wrong: features are not helpful, a\\nhyperparameter is not correctly tuned, the classifier is suffering from class\\nimbalance, etc…\\nDummyRegressor also implements four simple rules of thumb for regression:\\nmean always predicts the mean of the training targets.\\nmedian always predicts the median of the training targets.\\nquantile always predicts a user provided quantile of the training targets.\\nconstant always predicts a constant value that is provided by the user.\\nIn all these strategies, the predict method completely ignores\\nthe input data.\\n\\n3.4. Validation curves: plotting scores to evaluate models — scikit-learn 1.4.2 documentation\\n\\nInstall\\nUser Guide\\nAPI\\nExamples\\nCommunity\\nGetting Started\\nTutorial\\nWhat\\'s new\\nGlossary\\nDevelopment\\nFAQ\\nSupport\\nRelated packages\\nRoadmap\\nGovernance\\nAbout us\\nGitHub\\nOther Versions and Download\\nMore\\nGetting Started\\nTutorial\\nWhat\\'s new\\nGlossary\\nDevelopment\\nFAQ\\nSupport\\nRelated packages\\nRoadmap\\nGovernance\\nAbout us\\nGitHub\\nOther Versions and Download\\nToggle Menu\\nPrevUp\\nNext\\nscikit-learn 1.4.2\\nOther versions\\nPlease cite us if you use the software.\\n\\n3.4. Validation curves: plotting scores to evaluate models\\n\\n3.4.1. Validation curve\\n\\n3.4.2. Learning curve\\n\\n3.4. Validation curves: plotting scores to evaluate models¶\\n\\nEvery estimator has its advantages and drawbacks. Its generalization error\\ncan be decomposed in terms of bias, variance and noise. The bias of an\\nestimator is its average error for different training sets. The variance\\nof an estimator indicates how sensitive it is to varying training sets. Noise\\nis a property of the data.\\nIn the following plot, we see a function (f(x) = \\\\cos (\\\\frac{3}{2} \\\\pi x))\\nand some noisy samples from that function. We use three different estimators\\nto fit the function: linear regression with polynomial features of degree 1,\\n4 and 15. We see that the first estimator can at best provide only a poor fit\\nto the samples and the true function because it is too simple (high bias),\\nthe second estimator approximates it almost perfectly and the last estimator\\napproximates the training data perfectly but does not fit the true function\\nvery well, i.e. it is very sensitive to varying training data (high variance).\\nBias and variance are inherent properties of estimators and we usually have to\\nselect learning algorithms and hyperparameters so that both bias and variance\\nare as low as possible (see Bias-variance dilemma). Another way to reduce\\nthe variance of a model is to use more training data. However, you should only\\ncollect more training data if the true function is too complex to be\\napproximated by an estimator with a lower variance.\\nIn the simple one-dimensional problem that we have seen in the example it is\\neasy to see whether the estimator suffers from bias or variance. However, in\\nhigh-dimensional spaces, models can become very difficult to visualize. For\\nthis reason, it is often helpful to use the tools described below.\\nExamples:\\nUnderfitting vs. Overfitting\\nPlotting Validation Curves\\nPlotting Learning Curves and Checking Models’ Scalability\\n- 3.4.1. Validation curve¶\\nTo validate a model we need a scoring function (see Metrics and scoring: quantifying the quality of predictions),\\nfor example accuracy for classifiers. The proper way of choosing multiple\\nhyperparameters of an estimator is of course grid search or similar methods\\n(see Tuning the hyper-parameters of an estimator) that select the hyperparameter with the maximum score\\non a validation set or multiple validation sets. Note that if we optimize\\nthe hyperparameters based on a validation score the validation score is biased\\nand not a good estimate of the generalization any longer. To get a proper\\nestimate of the generalization we have to compute the score on another test\\nset.\\nHowever, it is sometimes helpful to plot the influence of a single\\nhyperparameter on the training score and the validation score to find out\\nwhether the estimator is overfitting or underfitting for some hyperparameter\\nvalues.\\nThe function validation_curve can help in this case:\\n\\nimport numpy as np\\nfrom sklearn.model_selection import validation_curve\\nfrom sklearn.datasets import load_iris\\nfrom sklearn.svm import SVC\\nnp.random.seed(0)\\nX, y = load_iris(return_X_y=True)\\nindices = np.arange(y.shape[0])\\nnp.random.shuffle(indices)\\nX, y = X[indices], y[indices]\\ntrain_scores, valid_scores = validation_curve(\\n...     SVC(kernel=\"linear\"), X, y, param_name=\"C\", param_range=np.logspace(-7, 3, 3),\\n... )\\ntrain_scores\\narray([[0.90..., 0.94..., 0.91..., 0.89..., 0.92...],\\n[0.9... , 0.92..., 0.93..., 0.92..., 0.93...],\\n[0.97..., 1...   , 0.98..., 0.97..., 0.99...]])\\nvalid_scores\\narray([[0.9..., 0.9... , 0.9... , 0.96..., 0.9... ],\\n[0.9..., 0.83..., 0.96..., 0.96..., 0.93...],\\n[1.... , 0.93..., 1....  , 1....  , 0.9... ]])\\nIf you intend to plot the validation curves only, the class\\nValidationCurveDisplay is more direct than\\nusing matplotlib manually on the results of a call to validation_curve.\\nYou can use the method\\nfrom_estimator similarly\\nto validation_curve to generate and plot the validation curve:\\nfrom sklearn.datasets import load_iris\\nfrom sklearn.model_selection import ValidationCurveDisplay\\nfrom sklearn.svm import SVC\\nfrom sklearn.utils import shuffle\\nX, y = load_iris(return_X_y=True)\\nX, y = shuffle(X, y, random_state=0)\\nValidationCurveDisplay.from_estimator(\\nSVC(kernel=\"linear\"), X, y, param_name=\"C\", param_range=np.logspace(-7, 3, 10)\\n)\\nIf the training score and the validation score are both low, the estimator will\\nbe underfitting. If the training score is high and the validation score is low,\\nthe estimator is overfitting and otherwise it is working very well. A low\\ntraining score and a high validation score is usually not possible. Underfitting,\\noverfitting, and a working model are shown in the in the plot below where we vary\\nthe parameter gamma of an SVM with an RBF kernel on the digits dataset.\\n- 3.4.2. Learning curve¶\\nA learning curve shows the validation and training score of an estimator\\nfor varying numbers of training samples. It is a tool to find out how much\\nwe benefit from adding more training data and whether the estimator suffers\\nmore from a variance error or a bias error. Consider the following example\\nwhere we plot the learning curve of a naive Bayes classifier and an SVM.\\nFor the naive Bayes, both the validation score and the training score\\nconverge to a value that is quite low with increasing size of the training\\nset. Thus, we will probably not benefit much from more training data.\\nIn contrast, for small amounts of data, the training score of the SVM is\\nmuch greater than the validation score. Adding more training samples will\\nmost likely increase generalization.\\nWe can use the function learning_curve to generate the values\\nthat are required to plot such a learning curve (number of samples\\nthat have been used, the average scores on the training sets and the\\naverage scores on the validation sets):\\nfrom sklearn.model_selection import learning_curve\\nfrom sklearn.svm import SVC\\ntrain_sizes, train_scores, valid_scores = learning_curve(\\n...     SVC(kernel=\\'linear\\'), X, y, train_sizes=[50, 80, 110], cv=5)\\ntrain_sizes\\narray([ 50, 80, 110])\\ntrain_scores\\narray([[0.98..., 0.98 , 0.98..., 0.98..., 0.98...],\\n[0.98..., 1.   , 0.98..., 0.98..., 0.98...],\\n[0.98..., 1.   , 0.98..., 0.98..., 0.99...]])\\nvalid_scores\\narray([[1. ,  0.93...,  1. ,  1. ,  0.96...],\\n[1. ,  0.96...,  1. ,  1. ,  0.96...],\\n[1. ,  0.96...,  1. ,  1. ,  0.96...]])\\nIf you intend to plot the learning curves only, the class\\nLearningCurveDisplay will be easier to use.\\nYou can use the method\\nfrom_estimator similarly\\nto learning_curve to generate and plot the learning curve:\\nfrom sklearn.datasets import load_iris\\nfrom sklearn.model_selection import LearningCurveDisplay\\nfrom sklearn.svm import SVC\\nfrom sklearn.utils import shuffle\\nX, y = load_iris(return_X_y=True)\\nX, y = shuffle(X, y, random_state=0)\\nLearningCurveDisplay.from_estimator(\\nSVC(kernel=\"linear\"), X, y, train_sizes=[50, 80, 110], cv=5)\\n\\n4.1. Partial Dependence and Individual Conditional Expectation plots — scikit-learn 1.4.2 documentation\\n\\nInstall\\nUser Guide\\nAPI\\nExamples\\nCommunity\\nGetting Started\\nTutorial\\nWhat\\'s new\\nGlossary\\nDevelopment\\nFAQ\\nSupport\\nRelated packages\\nRoadmap\\nGovernance\\nAbout us\\nGitHub\\nOther Versions and Download\\nMore\\nGetting Started\\nTutorial\\nWhat\\'s new\\nGlossary\\nDevelopment\\nFAQ\\nSupport\\nRelated packages\\nRoadmap\\nGovernance\\nAbout us\\nGitHub\\nOther Versions and Download\\nToggle Menu\\nPrevUp\\nNext\\nscikit-learn 1.4.2\\nOther versions\\nPlease cite us if you use the software.\\n\\n4.1. Partial Dependence and Individual Conditional Expectation plots\\n\\n4.1.1. Partial dependence plots\\n\\n4.1.2. Individual conditional expectation (ICE) plot\\n\\n4.1.3. Mathematical Definition\\n\\n4.1.4. Computation methods\\n\\n4.1. Partial Dependence and Individual Conditional Expectation plots¶\\n\\nPartial dependence plots (PDP) and individual conditional expectation (ICE)\\nplots can be used to visualize and analyze interaction between the target\\nresponse [1] and a set of input features of interest.\\nBoth PDPs [H2009] and ICEs [G2015] assume that the input features of interest\\nare independent from the complement features, and this assumption is often\\nviolated in practice. Thus, in the case of correlated features, we will\\ncreate absurd data points to compute the PDP/ICE [M2019].\\n- 4.1.1. Partial dependence plots¶\\nPartial dependence plots (PDP) show the dependence between the target response\\nand a set of input features of interest, marginalizing over the values\\nof all other input features (the ‘complement’ features). Intuitively, we can\\ninterpret the partial dependence as the expected target response as a\\nfunction of the input features of interest.\\nDue to the limits of human perception, the size of the set of input features of\\ninterest must be small (usually, one or two) thus the input features of interest\\nare usually chosen among the most important features.\\nThe figure below shows two one-way and one two-way partial dependence plots for\\nthe bike sharing dataset, with a\\nHistGradientBoostingRegressor:\\nOne-way PDPs tell us about the interaction between the target response and an input\\nfeature of interest (e.g. linear, non-linear). The left plot in the above figure\\nshows the effect of the temperature on the number of bike rentals; we can clearly see\\nthat a higher temperature is related with a higher number of bike rentals. Similarly, we\\ncould analyze the effect of the humidity on the number of bike rentals (middle plot).\\nThus, these interpretations are marginal, considering a feature at a time.\\nPDPs with two input features of interest show the interactions among the two features.\\nFor example, the two-variable PDP in the above figure shows the dependence of the number\\nof bike rentals on joint values of temperature and humidity. We can clearly see an\\ninteraction between the two features: with a temperature higher than 20 degrees Celsius,\\nmainly the humidity has a strong impact on the number of bike rentals. For lower\\ntemperatures, both the temperature and the humidity have an impact on the number of bike\\nrentals.\\nThe sklearn.inspection module provides a convenience function\\nfrom_estimator to create one-way and two-way partial\\ndependence plots. In the below example we show how to create a grid of\\npartial dependence plots: two one-way PDPs for the features 0 and 1\\nand a two-way PDP between the two features:\\n\\nfrom sklearn.datasets import make_hastie_10_2\\nfrom sklearn.ensemble import GradientBoostingClassifier\\nfrom sklearn.inspection import PartialDependenceDisplay\\nX, y = make_hastie_10_2(random_state=0)\\nclf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,\\n...     max_depth=1, random_state=0).fit(X, y)\\nfeatures = [0, 1, (0, 1)]\\nPartialDependenceDisplay.from_estimator(clf, X, features)\\n<...>\\nYou can access the newly created figure and Axes objects using plt.gcf()\\nand plt.gca().\\nTo make a partial dependence plot with categorical features, you need to specify\\nwhich features are categorical using the parameter categorical_features. This\\nparameter takes a list of indices, names of the categorical features or a boolean\\nmask. The graphical representation of partial dependence for categorical features is\\na bar plot or a 2D heatmap.\\nPDPs for multi-class classification\\nClick for more details\\n¶\\nFor multi-class classification, you need to set the class label for which\\nthe PDPs should be created via the target argument:\\nfrom sklearn.datasets import load_iris\\niris = load_iris()\\nmc_clf = GradientBoostingClassifier(n_estimators=10,\\n...     max_depth=1).fit(iris.data, iris.target)\\nfeatures = [3, 2, (3, 2)]\\nPartialDependenceDisplay.from_estimator(mc_clf, X, features, target=0)\\n<...>\\nThe same parameter target is used to specify the target in multi-output\\nregression settings.\\nIf you need the raw values of the partial dependence function rather than\\nthe plots, you can use the\\nsklearn.inspection.partial_dependence function:\\nfrom sklearn.inspection import partial_dependence\\nresults = partial_dependence(clf, X, [0])\\nresults[\"average\"]\\narray([[ 2.466...,  2.466..., ...\\nresults[\"values\"]\\n[array([-1.624..., -1.592..., ...\\nThe values at which the partial dependence should be evaluated are directly\\ngenerated from X. For 2-way partial dependence, a 2D-grid of values is\\ngenerated. The values field returned by\\nsklearn.inspection.partial_dependence gives the actual values\\nused in the grid for each input feature of interest. They also correspond to\\nthe axis of the plots.\\n- 4.1.2. Individual conditional expectation (ICE) plot¶\\nSimilar to a PDP, an individual conditional expectation (ICE) plot\\nshows the dependence between the target function and an input feature of\\ninterest. However, unlike a PDP, which shows the average effect of the input\\nfeature, an ICE plot visualizes the dependence of the prediction on a\\nfeature for each sample separately with one line per sample.\\nDue to the limits of human perception, only one input feature of interest is\\nsupported for ICE plots.\\nThe figures below show two ICE plots for the bike sharing dataset,\\nwith a HistGradientBoostingRegressor:.\\nThe figures plot the corresponding PD line overlaid on ICE lines.\\nWhile the PDPs are good at showing the average effect of the target features,\\nthey can obscure a heterogeneous relationship created by interactions.\\nWhen interactions are present the ICE plot will provide many more insights.\\nFor example, we see that the ICE for the temperature feature gives us some\\nadditional information: Some of the ICE lines are flat while some others\\nshows a decrease of the dependence for temperature above 35 degrees Celsius.\\nWe observe a similar pattern for the humidity feature: some of the ICE\\nlines show a sharp decrease when the humidity is above 80%.\\nThe sklearn.inspection module’s PartialDependenceDisplay.from_estimator\\nconvenience function can be used to create ICE plots by setting\\nkind=\\'individual\\'. In the example below, we show how to create a grid of\\nICE plots:\\nfrom sklearn.datasets import make_hastie_10_2\\nfrom sklearn.ensemble import GradientBoostingClassifier\\nfrom sklearn.inspection import PartialDependenceDisplay\\nX, y = make_hastie_10_2(random_state=0)\\nclf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,\\n...     max_depth=1, random_state=0).fit(X, y)\\nfeatures = [0, 1]\\nPartialDependenceDisplay.from_estimator(clf, X, features,\\n...     kind=\\'individual\\')\\n<...>\\nIn ICE plots it might not be easy to see the average effect of the input\\nfeature of interest. Hence, it is recommended to use ICE plots alongside\\nPDPs. They can be plotted together with\\nkind=\\'both\\'.\\nPartialDependenceDisplay.from_estimator(clf, X, features,\\n...     kind=\\'both\\')\\n<...>\\nIf there are too many lines in an ICE plot, it can be difficult to see\\ndifferences between individual samples and interpret the model. Centering the\\nICE at the first value on the x-axis, produces centered Individual Conditional\\nExpectation (cICE) plots [G2015]. This puts emphasis on the divergence of\\nindividual conditional expectations from the mean line, thus making it easier\\nto explore heterogeneous relationships. cICE plots can be plotted by setting\\ncentered=True:\\nPartialDependenceDisplay.from_estimator(clf, X, features,\\n...     kind=\\'both\\', centered=True)\\n<...>\\n- 4.1.3. Mathematical Definition¶\\nLet (X_S) be the set of input features of interest (i.e. the features\\nparameter) and let (X_C) be its complement.\\nThe partial dependence of the response (f) at a point (x_S) is\\ndefined as:\\n[\\\\begin{split}pd_{X_S}(x_S) &\\\\overset{def}{=} \\\\mathbb{E}{X_C}\\\\left[ f(x_S, X_C) \\\\right]\\\\\\n&= \\\\int f(x_S, x_C) p(x_C) dx_C,\\\\end{split}]\\nwhere (f(x_S, x_C)) is the response function (predict,\\npredict_proba or decision_function) for a given sample whose\\nvalues are defined by (x_S) for the features in (X_S), and by\\n(x_C) for the features in (X_C). Note that (x_S) and\\n(x_C) may be tuples.\\nComputing this integral for various values of (x_S) produces a PDP plot\\nas above. An ICE line is defined as a single (f(x{S}, x_{C}^{(i)}))\\nevaluated at (x_{S}).\\n- 4.1.4. Computation methods¶\\nThere are two main methods to approximate the integral above, namely the\\n‘brute’ and ‘recursion’ methods. The method parameter controls which method\\nto use.\\nThe ‘brute’ method is a generic method that works with any estimator. Note that\\ncomputing ICE plots is only supported with the ‘brute’ method. It\\napproximates the above integral by computing an average over the data X:\\n[pd_{X_S}(x_S) \\\\approx \\\\frac{1}{n_\\\\text{samples}} \\\\sum_{i=1}^n f(x_S, x_C^{(i)}),]\\nwhere (x_C^{(i)}) is the value of the i-th sample for the features in\\n(X_C). For each value of (x_S), this method requires a full pass\\nover the dataset X which is computationally intensive.\\nEach of the (f(x_{S}, x_{C}^{(i)})) corresponds to one ICE line evaluated\\nat (x_{S}). Computing this for multiple values of (x_{S}), one\\nobtains a full ICE line. As one can see, the average of the ICE lines\\ncorrespond to the partial dependence line.\\nThe ‘recursion’ method is faster than the ‘brute’ method, but it is only\\nsupported for PDP plots by some tree-based estimators. It is computed as\\nfollows. For a given point (x_S), a weighted tree traversal is performed:\\nif a split node involves an input feature of interest, the corresponding left\\nor right branch is followed; otherwise both branches are followed, each branch\\nbeing weighted by the fraction of training samples that entered that branch.\\nFinally, the partial dependence is given by a weighted average of all the\\nvisited leaves values.\\nWith the ‘brute’ method, the parameter X is used both for generating the\\ngrid of values (x_S) and the complement feature values (x_C).\\nHowever with the ‘recursion’ method, X is only used for the grid values:\\nimplicitly, the (x_C) values are those of the training data.\\nBy default, the ‘recursion’ method is used for plotting PDPs on tree-based\\nestimators that support it, and ‘brute’ is used for the rest.\\nNote\\nWhile both methods should be close in general, they might differ in some\\nspecific settings. The ‘brute’ method assumes the existence of the\\ndata points ((x_S, x_C^{(i)})). When the features are correlated,\\nsuch artificial samples may have a very low probability mass. The ‘brute’\\nand ‘recursion’ methods will likely disagree regarding the value of the\\npartial dependence, because they will treat these unlikely\\nsamples differently. Remember, however, that the primary assumption for\\ninterpreting PDPs is that the features should be independent.\\nExamples:\\nPartial Dependence and Individual Conditional Expectation Plots\\nFootnotes\\n[1]\\nFor classification, the target response may be the probability of a\\nclass (the positive class for binary classification), or the decision\\nfunction.\\nReferences\\n[H2009]\\nT. Hastie, R. Tibshirani and J. Friedman,\\nThe Elements of Statistical Learning,\\nSecond Edition, Section 10.13.2, Springer, 2009.\\n[M2019]\\nC. Molnar,\\nInterpretable Machine Learning,\\nSection 5.1, 2019.\\n[G2015]\\n(1,2)\\nA. Goldstein, A. Kapelner, J. Bleich, and E. Pitkin,\\n“Peeking Inside the Black Box: Visualizing Statistical\\nLearning With Plots of Individual Conditional Expectation”\\nJournal of Computational and Graphical Statistics,\\n24(1): 44-65, Springer, 2015.\\n\\n4.2. Permutation feature importance — scikit-learn 1.4.2 documentation\\n\\nInstall\\nUser Guide\\nAPI\\nExamples\\nCommunity\\nGetting Started\\nTutorial\\nWhat\\'s new\\nGlossary\\nDevelopment\\nFAQ\\nSupport\\nRelated packages\\nRoadmap\\nGovernance\\nAbout us\\nGitHub\\nOther Versions and Download\\nMore\\nGetting Started\\nTutorial\\nWhat\\'s new\\nGlossary\\nDevelopment\\nFAQ\\nSupport\\nRelated packages\\nRoadmap\\nGovernance\\nAbout us\\nGitHub\\nOther Versions and Download\\nToggle Menu\\nPrevUp\\nNext\\nscikit-learn 1.4.2\\nOther versions\\nPlease cite us if you use the software.\\n\\n4.2. Permutation feature importance\\n\\n4.2.1. Outline of the permutation importance algorithm\\n\\n4.2.2. Relation to impurity-based importance in trees\\n\\n4.2.3. Misleading values on strongly correlated features\\n\\n4.2. Permutation feature importance¶\\n\\nPermutation feature importance is a model inspection technique that measures the\\ncontribution of each feature to a fitted model’s statistical performance\\non a given tabular dataset. This technique is particularly useful for non-linear\\nor opaque estimators, and involves randomly shuffling the values of a\\nsingle feature and observing the resulting degradation of the model’s score\\n[1]. By breaking the relationship between the feature and the target, we\\ndetermine how much the model relies on such particular feature.\\nIn the following figures, we observe the effect of permuting features on the correlation\\nbetween the feature and the target and consequently on the model statistical\\nperformance.\\nOn the top figure, we observe that permuting a predictive feature breaks the\\ncorrelation between the feature and the target, and consequently the model\\nstatistical performance decreases. On the bottom figure, we observe that permuting\\na non-predictive feature does not significantly degrade the model statistical performance.\\nOne key advantage of permutation feature importance is that it is\\nmodel-agnostic, i.e. it can be applied to any fitted estimator. Moreover, it can\\nbe calculated multiple times with different permutations of the feature, further\\nproviding a measure of the variance in the estimated feature importances for the\\nspecific trained model.\\nThe figure below shows the permutation feature importance of a\\nRandomForestClassifier trained on an augmented\\nversion of the titanic dataset that contains a random_cat and a random_num\\nfeatures, i.e. a categrical and a numerical feature that are not correlated in\\nany way with the target variable:\\nWarning\\nFeatures that are deemed of low importance for a bad model (low\\ncross-validation score) could be very important for a good model.\\nTherefore it is always important to evaluate the predictive power of a model\\nusing a held-out set (or better with cross-validation) prior to computing\\nimportances. Permutation importance does not reflect to the intrinsic\\npredictive value of a feature by itself but how important this feature is\\nfor a particular model.\\nThe permutation_importance function calculates the feature importance\\nof estimators for a given dataset. The n_repeats parameter sets the\\nnumber of times a feature is randomly shuffled and returns a sample of feature\\nimportances.\\nLet’s consider the following trained regression model:\\n\\nfrom sklearn.datasets import load_diabetes\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.linear_model import Ridge\\ndiabetes = load_diabetes()\\nX_train, X_val, y_train, y_val = train_test_split(\\n...     diabetes.data, diabetes.target, random_state=0)\\n...\\nmodel = Ridge(alpha=1e-2).fit(X_train, y_train)\\nmodel.score(X_val, y_val)\\n0.356...\\nIts validation performance, measured via the (R^2) score, is\\nsignificantly larger than the chance level. This makes it possible to use the\\npermutation_importance function to probe which features are most\\npredictive:\\nfrom sklearn.inspection import permutation_importance\\nr = permutation_importance(model, X_val, y_val,\\n...                            n_repeats=30,\\n...                            random_state=0)\\n...\\nfor i in r.importances_mean.argsort()[::-1]:\\n...     if r.importances_mean[i] - 2 * r.importances_std[i] > 0:\\n...         print(f\"{diabetes.feature_names[i]:<8}\"\\n...               f\"{r.importances_mean[i]:.3f}\"\\n...               f\" +/- {r.importances_std[i]:.3f}\")\\n...\\ns5      0.204 +/- 0.050\\nbmi     0.176 +/- 0.048\\nbp      0.088 +/- 0.033\\nsex     0.056 +/- 0.023\\nNote that the importance values for the top features represent a large\\nfraction of the reference score of 0.356.\\nPermutation importances can be computed either on the training set or on a\\nheld-out testing or validation set. Using a held-out set makes it possible to\\nhighlight which features contribute the most to the generalization power of the\\ninspected model. Features that are important on the training set but not on the\\nheld-out set might cause the model to overfit.\\nThe permutation feature importance depends on the score function that is\\nspecified with the scoring argument. This argument accepts multiple scorers,\\nwhich is more computationally efficient than sequentially calling\\npermutation_importance several times with a different scorer, as it\\nreuses model predictions.\\nExample of permutation feature importance using multiple scorers\\nClick for more details\\n¶\\nIn the example below we use a list of metrics, but more input formats are\\npossible, as documented in Using multiple metric evaluation.\\nscoring = [\\'r2\\', \\'neg_mean_absolute_percentage_error\\', \\'neg_mean_squared_error\\']\\nr_multi = permutation_importance(\\n...     model, X_val, y_val, n_repeats=30, random_state=0, scoring=scoring)\\n...\\nfor metric in r_multi:\\n...     print(f\"{metric}\")\\n...     r = r_multi[metric]\\n...     for i in r.importances_mean.argsort()[::-1]:\\n...         if r.importances_mean[i] - 2 * r.importances_std[i] > 0:\\n...             print(f\"    {diabetes.feature_names[i]:<8}\"\\n...                   f\"{r.importances_mean[i]:.3f}\"\\n...                   f\" +/- {r.importances_std[i]:.3f}\")\\n...\\nr2\\ns5      0.204 +/- 0.050\\nbmi     0.176 +/- 0.048\\nbp      0.088 +/- 0.033\\nsex     0.056 +/- 0.023\\nneg_mean_absolute_percentage_error\\ns5      0.081 +/- 0.020\\nbmi     0.064 +/- 0.015\\nbp      0.029 +/- 0.010\\nneg_mean_squared_error\\ns5      1013.866 +/- 246.445\\nbmi     872.726 +/- 240.298\\nbp      438.663 +/- 163.022\\nsex     277.376 +/- 115.123\\nThe ranking of the features is approximately the same for different metrics even\\nif the scales of the importance values are very different. However, this is not\\nguaranteed and different metrics might lead to significantly different feature\\nimportances, in particular for models trained for imbalanced classification problems,\\nfor which the choice of the classification metric can be critical.\\n- 4.2.1. Outline of the permutation importance algorithm¶\\nInputs: fitted predictive model (m), tabular dataset (training or\\nvalidation) (D).\\nCompute the reference score (s) of the model (m) on data\\n(D) (for instance the accuracy for a classifier or the (R^2) for\\na regressor).\\nFor each feature (j) (column of (D)):\\nFor each repetition (k) in ({1, ..., K}):\\nRandomly shuffle column (j) of dataset (D) to generate a\\ncorrupted version of the data named (\\\\tilde{D}{k,j}).\\nCompute the score (s{k,j}) of model (m) on corrupted data\\n(\\\\tilde{D}{k,j}).\\nCompute importance (i_j) for feature (f_j) defined as:\\n[i_j = s - \\\\frac{1}{K} \\\\sum{k=1}^{K} s_{k,j}]\\n- 4.2.2. Relation to impurity-based importance in trees¶\\nTree-based models provide an alternative measure of feature importances\\nbased on the mean decrease in impurity\\n(MDI). Impurity is quantified by the splitting criterion of the decision trees\\n(Gini, Log Loss or Mean Squared Error). However, this method can give high\\nimportance to features that may not be predictive on unseen data when the model\\nis overfitting. Permutation-based feature importance, on the other hand, avoids\\nthis issue, since it can be computed on unseen data.\\nFurthermore, impurity-based feature importance for trees are strongly\\nbiased and favor high cardinality features (typically numerical features)\\nover low cardinality features such as binary features or categorical variables\\nwith a small number of possible categories.\\nPermutation-based feature importances do not exhibit such a bias. Additionally,\\nthe permutation feature importance may be computed with any performance metric\\non the model predictions and can be used to analyze any model class (not just\\ntree-based models).\\nThe following example highlights the limitations of impurity-based feature\\nimportance in contrast to permutation-based feature importance:\\nPermutation Importance vs Random Forest Feature Importance (MDI).\\n- 4.2.3. Misleading values on strongly correlated features¶\\nWhen two features are correlated and one of the features is permuted, the model\\nstill has access to the latter through its correlated feature. This results in a\\nlower reported importance value for both features, though they might actually\\nbe important.\\nThe figure below shows the permutation feature importance of a\\nRandomForestClassifier trained using the\\nBreast cancer wisconsin (diagnostic) dataset, which contains strongly correlated features. A\\nnaive interpretation would suggest that all features are unimportant:\\nOne way to handle the issue is to cluster features that are correlated and only\\nkeep one feature from each cluster.\\nFor more details on such strategy, see the example\\nPermutation Importance with Multicollinear or Correlated Features.\\nExamples:\\nPermutation Importance vs Random Forest Feature Importance (MDI)\\nPermutation Importance with Multicollinear or Correlated Features\\nReferences:\\n[1]\\nL. Breiman, “Random Forests”,\\nMachine Learning, 45(1), 5-32, 2001.\\n\\n5. Visualizations — scikit-learn 1.4.2 documentation\\n\\nInstall\\nUser Guide\\nAPI\\nExamples\\nCommunity\\nGetting Started\\nTutorial\\nWhat\\'s new\\nGlossary\\nDevelopment\\nFAQ\\nSupport\\nRelated packages\\nRoadmap\\nGovernance\\nAbout us\\nGitHub\\nOther Versions and Download\\nMore\\nGetting Started\\nTutorial\\nWhat\\'s new\\nGlossary\\nDevelopment\\nFAQ\\nSupport\\nRelated packages\\nRoadmap\\nGovernance\\nAbout us\\nGitHub\\nOther Versions and Download\\nToggle Menu\\nPrevUp\\nNext\\nscikit-learn 1.4.2\\nOther versions\\nPlease cite us if you use the software.\\nUser Guide\\n\\n1. Supervised learning\\n\\n2. Unsupervised learning\\n\\n3. Model selection and evaluation\\n\\n4. Inspection\\n\\n5. Visualizations\\n\\n6. Dataset transformations\\n\\n7. Dataset loading utilities\\n\\n8. Computing with scikit-learn\\n\\n9. Model persistence\\n\\n10. Common pitfalls and recommended practices\\n\\n11. Dispatching\\n\\n5. Visualizations¶\\n\\nScikit-learn defines a simple API for creating visualizations for machine\\nlearning. The key feature of this API is to allow for quick plotting and\\nvisual adjustments without recalculation. We provide Display classes that\\nexpose two methods for creating plots: from_estimator and\\nfrom_predictions. The from_estimator method will take a fitted estimator\\nand some data (X and y) and create a Display object. Sometimes, we would\\nlike to only compute the predictions once and one should use from_predictions\\ninstead. In the following example, we plot a ROC curve for a fitted support\\nvector machine:\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.svm import SVC\\nfrom sklearn.metrics import RocCurveDisplay\\nfrom sklearn.datasets import load_wine\\nX, y = load_wine(return_X_y=True)\\ny = y == 2  # make binary\\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\\nsvc = SVC(random_state=42)\\nsvc.fit(X_train, y_train)\\nsvc_disp = RocCurveDisplay.from_estimator(svc, X_test, y_test)\\nThe returned svc_disp object allows us to continue using the already computed\\nROC curve for SVC in future plots. In this case, the svc_disp is a\\nRocCurveDisplay that stores the computed values as\\nattributes called roc_auc, fpr, and tpr. Be aware that we could get\\nthe predictions from the support vector machine and then use from_predictions\\ninstead of from_estimator. Next, we train a random forest classifier and plot\\nthe previously computed roc curve again by using the plot method of the\\nDisplay object.\\nimport matplotlib.pyplot as plt\\nfrom sklearn.ensemble import RandomForestClassifier\\nrfc = RandomForestClassifier(n_estimators=10, random_state=42)\\nrfc.fit(X_train, y_train)\\nax = plt.gca()\\nrfc_disp = RocCurveDisplay.from_estimator(rfc, X_test, y_test, ax=ax, alpha=0.8)\\nsvc_disp.plot(ax=ax, alpha=0.8)\\nNotice that we pass alpha=0.8 to the plot functions to adjust the alpha\\nvalues of the curves.\\nExamples:\\nROC Curve with Visualization API\\nAdvanced Plotting With Partial Dependence\\nVisualizations with Display Objects\\nComparison of Calibration of Classifiers\\n\\n5.1. Available Plotting Utilities¶\\n\\n5.1.1. Display Objects¶\\ncalibration.CalibrationDisplay(prob_true, ...)\\nCalibration curve (also known as reliability diagram) visualization.\\ninspection.PartialDependenceDisplay(...[, ...])\\nPartial Dependence Plot (PDP).\\ninspection.DecisionBoundaryDisplay(, xx0, ...)\\nDecisions boundary visualization.\\nmetrics.ConfusionMatrixDisplay(...[, ...])\\nConfusion Matrix visualization.\\nmetrics.DetCurveDisplay(, fpr, fnr[, ...])\\nDET curve visualization.\\nmetrics.PrecisionRecallDisplay(precision, ...)\\nPrecision Recall visualization.\\nmetrics.PredictionErrorDisplay(, y_true, y_pred)\\nVisualization of the prediction error of a regression model.\\nmetrics.RocCurveDisplay(, fpr, tpr[, ...])\\nROC Curve visualization.\\nmodel_selection.LearningCurveDisplay(, ...)\\nLearning Curve visualization.\\nmodel_selection.ValidationCurveDisplay(, ...)\\nValidation Curve visualization.\\n\\n6.1. Pipelines and composite estimators — scikit-learn 1.4.2 documentation\\n\\nInstall\\nUser Guide\\nAPI\\nExamples\\nCommunity\\nGetting Started\\nTutorial\\nWhat\\'s new\\nGlossary\\nDevelopment\\nFAQ\\nSupport\\nRelated packages\\nRoadmap\\nGovernance\\nAbout us\\nGitHub\\nOther Versions and Download\\nMore\\nGetting Started\\nTutorial\\nWhat\\'s new\\nGlossary\\nDevelopment\\nFAQ\\nSupport\\nRelated packages\\nRoadmap\\nGovernance\\nAbout us\\nGitHub\\nOther Versions and Download\\nToggle Menu\\nPrevUp\\nNext\\nscikit-learn 1.4.2\\nOther versions\\nPlease cite us if you use the software.\\n\\n6.1. Pipelines and composite estimators\\n\\n6.1.1. Pipeline: chaining estimators\\n6.1.1.1. Usage\\n6.1.1.1.1. Build a pipeline\\n6.1.1.1.2. Access pipeline steps\\n6.1.1.1.3. Tracking feature names in a pipeline\\n6.1.1.1.4. Access to nested parameters\\n6.1.1.2. Caching transformers: avoid repeated computation\\n\\n6.1.2. Transforming target in regression\\n\\n6.1.3. FeatureUnion: composite feature spaces\\n6.1.3.1. Usage\\n\\n6.1.4. ColumnTransformer for heterogeneous data\\n\\n6.1.5. Visualizing Composite Estimators\\n\\n6.1. Pipelines and composite estimators¶\\n\\nTo build a composite estimator, transformers are usually combined with other\\ntransformers or with predictors (such as classifiers or regressors).\\nThe most common tool used for composing estimators is a Pipeline. Pipelines require all steps except the last to be a\\ntransformer. The last step can be anything, a transformer, a\\npredictor, or a clustering estimator which might have or not have a\\n.predict(...) method. A pipeline exposes all methods provided by the last\\nestimator: if the last step provides a transform method, then the pipeline\\nwould have a transform method and behave like a transformer. If the last step\\nprovides a predict method, then the pipeline would expose that method, and\\ngiven a data X, use all steps except the last to transform the data,\\nand then give that transformed data to the predict method of the last step of\\nthe pipeline. The class Pipeline is often used in combination with\\nColumnTransformer or\\nFeatureUnion which concatenate the output of transformers\\ninto a composite feature space.\\nTransformedTargetRegressor\\ndeals with transforming the target (i.e. log-transform y).\\n- 6.1.1. Pipeline: chaining estimators¶\\nPipeline can be used to chain multiple estimators\\ninto one. This is useful as there is often a fixed sequence\\nof steps in processing the data, for example feature selection, normalization\\nand classification. Pipeline serves multiple purposes here:\\nConvenience and encapsulationYou only have to call fit and predict once on your\\ndata to fit a whole sequence of estimators.\\nJoint parameter selectionYou can grid search\\nover parameters of all estimators in the pipeline at once.\\nSafetyPipelines help avoid leaking statistics from your test data into the\\ntrained model in cross-validation, by ensuring that the same samples are\\nused to train the transformers and predictors.\\nAll estimators in a pipeline, except the last one, must be transformers\\n(i.e. must have a transform method).\\nThe last estimator may be any type (transformer, classifier, etc.).\\nNote\\nCalling fit on the pipeline is the same as calling fit on\\neach estimator in turn, transform the input and pass it on to the next step.\\nThe pipeline has all the methods that the last estimator in the pipeline has,\\ni.e. if the last estimator is a classifier, the Pipeline can be used\\nas a classifier. If the last estimator is a transformer, again, so is the\\npipeline.\\n6.1.1.1. Usage¶\\n6.1.1.1.1. Build a pipeline¶\\nThe Pipeline is built using a list of (key, value) pairs, where\\nthe key is a string containing the name you want to give this step and value\\nis an estimator object:\\n\\nfrom sklearn.pipeline import Pipeline\\nfrom sklearn.svm import SVC\\nfrom sklearn.decomposition import PCA\\nestimators = [(\\'reduce_dim\\', PCA()), (\\'clf\\', SVC())]\\npipe = Pipeline(estimators)\\npipe\\nPipeline(steps=[(\\'reduce_dim\\', PCA()), (\\'clf\\', SVC())])\\nShorthand version using :func:make_pipeline\\nClick for more details\\n¶\\nThe utility function make_pipeline is a shorthand\\nfor constructing pipelines;\\nit takes a variable number of estimators and returns a pipeline,\\nfilling in the names automatically:\\nfrom sklearn.pipeline import make_pipeline\\nmake_pipeline(PCA(), SVC())\\nPipeline(steps=[(\\'pca\\', PCA()), (\\'svc\\', SVC())])\\n6.1.1.1.2. Access pipeline steps¶\\nThe estimators of a pipeline are stored as a list in the steps attribute.\\nA sub-pipeline can be extracted using the slicing notation commonly used\\nfor Python Sequences such as lists or strings (although only a step of 1 is\\npermitted). This is convenient for performing only some of the transformations\\n(or their inverse):\\npipe[:1]\\nPipeline(steps=[(\\'reduce_dim\\', PCA())])\\npipe[-1:]\\nPipeline(steps=[(\\'clf\\', SVC())])\\nAccessing a step by name or position\\nClick for more details\\n¶\\nA specific step can also be accessed by index or name by indexing (with [idx]) the\\npipeline:\\npipe.steps[0]\\n(\\'reduce_dim\\', PCA())\\npipe[0]\\nPCA()\\npipe[\\'reduce_dim\\']\\nPCA()\\nPipeline’s named_steps attribute allows accessing steps by name with tab\\ncompletion in interactive environments:\\npipe.named_steps.reduce_dim is pipe[\\'reduce_dim\\']\\nTrue\\n6.1.1.1.3. Tracking feature names in a pipeline¶\\nTo enable model inspection, Pipeline has a\\nget_feature_names_out() method, just like all transformers. You can use\\npipeline slicing to get the feature names going into each step:\\nfrom sklearn.datasets import load_iris\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.feature_selection import SelectKBest\\niris = load_iris()\\npipe = Pipeline(steps=[\\n...    (\\'select\\', SelectKBest(k=2)),\\n...    (\\'clf\\', LogisticRegression())])\\npipe.fit(iris.data, iris.target)\\nPipeline(steps=[(\\'select\\', SelectKBest(...)), (\\'clf\\', LogisticRegression(...))])\\npipe[:-1].get_feature_names_out()\\narray([\\'x2\\', \\'x3\\'], ...)\\nCustomize feature names\\nClick for more details\\n¶\\nYou can also provide custom feature names for the input data using\\nget_feature_names_out:\\npipe[:-1].get_feature_names_out(iris.feature_names)\\narray([\\'petal length (cm)\\', \\'petal width (cm)\\'], ...)\\n6.1.1.1.4. Access to nested parameters¶\\nIt is common to adjust the parameters of an estimator within a pipeline. This parameter\\nis therefore nested because it belongs to a particular sub-step. Parameters of the\\nestimators in the pipeline are accessible using the __\\nsyntax:\\npipe = Pipeline(steps=[(\"reduce_dim\", PCA()), (\"clf\", SVC())])\\npipe.set_params(clf__C=10)\\nPipeline(steps=[(\\'reduce_dim\\', PCA()), (\\'clf\\', SVC(C=10))])\\nWhen does it matter?\\nClick for more details\\n¶\\nThis is particularly important for doing grid searches:\\nfrom sklearn.model_selection import GridSearchCV\\nparam_grid = dict(reduce_dim__n_components=[2, 5, 10],\\n...                   clf__C=[0.1, 10, 100])\\ngrid_search = GridSearchCV(pipe, param_grid=param_grid)\\nIndividual steps may also be replaced as parameters, and non-final steps may be\\nignored by setting them to \\'passthrough\\':\\nparam_grid = dict(reduce_dim=[\\'passthrough\\', PCA(5), PCA(10)],\\n...                   clf=[SVC(), LogisticRegression()],\\n...                   clf__C=[0.1, 10, 100])\\ngrid_search = GridSearchCV(pipe, param_grid=param_grid)\\nSee Also:\\nComposite estimators and parameter spaces\\nExamples:\\nPipeline ANOVA SVM\\nSample pipeline for text feature extraction and evaluation\\nPipelining: chaining a PCA and a logistic regression\\nExplicit feature map approximation for RBF kernels\\nSVM-Anova: SVM with univariate feature selection\\nSelecting dimensionality reduction with Pipeline and GridSearchCV\\nDisplaying Pipelines\\n6.1.1.2. Caching transformers: avoid repeated computation¶\\nFitting transformers may be computationally expensive. With its\\nmemory parameter set, Pipeline will cache each transformer\\nafter calling fit.\\nThis feature is used to avoid computing the fit transformers within a pipeline\\nif the parameters and input data are identical. A typical example is the case of\\na grid search in which the transformers can be fitted only once and reused for\\neach configuration. The last step will never be cached, even if it is a transformer.\\nThe parameter memory is needed in order to cache the transformers.\\nmemory can be either a string containing the directory where to cache the\\ntransformers or a joblib.Memory\\nobject:\\nfrom tempfile import mkdtemp\\nfrom shutil import rmtree\\nfrom sklearn.decomposition import PCA\\nfrom sklearn.svm import SVC\\nfrom sklearn.pipeline import Pipeline\\nestimators = [(\\'reduce_dim\\', PCA()), (\\'clf\\', SVC())]\\ncachedir = mkdtemp()\\npipe = Pipeline(estimators, memory=cachedir)\\npipe\\nPipeline(memory=...,\\nsteps=[(\\'reduce_dim\\', PCA()), (\\'clf\\', SVC())])\\n\\nClear the cache directory when you don\\'t need it anymore\\n\\nrmtree(cachedir)\\nWarning: Side effect of caching transformers\\nClick for more details\\n¶\\nUsing a Pipeline without cache enabled, it is possible to\\ninspect the original instance such as:\\nfrom sklearn.datasets import load_digits\\nX_digits, y_digits = load_digits(return_X_y=True)\\npca1 = PCA()\\nsvm1 = SVC()\\npipe = Pipeline([(\\'reduce_dim\\', pca1), (\\'clf\\', svm1)])\\npipe.fit(X_digits, y_digits)\\nPipeline(steps=[(\\'reduce_dim\\', PCA()), (\\'clf\\', SVC())])\\n\\nThe pca instance can be inspected directly\\n\\nprint(pca1.components_)\\n[[-1.77484909e-19  ... 4.07058917e-18]]\\nEnabling caching triggers a clone of the transformers before fitting.\\nTherefore, the transformer instance given to the pipeline cannot be\\ninspected directly.\\nIn following example, accessing the PCA\\ninstance pca2 will raise an AttributeError since pca2 will be an\\nunfitted transformer.\\nInstead, use the attribute named_steps to inspect estimators within\\nthe pipeline:\\ncachedir = mkdtemp()\\npca2 = PCA()\\nsvm2 = SVC()\\ncached_pipe = Pipeline([(\\'reduce_dim\\', pca2), (\\'clf\\', svm2)],\\n...                        memory=cachedir)\\ncached_pipe.fit(X_digits, y_digits)\\nPipeline(memory=...,\\nsteps=[(\\'reduce_dim\\', PCA()), (\\'clf\\', SVC())])\\nprint(cached_pipe.named_steps[\\'reduce_dim\\'].components_)\\n[[-1.77484909e-19  ... 4.07058917e-18]]\\n\\nRemove the cache directory\\n\\nrmtree(cachedir)\\nExamples:\\nSelecting dimensionality reduction with Pipeline and GridSearchCV\\n- 6.1.2. Transforming target in regression¶\\nTransformedTargetRegressor transforms the\\ntargets y before fitting a regression model. The predictions are mapped\\nback to the original space via an inverse transform. It takes as an argument\\nthe regressor that will be used for prediction, and the transformer that will\\nbe applied to the target variable:\\nimport numpy as np\\nfrom sklearn.datasets import fetch_california_housing\\nfrom sklearn.compose import TransformedTargetRegressor\\nfrom sklearn.preprocessing import QuantileTransformer\\nfrom sklearn.linear_model import LinearRegression\\nfrom sklearn.model_selection import train_test_split\\nX, y = fetch_california_housing(return_X_y=True)\\nX, y = X[:2000, :], y[:2000]  # select a subset of data\\ntransformer = QuantileTransformer(output_distribution=\\'normal\\')\\nregressor = LinearRegression()\\nregr = TransformedTargetRegressor(regressor=regressor,\\n...                                   transformer=transformer)\\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\\nregr.fit(X_train, y_train)\\nTransformedTargetRegressor(...)\\nprint(\\'R2 score: {0:.2f}\\'.format(regr.score(X_test, y_test)))\\nR2 score: 0.61\\nraw_target_regr = LinearRegression().fit(X_train, y_train)\\nprint(\\'R2 score: {0:.2f}\\'.format(raw_target_regr.score(X_test, y_test)))\\nR2 score: 0.59\\nFor simple transformations, instead of a Transformer object, a pair of\\nfunctions can be passed, defining the transformation and its inverse mapping:\\ndef func(x):\\n...     return np.log(x)\\ndef inverse_func(x):\\n...     return np.exp(x)\\nSubsequently, the object is created as:\\nregr = TransformedTargetRegressor(regressor=regressor,\\n...                                   func=func,\\n...                                   inverse_func=inverse_func)\\nregr.fit(X_train, y_train)\\nTransformedTargetRegressor(...)\\nprint(\\'R2 score: {0:.2f}\\'.format(regr.score(X_test, y_test)))\\nR2 score: 0.51\\nBy default, the provided functions are checked at each fit to be the inverse of\\neach other. However, it is possible to bypass this checking by setting\\ncheck_inverse to False:\\ndef inverse_func(x):\\n...     return x\\nregr = TransformedTargetRegressor(regressor=regressor,\\n...                                   func=func,\\n...                                   inverse_func=inverse_func,\\n...                                   check_inverse=False)\\nregr.fit(X_train, y_train)\\nTransformedTargetRegressor(...)\\nprint(\\'R2 score: {0:.2f}\\'.format(regr.score(X_test, y_test)))\\nR2 score: -1.57\\nNote\\nThe transformation can be triggered by setting either transformer or the\\npair of functions func and inverse_func. However, setting both\\noptions will raise an error.\\nExamples:\\nEffect of transforming the targets in regression model\\n- 6.1.3. FeatureUnion: composite feature spaces¶\\nFeatureUnion combines several transformer objects into a new\\ntransformer that combines their output. A FeatureUnion takes\\na list of transformer objects. During fitting, each of these\\nis fit to the data independently. The transformers are applied in parallel,\\nand the feature matrices they output are concatenated side-by-side into a\\nlarger matrix.\\nWhen you want to apply different transformations to each field of the data,\\nsee the related class ColumnTransformer\\n(see user guide).\\nFeatureUnion serves the same purposes as Pipeline -\\nconvenience and joint parameter estimation and validation.\\nFeatureUnion and Pipeline can be combined to\\ncreate complex models.\\n(A FeatureUnion has no way of checking whether two transformers\\nmight produce identical features. It only produces a union when the\\nfeature sets are disjoint, and making sure they are is the caller’s\\nresponsibility.)\\n6.1.3.1. Usage¶\\nA FeatureUnion is built using a list of (key, value) pairs,\\nwhere the key is the name you want to give to a given transformation\\n(an arbitrary string; it only serves as an identifier)\\nand value is an estimator object:\\nfrom sklearn.pipeline import FeatureUnion\\nfrom sklearn.decomposition import PCA\\nfrom sklearn.decomposition import KernelPCA\\nestimators = [(\\'linear_pca\\', PCA()), (\\'kernel_pca\\', KernelPCA())]\\ncombined = FeatureUnion(estimators)\\ncombined\\nFeatureUnion(transformer_list=[(\\'linear_pca\\', PCA()),\\n(\\'kernel_pca\\', KernelPCA())])\\nLike pipelines, feature unions have a shorthand constructor called\\nmake_union that does not require explicit naming of the components.\\nLike Pipeline, individual steps may be replaced using set_params,\\nand ignored by setting to \\'drop\\':\\ncombined.set_params(kernel_pca=\\'drop\\')\\nFeatureUnion(transformer_list=[(\\'linear_pca\\', PCA()),\\n(\\'kernel_pca\\', \\'drop\\')])\\nExamples:\\nConcatenating multiple feature extraction methods\\n- 6.1.4. ColumnTransformer for heterogeneous data¶\\nMany datasets contain features of different types, say text, floats, and dates,\\nwhere each type of feature requires separate preprocessing or feature\\nextraction steps.  Often it is easiest to preprocess data before applying\\nscikit-learn methods, for example using pandas.\\nProcessing your data before passing it to scikit-learn might be problematic for\\none of the following reasons:\\nIncorporating statistics from test data into the preprocessors makes\\ncross-validation scores unreliable (known as data leakage),\\nfor example in the case of scalers or imputing missing values.\\nYou may want to include the parameters of the preprocessors in a\\nparameter search.\\nThe ColumnTransformer helps performing different\\ntransformations for different columns of the data, within a\\nPipeline that is safe from data leakage and that can\\nbe parametrized. ColumnTransformer works on\\narrays, sparse matrices, and\\npandas DataFrames.\\nTo each column, a different transformation can be applied, such as\\npreprocessing or a specific feature extraction method:\\nimport pandas as pd\\nX = pd.DataFrame(\\n...     {\\'city\\': [\\'London\\', \\'London\\', \\'Paris\\', \\'Sallisaw\\'],\\n...      \\'title\\': [\"His Last Bow\", \"How Watson Learned the Trick\",\\n...                \"A Moveable Feast\", \"The Grapes of Wrath\"],\\n...      \\'expert_rating\\': [5, 3, 4, 5],\\n...      \\'user_rating\\': [4, 5, 4, 3]})\\nFor this data, we might want to encode the \\'city\\' column as a categorical\\nvariable using OneHotEncoder but apply a\\nCountVectorizer to the \\'title\\' column.\\nAs we might use multiple feature extraction methods on the same column, we give\\neach transformer a unique name, say \\'city_category\\' and \\'title_bow\\'.\\nBy default, the remaining rating columns are ignored (remainder=\\'drop\\'):\\nfrom sklearn.compose import ColumnTransformer\\nfrom sklearn.feature_extraction.text import CountVectorizer\\nfrom sklearn.preprocessing import OneHotEncoder\\ncolumn_trans = ColumnTransformer(\\n...     [(\\'categories\\', OneHotEncoder(dtype=\\'int\\'), [\\'city\\']),\\n...      (\\'title_bow\\', CountVectorizer(), \\'title\\')],\\n...     remainder=\\'drop\\', verbose_feature_names_out=False)\\ncolumn_trans.fit(X)\\nColumnTransformer(transformers=[(\\'categories\\', OneHotEncoder(dtype=\\'int\\'),\\n[\\'city\\']),\\n(\\'title_bow\\', CountVectorizer(), \\'title\\')],\\nverbose_feature_names_out=False)\\ncolumn_trans.get_feature_names_out()\\narray([\\'city_London\\', \\'city_Paris\\', \\'city_Sallisaw\\', \\'bow\\', \\'feast\\',\\n\\'grapes\\', \\'his\\', \\'how\\', \\'last\\', \\'learned\\', \\'moveable\\', \\'of\\', \\'the\\',\\n\\'trick\\', \\'watson\\', \\'wrath\\'], ...)\\ncolumn_trans.transform(X).toarray()\\narray([[1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0],\\n[1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0],\\n[0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\\n[0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1]]...)\\nIn the above example, the\\nCountVectorizer expects a 1D array as\\ninput and therefore the columns were specified as a string (\\'title\\').\\nHowever, OneHotEncoder\\nas most of other transformers expects 2D data, therefore in that case you need\\nto specify the column as a list of strings ([\\'city\\']).\\nApart from a scalar or a single item list, the column selection can be specified\\nas a list of multiple items, an integer array, a slice, a boolean mask, or\\nwith a make_column_selector. The\\nmake_column_selector is used to select columns based\\non data type or column name:\\nfrom sklearn.preprocessing import StandardScaler\\nfrom sklearn.compose import make_column_selector\\nct = ColumnTransformer([\\n...       (\\'scale\\', StandardScaler(),\\n...       make_column_selector(dtype_include=np.number)),\\n...       (\\'onehot\\',\\n...       OneHotEncoder(),\\n...       make_column_selector(pattern=\\'city\\', dtype_include=object))])\\nct.fit_transform(X)\\narray([[ 0.904...,  0.      ,  1. ,  0. ,  0. ],\\n[-1.507...,  1.414...,  1. ,  0. ,  0. ],\\n[-0.301...,  0.      ,  0. ,  1. ,  0. ],\\n[ 0.904..., -1.414...,  0. ,  0. ,  1. ]])\\nStrings can reference columns if the input is a DataFrame, integers are always\\ninterpreted as the positional columns.\\nWe can keep the remaining rating columns by setting\\nremainder=\\'passthrough\\'. The values are appended to the end of the\\ntransformation:\\ncolumn_trans = ColumnTransformer(\\n...     [(\\'city_category\\', OneHotEncoder(dtype=\\'int\\'),[\\'city\\']),\\n...      (\\'title_bow\\', CountVectorizer(), \\'title\\')],\\n...     remainder=\\'passthrough\\')\\ncolumn_trans.fit_transform(X)\\narray([[1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 5, 4],\\n[1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 3, 5],\\n[0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 4, 4],\\n[0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 5, 3]]...)\\nThe remainder parameter can be set to an estimator to transform the\\nremaining rating columns. The transformed values are appended to the end of\\nthe transformation:\\nfrom sklearn.preprocessing import MinMaxScaler\\ncolumn_trans = ColumnTransformer(\\n...     [(\\'city_category\\', OneHotEncoder(), [\\'city\\']),\\n...      (\\'title_bow\\', CountVectorizer(), \\'title\\')],\\n...     remainder=MinMaxScaler())\\ncolumn_trans.fit_transform(X)[:, -2:]\\narray([[1. , 0.5],\\n[0. , 1. ],\\n[0.5, 0.5],\\n[1. , 0. ]])\\nThe make_column_transformer function is available\\nto more easily create a ColumnTransformer object.\\nSpecifically, the names will be given automatically. The equivalent for the\\nabove example would be:\\nfrom sklearn.compose import make_column_transformer\\ncolumn_trans = make_column_transformer(\\n...     (OneHotEncoder(), [\\'city\\']),\\n...     (CountVectorizer(), \\'title\\'),\\n...     remainder=MinMaxScaler())\\ncolumn_trans\\nColumnTransformer(remainder=MinMaxScaler(),\\ntransformers=[(\\'onehotencoder\\', OneHotEncoder(), [\\'city\\']),\\n(\\'countvectorizer\\', CountVectorizer(),\\n\\'title\\')])\\nIf ColumnTransformer is fitted with a dataframe\\nand the dataframe only has string column names, then transforming a dataframe\\nwill use the column names to select the columns:\\nct = ColumnTransformer(\\n...          [(\"scale\", StandardScaler(), [\"expert_rating\"])]).fit(X)\\nX_new = pd.DataFrame({\"expert_rating\": [5, 6, 1],\\n...                       \"ignored_new_col\": [1.2, 0.3, -0.1]})\\nct.transform(X_new)\\narray([[ 0.9...],\\n[ 2.1...],\\n[-3.9...]])\\n- 6.1.5. Visualizing Composite Estimators¶\\nEstimators are displayed with an HTML representation when shown in a\\njupyter notebook. This is useful to diagnose or visualize a Pipeline with\\nmany estimators. This visualization is activated by default:\\ncolumn_trans\\nIt can be deactivated by setting the display option in set_config\\nto ‘text’:\\nfrom sklearn import set_config\\nset_config(display=\\'text\\')\\n\\ndisplays text representation in a jupyter context\\n\\ncolumn_trans\\nAn example of the HTML output can be seen in the\\nHTML representation of Pipeline section of\\nColumn Transformer with Mixed Types.\\nAs an alternative, the HTML can be written to a file using\\nestimator_html_repr:\\nfrom sklearn.utils import estimator_html_repr\\nwith open(\\'my_estimator.html\\', \\'w\\') as f:\\n...     f.write(estimator_html_repr(clf))\\nExamples:\\nColumn Transformer with Heterogeneous Data Sources\\nColumn Transformer with Mixed Types\\n\\n6.2. Feature extraction — scikit-learn 1.4.2 documentation\\n\\nInstall\\nUser Guide\\nAPI\\nExamples\\nCommunity\\nGetting Started\\nTutorial\\nWhat\\'s new\\nGlossary\\nDevelopment\\nFAQ\\nSupport\\nRelated packages\\nRoadmap\\nGovernance\\nAbout us\\nGitHub\\nOther Versions and Download\\nMore\\nGetting Started\\nTutorial\\nWhat\\'s new\\nGlossary\\nDevelopment\\nFAQ\\nSupport\\nRelated packages\\nRoadmap\\nGovernance\\nAbout us\\nGitHub\\nOther Versions and Download\\nToggle Menu\\nPrevUp\\nNext\\nscikit-learn 1.4.2\\nOther versions\\nPlease cite us if you use the software.\\n\\n6.2. Feature extraction\\n\\n6.2.1. Loading features from dicts\\n\\n6.2.2. Feature hashing\\n\\n6.2.3. Text feature extraction\\n6.2.3.1. The Bag of Words representation\\n6.2.3.2. Sparsity\\n6.2.3.3. Common Vectorizer usage\\n6.2.3.4. Using stop words\\n6.2.3.5. Tf–idf term weighting\\n6.2.3.6. Decoding text files\\n6.2.3.7. Applications and examples\\n6.2.3.8. Limitations of the Bag of Words representation\\n6.2.3.9. Vectorizing a large text corpus with the hashing trick\\n6.2.3.10. Customizing the vectorizer classes\\n\\n6.2.4. Image feature extraction\\n6.2.4.1. Patch extraction\\n6.2.4.2. Connectivity graph of an image\\n\\n6.2. Feature extraction¶\\n\\nThe sklearn.feature_extraction module can be used to extract\\nfeatures in a format supported by machine learning algorithms from datasets\\nconsisting of formats such as text and image.\\nNote\\nFeature extraction is very different from Feature selection:\\nthe former consists in transforming arbitrary data, such as text or\\nimages, into numerical features usable for machine learning. The latter\\nis a machine learning technique applied on these features.\\n- 6.2.1. Loading features from dicts¶\\nThe class DictVectorizer can be used to convert feature\\narrays represented as lists of standard Python dict objects to the\\nNumPy/SciPy representation used by scikit-learn estimators.\\nWhile not particularly fast to process, Python’s dict has the\\nadvantages of being convenient to use, being sparse (absent features\\nneed not be stored) and storing feature names in addition to values.\\nDictVectorizer implements what is called one-of-K or “one-hot”\\ncoding for categorical (aka nominal, discrete) features. Categorical\\nfeatures are “attribute-value” pairs where the value is restricted\\nto a list of discrete possibilities without ordering (e.g. topic\\nidentifiers, types of objects, tags, names…).\\nIn the following, “city” is a categorical attribute while “temperature”\\nis a traditional numerical feature:\\n\\nmeasurements = [\\n...     {\\'city\\': \\'Dubai\\', \\'temperature\\': 33.},\\n...     {\\'city\\': \\'London\\', \\'temperature\\': 12.},\\n...     {\\'city\\': \\'San Francisco\\', \\'temperature\\': 18.},\\n... ]\\nfrom sklearn.feature_extraction import DictVectorizer\\nvec = DictVectorizer()\\nvec.fit_transform(measurements).toarray()\\narray([[ 1.,  0.,  0., 33.],\\n[ 0.,  1.,  0., 12.],\\n[ 0.,  0.,  1., 18.]])\\nvec.get_feature_names_out()\\narray([\\'city=Dubai\\', \\'city=London\\', \\'city=San Francisco\\', \\'temperature\\'], ...)\\nDictVectorizer accepts multiple string values for one\\nfeature, like, e.g., multiple categories for a movie.\\nAssume a database classifies each movie using some categories (not mandatories)\\nand its year of release.\\nmovie_entry = [{\\'category\\': [\\'thriller\\', \\'drama\\'], \\'year\\': 2003},\\n...                {\\'category\\': [\\'animation\\', \\'family\\'], \\'year\\': 2011},\\n...                {\\'year\\': 1974}]\\nvec.fit_transform(movie_entry).toarray()\\narray([[0.000e+00, 1.000e+00, 0.000e+00, 1.000e+00, 2.003e+03],\\n[1.000e+00, 0.000e+00, 1.000e+00, 0.000e+00, 2.011e+03],\\n[0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 1.974e+03]])\\nvec.get_feature_names_out()\\narray([\\'category=animation\\', \\'category=drama\\', \\'category=family\\',\\n\\'category=thriller\\', \\'year\\'], ...)\\nvec.transform({\\'category\\': [\\'thriller\\'],\\n...                \\'unseen_feature\\': \\'3\\'}).toarray()\\narray([[0., 0., 0., 1., 0.]])\\nDictVectorizer is also a useful representation transformation\\nfor training sequence classifiers in Natural Language Processing models\\nthat typically work by extracting feature windows around a particular\\nword of interest.\\nFor example, suppose that we have a first algorithm that extracts Part of\\nSpeech (PoS) tags that we want to use as complementary tags for training\\na sequence classifier (e.g. a chunker). The following dict could be\\nsuch a window of features extracted around the word ‘sat’ in the sentence\\n‘The cat sat on the mat.’:\\npos_window = [\\n...     {\\n...         \\'word-2\\': \\'the\\',\\n...         \\'pos-2\\': \\'DT\\',\\n...         \\'word-1\\': \\'cat\\',\\n...         \\'pos-1\\': \\'NN\\',\\n...         \\'word+1\\': \\'on\\',\\n...         \\'pos+1\\': \\'PP\\',\\n...     },\\n...     # in a real application one would extract many such dictionaries\\n... ]\\nThis description can be vectorized into a sparse two-dimensional matrix\\nsuitable for feeding into a classifier (maybe after being piped into a\\nTfidfTransformer for normalization):\\nvec = DictVectorizer()\\npos_vectorized = vec.fit_transform(pos_window)\\npos_vectorized\\n<1x6 sparse matrix of type \\'<... \\'numpy.float64\\'>\\'\\nwith 6 stored elements in Compressed Sparse ... format>\\npos_vectorized.toarray()\\narray([[1., 1., 1., 1., 1., 1.]])\\nvec.get_feature_names_out()\\narray([\\'pos+1=PP\\', \\'pos-1=NN\\', \\'pos-2=DT\\', \\'word+1=on\\', \\'word-1=cat\\',\\n\\'word-2=the\\'], ...)\\nAs you can imagine, if one extracts such a context around each individual\\nword of a corpus of documents the resulting matrix will be very wide\\n(many one-hot-features) with most of them being valued to zero most\\nof the time. So as to make the resulting data structure able to fit in\\nmemory the DictVectorizer class uses a scipy.sparse matrix by\\ndefault instead of a numpy.ndarray.\\n- 6.2.2. Feature hashing¶\\nThe class FeatureHasher is a high-speed, low-memory vectorizer that\\nuses a technique known as\\nfeature hashing,\\nor the “hashing trick”.\\nInstead of building a hash table of the features encountered in training,\\nas the vectorizers do, instances of FeatureHasher\\napply a hash function to the features\\nto determine their column index in sample matrices directly.\\nThe result is increased speed and reduced memory usage,\\nat the expense of inspectability;\\nthe hasher does not remember what the input features looked like\\nand has no inverse_transform method.\\nSince the hash function might cause collisions between (unrelated) features,\\na signed hash function is used and the sign of the hash value\\ndetermines the sign of the value stored in the output matrix for a feature.\\nThis way, collisions are likely to cancel out rather than accumulate error,\\nand the expected mean of any output feature’s value is zero. This mechanism\\nis enabled by default with alternate_sign=True and is particularly useful\\nfor small hash table sizes (n_features < 10000). For large hash table\\nsizes, it can be disabled, to allow the output to be passed to estimators like\\nMultinomialNB or\\nchi2\\nfeature selectors that expect non-negative inputs.\\nFeatureHasher accepts either mappings\\n(like Python’s dict and its variants in the collections module),\\n(feature, value) pairs, or strings,\\ndepending on the constructor parameter input_type.\\nMapping are treated as lists of (feature, value) pairs,\\nwhile single strings have an implicit value of 1,\\nso [\\'feat1\\', \\'feat2\\', \\'feat3\\'] is interpreted as\\n[(\\'feat1\\', 1), (\\'feat2\\', 1), (\\'feat3\\', 1)].\\nIf a single feature occurs multiple times in a sample,\\nthe associated values will be summed\\n(so (\\'feat\\', 2) and (\\'feat\\', 3.5) become (\\'feat\\', 5.5)).\\nThe output from FeatureHasher is always a scipy.sparse matrix\\nin the CSR format.\\nFeature hashing can be employed in document classification,\\nbut unlike CountVectorizer,\\nFeatureHasher does not do word\\nsplitting or any other preprocessing except Unicode-to-UTF-8 encoding;\\nsee Vectorizing a large text corpus with the hashing trick, below, for a combined tokenizer/hasher.\\nAs an example, consider a word-level natural language processing task\\nthat needs features extracted from (token, part_of_speech) pairs.\\nOne could use a Python generator function to extract features:\\ndef token_features(token, part_of_speech):\\nif token.isdigit():\\nyield \"numeric\"\\nelse:\\nyield \"token={}\".format(token.lower())\\nyield \"token,pos={},{}\".format(token, part_of_speech)\\nif token[0].isupper():\\nyield \"uppercase_initial\"\\nif token.isupper():\\nyield \"all_uppercase\"\\nyield \"pos={}\".format(part_of_speech)\\nThen, the raw_X to be fed to FeatureHasher.transform\\ncan be constructed using:\\nraw_X = (token_features(tok, pos_tagger(tok)) for tok in corpus)\\nand fed to a hasher with:\\nhasher = FeatureHasher(input_type=\\'string\\')\\nX = hasher.transform(raw_X)\\nto get a scipy.sparse matrix X.\\nNote the use of a generator comprehension,\\nwhich introduces laziness into the feature extraction:\\ntokens are only processed on demand from the hasher.\\nImplementation details\\nClick for more details\\n¶\\nFeatureHasher uses the signed 32-bit variant of MurmurHash3.\\nAs a result (and because of limitations in scipy.sparse),\\nthe maximum number of features supported is currently (2^{31} - 1).\\nThe original formulation of the hashing trick by Weinberger et al.\\nused two separate hash functions (h) and (\\\\xi)\\nto determine the column index and sign of a feature, respectively.\\nThe present implementation works under the assumption\\nthat the sign bit of MurmurHash3 is independent of its other bits.\\nSince a simple modulo is used to transform the hash function to a column index,\\nit is advisable to use a power of two as the n_features parameter;\\notherwise the features will not be mapped evenly to the columns.\\nReferences:\\nMurmurHash3.\\nReferences:\\nKilian Weinberger, Anirban Dasgupta, John Langford, Alex Smola and\\nJosh Attenberg (2009). Feature hashing for large scale multitask learning. Proc. ICML.\\n- 6.2.3. Text feature extraction¶\\n6.2.3.1. The Bag of Words representation¶\\nText Analysis is a major application field for machine learning\\nalgorithms. However the raw data, a sequence of symbols cannot be fed\\ndirectly to the algorithms themselves as most of them expect numerical\\nfeature vectors with a fixed size rather than the raw text documents\\nwith variable length.\\nIn order to address this, scikit-learn provides utilities for the most\\ncommon ways to extract numerical features from text content, namely:\\ntokenizing strings and giving an integer id for each possible token,\\nfor instance by using white-spaces and punctuation as token separators.\\ncounting the occurrences of tokens in each document.\\nnormalizing and weighting with diminishing importance tokens that\\noccur in the majority of samples / documents.\\nIn this scheme, features and samples are defined as follows:\\neach individual token occurrence frequency (normalized or not)\\nis treated as a feature.\\nthe vector of all the token frequencies for a given document is\\nconsidered a multivariate sample.\\nA corpus of documents can thus be represented by a matrix with one row\\nper document and one column per token (e.g. word) occurring in the corpus.\\nWe call vectorization the general process of turning a collection\\nof text documents into numerical feature vectors. This specific strategy\\n(tokenization, counting and normalization) is called the Bag of Words\\nor “Bag of n-grams” representation. Documents are described by word\\noccurrences while completely ignoring the relative position information\\nof the words in the document.\\n6.2.3.2. Sparsity¶\\nAs most documents will typically use a very small subset of the words used in\\nthe corpus, the resulting matrix will have many feature values that are\\nzeros (typically more than 99% of them).\\nFor instance a collection of 10,000 short text documents (such as emails)\\nwill use a vocabulary with a size in the order of 100,000 unique words in\\ntotal while each document will use 100 to 1000 unique words individually.\\nIn order to be able to store such a matrix in memory but also to speed\\nup algebraic operations matrix / vector, implementations will typically\\nuse a sparse representation such as the implementations available in the\\nscipy.sparse package.\\n6.2.3.3. Common Vectorizer usage¶\\nCountVectorizer implements both tokenization and occurrence\\ncounting in a single class:\\nfrom sklearn.feature_extraction.text import CountVectorizer\\nThis model has many parameters, however the default values are quite\\nreasonable (please see  the reference documentation for the details):\\nvectorizer = CountVectorizer()\\nvectorizer\\nCountVectorizer()\\nLet’s use it to tokenize and count the word occurrences of a minimalistic\\ncorpus of text documents:\\ncorpus = [\\n...     \\'This is the first document.\\',\\n...     \\'This is the second second document.\\',\\n...     \\'And the third one.\\',\\n...     \\'Is this the first document?\\',\\n... ]\\nX = vectorizer.fit_transform(corpus)\\nX\\n<4x9 sparse matrix of type \\'<... \\'numpy.int64\\'>\\'\\nwith 19 stored elements in Compressed Sparse ... format>\\nThe default configuration tokenizes the string by extracting words of\\nat least 2 letters. The specific function that does this step can be\\nrequested explicitly:\\nanalyze = vectorizer.build_analyzer()\\nanalyze(\"This is a text document to analyze.\") == (\\n...     [\\'this\\', \\'is\\', \\'text\\', \\'document\\', \\'to\\', \\'analyze\\'])\\nTrue\\nEach term found by the analyzer during the fit is assigned a unique\\ninteger index corresponding to a column in the resulting matrix. This\\ninterpretation of the columns can be retrieved as follows:\\nvectorizer.get_feature_names_out()\\narray([\\'and\\', \\'document\\', \\'first\\', \\'is\\', \\'one\\', \\'second\\', \\'the\\',\\n\\'third\\', \\'this\\'], ...)\\nX.toarray()\\narray([[0, 1, 1, 1, 0, 0, 1, 0, 1],\\n[0, 1, 0, 1, 0, 2, 1, 0, 1],\\n[1, 0, 0, 0, 1, 0, 1, 1, 0],\\n[0, 1, 1, 1, 0, 0, 1, 0, 1]]...)\\nThe converse mapping from feature name to column index is stored in the\\nvocabulary_ attribute of the vectorizer:\\nvectorizer.vocabulary_.get(\\'document\\')\\n1\\nHence words that were not seen in the training corpus will be completely\\nignored in future calls to the transform method:\\nvectorizer.transform([\\'Something completely new.\\']).toarray()\\narray([[0, 0, 0, 0, 0, 0, 0, 0, 0]]...)\\nNote that in the previous corpus, the first and the last documents have\\nexactly the same words hence are encoded in equal vectors. In particular\\nwe lose the information that the last document is an interrogative form. To\\npreserve some of the local ordering information we can extract 2-grams\\nof words in addition to the 1-grams (individual words):\\nbigram_vectorizer = CountVectorizer(ngram_range=(1, 2),\\n...                                     token_pattern=r\\'\\\\b\\\\w+\\\\b\\', min_df=1)\\nanalyze = bigram_vectorizer.build_analyzer()\\nanalyze(\\'Bi-grams are cool!\\') == (\\n...     [\\'bi\\', \\'grams\\', \\'are\\', \\'cool\\', \\'bi grams\\', \\'grams are\\', \\'are cool\\'])\\nTrue\\nThe vocabulary extracted by this vectorizer is hence much bigger and\\ncan now resolve ambiguities encoded in local positioning patterns:\\nX_2 = bigram_vectorizer.fit_transform(corpus).toarray()\\nX_2\\narray([[0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0],\\n[0, 0, 1, 0, 0, 1, 1, 0, 0, 2, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0],\\n[1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0],\\n[0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1]]...)\\nIn particular the interrogative form “Is this” is only present in the\\nlast document:\\nfeature_index = bigram_vectorizer.vocabulary_.get(\\'is this\\')\\nX_2[:, feature_index]\\narray([0, 0, 0, 1]...)\\n6.2.3.4. Using stop words¶\\nStop words are words like “and”, “the”, “him”, which are presumed to be\\nuninformative in representing the content of a text, and which may be\\nremoved to avoid them being construed as signal for prediction.  Sometimes,\\nhowever, similar words are useful for prediction, such as in classifying\\nwriting style or personality.\\nThere are several known issues in our provided ‘english’ stop word list. It\\ndoes not aim to be a general, ‘one-size-fits-all’ solution as some tasks\\nmay require a more custom solution. See [NQY18] for more details.\\nPlease take care in choosing a stop word list.\\nPopular stop word lists may include words that are highly informative to\\nsome tasks, such as computer.\\nYou should also make sure that the stop word list has had the same\\npreprocessing and tokenization applied as the one used in the vectorizer.\\nThe word we’ve is split into we and ve by CountVectorizer’s default\\ntokenizer, so if we’ve is in stop_words, but ve is not, ve will\\nbe retained from we’ve in transformed text.  Our vectorizers will try to\\nidentify and warn about some kinds of inconsistencies.\\nReferences\\n[NQY18]\\nJ. Nothman, H. Qin and R. Yurchak (2018).\\n“Stop Word Lists in Free Open-source Software Packages”.\\nIn Proc. Workshop for NLP Open Source Software.\\n6.2.3.5. Tf–idf term weighting¶\\nIn a large text corpus, some words will be very present (e.g. “the”, “a”,\\n“is” in English) hence carrying very little meaningful information about\\nthe actual contents of the document. If we were to feed the direct count\\ndata directly to a classifier those very frequent terms would shadow\\nthe frequencies of rarer yet more interesting terms.\\nIn order to re-weight the count features into floating point values\\nsuitable for usage by a classifier it is very common to use the tf–idf\\ntransform.\\nTf means term-frequency while tf–idf means term-frequency times\\ninverse document-frequency:\\n(\\\\text{tf-idf(t,d)}=\\\\text{tf(t,d)} \\\\times \\\\text{idf(t)}).\\nUsing the TfidfTransformer’s default settings,\\nTfidfTransformer(norm=\\'l2\\', use_idf=True, smooth_idf=True, sublinear_tf=False)\\nthe term frequency, the number of times a term occurs in a given document,\\nis multiplied with idf component, which is computed as\\n(\\\\text{idf}(t) = \\\\log{\\\\frac{1 + n}{1+\\\\text{df}(t)}} + 1),\\nwhere (n) is the total number of documents in the document set, and\\n(\\\\text{df}(t)) is the number of documents in the document set that\\ncontain term (t). The resulting tf-idf vectors are then normalized by the\\nEuclidean norm:\\n(v_{norm} = \\\\frac{v}{||v||2} = \\\\frac{v}{\\\\sqrt{v{_1}^2 +\\nv{_2}^2 + \\\\dots + v{_n}^2}}).\\nThis was originally a term weighting scheme developed for information retrieval\\n(as a ranking function for search engines results) that has also found good\\nuse in document classification and clustering.\\nThe following sections contain further explanations and examples that\\nillustrate how the tf-idfs are computed exactly and how the tf-idfs\\ncomputed in scikit-learn’s TfidfTransformer\\nand TfidfVectorizer differ slightly from the standard textbook\\nnotation that defines the idf as\\n(\\\\text{idf}(t) = \\\\log{\\\\frac{n}{1+\\\\text{df}(t)}}.)\\nIn the TfidfTransformer and TfidfVectorizer\\nwith smooth_idf=False, the\\n“1” count is added to the idf instead of the idf’s denominator:\\n(\\\\text{idf}(t) = \\\\log{\\\\frac{n}{\\\\text{df}(t)}} + 1)\\nThis normalization is implemented by the TfidfTransformer\\nclass:\\nfrom sklearn.feature_extraction.text import TfidfTransformer\\ntransformer = TfidfTransformer(smooth_idf=False)\\ntransformer\\nTfidfTransformer(smooth_idf=False)\\nAgain please see the reference documentation for the details on all the parameters.\\nNumeric example of a tf-idf matrix\\nClick for more details\\n¶\\nLet’s take an example with the following counts. The first term is present\\n100% of the time hence not very interesting. The two other features only\\nin less than 50% of the time hence probably more representative of the\\ncontent of the documents:\\ncounts = [[3, 0, 1],\\n...           [2, 0, 0],\\n...           [3, 0, 0],\\n...           [4, 0, 0],\\n...           [3, 2, 0],\\n...           [3, 0, 2]]\\n...\\ntfidf = transformer.fit_transform(counts)\\ntfidf\\n<6x3 sparse matrix of type \\'<... \\'numpy.float64\\'>\\'\\nwith 9 stored elements in Compressed Sparse ... format>\\ntfidf.toarray()\\narray([[0.81940995, 0.        , 0.57320793],\\n[1.        , 0.        , 0.        ],\\n[1.        , 0.        , 0.        ],\\n[1.        , 0.        , 0.        ],\\n[0.47330339, 0.88089948, 0.        ],\\n[0.58149261, 0.        , 0.81355169]])\\nEach row is normalized to have unit Euclidean norm:\\n(v{norm} = \\\\frac{v}{||v||2} = \\\\frac{v}{\\\\sqrt{v{_1}^2 +\\nv{_2}^2 + \\\\dots + v{_n}^2}})\\nFor example, we can compute the tf-idf of the first term in the first\\ndocument in the counts array as follows:\\n(n = 6)\\n(\\\\text{df}(t){\\\\text{term1}} = 6)\\n(\\\\text{idf}(t){\\\\text{term1}} =\\n\\\\log \\\\frac{n}{\\\\text{df}(t)} + 1 = \\\\log(1)+1 = 1)\\n(\\\\text{tf-idf}{\\\\text{term1}} = \\\\text{tf} \\\\times \\\\text{idf} = 3 \\\\times 1 = 3)\\nNow, if we repeat this computation for the remaining 2 terms in the document,\\nwe get\\n(\\\\text{tf-idf}{\\\\text{term2}} = 0 \\\\times (\\\\log(6/1)+1) = 0)\\n(\\\\text{tf-idf}{\\\\text{term3}} = 1 \\\\times (\\\\log(6/2)+1) \\\\approx 2.0986)\\nand the vector of raw tf-idfs:\\n(\\\\text{tf-idf}{\\\\text{raw}} = [3, 0, 2.0986].)\\nThen, applying the Euclidean (L2) norm, we obtain the following tf-idfs\\nfor document 1:\\n(\\\\frac{[3, 0, 2.0986]}{\\\\sqrt{\\\\big(3^2 + 0^2 + 2.0986^2\\\\big)}}\\n= [ 0.819,  0,  0.573].)\\nFurthermore, the default parameter smooth_idf=True adds “1” to the numerator\\nand  denominator as if an extra document was seen containing every term in the\\ncollection exactly once, which prevents zero divisions:\\n(\\\\text{idf}(t) = \\\\log{\\\\frac{1 + n}{1+\\\\text{df}(t)}} + 1)\\nUsing this modification, the tf-idf of the third term in document 1 changes to\\n1.8473:\\n(\\\\text{tf-idf}{\\\\text{term3}} = 1 \\\\times \\\\log(7/3)+1 \\\\approx 1.8473)\\nAnd the L2-normalized tf-idf changes to\\n(\\\\frac{[3, 0, 1.8473]}{\\\\sqrt{\\\\big(3^2 + 0^2 + 1.8473^2\\\\big)}}\\n= [0.8515, 0, 0.5243]):\\ntransformer = TfidfTransformer()\\ntransformer.fit_transform(counts).toarray()\\narray([[0.85151335, 0.        , 0.52433293],\\n[1.        , 0.        , 0.        ],\\n[1.        , 0.        , 0.        ],\\n[1.        , 0.        , 0.        ],\\n[0.55422893, 0.83236428, 0.        ],\\n[0.63035731, 0.        , 0.77630514]])\\nThe weights of each\\nfeature computed by the fit method call are stored in a model\\nattribute:\\ntransformer.idf_\\narray([1. ..., 2.25..., 1.84...])\\nAs tf–idf is very often used for text features, there is also another\\nclass called TfidfVectorizer that combines all the options of\\nCountVectorizer and TfidfTransformer in a single model:\\nfrom sklearn.feature_extraction.text import TfidfVectorizer\\nvectorizer = TfidfVectorizer()\\nvectorizer.fit_transform(corpus)\\n<4x9 sparse matrix of type \\'<... \\'numpy.float64\\'>\\'\\nwith 19 stored elements in Compressed Sparse ... format>\\nWhile the tf–idf normalization is often very useful, there might\\nbe cases where the binary occurrence markers might offer better\\nfeatures. This can be achieved by using the binary parameter\\nof CountVectorizer. In particular, some estimators such as\\nBernoulli Naive Bayes explicitly model discrete boolean random\\nvariables. Also, very short texts are likely to have noisy tf–idf values\\nwhile the binary occurrence info is more stable.\\nAs usual the best way to adjust the feature extraction parameters\\nis to use a cross-validated grid search, for instance by pipelining the\\nfeature extractor with a classifier:\\nSample pipeline for text feature extraction and evaluation\\n6.2.3.6. Decoding text files¶\\nText is made of characters, but files are made of bytes. These bytes represent\\ncharacters according to some encoding. To work with text files in Python,\\ntheir bytes must be decoded to a character set called Unicode.\\nCommon encodings are ASCII, Latin-1 (Western Europe), KOI8-R (Russian)\\nand the universal encodings UTF-8 and UTF-16. Many others exist.\\nNote\\nAn encoding can also be called a ‘character set’,\\nbut this term is less accurate: several encodings can exist\\nfor a single character set.\\nThe text feature extractors in scikit-learn know how to decode text files,\\nbut only if you tell them what encoding the files are in.\\nThe CountVectorizer takes an encoding parameter for this purpose.\\nFor modern text files, the correct encoding is probably UTF-8,\\nwhich is therefore the default (encoding=\"utf-8\").\\nIf the text you are loading is not actually encoded with UTF-8, however,\\nyou will get a UnicodeDecodeError.\\nThe vectorizers can be told to be silent about decoding errors\\nby setting the decode_error parameter to either \"ignore\"\\nor \"replace\". See the documentation for the Python function\\nbytes.decode for more details\\n(type help(bytes.decode) at the Python prompt).\\nTroubleshooting decoding text\\nClick for more details\\n¶\\nIf you are having trouble decoding text, here are some things to try:\\nFind out what the actual encoding of the text is. The file might come\\nwith a header or README that tells you the encoding, or there might be some\\nstandard encoding you can assume based on where the text comes from.\\nYou may be able to find out what kind of encoding it is in general\\nusing the UNIX command file. The Python chardet module comes with\\na script called chardetect.py that will guess the specific encoding,\\nthough you cannot rely on its guess being correct.\\nYou could try UTF-8 and disregard the errors. You can decode byte\\nstrings with bytes.decode(errors=\\'replace\\') to replace all\\ndecoding errors with a meaningless character, or set\\ndecode_error=\\'replace\\' in the vectorizer. This may damage the\\nusefulness of your features.\\nReal text may come from a variety of sources that may have used different\\nencodings, or even be sloppily decoded in a different encoding than the\\none it was encoded with. This is common in text retrieved from the Web.\\nThe Python package ftfy can automatically sort out some classes of\\ndecoding errors, so you could try decoding the unknown text as latin-1\\nand then using ftfy to fix errors.\\nIf the text is in a mish-mash of encodings that is simply too hard to sort\\nout (which is the case for the 20 Newsgroups dataset), you can fall back on\\na simple single-byte encoding such as latin-1. Some text may display\\nincorrectly, but at least the same sequence of bytes will always represent\\nthe same feature.\\nFor example, the following snippet uses chardet\\n(not shipped with scikit-learn, must be installed separately)\\nto figure out the encoding of three texts.\\nIt then vectorizes the texts and prints the learned vocabulary.\\nThe output is not shown here.\\nimport chardet\\ntext1 = b\"Sei mir gegr\\\\xc3\\\\xbc\\\\xc3\\\\x9ft mein Sauerkraut\"\\ntext2 = b\"holdselig sind deine Ger\\\\xfcche\"\\ntext3 = b\"\\\\xff\\\\xfeA\\\\x00u\\\\x00f\\\\x00 \\\\x00F\\\\x00l\\\\x00\\\\xfc\\\\x00g\\\\x00e\\\\x00l\\\\x00n\\\\x00 \\\\x00d\\\\x00e\\\\x00s\\\\x00 \\\\x00G\\\\x00e\\\\x00s\\\\x00a\\\\x00n\\\\x00g\\\\x00e\\\\x00s\\\\x00,\\\\x00 \\\\x00H\\\\x00e\\\\x00r\\\\x00z\\\\x00l\\\\x00i\\\\x00e\\\\x00b\\\\x00c\\\\x00h\\\\x00e\\\\x00n\\\\x00,\\\\x00 \\\\x00t\\\\x00r\\\\x00a\\\\x00g\\\\x00 \\\\x00i\\\\x00c\\\\x00h\\\\x00 \\\\x00d\\\\x00i\\\\x00c\\\\x00h\\\\x00 \\\\x00f\\\\x00o\\\\x00r\\\\x00t\\\\x00\"\\ndecoded = [x.decode(chardet.detect(x)[\\'encoding\\'])\\n...            for x in (text1, text2, text3)]\\nv = CountVectorizer().fit(decoded).vocabulary_\\nfor term in v: print(v)\\n(Depending on the version of chardet, it might get the first one wrong.)\\nFor an introduction to Unicode and character encodings in general,\\nsee Joel Spolsky’s Absolute Minimum Every Software Developer Must Know\\nAbout Unicode.\\n6.2.3.7. Applications and examples¶\\nThe bag of words representation is quite simplistic but surprisingly\\nuseful in practice.\\nIn particular in a supervised setting it can be successfully combined\\nwith fast and scalable linear models to train document classifiers,\\nfor instance:\\nClassification of text documents using sparse features\\nIn an unsupervised setting it can be used to group similar documents\\ntogether by applying clustering algorithms such as K-means:\\nClustering text documents using k-means\\nFinally it is possible to discover the main topics of a corpus by\\nrelaxing the hard assignment constraint of clustering, for instance by\\nusing Non-negative matrix factorization (NMF or NNMF):\\nTopic extraction with Non-negative Matrix Factorization and Latent Dirichlet Allocation\\n6.2.3.8. Limitations of the Bag of Words representation¶\\nA collection of unigrams (what bag of words is) cannot capture phrases\\nand multi-word expressions, effectively disregarding any word order\\ndependence. Additionally, the bag of words model doesn’t account for potential\\nmisspellings or word derivations.\\nN-grams to the rescue! Instead of building a simple collection of\\nunigrams (n=1), one might prefer a collection of bigrams (n=2), where\\noccurrences of pairs of consecutive words are counted.\\nOne might alternatively consider a collection of character n-grams, a\\nrepresentation resilient against misspellings and derivations.\\nFor example, let’s say we’re dealing with a corpus of two documents:\\n[\\'words\\', \\'wprds\\']. The second document contains a misspelling\\nof the word ‘words’.\\nA simple bag of words representation would consider these two as\\nvery distinct documents, differing in both of the two possible features.\\nA character 2-gram representation, however, would find the documents\\nmatching in 4 out of 8 features, which may help the preferred classifier\\ndecide better:\\nngram_vectorizer = CountVectorizer(analyzer=\\'char_wb\\', ngram_range=(2, 2))\\ncounts = ngram_vectorizer.fit_transform([\\'words\\', \\'wprds\\'])\\nngram_vectorizer.get_feature_names_out()\\narray([\\' w\\', \\'ds\\', \\'or\\', \\'pr\\', \\'rd\\', \\'s \\', \\'wo\\', \\'wp\\'], ...)\\ncounts.toarray().astype(int)\\narray([[1, 1, 1, 0, 1, 1, 1, 0],\\n[1, 1, 0, 1, 1, 1, 0, 1]])\\nIn the above example, char_wb analyzer is used, which creates n-grams\\nonly from characters inside word boundaries (padded with space on each\\nside). The char analyzer, alternatively, creates n-grams that\\nspan across words:\\nngram_vectorizer = CountVectorizer(analyzer=\\'char_wb\\', ngram_range=(5, 5))\\nngram_vectorizer.fit_transform([\\'jumpy fox\\'])\\n<1x4 sparse matrix of type \\'<... \\'numpy.int64\\'>\\'\\nwith 4 stored elements in Compressed Sparse ... format>\\nngram_vectorizer.get_feature_names_out()\\narray([\\' fox \\', \\' jump\\', \\'jumpy\\', \\'umpy \\'], ...)\\nngram_vectorizer = CountVectorizer(analyzer=\\'char\\', ngram_range=(5, 5))\\nngram_vectorizer.fit_transform([\\'jumpy fox\\'])\\n<1x5 sparse matrix of type \\'<... \\'numpy.int64\\'>\\'\\nwith 5 stored elements in Compressed Sparse ... format>\\nngram_vectorizer.get_feature_names_out()\\narray([\\'jumpy\\', \\'mpy f\\', \\'py fo\\', \\'umpy \\', \\'y fox\\'], ...)\\nThe word boundaries-aware variant char_wb is especially interesting\\nfor languages that use white-spaces for word separation as it generates\\nsignificantly less noisy features than the raw char variant in\\nthat case. For such languages it can increase both the predictive\\naccuracy and convergence speed of classifiers trained using such\\nfeatures while retaining the robustness with regards to misspellings and\\nword derivations.\\nWhile some local positioning information can be preserved by extracting\\nn-grams instead of individual words, bag of words and bag of n-grams\\ndestroy most of the inner structure of the document and hence most of\\nthe meaning carried by that internal structure.\\nIn order to address the wider task of Natural Language Understanding,\\nthe local structure of sentences and paragraphs should thus be taken\\ninto account. Many such models will thus be casted as “Structured output”\\nproblems which are currently outside of the scope of scikit-learn.\\n6.2.3.9. Vectorizing a large text corpus with the hashing trick¶\\nThe above vectorization scheme is simple but the fact that it holds an in-\\nmemory mapping from the string tokens to the integer feature indices (the\\nvocabulary_ attribute) causes several problems when dealing with large\\ndatasets:\\nthe larger the corpus, the larger the vocabulary will grow and hence the\\nmemory use too,\\nfitting requires the allocation of intermediate data structures\\nof size proportional to that of the original dataset.\\nbuilding the word-mapping requires a full pass over the dataset hence it is\\nnot possible to fit text classifiers in a strictly online manner.\\npickling and un-pickling vectorizers with a large vocabulary_ can be very\\nslow (typically much slower than pickling / un-pickling flat data structures\\nsuch as a NumPy array of the same size),\\nit is not easily possible to split the vectorization work into concurrent sub\\ntasks as the vocabulary_ attribute would have to be a shared state with a\\nfine grained synchronization barrier: the mapping from token string to\\nfeature index is dependent on ordering of the first occurrence of each token\\nhence would have to be shared, potentially harming the concurrent workers’\\nperformance to the point of making them slower than the sequential variant.\\nIt is possible to overcome those limitations by combining the “hashing trick”\\n(Feature hashing) implemented by the\\nFeatureHasher class and the text\\npreprocessing and tokenization features of the CountVectorizer.\\nThis combination is implementing in HashingVectorizer,\\na transformer class that is mostly API compatible with CountVectorizer.\\nHashingVectorizer is stateless,\\nmeaning that you don’t have to call fit on it:\\nfrom sklearn.feature_extraction.text import HashingVectorizer\\nhv = HashingVectorizer(n_features=10)\\nhv.transform(corpus)\\n<4x10 sparse matrix of type \\'<... \\'numpy.float64\\'>\\'\\nwith 16 stored elements in Compressed Sparse ... format>\\nYou can see that 16 non-zero feature tokens were extracted in the vector\\noutput: this is less than the 19 non-zeros extracted previously by the\\nCountVectorizer on the same toy corpus. The discrepancy comes from\\nhash function collisions because of the low value of the n_features parameter.\\nIn a real world setting, the n_features parameter can be left to its\\ndefault value of 2 ** 20 (roughly one million possible features). If memory\\nor downstream models size is an issue selecting a lower value such as 2 **\\n18 might help without introducing too many additional collisions on typical\\ntext classification tasks.\\nNote that the dimensionality does not affect the CPU training time of\\nalgorithms which operate on CSR matrices (LinearSVC(dual=True),\\nPerceptron, SGDClassifier, PassiveAggressive) but it does for\\nalgorithms that work with CSC matrices (LinearSVC(dual=False), Lasso(),\\netc.).\\nLet’s try again with the default setting:\\nhv = HashingVectorizer()\\nhv.transform(corpus)\\n<4x1048576 sparse matrix of type \\'<... \\'numpy.float64\\'>\\'\\nwith 19 stored elements in Compressed Sparse ... format>\\nWe no longer get the collisions, but this comes at the expense of a much larger\\ndimensionality of the output space.\\nOf course, other terms than the 19 used here\\nmight still collide with each other.\\nThe HashingVectorizer also comes with the following limitations:\\nit is not possible to invert the model (no inverse_transform method),\\nnor to access the original string representation of the features,\\nbecause of the one-way nature of the hash function that performs the mapping.\\nit does not provide IDF weighting as that would introduce statefulness in the\\nmodel. A TfidfTransformer can be appended to it in a pipeline if\\nrequired.\\nPerforming out-of-core scaling with HashingVectorizer\\nClick for more details\\n¶\\nAn interesting development of using a HashingVectorizer is the ability\\nto perform out-of-core scaling. This means that we can learn from data that\\ndoes not fit into the computer’s main memory.\\nA strategy to implement out-of-core scaling is to stream data to the estimator\\nin mini-batches. Each mini-batch is vectorized using HashingVectorizer\\nso as to guarantee that the input space of the estimator has always the same\\ndimensionality. The amount of memory used at any time is thus bounded by the\\nsize of a mini-batch. Although there is no limit to the amount of data that can\\nbe ingested using such an approach, from a practical point of view the learning\\ntime is often limited by the CPU time one wants to spend on the task.\\nFor a full-fledged example of out-of-core scaling in a text classification\\ntask see Out-of-core classification of text documents.\\n6.2.3.10. Customizing the vectorizer classes¶\\nIt is possible to customize the behavior by passing a callable\\nto the vectorizer constructor:\\ndef my_tokenizer(s):\\n...     return s.split()\\n...\\nvectorizer = CountVectorizer(tokenizer=my_tokenizer)\\nvectorizer.build_analyzer()(u\"Some... punctuation!\") == (\\n...     [\\'some...\\', \\'punctuation!\\'])\\nTrue\\nIn particular we name:\\npreprocessor: a callable that takes an entire document as input (as a\\nsingle string), and returns a possibly transformed version of the document,\\nstill as an entire string. This can be used to remove HTML tags, lowercase\\nthe entire document, etc.\\ntokenizer: a callable that takes the output from the preprocessor\\nand splits it into tokens, then returns a list of these.\\nanalyzer: a callable that replaces the preprocessor and tokenizer.\\nThe default analyzers all call the preprocessor and tokenizer, but custom\\nanalyzers will skip this. N-gram extraction and stop word filtering take\\nplace at the analyzer level, so a custom analyzer may have to reproduce\\nthese steps.\\n(Lucene users might recognize these names, but be aware that scikit-learn\\nconcepts may not map one-to-one onto Lucene concepts.)\\nTo make the preprocessor, tokenizer and analyzers aware of the model\\nparameters it is possible to derive from the class and override the\\nbuild_preprocessor, build_tokenizer and build_analyzer\\nfactory methods instead of passing custom functions.\\nTips and tricks\\nClick for more details\\n¶\\nSome tips and tricks:\\nIf documents are pre-tokenized by an external package, then store them in\\nfiles (or strings) with the tokens separated by whitespace and pass\\nanalyzer=str.split\\nFancy token-level analysis such as stemming, lemmatizing, compound\\nsplitting, filtering based on part-of-speech, etc. are not included in the\\nscikit-learn codebase, but can be added by customizing either the\\ntokenizer or the analyzer.\\nHere’s a CountVectorizer with a tokenizer and lemmatizer using\\nNLTK:\\nfrom nltk import word_tokenize\\nfrom nltk.stem import WordNetLemmatizer\\nclass LemmaTokenizer:\\n...     def init(self):\\n...         self.wnl = WordNetLemmatizer()\\n...     def call(self, doc):\\n...         return [self.wnl.lemmatize(t) for t in word_tokenize(doc)]\\n...\\nvect = CountVectorizer(tokenizer=LemmaTokenizer())\\n(Note that this will not filter out punctuation.)\\nThe following example will, for instance, transform some British spelling\\nto American spelling:\\nimport re\\ndef to_british(tokens):\\n...     for t in tokens:\\n...         t = re.sub(r\"(...)our$\", r\"\\\\1or\", t)\\n...         t = re.sub(r\"([bt])re$\", r\"\\\\1er\", t)\\n...         t = re.sub(r\"([iy])s(e$|ing|ation)\", r\"\\\\1z\\\\2\", t)\\n...         t = re.sub(r\"ogue$\", \"og\", t)\\n...         yield t\\n...\\nclass CustomVectorizer(CountVectorizer):\\n...     def build_tokenizer(self):\\n...         tokenize = super().build_tokenizer()\\n...         return lambda doc: list(to_british(tokenize(doc)))\\n...\\nprint(CustomVectorizer().build_analyzer()(u\"color colour\"))\\n[...\\'color\\', ...\\'color\\']\\nfor other styles of preprocessing; examples include stemming, lemmatization,\\nor normalizing numerical tokens, with the latter illustrated in:\\nBiclustering documents with the Spectral Co-clustering algorithm\\nCustomizing the vectorizer can also be useful when handling Asian languages\\nthat do not use an explicit word separator such as whitespace.\\n- 6.2.4. Image feature extraction¶\\n6.2.4.1. Patch extraction¶\\nThe extract_patches_2d function extracts patches from an image stored\\nas a two-dimensional array, or three-dimensional with color information along\\nthe third axis. For rebuilding an image from all its patches, use\\nreconstruct_from_patches_2d. For example let us generate a 4x4 pixel\\npicture with 3 color channels (e.g. in RGB format):\\nimport numpy as np\\nfrom sklearn.feature_extraction import image\\none_image = np.arange(4 * 4 * 3).reshape((4, 4, 3))\\none_image[:, :, 0]  # R channel of a fake RGB picture\\narray([[ 0,  3,  6,  9],\\n[12, 15, 18, 21],\\n[24, 27, 30, 33],\\n[36, 39, 42, 45]])\\npatches = image.extract_patches_2d(one_image, (2, 2), max_patches=2,\\n...     random_state=0)\\npatches.shape\\n(2, 2, 2, 3)\\npatches[:, :, :, 0]\\narray([[[ 0,  3],\\n[12, 15]],\\n[[15, 18],\\n[27, 30]]])\\npatches = image.extract_patches_2d(one_image, (2, 2))\\npatches.shape\\n(9, 2, 2, 3)\\npatches[4, :, :, 0]\\narray([[15, 18],\\n[27, 30]])\\nLet us now try to reconstruct the original image from the patches by averaging\\non overlapping areas:\\nreconstructed = image.reconstruct_from_patches_2d(patches, (4, 4, 3))\\nnp.testing.assert_array_equal(one_image, reconstructed)\\nThe PatchExtractor class works in the same way as\\nextract_patches_2d, only it supports multiple images as input. It is\\nimplemented as a scikit-learn transformer, so it can be used in pipelines. See:\\nfive_images = np.arange(5 * 4 * 4 * 3).reshape(5, 4, 4, 3)\\npatches = image.PatchExtractor(patch_size=(2, 2)).transform(five_images)\\npatches.shape\\n(45, 2, 2, 3)\\n6.2.4.2. Connectivity graph of an image¶\\nSeveral estimators in the scikit-learn can use connectivity information between\\nfeatures or samples. For instance Ward clustering\\n(Hierarchical clustering) can cluster together only neighboring pixels\\nof an image, thus forming contiguous patches:\\nFor this purpose, the estimators use a ‘connectivity’ matrix, giving\\nwhich samples are connected.\\nThe function img_to_graph returns such a matrix from a 2D or 3D\\nimage. Similarly, grid_to_graph build a connectivity matrix for\\nimages given the shape of these image.\\nThese matrices can be used to impose connectivity in estimators that use\\nconnectivity information, such as Ward clustering\\n(Hierarchical clustering), but also to build precomputed kernels,\\nor similarity matrices.\\nNote\\nExamples\\nA demo of structured Ward hierarchical clustering on an image of coins\\nSpectral clustering for image segmentation\\nFeature agglomeration vs. univariate selection\\n\\n6.3. Preprocessing data — scikit-learn 1.4.2 documentation\\n\\nInstall\\nUser Guide\\nAPI\\nExamples\\nCommunity\\nGetting Started\\nTutorial\\nWhat\\'s new\\nGlossary\\nDevelopment\\nFAQ\\nSupport\\nRelated packages\\nRoadmap\\nGovernance\\nAbout us\\nGitHub\\nOther Versions and Download\\nMore\\nGetting Started\\nTutorial\\nWhat\\'s new\\nGlossary\\nDevelopment\\nFAQ\\nSupport\\nRelated packages\\nRoadmap\\nGovernance\\nAbout us\\nGitHub\\nOther Versions and Download\\nToggle Menu\\nPrevUp\\nNext\\nscikit-learn 1.4.2\\nOther versions\\nPlease cite us if you use the software.\\n\\n6.3. Preprocessing data\\n\\n6.3.1. Standardization, or mean removal and variance scaling\\n6.3.1.1. Scaling features to a range\\n6.3.1.2. Scaling sparse data\\n6.3.1.3. Scaling data with outliers\\n6.3.1.4. Centering kernel matrices\\n\\n6.3.2. Non-linear transformation\\n6.3.2.1. Mapping to a Uniform distribution\\n6.3.2.2. Mapping to a Gaussian distribution\\n\\n6.3.3. Normalization\\n\\n6.3.4. Encoding categorical features\\n6.3.4.1. Infrequent categories\\n6.3.4.2. Target Encoder\\n\\n6.3.5. Discretization\\n6.3.5.1. K-bins discretization\\n6.3.5.2. Feature binarization\\n\\n6.3.6. Imputation of missing values\\n\\n6.3.7. Generating polynomial features\\n6.3.7.1. Polynomial features\\n6.3.7.2. Spline transformer\\n\\n6.3.8. Custom transformers\\n\\n6.3. Preprocessing data¶\\n\\nThe sklearn.preprocessing package provides several common\\nutility functions and transformer classes to change raw feature vectors\\ninto a representation that is more suitable for the downstream estimators.\\nIn general, many learning algorithms such as linear models benefit from standardization of the data set\\n(see Importance of Feature Scaling).\\nIf some outliers are present in the set, robust scalers or other transformers can\\nbe more appropriate. The behaviors of the different scalers, transformers, and\\nnormalizers on a dataset containing marginal outliers is highlighted in\\nCompare the effect of different scalers on data with outliers.\\n- 6.3.1. Standardization, or mean removal and variance scaling¶\\nStandardization of datasets is a common requirement for many\\nmachine learning estimators implemented in scikit-learn; they might behave\\nbadly if the individual features do not more or less look like standard\\nnormally distributed data: Gaussian with zero mean and unit variance.\\nIn practice we often ignore the shape of the distribution and just\\ntransform the data to center it by removing the mean value of each\\nfeature, then scale it by dividing non-constant features by their\\nstandard deviation.\\nFor instance, many elements used in the objective function of\\na learning algorithm (such as the RBF kernel of Support Vector\\nMachines or the l1 and l2 regularizers of linear models) may assume that\\nall features are centered around zero or have variance in the same\\norder. If a feature has a variance that is orders of magnitude larger\\nthan others, it might dominate the objective function and make the\\nestimator unable to learn from other features correctly as expected.\\nThe preprocessing module provides the\\nStandardScaler utility class, which is a quick and\\neasy way to perform the following operation on an array-like\\ndataset:\\n\\nfrom sklearn import preprocessing\\nimport numpy as np\\nX_train = np.array([[ 1., -1.,  2.],\\n...                     [ 2.,  0.,  0.],\\n...                     [ 0.,  1., -1.]])\\nscaler = preprocessing.StandardScaler().fit(X_train)\\nscaler\\nStandardScaler()\\nscaler.mean_\\narray([1. ..., 0. ..., 0.33...])\\nscaler.scale_\\narray([0.81..., 0.81..., 1.24...])\\nX_scaled = scaler.transform(X_train)\\nX_scaled\\narray([[ 0.  ..., -1.22...,  1.33...],\\n[ 1.22...,  0.  ..., -0.26...],\\n[-1.22...,  1.22..., -1.06...]])\\nScaled data has zero mean and unit variance:\\nX_scaled.mean(axis=0)\\narray([0., 0., 0.])\\nX_scaled.std(axis=0)\\narray([1., 1., 1.])\\nThis class implements the Transformer API to compute the mean and\\nstandard deviation on a training set so as to be able to later re-apply the\\nsame transformation on the testing set. This class is hence suitable for\\nuse in the early steps of a Pipeline:\\nfrom sklearn.datasets import make_classification\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.pipeline import make_pipeline\\nfrom sklearn.preprocessing import StandardScaler\\nX, y = make_classification(random_state=42)\\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\\npipe = make_pipeline(StandardScaler(), LogisticRegression())\\npipe.fit(X_train, y_train)  # apply scaling on training data\\nPipeline(steps=[(\\'standardscaler\\', StandardScaler()),\\n(\\'logisticregression\\', LogisticRegression())])\\npipe.score(X_test, y_test)  # apply scaling on testing data, without leaking training data.\\n0.96\\nIt is possible to disable either centering or scaling by either\\npassing with_mean=False or with_std=False to the constructor\\nof StandardScaler.\\n6.3.1.1. Scaling features to a range¶\\nAn alternative standardization is scaling features to\\nlie between a given minimum and maximum value, often between zero and one,\\nor so that the maximum absolute value of each feature is scaled to unit size.\\nThis can be achieved using MinMaxScaler or MaxAbsScaler,\\nrespectively.\\nThe motivation to use this scaling include robustness to very small\\nstandard deviations of features and preserving zero entries in sparse data.\\nHere is an example to scale a toy data matrix to the [0, 1] range:\\nX_train = np.array([[ 1., -1.,  2.],\\n...                     [ 2.,  0.,  0.],\\n...                     [ 0.,  1., -1.]])\\n...\\nmin_max_scaler = preprocessing.MinMaxScaler()\\nX_train_minmax = min_max_scaler.fit_transform(X_train)\\nX_train_minmax\\narray([[0.5       , 0.        , 1.        ],\\n[1.        , 0.5       , 0.33333333],\\n[0.        , 1.        , 0.        ]])\\nThe same instance of the transformer can then be applied to some new test data\\nunseen during the fit call: the same scaling and shifting operations will be\\napplied to be consistent with the transformation performed on the train data:\\nX_test = np.array([[-3., -1.,  4.]])\\nX_test_minmax = min_max_scaler.transform(X_test)\\nX_test_minmax\\narray([[-1.5       ,  0.        ,  1.66666667]])\\nIt is possible to introspect the scaler attributes to find about the exact\\nnature of the transformation learned on the training data:\\nmin_max_scaler.scale_\\narray([0.5       , 0.5       , 0.33...])\\nmin_max_scaler.min_\\narray([0.        , 0.5       , 0.33...])\\nIf MinMaxScaler is given an explicit feature_range=(min, max) the\\nfull formula is:\\nX_std = (X - X.min(axis=0)) / (X.max(axis=0) - X.min(axis=0))\\nX_scaled = X_std * (max - min) + min\\nMaxAbsScaler works in a very similar fashion, but scales in a way\\nthat the training data lies within the range [-1, 1] by dividing through\\nthe largest maximum value in each feature. It is meant for data\\nthat is already centered at zero or sparse data.\\nHere is how to use the toy data from the previous example with this scaler:\\nX_train = np.array([[ 1., -1.,  2.],\\n...                     [ 2.,  0.,  0.],\\n...                     [ 0.,  1., -1.]])\\n...\\nmax_abs_scaler = preprocessing.MaxAbsScaler()\\nX_train_maxabs = max_abs_scaler.fit_transform(X_train)\\nX_train_maxabs\\narray([[ 0.5, -1. ,  1. ],\\n[ 1. ,  0. ,  0. ],\\n[ 0. ,  1. , -0.5]])\\nX_test = np.array([[ -3., -1.,  4.]])\\nX_test_maxabs = max_abs_scaler.transform(X_test)\\nX_test_maxabs\\narray([[-1.5, -1. ,  2. ]])\\nmax_abs_scaler.scale_\\narray([2.,  1.,  2.])\\n6.3.1.2. Scaling sparse data¶\\nCentering sparse data would destroy the sparseness structure in the data, and\\nthus rarely is a sensible thing to do. However, it can make sense to scale\\nsparse inputs, especially if features are on different scales.\\nMaxAbsScaler was specifically designed for scaling\\nsparse data, and is the recommended way to go about this.\\nHowever, StandardScaler can accept scipy.sparse\\nmatrices  as input, as long as with_mean=False is explicitly passed\\nto the constructor. Otherwise a ValueError will be raised as\\nsilently centering would break the sparsity and would often crash the\\nexecution by allocating excessive amounts of memory unintentionally.\\nRobustScaler cannot be fitted to sparse inputs, but you can use\\nthe transform method on sparse inputs.\\nNote that the scalers accept both Compressed Sparse Rows and Compressed\\nSparse Columns format (see scipy.sparse.csr_matrix and\\nscipy.sparse.csc_matrix). Any other sparse input will be converted to\\nthe Compressed Sparse Rows representation.  To avoid unnecessary memory\\ncopies, it is recommended to choose the CSR or CSC representation upstream.\\nFinally, if the centered data is expected to be small enough, explicitly\\nconverting the input to an array using the toarray method of sparse matrices\\nis another option.\\n6.3.1.3. Scaling data with outliers¶\\nIf your data contains many outliers, scaling using the mean and variance\\nof the data is likely to not work very well. In these cases, you can use\\nRobustScaler as a drop-in replacement instead. It uses\\nmore robust estimates for the center and range of your data.\\nReferences:\\nFurther discussion on the importance of centering and scaling data is\\navailable on this FAQ: Should I normalize/standardize/rescale the data?\\nScaling vs Whitening\\nIt is sometimes not enough to center and scale the features\\nindependently, since a downstream model can further make some assumption\\non the linear independence of the features.\\nTo address this issue you can use PCA with\\nwhiten=True to further remove the linear correlation across features.\\n6.3.1.4. Centering kernel matrices¶\\nIf you have a kernel matrix of a kernel (K) that computes a dot product\\nin a feature space (possibly implicitly) defined by a function\\n(\\\\phi(\\\\cdot)), a KernelCenterer can transform the kernel matrix\\nso that it contains inner products in the feature space defined by (\\\\phi)\\nfollowed by the removal of the mean in that space. In other words,\\nKernelCenterer computes the centered Gram matrix associated to a\\npositive semidefinite kernel (K).\\nMathematical formulation\\nWe can have a look at the mathematical formulation now that we have the\\nintuition. Let (K) be a kernel matrix of shape (n_samples, n_samples)\\ncomputed from (X), a data matrix of shape (n_samples, n_features),\\nduring the fit step. (K) is defined by\\n[K(X, X) = \\\\phi(X) . \\\\phi(X)^{T}]\\n(\\\\phi(X)) is a function mapping of (X) to a Hilbert space. A\\ncentered kernel (\\\\tilde{K}) is defined as:\\n[\\\\tilde{K}(X, X) = \\\\tilde{\\\\phi}(X) . \\\\tilde{\\\\phi}(X)^{T}]\\nwhere (\\\\tilde{\\\\phi}(X)) results from centering (\\\\phi(X)) in the\\nHilbert space.\\nThus, one could compute (\\\\tilde{K}) by mapping (X) using the\\nfunction (\\\\phi(\\\\cdot)) and center the data in this new space. However,\\nkernels are often used because they allows some algebra calculations that\\navoid computing explicitly this mapping using (\\\\phi(\\\\cdot)). Indeed, one\\ncan implicitly center as shown in Appendix B in [Scholkopf1998]:\\n[\\\\tilde{K} = K - 1_{\\\\text{n}{samples}} K - K 1{\\\\text{n}{samples}} + 1{\\\\text{n}{samples}} K 1{\\\\text{n}{samples}}]\\n(1{\\\\text{n}{samples}}) is a matrix of (n_samples, n_samples) where\\nall entries are equal to (\\\\frac{1}{\\\\text{n}{samples}}). In the\\ntransform step, the kernel becomes (K_{test}(X, Y)) defined as:\\n[K_{test}(X, Y) = \\\\phi(Y) . \\\\phi(X)^{T}]\\n(Y) is the test dataset of shape (n_samples_test, n_features) and thus\\n(K_{test}) is of shape (n_samples_test, n_samples). In this case,\\ncentering (K_{test}) is done as:\\n[\\\\tilde{K}{test}(X, Y) = K{test} - 1\\'{\\\\text{n}{samples}} K - K_{test} 1_{\\\\text{n}{samples}} + 1\\'{\\\\text{n}{samples}} K 1{\\\\text{n}{samples}}]\\n(1\\'{\\\\text{n}{samples}}) is a matrix of shape\\n(n_samples_test, n_samples) where all entries are equal to\\n(\\\\frac{1}{\\\\text{n}{samples}}).\\nReferences\\n[Scholkopf1998]\\nB. Schölkopf, A. Smola, and K.R. Müller,\\n“Nonlinear component analysis as a kernel eigenvalue problem.”\\nNeural computation 10.5 (1998): 1299-1319.\\n- 6.3.2. Non-linear transformation¶\\nTwo types of transformations are available: quantile transforms and power\\ntransforms. Both quantile and power transforms are based on monotonic\\ntransformations of the features and thus preserve the rank of the values\\nalong each feature.\\nQuantile transforms put all features into the same desired distribution based\\non the formula (G^{-1}(F(X))) where (F) is the cumulative\\ndistribution function of the feature and (G^{-1}) the\\nquantile function of the\\ndesired output distribution (G). This formula is using the two following\\nfacts: (i) if (X) is a random variable with a continuous cumulative\\ndistribution function (F) then (F(X)) is uniformly distributed on\\n([0,1]); (ii) if (U) is a random variable with uniform distribution\\non ([0,1]) then (G^{-1}(U)) has distribution (G). By performing\\na rank transformation, a quantile transform smooths out unusual distributions\\nand is less influenced by outliers than scaling methods. It does, however,\\ndistort correlations and distances within and across features.\\nPower transforms are a family of parametric transformations that aim to map\\ndata from any distribution to as close to a Gaussian distribution.\\n6.3.2.1. Mapping to a Uniform distribution¶\\nQuantileTransformer provides a non-parametric\\ntransformation to map the data to a uniform distribution\\nwith values between 0 and 1:\\nfrom sklearn.datasets import load_iris\\nfrom sklearn.model_selection import train_test_split\\nX, y = load_iris(return_X_y=True)\\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\\nquantile_transformer = preprocessing.QuantileTransformer(random_state=0)\\nX_train_trans = quantile_transformer.fit_transform(X_train)\\nX_test_trans = quantile_transformer.transform(X_test)\\nnp.percentile(X_train[:, 0], [0, 25, 50, 75, 100])\\narray([ 4.3,  5.1,  5.8,  6.5,  7.9])\\nThis feature corresponds to the sepal length in cm. Once the quantile\\ntransformation applied, those landmarks approach closely the percentiles\\npreviously defined:\\nnp.percentile(X_train_trans[:, 0], [0, 25, 50, 75, 100])\\n...\\narray([ 0.00... ,  0.24...,  0.49...,  0.73...,  0.99... ])\\nThis can be confirmed on a independent testing set with similar remarks:\\nnp.percentile(X_test[:, 0], [0, 25, 50, 75, 100])\\n...\\narray([ 4.4  ,  5.125,  5.75 ,  6.175,  7.3  ])\\nnp.percentile(X_test_trans[:, 0], [0, 25, 50, 75, 100])\\n...\\narray([ 0.01...,  0.25...,  0.46...,  0.60... ,  0.94...])\\n6.3.2.2. Mapping to a Gaussian distribution¶\\nIn many modeling scenarios, normality of the features in a dataset is desirable.\\nPower transforms are a family of parametric, monotonic transformations that aim\\nto map data from any distribution to as close to a Gaussian distribution as\\npossible in order to stabilize variance and minimize skewness.\\nPowerTransformer currently provides two such power transformations,\\nthe Yeo-Johnson transform and the Box-Cox transform.\\nThe Yeo-Johnson transform is given by:\\n[\\\\begin{split}x_i^{(\\\\lambda)} =\\n\\\\begin{cases}\\n[(x_i + 1)^\\\\lambda - 1] / \\\\lambda & \\\\text{if } \\\\lambda \\\\neq 0, x_i \\\\geq 0, \\\\[8pt]\\n\\\\ln{(x_i + 1)} & \\\\text{if } \\\\lambda = 0, x_i \\\\geq 0 \\\\[8pt]\\n-[(-x_i + 1)^{2 - \\\\lambda} - 1] / (2 - \\\\lambda) & \\\\text{if } \\\\lambda \\\\neq 2, x_i < 0, \\\\[8pt]\\n- \\\\ln (- x_i + 1) & \\\\text{if } \\\\lambda = 2, x_i < 0\\n\\\\end{cases}\\\\end{split}]\\nwhile the Box-Cox transform is given by:\\n[\\\\begin{split}x_i^{(\\\\lambda)} =\\n\\\\begin{cases}\\n\\\\dfrac{x_i^\\\\lambda - 1}{\\\\lambda} & \\\\text{if } \\\\lambda \\\\neq 0, \\\\[8pt]\\n\\\\ln{(x_i)} & \\\\text{if } \\\\lambda = 0,\\n\\\\end{cases}\\\\end{split}]\\nBox-Cox can only be applied to strictly positive data. In both methods, the\\ntransformation is parameterized by (\\\\lambda), which is determined through\\nmaximum likelihood estimation. Here is an example of using Box-Cox to map\\nsamples drawn from a lognormal distribution to a normal distribution:\\npt = preprocessing.PowerTransformer(method=\\'box-cox\\', standardize=False)\\nX_lognormal = np.random.RandomState(616).lognormal(size=(3, 3))\\nX_lognormal\\narray([[1.28..., 1.18..., 0.84...],\\n[0.94..., 1.60..., 0.38...],\\n[1.35..., 0.21..., 1.09...]])\\npt.fit_transform(X_lognormal)\\narray([[ 0.49...,  0.17..., -0.15...],\\n[-0.05...,  0.58..., -0.57...],\\n[ 0.69..., -0.84...,  0.10...]])\\nWhile the above example sets the standardize option to False,\\nPowerTransformer will apply zero-mean, unit-variance normalization\\nto the transformed output by default.\\nBelow are examples of Box-Cox and Yeo-Johnson applied to various probability\\ndistributions.  Note that when applied to certain distributions, the power\\ntransforms achieve very Gaussian-like results, but with others, they are\\nineffective. This highlights the importance of visualizing the data before and\\nafter transformation.\\nIt is also possible to map data to a normal distribution using\\nQuantileTransformer by setting output_distribution=\\'normal\\'.\\nUsing the earlier example with the iris dataset:\\nquantile_transformer = preprocessing.QuantileTransformer(\\n...     output_distribution=\\'normal\\', random_state=0)\\nX_trans = quantile_transformer.fit_transform(X)\\nquantile_transformer.quantiles_\\narray([[4.3, 2. , 1. , 0.1],\\n[4.4, 2.2, 1.1, 0.1],\\n[4.4, 2.2, 1.2, 0.1],\\n...,\\n[7.7, 4.1, 6.7, 2.5],\\n[7.7, 4.2, 6.7, 2.5],\\n[7.9, 4.4, 6.9, 2.5]])\\nThus the median of the input becomes the mean of the output, centered at 0. The\\nnormal output is clipped so that the input’s minimum and maximum —\\ncorresponding to the 1e-7 and 1 - 1e-7 quantiles respectively — do not\\nbecome infinite under the transformation.\\n- 6.3.3. Normalization¶\\nNormalization is the process of scaling individual samples to have\\nunit norm. This process can be useful if you plan to use a quadratic form\\nsuch as the dot-product or any other kernel to quantify the similarity\\nof any pair of samples.\\nThis assumption is the base of the Vector Space Model often used in text\\nclassification and clustering contexts.\\nThe function normalize provides a quick and easy way to perform this\\noperation on a single array-like dataset, either using the l1, l2, or\\nmax norms:\\nX = [[ 1., -1.,  2.],\\n...      [ 2.,  0.,  0.],\\n...      [ 0.,  1., -1.]]\\nX_normalized = preprocessing.normalize(X, norm=\\'l2\\')\\nX_normalized\\narray([[ 0.40..., -0.40...,  0.81...],\\n[ 1.  ...,  0.  ...,  0.  ...],\\n[ 0.  ...,  0.70..., -0.70...]])\\nThe preprocessing module further provides a utility class\\nNormalizer that implements the same operation using the\\nTransformer API (even though the fit method is useless in this case:\\nthe class is stateless as this operation treats samples independently).\\nThis class is hence suitable for use in the early steps of a\\nPipeline:\\nnormalizer = preprocessing.Normalizer().fit(X)  # fit does nothing\\nnormalizer\\nNormalizer()\\nThe normalizer instance can then be used on sample vectors as any transformer:\\nnormalizer.transform(X)\\narray([[ 0.40..., -0.40...,  0.81...],\\n[ 1.  ...,  0.  ...,  0.  ...],\\n[ 0.  ...,  0.70..., -0.70...]])\\nnormalizer.transform([[-1.,  1., 0.]])\\narray([[-0.70...,  0.70...,  0.  ...]])\\nNote: L2 normalization is also known as spatial sign preprocessing.\\nSparse input\\nnormalize and Normalizer accept both dense array-like\\nand sparse matrices from scipy.sparse as input.\\nFor sparse input the data is converted to the Compressed Sparse Rows\\nrepresentation (see scipy.sparse.csr_matrix) before being fed to\\nefficient Cython routines. To avoid unnecessary memory copies, it is\\nrecommended to choose the CSR representation upstream.\\n- 6.3.4. Encoding categorical features¶\\nOften features are not given as continuous values but categorical.\\nFor example a person could have features [\"male\", \"female\"],\\n[\"from Europe\", \"from US\", \"from Asia\"],\\n[\"uses Firefox\", \"uses Chrome\", \"uses Safari\", \"uses Internet Explorer\"].\\nSuch features can be efficiently coded as integers, for instance\\n[\"male\", \"from US\", \"uses Internet Explorer\"] could be expressed as\\n[0, 1, 3] while [\"female\", \"from Asia\", \"uses Chrome\"] would be\\n[1, 2, 1].\\nTo convert categorical features to such integer codes, we can use the\\nOrdinalEncoder. This estimator transforms each categorical feature to one\\nnew feature of integers (0 to n_categories - 1):\\nenc = preprocessing.OrdinalEncoder()\\nX = [[\\'male\\', \\'from US\\', \\'uses Safari\\'], [\\'female\\', \\'from Europe\\', \\'uses Firefox\\']]\\nenc.fit(X)\\nOrdinalEncoder()\\nenc.transform([[\\'female\\', \\'from US\\', \\'uses Safari\\']])\\narray([[0., 1., 1.]])\\nSuch integer representation can, however, not be used directly with all\\nscikit-learn estimators, as these expect continuous input, and would interpret\\nthe categories as being ordered, which is often not desired (i.e. the set of\\nbrowsers was ordered arbitrarily).\\nBy default, OrdinalEncoder will also passthrough missing values that\\nare indicated by np.nan.\\nenc = preprocessing.OrdinalEncoder()\\nX = [[\\'male\\'], [\\'female\\'], [np.nan], [\\'female\\']]\\nenc.fit_transform(X)\\narray([[ 1.],\\n[ 0.],\\n[nan],\\n[ 0.]])\\nOrdinalEncoder provides a parameter encoded_missing_value to encode\\nthe missing values without the need to create a pipeline and using\\nSimpleImputer.\\nenc = preprocessing.OrdinalEncoder(encoded_missing_value=-1)\\nX = [[\\'male\\'], [\\'female\\'], [np.nan], [\\'female\\']]\\nenc.fit_transform(X)\\narray([[ 1.],\\n[ 0.],\\n[-1.],\\n[ 0.]])\\nThe above processing is equivalent to the following pipeline:\\nfrom sklearn.pipeline import Pipeline\\nfrom sklearn.impute import SimpleImputer\\nenc = Pipeline(steps=[\\n...     (\"encoder\", preprocessing.OrdinalEncoder()),\\n...     (\"imputer\", SimpleImputer(strategy=\"constant\", fill_value=-1)),\\n... ])\\nenc.fit_transform(X)\\narray([[ 1.],\\n[ 0.],\\n[-1.],\\n[ 0.]])\\nAnother possibility to convert categorical features to features that can be used\\nwith scikit-learn estimators is to use a one-of-K, also known as one-hot or\\ndummy encoding.\\nThis type of encoding can be obtained with the OneHotEncoder,\\nwhich transforms each categorical feature with\\nn_categories possible values into n_categories binary features, with\\none of them 1, and all others 0.\\nContinuing the example above:\\nenc = preprocessing.OneHotEncoder()\\nX = [[\\'male\\', \\'from US\\', \\'uses Safari\\'], [\\'female\\', \\'from Europe\\', \\'uses Firefox\\']]\\nenc.fit(X)\\nOneHotEncoder()\\nenc.transform([[\\'female\\', \\'from US\\', \\'uses Safari\\'],\\n...                [\\'male\\', \\'from Europe\\', \\'uses Safari\\']]).toarray()\\narray([[1., 0., 0., 1., 0., 1.],\\n[0., 1., 1., 0., 0., 1.]])\\nBy default, the values each feature can take is inferred automatically\\nfrom the dataset and can be found in the categories_ attribute:\\nenc.categories_\\n[array([\\'female\\', \\'male\\'], dtype=object), array([\\'from Europe\\', \\'from US\\'], dtype=object), array([\\'uses Firefox\\', \\'uses Safari\\'], dtype=object)]\\nIt is possible to specify this explicitly using the parameter categories.\\nThere are two genders, four possible continents and four web browsers in our\\ndataset:\\ngenders = [\\'female\\', \\'male\\']\\nlocations = [\\'from Africa\\', \\'from Asia\\', \\'from Europe\\', \\'from US\\']\\nbrowsers = [\\'uses Chrome\\', \\'uses Firefox\\', \\'uses IE\\', \\'uses Safari\\']\\nenc = preprocessing.OneHotEncoder(categories=[genders, locations, browsers])\\n\\nNote that for there are missing categorical values for the 2nd and 3rd\\n\\nfeature\\n\\nX = [[\\'male\\', \\'from US\\', \\'uses Safari\\'], [\\'female\\', \\'from Europe\\', \\'uses Firefox\\']]\\nenc.fit(X)\\nOneHotEncoder(categories=[[\\'female\\', \\'male\\'],\\n[\\'from Africa\\', \\'from Asia\\', \\'from Europe\\',\\n\\'from US\\'],\\n[\\'uses Chrome\\', \\'uses Firefox\\', \\'uses IE\\',\\n\\'uses Safari\\']])\\nenc.transform([[\\'female\\', \\'from Asia\\', \\'uses Chrome\\']]).toarray()\\narray([[1., 0., 0., 1., 0., 0., 1., 0., 0., 0.]])\\nIf there is a possibility that the training data might have missing categorical\\nfeatures, it can often be better to specify\\nhandle_unknown=\\'infrequent_if_exist\\' instead of setting the categories\\nmanually as above. When handle_unknown=\\'infrequent_if_exist\\' is specified\\nand unknown categories are encountered during transform, no error will be\\nraised but the resulting one-hot encoded columns for this feature will be all\\nzeros or considered as an infrequent category if enabled.\\n(handle_unknown=\\'infrequent_if_exist\\' is only supported for one-hot\\nencoding):\\nenc = preprocessing.OneHotEncoder(handle_unknown=\\'infrequent_if_exist\\')\\nX = [[\\'male\\', \\'from US\\', \\'uses Safari\\'], [\\'female\\', \\'from Europe\\', \\'uses Firefox\\']]\\nenc.fit(X)\\nOneHotEncoder(handle_unknown=\\'infrequent_if_exist\\')\\nenc.transform([[\\'female\\', \\'from Asia\\', \\'uses Chrome\\']]).toarray()\\narray([[1., 0., 0., 0., 0., 0.]])\\nIt is also possible to encode each column into n_categories - 1 columns\\ninstead of n_categories columns by using the drop parameter. This\\nparameter allows the user to specify a category for each feature to be dropped.\\nThis is useful to avoid co-linearity in the input matrix in some classifiers.\\nSuch functionality is useful, for example, when using non-regularized\\nregression (LinearRegression),\\nsince co-linearity would cause the covariance matrix to be non-invertible:\\nX = [[\\'male\\', \\'from US\\', \\'uses Safari\\'],\\n...      [\\'female\\', \\'from Europe\\', \\'uses Firefox\\']]\\ndrop_enc = preprocessing.OneHotEncoder(drop=\\'first\\').fit(X)\\ndrop_enc.categories_\\n[array([\\'female\\', \\'male\\'], dtype=object), array([\\'from Europe\\', \\'from US\\'], dtype=object),\\narray([\\'uses Firefox\\', \\'uses Safari\\'], dtype=object)]\\ndrop_enc.transform(X).toarray()\\narray([[1., 1., 1.],\\n[0., 0., 0.]])\\nOne might want to drop one of the two columns only for features with 2\\ncategories. In this case, you can set the parameter drop=\\'if_binary\\'.\\nX = [[\\'male\\', \\'US\\', \\'Safari\\'],\\n...      [\\'female\\', \\'Europe\\', \\'Firefox\\'],\\n...      [\\'female\\', \\'Asia\\', \\'Chrome\\']]\\ndrop_enc = preprocessing.OneHotEncoder(drop=\\'if_binary\\').fit(X)\\ndrop_enc.categories_\\n[array([\\'female\\', \\'male\\'], dtype=object), array([\\'Asia\\', \\'Europe\\', \\'US\\'], dtype=object),\\narray([\\'Chrome\\', \\'Firefox\\', \\'Safari\\'], dtype=object)]\\ndrop_enc.transform(X).toarray()\\narray([[1., 0., 0., 1., 0., 0., 1.],\\n[0., 0., 1., 0., 0., 1., 0.],\\n[0., 1., 0., 0., 1., 0., 0.]])\\nIn the transformed X, the first column is the encoding of the feature with\\ncategories “male”/”female”, while the remaining 6 columns is the encoding of\\nthe 2 features with respectively 3 categories each.\\nWhen handle_unknown=\\'ignore\\' and drop is not None, unknown categories will\\nbe encoded as all zeros:\\ndrop_enc = preprocessing.OneHotEncoder(drop=\\'first\\',\\n...                                        handle_unknown=\\'ignore\\').fit(X)\\nX_test = [[\\'unknown\\', \\'America\\', \\'IE\\']]\\ndrop_enc.transform(X_test).toarray()\\narray([[0., 0., 0., 0., 0.]])\\nAll the categories in X_test are unknown during transform and will be mapped\\nto all zeros. This means that unknown categories will have the same mapping as\\nthe dropped category. OneHotEncoder.inverse_transform will map all zeros\\nto the dropped category if a category is dropped and None if a category is\\nnot dropped:\\ndrop_enc = preprocessing.OneHotEncoder(drop=\\'if_binary\\', sparse_output=False,\\n...                                        handle_unknown=\\'ignore\\').fit(X)\\nX_test = [[\\'unknown\\', \\'America\\', \\'IE\\']]\\nX_trans = drop_enc.transform(X_test)\\nX_trans\\narray([[0., 0., 0., 0., 0., 0., 0.]])\\ndrop_enc.inverse_transform(X_trans)\\narray([[\\'female\\', None, None]], dtype=object)\\nOneHotEncoder supports categorical features with missing values by\\nconsidering the missing values as an additional category:\\nX = [[\\'male\\', \\'Safari\\'],\\n...      [\\'female\\', None],\\n...      [np.nan, \\'Firefox\\']]\\nenc = preprocessing.OneHotEncoder(handle_unknown=\\'error\\').fit(X)\\nenc.categories_\\n[array([\\'female\\', \\'male\\', nan], dtype=object),\\narray([\\'Firefox\\', \\'Safari\\', None], dtype=object)]\\nenc.transform(X).toarray()\\narray([[0., 1., 0., 0., 1., 0.],\\n[1., 0., 0., 0., 0., 1.],\\n[0., 0., 1., 1., 0., 0.]])\\nIf a feature contains both np.nan and None, they will be considered\\nseparate categories:\\nX = [[\\'Safari\\'], [None], [np.nan], [\\'Firefox\\']]\\nenc = preprocessing.OneHotEncoder(handle_unknown=\\'error\\').fit(X)\\nenc.categories_\\n[array([\\'Firefox\\', \\'Safari\\', None, nan], dtype=object)]\\nenc.transform(X).toarray()\\narray([[0., 1., 0., 0.],\\n[0., 0., 1., 0.],\\n[0., 0., 0., 1.],\\n[1., 0., 0., 0.]])\\nSee Loading features from dicts for categorical features that are\\nrepresented as a dict, not as scalars.\\n6.3.4.1. Infrequent categories¶\\nOneHotEncoder and OrdinalEncoder support aggregating\\ninfrequent categories into a single output for each feature. The parameters to\\nenable the gathering of infrequent categories are min_frequency and\\nmax_categories.\\nmin_frequency is either an  integer greater or equal to 1, or a float in\\nthe interval (0.0, 1.0). If min_frequency is an integer, categories with\\na cardinality smaller than min_frequency  will be considered infrequent.\\nIf min_frequency is a float, categories with a cardinality smaller than\\nthis fraction of the total number of samples will be considered infrequent.\\nThe default value is 1, which means every category is encoded separately.\\nmax_categories is either None or any integer greater than 1. This\\nparameter sets an upper limit to the number of output features for each\\ninput feature. max_categories includes the feature that combines\\ninfrequent categories.\\nIn the following example with OrdinalEncoder, the categories \\'dog\\' and\\n\\'snake\\' are considered infrequent:\\nX = np.array([[\\'dog\\'] * 5 + [\\'cat\\'] * 20 + [\\'rabbit\\'] * 10 +\\n...               [\\'snake\\'] * 3], dtype=object).T\\nenc = preprocessing.OrdinalEncoder(min_frequency=6).fit(X)\\nenc.infrequent_categories_\\n[array([\\'dog\\', \\'snake\\'], dtype=object)]\\nenc.transform(np.array([[\\'dog\\'], [\\'cat\\'], [\\'rabbit\\'], [\\'snake\\']]))\\narray([[2.],\\n[0.],\\n[1.],\\n[2.]])\\nOrdinalEncoder’s max_categories do not take into account missing\\nor unknown categories. Setting unknown_value or encoded_missing_value to an\\ninteger will increase the number of unique integer codes by one each. This can\\nresult in up to max_categories + 2 integer codes. In the following example,\\n“a” and “d” are considered infrequent and grouped together into a single\\ncategory, “b” and “c” are their own categories, unknown values are encoded as 3\\nand missing values are encoded as 4.\\nX_train = np.array(\\n...     [[\"a\"] * 5 + [\"b\"] * 20 + [\"c\"] * 10 + [\"d\"] * 3 + [np.nan]],\\n...     dtype=object).T\\nenc = preprocessing.OrdinalEncoder(\\n...     handle_unknown=\"use_encoded_value\", unknown_value=3,\\n...     max_categories=3, encoded_missing_value=4)\\n_ = enc.fit(X_train)\\nX_test = np.array([[\"a\"], [\"b\"], [\"c\"], [\"d\"], [\"e\"], [np.nan]], dtype=object)\\nenc.transform(X_test)\\narray([[2.],\\n[0.],\\n[1.],\\n[2.],\\n[3.],\\n[4.]])\\nSimilarity, OneHotEncoder can be configured to group together infrequent\\ncategories:\\nenc = preprocessing.OneHotEncoder(min_frequency=6, sparse_output=False).fit(X)\\nenc.infrequent_categories_\\n[array([\\'dog\\', \\'snake\\'], dtype=object)]\\nenc.transform(np.array([[\\'dog\\'], [\\'cat\\'], [\\'rabbit\\'], [\\'snake\\']]))\\narray([[0., 0., 1.],\\n[1., 0., 0.],\\n[0., 1., 0.],\\n[0., 0., 1.]])\\nBy setting handle_unknown to \\'infrequent_if_exist\\', unknown categories will\\nbe considered infrequent:\\nenc = preprocessing.OneHotEncoder(\\n...    handle_unknown=\\'infrequent_if_exist\\', sparse_output=False, min_frequency=6)\\nenc = enc.fit(X)\\nenc.transform(np.array([[\\'dragon\\']]))\\narray([[0., 0., 1.]])\\nOneHotEncoder.get_feature_names_out uses ‘infrequent’ as the infrequent\\nfeature name:\\nenc.get_feature_names_out()\\narray([\\'x0_cat\\', \\'x0_rabbit\\', \\'x0_infrequent_sklearn\\'], dtype=object)\\nWhen \\'handle_unknown\\' is set to \\'infrequent_if_exist\\' and an unknown\\ncategory is encountered in transform:\\nIf infrequent category support was not configured or there was no\\ninfrequent category during training, the resulting one-hot encoded columns\\nfor this feature will be all zeros. In the inverse transform, an unknown\\ncategory will be denoted as None.\\nIf there is an infrequent category during training, the unknown category\\nwill be considered infrequent. In the inverse transform, ‘infrequent_sklearn’\\nwill be used to represent the infrequent category.\\nInfrequent categories can also be configured using max_categories. In the\\nfollowing example, we set max_categories=2 to limit the number of features in\\nthe output. This will result in all but the \\'cat\\' category to be considered\\ninfrequent, leading to two features, one for \\'cat\\' and one for infrequent\\ncategories - which are all the others:\\nenc = preprocessing.OneHotEncoder(max_categories=2, sparse_output=False)\\nenc = enc.fit(X)\\nenc.transform([[\\'dog\\'], [\\'cat\\'], [\\'rabbit\\'], [\\'snake\\']])\\narray([[0., 1.],\\n[1., 0.],\\n[0., 1.],\\n[0., 1.]])\\nIf both max_categories and min_frequency are non-default values, then\\ncategories are selected based on min_frequency first and max_categories\\ncategories are kept. In the following example, min_frequency=4 considers\\nonly snake to be infrequent, but max_categories=3, forces dog to also be\\ninfrequent:\\nenc = preprocessing.OneHotEncoder(min_frequency=4, max_categories=3, sparse_output=False)\\nenc = enc.fit(X)\\nenc.transform([[\\'dog\\'], [\\'cat\\'], [\\'rabbit\\'], [\\'snake\\']])\\narray([[0., 0., 1.],\\n[1., 0., 0.],\\n[0., 1., 0.],\\n[0., 0., 1.]])\\nIf there are infrequent categories with the same cardinality at the cutoff of\\nmax_categories, then then the first max_categories are taken based on lexicon\\nordering. In the following example, “b”, “c”, and “d”, have the same cardinality\\nand with max_categories=2, “b” and “c” are infrequent because they have a higher\\nlexicon order.\\nX = np.asarray([[\"a\"] * 20 + [\"b\"] * 10 + [\"c\"] * 10 + [\"d\"] * 10], dtype=object).T\\nenc = preprocessing.OneHotEncoder(max_categories=3).fit(X)\\nenc.infrequent_categories_\\n[array([\\'b\\', \\'c\\'], dtype=object)]\\n6.3.4.2. Target Encoder¶\\nThe TargetEncoder uses the target mean conditioned on the categorical\\nfeature for encoding unordered categories, i.e. nominal categories [PAR]\\n[MIC]. This encoding scheme is useful with categorical features with high\\ncardinality, where one-hot encoding would inflate the feature space making it\\nmore expensive for a downstream model to process. A classical example of high\\ncardinality categories are location based such as zip code or region. For the\\nbinary classification target, the target encoding is given by:\\n[S_i = \\\\lambda_i\\\\frac{n_{iY}}{n_i} + (1 - \\\\lambda_i)\\\\frac{n_Y}{n}]\\nwhere (S_i) is the encoding for category (i), (n_{iY}) is the\\nnumber of observations with (Y=1) and category (i), (n_i) is\\nthe number of observations with category (i), (n_Y) is the number of\\nobservations with (Y=1), (n) is the number of observations, and\\n(\\\\lambda_i) is a shrinkage factor for category (i). The shrinkage\\nfactor is given by:\\n[\\\\lambda_i = \\\\frac{n_i}{m + n_i}]\\nwhere (m) is a smoothing factor, which is controlled with the smooth\\nparameter in TargetEncoder. Large smoothing factors will put more\\nweight on the global mean. When smooth=\"auto\", the smoothing factor is\\ncomputed as an empirical Bayes estimate: (m=\\\\sigma_i^2/\\\\tau^2), where\\n(\\\\sigma_i^2) is the variance of y with category (i) and\\n(\\\\tau^2) is the global variance of y.\\nFor multiclass classification targets, the formulation is similar to binary\\nclassification:\\n[S_{ij} = \\\\lambda_i\\\\frac{n_{iY_j}}{n_i} + (1 - \\\\lambda_i)\\\\frac{n_{Y_j}}{n}]\\nwhere (S_{ij}) is the encoding for category (i) and class (j),\\n(n_{iY_j}) is the number of observations with (Y=j) and category\\n(i), (n_i) is the number of observations with category (i),\\n(n_{Y_j}) is the number of observations with (Y=j), (n) is the\\nnumber of observations, and (\\\\lambda_i) is a shrinkage factor for category\\n(i).\\nFor continuous targets, the formulation is similar to binary classification:\\n[S_i = \\\\lambda_i\\\\frac{\\\\sum_{k\\\\in L_i}Y_k}{n_i} + (1 - \\\\lambda_i)\\\\frac{\\\\sum_{k=1}^{n}Y_k}{n}]\\nwhere (L_i) is the set of observations with category (i) and\\n(n_i) is the number of observations with category (i).\\nfit_transform internally relies on a cross fitting\\nscheme to prevent target information from leaking into the train-time\\nrepresentation, especially for non-informative high-cardinality categorical\\nvariables, and help prevent the downstream model from overfitting spurious\\ncorrelations. Note that as a result, fit(X, y).transform(X) does not equal\\nfit_transform(X, y). In fit_transform, the training\\ndata is split into k folds (determined by the cv parameter) and each fold is\\nencoded using the encodings learnt using the other k-1 folds. The following\\ndiagram shows the cross fitting scheme in\\nfit_transform with the default cv=5:\\nfit_transform also learns a ‘full data’ encoding using\\nthe whole training set. This is never used in\\nfit_transform but is saved to the attribute encodings_,\\nfor use when transform is called. Note that the encodings\\nlearned for each fold during the cross fitting scheme are not saved to\\nan attribute.\\nThe fit method does not use any cross fitting\\nschemes and learns one encoding on the entire training set, which is used to\\nencode categories in transform.\\nThis encoding is the same as the ‘full data’\\nencoding learned in fit_transform.\\nNote\\nTargetEncoder considers missing values, such as np.nan or None,\\nas another category and encodes them like any other category. Categories\\nthat are not seen during fit are encoded with the target mean, i.e.\\ntarget_mean_.\\nExamples:\\nComparing Target Encoder with Other Encoders\\nTarget Encoder’s Internal Cross fitting\\nReferences\\n[MIC]\\nMicci-Barreca, Daniele. “A preprocessing scheme for high-cardinality\\ncategorical attributes in classification and prediction problems”\\nSIGKDD Explor. Newsl. 3, 1 (July 2001), 27–32.\\n[PAR]\\nPargent, F., Pfisterer, F., Thomas, J. et al. “Regularized target\\nencoding outperforms traditional methods in supervised machine learning with\\nhigh cardinality features” Comput Stat 37, 2671–2692 (2022)\\n- 6.3.5. Discretization¶\\nDiscretization\\n(otherwise known as quantization or binning) provides a way to partition continuous\\nfeatures into discrete values. Certain datasets with continuous features\\nmay benefit from discretization, because discretization can transform the dataset\\nof continuous attributes to one with only nominal attributes.\\nOne-hot encoded discretized features can make a model more expressive, while\\nmaintaining interpretability. For instance, pre-processing with a discretizer\\ncan introduce nonlinearity to linear models. For more advanced possibilities,\\nin particular smooth ones, see Generating polynomial features further\\nbelow.\\n6.3.5.1. K-bins discretization¶\\nKBinsDiscretizer discretizes features into k bins:\\nX = np.array([[ -3., 5., 15 ],\\n...               [  0., 6., 14 ],\\n...               [  6., 3., 11 ]])\\nest = preprocessing.KBinsDiscretizer(n_bins=[3, 2, 2], encode=\\'ordinal\\').fit(X)\\nBy default the output is one-hot encoded into a sparse matrix\\n(See Encoding categorical features)\\nand this can be configured with the encode parameter.\\nFor each feature, the bin edges are computed during fit and together with\\nthe number of bins, they will define the intervals. Therefore, for the current\\nexample, these intervals are defined as:\\nfeature 1: ({[-\\\\infty, -1), [-1, 2), [2, \\\\infty)})\\nfeature 2: ({[-\\\\infty, 5), [5, \\\\infty)})\\nfeature 3: ({[-\\\\infty, 14), [14, \\\\infty)})\\nBased on these bin intervals, X is transformed as follows:\\nest.transform(X)\\narray([[ 0., 1., 1.],\\n[ 1., 1., 1.],\\n[ 2., 0., 0.]])\\nThe resulting dataset contains ordinal attributes which can be further used\\nin a Pipeline.\\nDiscretization is similar to constructing histograms for continuous data.\\nHowever, histograms focus on counting features which fall into particular\\nbins, whereas discretization focuses on assigning feature values to these bins.\\nKBinsDiscretizer implements different binning strategies, which can be\\nselected with the strategy parameter. The ‘uniform’ strategy uses\\nconstant-width bins. The ‘quantile’ strategy uses the quantiles values to have\\nequally populated bins in each feature. The ‘kmeans’ strategy defines bins based\\non a k-means clustering procedure performed on each feature independently.\\nBe aware that one can specify custom bins by passing a callable defining the\\ndiscretization strategy to FunctionTransformer.\\nFor instance, we can use the Pandas function pandas.cut:\\nimport pandas as pd\\nimport numpy as np\\nfrom sklearn import preprocessing\\n\\nbins = [0, 1, 13, 20, 60, np.inf]\\nlabels = [\\'infant\\', \\'kid\\', \\'teen\\', \\'adult\\', \\'senior citizen\\']\\ntransformer = preprocessing.FunctionTransformer(\\n...     pd.cut, kw_args={\\'bins\\': bins, \\'labels\\': labels, \\'retbins\\': False}\\n... )\\nX = np.array([0.2, 2, 15, 25, 97])\\ntransformer.fit_transform(X)\\n[\\'infant\\', \\'kid\\', \\'teen\\', \\'adult\\', \\'senior citizen\\']\\nCategories (5, object): [\\'infant\\' < \\'kid\\' < \\'teen\\' < \\'adult\\' < \\'senior citizen\\']\\nExamples:\\nUsing KBinsDiscretizer to discretize continuous features\\nFeature discretization\\nDemonstrating the different strategies of KBinsDiscretizer\\n6.3.5.2. Feature binarization¶\\nFeature binarization is the process of thresholding numerical\\nfeatures to get boolean values. This can be useful for downstream\\nprobabilistic estimators that make assumption that the input data\\nis distributed according to a multi-variate Bernoulli distribution. For instance,\\nthis is the case for the BernoulliRBM.\\nIt is also common among the text processing community to use binary\\nfeature values (probably to simplify the probabilistic reasoning) even\\nif normalized counts (a.k.a. term frequencies) or TF-IDF valued features\\noften perform slightly better in practice.\\nAs for the Normalizer, the utility class\\nBinarizer is meant to be used in the early stages of\\nPipeline. The fit method does nothing\\nas each sample is treated independently of others:\\nX = [[ 1., -1.,  2.],\\n...      [ 2.,  0.,  0.],\\n...      [ 0.,  1., -1.]]\\nbinarizer = preprocessing.Binarizer().fit(X)  # fit does nothing\\nbinarizer\\nBinarizer()\\nbinarizer.transform(X)\\narray([[1., 0., 1.],\\n[1., 0., 0.],\\n[0., 1., 0.]])\\nIt is possible to adjust the threshold of the binarizer:\\nbinarizer = preprocessing.Binarizer(threshold=1.1)\\nbinarizer.transform(X)\\narray([[0., 0., 1.],\\n[1., 0., 0.],\\n[0., 0., 0.]])\\nAs for the Normalizer class, the preprocessing module\\nprovides a companion function binarize\\nto be used when the transformer API is not necessary.\\nNote that the Binarizer is similar to the KBinsDiscretizer\\nwhen k = 2, and when the bin edge is at the value threshold.\\nSparse input\\nbinarize and Binarizer accept both dense array-like\\nand sparse matrices from scipy.sparse as input.\\nFor sparse input the data is converted to the Compressed Sparse Rows\\nrepresentation (see scipy.sparse.csr_matrix).\\nTo avoid unnecessary memory copies, it is recommended to choose the CSR\\nrepresentation upstream.\\n- 6.3.6. Imputation of missing values¶\\nTools for imputing missing values are discussed at Imputation of missing values.\\n- 6.3.7. Generating polynomial features¶\\nOften it’s useful to add complexity to a model by considering nonlinear\\nfeatures of the input data. We show two possibilities that are both based on\\npolynomials: The first one uses pure polynomials, the second one uses splines,\\ni.e. piecewise polynomials.\\n6.3.7.1. Polynomial features¶\\nA simple and common method to use is polynomial features, which can get\\nfeatures’ high-order and interaction terms. It is implemented in\\nPolynomialFeatures:\\nimport numpy as np\\nfrom sklearn.preprocessing import PolynomialFeatures\\nX = np.arange(6).reshape(3, 2)\\nX\\narray([[0, 1],\\n[2, 3],\\n[4, 5]])\\npoly = PolynomialFeatures(2)\\npoly.fit_transform(X)\\narray([[ 1.,  0.,  1.,  0.,  0.,  1.],\\n[ 1.,  2.,  3.,  4.,  6.,  9.],\\n[ 1.,  4.,  5., 16., 20., 25.]])\\nThe features of X have been transformed from ((X_1, X_2)) to\\n((1, X_1, X_2, X_1^2, X_1X_2, X_2^2)).\\nIn some cases, only interaction terms among features are required, and it can\\nbe gotten with the setting interaction_only=True:\\nX = np.arange(9).reshape(3, 3)\\nX\\narray([[0, 1, 2],\\n[3, 4, 5],\\n[6, 7, 8]])\\npoly = PolynomialFeatures(degree=3, interaction_only=True)\\npoly.fit_transform(X)\\narray([[  1.,   0.,   1.,   2.,   0.,   0.,   2.,   0.],\\n[  1.,   3.,   4.,   5.,  12.,  15.,  20.,  60.],\\n[  1.,   6.,   7.,   8.,  42.,  48.,  56., 336.]])\\nThe features of X have been transformed from ((X_1, X_2, X_3)) to\\n((1, X_1, X_2, X_3, X_1X_2, X_1X_3, X_2X_3, X_1X_2X_3)).\\nNote that polynomial features are used implicitly in kernel methods (e.g., SVC,\\nKernelPCA) when using polynomial Kernel functions.\\nSee Polynomial and Spline interpolation\\nfor Ridge regression using created polynomial features.\\n6.3.7.2. Spline transformer¶\\nAnother way to add nonlinear terms instead of pure polynomials of features is\\nto generate spline basis functions for each feature with the\\nSplineTransformer. Splines are piecewise polynomials, parametrized by\\ntheir polynomial degree and the positions of the knots. The\\nSplineTransformer implements a B-spline basis, cf. the references\\nbelow.\\nNote\\nThe SplineTransformer treats each feature separately, i.e. it\\nwon’t give you interaction terms.\\nSome of the advantages of splines over polynomials are:\\nB-splines are very flexible and robust if you keep a fixed low degree,\\nusually 3, and parsimoniously adapt the number of knots. Polynomials\\nwould need a higher degree, which leads to the next point.\\nB-splines do not have oscillatory behaviour at the boundaries as have\\npolynomials (the higher the degree, the worse). This is known as Runge’s\\nphenomenon.\\nB-splines provide good options for extrapolation beyond the boundaries,\\ni.e. beyond the range of fitted values. Have a look at the option\\nextrapolation.\\nB-splines generate a feature matrix with a banded structure. For a single\\nfeature, every row contains only degree + 1 non-zero elements, which\\noccur consecutively and are even positive. This results in a matrix with\\ngood numerical properties, e.g. a low condition number, in sharp contrast\\nto a matrix of polynomials, which goes under the name\\nVandermonde matrix.\\nA low condition number is important for stable algorithms of linear\\nmodels.\\nThe following code snippet shows splines in action:\\nimport numpy as np\\nfrom sklearn.preprocessing import SplineTransformer\\nX = np.arange(5).reshape(5, 1)\\nX\\narray([[0],\\n[1],\\n[2],\\n[3],\\n[4]])\\nspline = SplineTransformer(degree=2, n_knots=3)\\nspline.fit_transform(X)\\narray([[0.5  , 0.5  , 0.   , 0.   ],\\n[0.125, 0.75 , 0.125, 0.   ],\\n[0.   , 0.5  , 0.5  , 0.   ],\\n[0.   , 0.125, 0.75 , 0.125],\\n[0.   , 0.   , 0.5  , 0.5  ]])\\nAs the X is sorted, one can easily see the banded matrix output. Only the\\nthree middle diagonals are non-zero for degree=2. The higher the degree,\\nthe more overlapping of the splines.\\nInterestingly, a SplineTransformer of degree=0 is the same as\\nKBinsDiscretizer with\\nencode=\\'onehot-dense\\' and n_bins = n_knots - 1 if\\nknots = strategy.\\nExamples:\\nPolynomial and Spline interpolation\\nTime-related feature engineering\\nReferences:\\nEilers, P., & Marx, B. (1996). Flexible Smoothing with B-splines and\\nPenalties. Statist. Sci. 11 (1996), no. 2, 89–121.\\nPerperoglou, A., Sauerbrei, W., Abrahamowicz, M. et al. A review of\\nspline function procedures in R.\\nBMC Med Res Methodol 19, 46 (2019).\\n- 6.3.8. Custom transformers¶\\nOften, you will want to convert an existing Python function into a transformer\\nto assist in data cleaning or processing. You can implement a transformer from\\nan arbitrary function with FunctionTransformer. For example, to build\\na transformer that applies a log transformation in a pipeline, do:\\nimport numpy as np\\nfrom sklearn.preprocessing import FunctionTransformer\\ntransformer = FunctionTransformer(np.log1p, validate=True)\\nX = np.array([[0, 1], [2, 3]])\\n\\nSince FunctionTransformer is no-op during fit, we can call transform directly\\n\\ntransformer.transform(X)\\narray([[0.        , 0.69314718],\\n[1.09861229, 1.38629436]])\\nYou can ensure that func and inverse_func are the inverse of each other\\nby setting check_inverse=True and calling fit before\\ntransform. Please note that a warning is raised and can be turned into an\\nerror with a filterwarnings:\\nimport warnings\\nwarnings.filterwarnings(\"error\", message=\".check_inverse.\",\\n...                         category=UserWarning, append=False)\\nFor a full code example that demonstrates using a FunctionTransformer\\nto extract features from text data see\\nColumn Transformer with Heterogeneous Data Sources and\\nTime-related feature engineering.\\n\\n6.4. Imputation of missing values — scikit-learn 1.4.2 documentation\\n\\nInstall\\nUser Guide\\nAPI\\nExamples\\nCommunity\\nGetting Started\\nTutorial\\nWhat\\'s new\\nGlossary\\nDevelopment\\nFAQ\\nSupport\\nRelated packages\\nRoadmap\\nGovernance\\nAbout us\\nGitHub\\nOther Versions and Download\\nMore\\nGetting Started\\nTutorial\\nWhat\\'s new\\nGlossary\\nDevelopment\\nFAQ\\nSupport\\nRelated packages\\nRoadmap\\nGovernance\\nAbout us\\nGitHub\\nOther Versions and Download\\nToggle Menu\\nPrevUp\\nNext\\nscikit-learn 1.4.2\\nOther versions\\nPlease cite us if you use the software.\\n\\n6.4. Imputation of missing values\\n\\n6.4.1. Univariate vs. Multivariate Imputation\\n\\n6.4.2. Univariate feature imputation\\n\\n6.4.3. Multivariate feature imputation\\n6.4.3.1. Flexibility of IterativeImputer\\n6.4.3.2. Multiple vs. Single Imputation\\n6.4.3.3. References\\n\\n6.4.4. Nearest neighbors imputation\\n\\n6.4.5. Keeping the number of features constant\\n\\n6.4.6. Marking imputed values\\n\\n6.4.7. Estimators that handle NaN values\\n\\n6.4. Imputation of missing values¶\\n\\nFor various reasons, many real world datasets contain missing values, often\\nencoded as blanks, NaNs or other placeholders. Such datasets however are\\nincompatible with scikit-learn estimators which assume that all values in an\\narray are numerical, and that all have and hold meaning. A basic strategy to\\nuse incomplete datasets is to discard entire rows and/or columns containing\\nmissing values. However, this comes at the price of losing data which may be\\nvaluable (even though incomplete). A better strategy is to impute the missing\\nvalues, i.e., to infer them from the known part of the data. See the\\nglossary entry on imputation.\\n- 6.4.1. Univariate vs. Multivariate Imputation¶\\nOne type of imputation algorithm is univariate, which imputes values in the\\ni-th feature dimension using only non-missing values in that feature dimension\\n(e.g. SimpleImputer). By contrast, multivariate imputation\\nalgorithms use the entire set of available feature dimensions to estimate the\\nmissing values (e.g. IterativeImputer).\\n- 6.4.2. Univariate feature imputation¶\\nThe SimpleImputer class provides basic strategies for imputing missing\\nvalues. Missing values can be imputed with a provided constant value, or using\\nthe statistics (mean, median or most frequent) of each column in which the\\nmissing values are located. This class also allows for different missing values\\nencodings.\\nThe following snippet demonstrates how to replace missing values,\\nencoded as np.nan, using the mean value of the columns (axis 0)\\nthat contain the missing values:\\n\\nimport numpy as np\\nfrom sklearn.impute import SimpleImputer\\nimp = SimpleImputer(missing_values=np.nan, strategy=\\'mean\\')\\nimp.fit([[1, 2], [np.nan, 3], [7, 6]])\\nSimpleImputer()\\nX = [[np.nan, 2], [6, np.nan], [7, 6]]\\nprint(imp.transform(X))\\n[[4.          2.        ]\\n[6.          3.666...]\\n[7.          6.        ]]\\nThe SimpleImputer class also supports sparse matrices:\\nimport scipy.sparse as sp\\nX = sp.csc_matrix([[1, 2], [0, -1], [8, 4]])\\nimp = SimpleImputer(missing_values=-1, strategy=\\'mean\\')\\nimp.fit(X)\\nSimpleImputer(missing_values=-1)\\nX_test = sp.csc_matrix([[-1, 2], [6, -1], [7, 6]])\\nprint(imp.transform(X_test).toarray())\\n[[3. 2.]\\n[6. 3.]\\n[7. 6.]]\\nNote that this format is not meant to be used to implicitly store missing\\nvalues in the matrix because it would densify it at transform time. Missing\\nvalues encoded by 0 must be used with dense input.\\nThe SimpleImputer class also supports categorical data represented as\\nstring values or pandas categoricals when using the \\'most_frequent\\' or\\n\\'constant\\' strategy:\\nimport pandas as pd\\ndf = pd.DataFrame([[\"a\", \"x\"],\\n...                    [np.nan, \"y\"],\\n...                    [\"a\", np.nan],\\n...                    [\"b\", \"y\"]], dtype=\"category\")\\n...\\nimp = SimpleImputer(strategy=\"most_frequent\")\\nprint(imp.fit_transform(df))\\n[[\\'a\\' \\'x\\']\\n[\\'a\\' \\'y\\']\\n[\\'a\\' \\'y\\']\\n[\\'b\\' \\'y\\']]\\nFor another example on usage, see Imputing missing values before building an estimator.\\n- 6.4.3. Multivariate feature imputation¶\\nA more sophisticated approach is to use the IterativeImputer class,\\nwhich models each feature with missing values as a function of other features,\\nand uses that estimate for imputation. It does so in an iterated round-robin\\nfashion: at each step, a feature column is designated as output y and the\\nother feature columns are treated as inputs X. A regressor is fit on (X,\\ny) for known y. Then, the regressor is used to predict the missing values\\nof y.  This is done for each feature in an iterative fashion, and then is\\nrepeated for max_iter imputation rounds. The results of the final\\nimputation round are returned.\\nNote\\nThis estimator is still experimental for now: default parameters or\\ndetails of behaviour might change without any deprecation cycle. Resolving\\nthe following issues would help stabilize IterativeImputer:\\nconvergence criteria (#14338), default estimators (#13286),\\nand use of random state (#15611). To use it, you need to explicitly\\nimport enable_iterative_imputer.\\nimport numpy as np\\nfrom sklearn.experimental import enable_iterative_imputer\\nfrom sklearn.impute import IterativeImputer\\nimp = IterativeImputer(max_iter=10, random_state=0)\\nimp.fit([[1, 2], [3, 6], [4, 8], [np.nan, 3], [7, np.nan]])\\nIterativeImputer(random_state=0)\\nX_test = [[np.nan, 2], [6, np.nan], [np.nan, 6]]\\n\\nthe model learns that the second feature is double the first\\n\\nprint(np.round(imp.transform(X_test)))\\n[[ 1.  2.]\\n[ 6. 12.]\\n[ 3.  6.]]\\nBoth SimpleImputer and IterativeImputer can be used in a\\nPipeline as a way to build a composite estimator that supports imputation.\\nSee Imputing missing values before building an estimator.\\n6.4.3.1. Flexibility of IterativeImputer¶\\nThere are many well-established imputation packages in the R data science\\necosystem: Amelia, mi, mice, missForest, etc. missForest is popular, and turns\\nout to be a particular instance of different sequential imputation algorithms\\nthat can all be implemented with IterativeImputer by passing in\\ndifferent regressors to be used for predicting missing feature values. In the\\ncase of missForest, this regressor is a Random Forest.\\nSee Imputing missing values with variants of IterativeImputer.\\n6.4.3.2. Multiple vs. Single Imputation¶\\nIn the statistics community, it is common practice to perform multiple\\nimputations, generating, for example, m separate imputations for a single\\nfeature matrix. Each of these m imputations is then put through the\\nsubsequent analysis pipeline (e.g. feature engineering, clustering, regression,\\nclassification). The m final analysis results (e.g. held-out validation\\nerrors) allow the data scientist to obtain understanding of how analytic\\nresults may differ as a consequence of the inherent uncertainty caused by the\\nmissing values. The above practice is called multiple imputation.\\nOur implementation of IterativeImputer was inspired by the R MICE\\npackage (Multivariate Imputation by Chained Equations) [1], but differs from\\nit by returning a single imputation instead of multiple imputations.  However,\\nIterativeImputer can also be used for multiple imputations by applying\\nit repeatedly to the same dataset with different random seeds when\\nsample_posterior=True. See [2], chapter 4 for more discussion on multiple\\nvs. single imputations.\\nIt is still an open problem as to how useful single vs. multiple imputation is\\nin the context of prediction and classification when the user is not\\ninterested in measuring uncertainty due to missing values.\\nNote that a call to the transform method of IterativeImputer is\\nnot allowed to change the number of samples. Therefore multiple imputations\\ncannot be achieved by a single call to transform.\\n6.4.3.3. References¶\\n[1]\\nStef van Buuren, Karin Groothuis-Oudshoorn (2011). “mice: Multivariate\\nImputation by Chained Equations in R”. Journal of Statistical Software 45:\\n1-67.\\n[2]\\nRoderick J A Little and Donald B Rubin (1986). “Statistical Analysis\\nwith Missing Data”. John Wiley & Sons, Inc., New York, NY, USA.\\n- 6.4.4. Nearest neighbors imputation¶\\nThe KNNImputer class provides imputation for filling in missing values\\nusing the k-Nearest Neighbors approach. By default, a euclidean distance metric\\nthat supports missing values,\\nnan_euclidean_distances, is used to find the\\nnearest neighbors. Each missing feature is imputed using values from\\nn_neighbors nearest neighbors that have a value for the feature. The\\nfeature of the neighbors are averaged uniformly or weighted by distance to each\\nneighbor. If a sample has more than one feature missing, then the neighbors for\\nthat sample can be different depending on the particular feature being imputed.\\nWhen the number of available neighbors is less than n_neighbors and there are\\nno defined distances to the training set, the training set average for that\\nfeature is used during imputation. If there is at least one neighbor with a\\ndefined distance, the weighted or unweighted average of the remaining neighbors\\nwill be used during imputation. If a feature is always missing in training, it\\nis removed during transform. For more information on the methodology, see\\nref. [OL2001].\\nThe following snippet demonstrates how to replace missing values,\\nencoded as np.nan, using the mean feature value of the two nearest\\nneighbors of samples with missing values:\\nimport numpy as np\\nfrom sklearn.impute import KNNImputer\\nnan = np.nan\\nX = [[1, 2, nan], [3, 4, 3], [nan, 6, 5], [8, 8, 7]]\\nimputer = KNNImputer(n_neighbors=2, weights=\"uniform\")\\nimputer.fit_transform(X)\\narray([[1. , 2. , 4. ],\\n[3. , 4. , 3. ],\\n[5.5, 6. , 5. ],\\n[8. , 8. , 7. ]])\\nFor another example on usage, see Imputing missing values before building an estimator.\\nReferences\\n[OL2001]\\nOlga Troyanskaya, Michael Cantor, Gavin Sherlock, Pat Brown,\\nTrevor Hastie, Robert Tibshirani, David Botstein and Russ B. Altman,\\nMissing value estimation methods for DNA microarrays, BIOINFORMATICS\\nVol. 17 no. 6, 2001 Pages 520-525.\\n- 6.4.5. Keeping the number of features constant¶\\nBy default, the scikit-learn imputers will drop fully empty features, i.e.\\ncolumns containing only missing values. For instance:\\nimputer = SimpleImputer()\\nX = np.array([[np.nan, 1], [np.nan, 2], [np.nan, 3]])\\nimputer.fit_transform(X)\\narray([[1.],\\n[2.],\\n[3.]])\\nThe first feature in X containing only np.nan was dropped after the\\nimputation. While this feature will not help in predictive setting, dropping\\nthe columns will change the shape of X which could be problematic when using\\nimputers in a more complex machine-learning pipeline. The parameter\\nkeep_empty_features offers the option to keep the empty features by imputing\\nwith a constant values. In most of the cases, this constant value is zero:\\nimputer.set_params(keep_empty_features=True)\\nSimpleImputer(keep_empty_features=True)\\nimputer.fit_transform(X)\\narray([[0., 1.],\\n[0., 2.],\\n[0., 3.]])\\n- 6.4.6. Marking imputed values¶\\nThe MissingIndicator transformer is useful to transform a dataset into\\ncorresponding binary matrix indicating the presence of missing values in the\\ndataset. This transformation is useful in conjunction with imputation. When\\nusing imputation, preserving the information about which values had been\\nmissing can be informative. Note that both the SimpleImputer and\\nIterativeImputer have the boolean parameter add_indicator\\n(False by default) which when set to True provides a convenient way of\\nstacking the output of the MissingIndicator transformer with the\\noutput of the imputer.\\nNaN is usually used as the placeholder for missing values. However, it\\nenforces the data type to be float. The parameter missing_values allows to\\nspecify other placeholder such as integer. In the following example, we will\\nuse -1 as missing values:\\nfrom sklearn.impute import MissingIndicator\\nX = np.array([[-1, -1, 1, 3],\\n...               [4, -1, 0, -1],\\n...               [8, -1, 1, 0]])\\nindicator = MissingIndicator(missing_values=-1)\\nmask_missing_values_only = indicator.fit_transform(X)\\nmask_missing_values_only\\narray([[ True,  True, False],\\n[False,  True,  True],\\n[False,  True, False]])\\nThe features parameter is used to choose the features for which the mask is\\nconstructed. By default, it is \\'missing-only\\' which returns the imputer\\nmask of the features containing missing values at fit time:\\nindicator.features_\\narray([0, 1, 3])\\nThe features parameter can be set to \\'all\\' to return all features\\nwhether or not they contain missing values:\\nindicator = MissingIndicator(missing_values=-1, features=\"all\")\\nmask_all = indicator.fit_transform(X)\\nmask_all\\narray([[ True,  True, False, False],\\n[False,  True, False,  True],\\n[False,  True, False, False]])\\nindicator.features_\\narray([0, 1, 2, 3])\\nWhen using the MissingIndicator in a\\nPipeline, be sure to use the\\nFeatureUnion or\\nColumnTransformer to add the indicator features to\\nthe regular features. First we obtain the iris dataset, and add some missing\\nvalues to it.\\nfrom sklearn.datasets import load_iris\\nfrom sklearn.impute import SimpleImputer, MissingIndicator\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.pipeline import FeatureUnion, make_pipeline\\nfrom sklearn.tree import DecisionTreeClassifier\\nX, y = load_iris(return_X_y=True)\\nmask = np.random.randint(0, 2, size=X.shape).astype(bool)\\nX[mask] = np.nan\\nX_train, X_test, y_train, _ = train_test_split(X, y, test_size=100,\\n...                                                random_state=0)\\nNow we create a FeatureUnion. All features will be\\nimputed using SimpleImputer, in order to enable classifiers to work\\nwith this data. Additionally, it adds the indicator variables from\\nMissingIndicator.\\ntransformer = FeatureUnion(\\n...     transformer_list=[\\n...         (\\'features\\', SimpleImputer(strategy=\\'mean\\')),\\n...         (\\'indicators\\', MissingIndicator())])\\ntransformer = transformer.fit(X_train, y_train)\\nresults = transformer.transform(X_test)\\nresults.shape\\n(100, 8)\\nOf course, we cannot use the transformer to make any predictions. We should\\nwrap this in a Pipeline with a classifier (e.g., a\\nDecisionTreeClassifier) to be able to make predictions.\\nclf = make_pipeline(transformer, DecisionTreeClassifier())\\nclf = clf.fit(X_train, y_train)\\nresults = clf.predict(X_test)\\nresults.shape\\n(100,)\\n- 6.4.7. Estimators that handle NaN values¶\\nSome estimators are designed to handle NaN values without preprocessing.\\nBelow is the list of these estimators, classified by type\\n(cluster, regressor, classifier, transform):\\nEstimators that allow NaN values for type cluster:\\nHDBSCAN\\nEstimators that allow NaN values for type regressor:\\nBaggingRegressor\\nDecisionTreeRegressor\\nHistGradientBoostingRegressor\\nRandomForestRegressor\\nStackingRegressor\\nVotingRegressor\\nEstimators that allow NaN values for type classifier:\\nBaggingClassifier\\nDecisionTreeClassifier\\nHistGradientBoostingClassifier\\nRandomForestClassifier\\nStackingClassifier\\nVotingClassifier\\nEstimators that allow NaN values for type transformer:\\nIterativeImputer\\nKNNImputer\\nMaxAbsScaler\\nMinMaxScaler\\nMissingIndicator\\nOneHotEncoder\\nOrdinalEncoder\\nPowerTransformer\\nQuantileTransformer\\nRobustScaler\\nSimpleImputer\\nStackingClassifier\\nStackingRegressor\\nStandardScaler\\nTargetEncoder\\nVarianceThreshold\\nVotingClassifier\\nVotingRegressor\\n\\n6.5. Unsupervised dimensionality reduction — scikit-learn 1.4.2 documentation\\n\\nInstall\\nUser Guide\\nAPI\\nExamples\\nCommunity\\nGetting Started\\nTutorial\\nWhat\\'s new\\nGlossary\\nDevelopment\\nFAQ\\nSupport\\nRelated packages\\nRoadmap\\nGovernance\\nAbout us\\nGitHub\\nOther Versions and Download\\nMore\\nGetting Started\\nTutorial\\nWhat\\'s new\\nGlossary\\nDevelopment\\nFAQ\\nSupport\\nRelated packages\\nRoadmap\\nGovernance\\nAbout us\\nGitHub\\nOther Versions and Download\\nToggle Menu\\nPrevUp\\nNext\\nscikit-learn 1.4.2\\nOther versions\\nPlease cite us if you use the software.\\n\\n6.5. Unsupervised dimensionality reduction\\n\\n6.5.1. PCA: principal component analysis\\n\\n6.5.2. Random projections\\n\\n6.5.3. Feature agglomeration\\n\\n6.5. Unsupervised dimensionality reduction¶\\n\\nIf your number of features is high, it may be useful to reduce it with an\\nunsupervised step prior to supervised steps. Many of the\\nUnsupervised learning methods implement a transform method that\\ncan be used to reduce the dimensionality. Below we discuss two specific\\nexample of this pattern that are heavily used.\\nPipelining\\nThe unsupervised data reduction and the supervised estimator can be\\nchained in one step. See Pipeline: chaining estimators.\\n- 6.5.1. PCA: principal component analysis¶\\ndecomposition.PCA looks for a combination of features that\\ncapture well the variance of the original features. See Decomposing signals in components (matrix factorization problems).\\nExamples\\nFaces recognition example using eigenfaces and SVMs\\n- 6.5.2. Random projections¶\\nThe module: random_projection provides several tools for data\\nreduction by random projections. See the relevant section of the\\ndocumentation: Random Projection.\\nExamples\\nThe Johnson-Lindenstrauss bound for embedding with random projections\\n- 6.5.3. Feature agglomeration¶\\ncluster.FeatureAgglomeration applies\\nHierarchical clustering to group together features that behave\\nsimilarly.\\nExamples\\nFeature agglomeration vs. univariate selection\\nFeature agglomeration\\nFeature scaling\\nNote that if features have very different scaling or statistical\\nproperties, cluster.FeatureAgglomeration may not be able to\\ncapture the links between related features. Using a\\npreprocessing.StandardScaler can be useful in these settings.\\n\\n6.6. Random Projection — scikit-learn 1.4.2 documentation\\n\\nInstall\\nUser Guide\\nAPI\\nExamples\\nCommunity\\nGetting Started\\nTutorial\\nWhat\\'s new\\nGlossary\\nDevelopment\\nFAQ\\nSupport\\nRelated packages\\nRoadmap\\nGovernance\\nAbout us\\nGitHub\\nOther Versions and Download\\nMore\\nGetting Started\\nTutorial\\nWhat\\'s new\\nGlossary\\nDevelopment\\nFAQ\\nSupport\\nRelated packages\\nRoadmap\\nGovernance\\nAbout us\\nGitHub\\nOther Versions and Download\\nToggle Menu\\nPrevUp\\nNext\\nscikit-learn 1.4.2\\nOther versions\\nPlease cite us if you use the software.\\n\\n6.6. Random Projection\\n\\n6.6.1. The Johnson-Lindenstrauss lemma\\n\\n6.6.2. Gaussian random projection\\n\\n6.6.3. Sparse random projection\\n\\n6.6.4. Inverse Transform\\n\\n6.6. Random Projection¶\\n\\nThe sklearn.random_projection module implements a simple and\\ncomputationally efficient way to reduce the dimensionality of the data by\\ntrading a controlled amount of accuracy (as additional variance) for faster\\nprocessing times and smaller model sizes. This module implements two types of\\nunstructured random matrix:\\nGaussian random matrix and\\nsparse random matrix.\\nThe dimensions and distribution of random projections matrices are\\ncontrolled so as to preserve the pairwise distances between any two\\nsamples of the dataset. Thus random projection is a suitable approximation\\ntechnique for distance based method.\\nReferences:\\nSanjoy Dasgupta. 2000.\\nExperiments with random projection.\\nIn Proceedings of the Sixteenth conference on Uncertainty in artificial\\nintelligence (UAI’00), Craig Boutilier and Moisés Goldszmidt (Eds.). Morgan\\nKaufmann Publishers Inc., San Francisco, CA, USA, 143-151.\\nElla Bingham and Heikki Mannila. 2001.\\nRandom projection in dimensionality reduction: applications to image and text data.\\nIn Proceedings of the seventh ACM SIGKDD international conference on\\nKnowledge discovery and data mining (KDD ‘01). ACM, New York, NY, USA,\\n245-250.\\n- 6.6.1. The Johnson-Lindenstrauss lemma¶\\nThe main theoretical result behind the efficiency of random projection is the\\nJohnson-Lindenstrauss lemma (quoting Wikipedia):\\nIn mathematics, the Johnson-Lindenstrauss lemma is a result\\nconcerning low-distortion embeddings of points from high-dimensional\\ninto low-dimensional Euclidean space. The lemma states that a small set\\nof points in a high-dimensional space can be embedded into a space of\\nmuch lower dimension in such a way that distances between the points are\\nnearly preserved. The map used for the embedding is at least Lipschitz,\\nand can even be taken to be an orthogonal projection.\\nKnowing only the number of samples, the\\njohnson_lindenstrauss_min_dim estimates\\nconservatively the minimal size of the random subspace to guarantee a\\nbounded distortion introduced by the random projection:\\n\\nfrom sklearn.random_projection import johnson_lindenstrauss_min_dim\\njohnson_lindenstrauss_min_dim(n_samples=1e6, eps=0.5)\\n663\\njohnson_lindenstrauss_min_dim(n_samples=1e6, eps=[0.5, 0.1, 0.01])\\narray([    663,   11841, 1112658])\\njohnson_lindenstrauss_min_dim(n_samples=[1e4, 1e5, 1e6], eps=0.1)\\narray([ 7894,  9868, 11841])\\nExample:\\nSee The Johnson-Lindenstrauss bound for embedding with random projections\\nfor a theoretical explication on the Johnson-Lindenstrauss lemma and an\\nempirical validation using sparse random matrices.\\nReferences:\\nSanjoy Dasgupta and Anupam Gupta, 1999.\\nAn elementary proof of the Johnson-Lindenstrauss Lemma.\\n- 6.6.2. Gaussian random projection¶\\nThe GaussianRandomProjection reduces the\\ndimensionality by projecting the original input space on a randomly generated\\nmatrix where components are drawn from the following distribution\\n(N(0, \\\\frac{1}{n_{components}})).\\nHere a small excerpt which illustrates how to use the Gaussian random\\nprojection transformer:\\nimport numpy as np\\nfrom sklearn import random_projection\\nX = np.random.rand(100, 10000)\\ntransformer = random_projection.GaussianRandomProjection()\\nX_new = transformer.fit_transform(X)\\nX_new.shape\\n(100, 3947)\\n- 6.6.3. Sparse random projection¶\\nThe SparseRandomProjection reduces the\\ndimensionality by projecting the original input space using a sparse\\nrandom matrix.\\nSparse random matrices are an alternative to dense Gaussian random\\nprojection matrix that guarantees similar embedding quality while being much\\nmore memory efficient and allowing faster computation of the projected data.\\nIf we define s = 1 / density, the elements of the random matrix\\nare drawn from\\n[\\\\begin{split}\\\\left{\\n\\\\begin{array}{c c l}\\n-\\\\sqrt{\\\\frac{s}{n_{\\\\text{components}}}} & & 1 / 2s\\\\\\n0 &\\\\text{with probability}  & 1 - 1 / s \\\\\\n+\\\\sqrt{\\\\frac{s}{n_{\\\\text{components}}}} & & 1 / 2s\\\\\\n\\\\end{array}\\n\\\\right.\\\\end{split}]\\nwhere (n_{\\\\text{components}}) is the size of the projected subspace.\\nBy default the density of non zero elements is set to the minimum density as\\nrecommended by Ping Li et al.: (1 / \\\\sqrt{n_{\\\\text{features}}}).\\nHere a small excerpt which illustrates how to use the sparse random\\nprojection transformer:\\nimport numpy as np\\nfrom sklearn import random_projection\\nX = np.random.rand(100, 10000)\\ntransformer = random_projection.SparseRandomProjection()\\nX_new = transformer.fit_transform(X)\\nX_new.shape\\n(100, 3947)\\nReferences:\\nD. Achlioptas. 2003.\\nDatabase-friendly random projections: Johnson-Lindenstrauss  with binary\\ncoins.\\nJournal of Computer and System Sciences 66 (2003) 671–687\\nPing Li, Trevor J. Hastie, and Kenneth W. Church. 2006.\\nVery sparse random projections.\\nIn Proceedings of the 12th ACM SIGKDD international conference on\\nKnowledge discovery and data mining (KDD ‘06). ACM, New York, NY, USA,\\n287-296.\\n- 6.6.4. Inverse Transform¶\\nThe random projection transformers have compute_inverse_components parameter. When\\nset to True, after creating the random components_ matrix during fitting,\\nthe transformer computes the pseudo-inverse of this matrix and stores it as\\ninverse_components_. The inverse_components_ matrix has shape\\n(n_{features} \\\\times n_{components}), and it is always a dense matrix,\\nregardless of whether the components matrix is sparse or dense. So depending on\\nthe number of features and components, it may use a lot of memory.\\nWhen the inverse_transform method is called, it computes the product of the\\ninput X and the transpose of the inverse components. If the inverse components have\\nbeen computed during fit, they are reused at each call to inverse_transform.\\nOtherwise they are recomputed each time, which can be costly. The result is always\\ndense, even if X is sparse.\\nHere a small code example which illustrates how to use the inverse transform\\nfeature:\\nimport numpy as np\\nfrom sklearn.random_projection import SparseRandomProjection\\nX = np.random.rand(100, 10000)\\ntransformer = SparseRandomProjection(\\n...   compute_inverse_components=True\\n... )\\n...\\nX_new = transformer.fit_transform(X)\\nX_new.shape\\n(100, 3947)\\nX_new_inversed = transformer.inverse_transform(X_new)\\nX_new_inversed.shape\\n(100, 10000)\\nX_new_again = transformer.transform(X_new_inversed)\\nnp.allclose(X_new, X_new_again)\\nTrue\\n\\n6.7. Kernel Approximation — scikit-learn 1.4.2 documentation\\n\\nInstall\\nUser Guide\\nAPI\\nExamples\\nCommunity\\nGetting Started\\nTutorial\\nWhat\\'s new\\nGlossary\\nDevelopment\\nFAQ\\nSupport\\nRelated packages\\nRoadmap\\nGovernance\\nAbout us\\nGitHub\\nOther Versions and Download\\nMore\\nGetting Started\\nTutorial\\nWhat\\'s new\\nGlossary\\nDevelopment\\nFAQ\\nSupport\\nRelated packages\\nRoadmap\\nGovernance\\nAbout us\\nGitHub\\nOther Versions and Download\\nToggle Menu\\nPrevUp\\nNext\\nscikit-learn 1.4.2\\nOther versions\\nPlease cite us if you use the software.\\n\\n6.7. Kernel Approximation\\n\\n6.7.1. Nystroem Method for Kernel Approximation\\n\\n6.7.2. Radial Basis Function Kernel\\n\\n6.7.3. Additive Chi Squared Kernel\\n\\n6.7.4. Skewed Chi Squared Kernel\\n\\n6.7.5. Polynomial Kernel Approximation via Tensor Sketch\\n\\n6.7.6. Mathematical Details\\n\\n6.7. Kernel Approximation¶\\n\\nThis submodule contains functions that approximate the feature mappings that\\ncorrespond to certain kernels, as they are used for example in support vector\\nmachines (see Support Vector Machines).\\nThe following feature functions perform non-linear transformations of the\\ninput, which can serve as a basis for linear classification or other\\nalgorithms.\\nThe advantage of using approximate explicit feature maps compared to the\\nkernel trick,\\nwhich makes use of feature maps implicitly, is that explicit mappings\\ncan be better suited for online learning and can significantly reduce the cost\\nof learning with very large datasets.\\nStandard kernelized SVMs do not scale well to large datasets, but using an\\napproximate kernel map it is possible to use much more efficient linear SVMs.\\nIn particular, the combination of kernel map approximations with\\nSGDClassifier can make non-linear learning on large datasets possible.\\nSince there has not been much empirical work using approximate embeddings, it\\nis advisable to compare results against exact kernel methods when possible.\\nSee also\\nPolynomial regression: extending linear models with basis functions for an exact polynomial transformation.\\n- 6.7.1. Nystroem Method for Kernel Approximation¶\\nThe Nystroem method, as implemented in Nystroem is a general method for\\nreduced rank approximations of kernels. It achieves this by subsampling without\\nreplacement rows/columns of the data on which the kernel is evaluated. While the\\ncomputational complexity of the exact method is\\n(\\\\mathcal{O}(n^3_{\\\\text{samples}})), the complexity of the approximation\\nis (\\\\mathcal{O}(n^2_{\\\\text{components}} \\\\cdot n_{\\\\text{samples}})), where\\none can set (n_{\\\\text{components}} \\\\ll n_{\\\\text{samples}}) without a\\nsignificative decrease in performance [WS2001].\\nWe can construct the eigendecomposition of the kernel matrix (K), based\\non the features of the data, and then split it into sampled and unsampled data\\npoints.\\n[\\\\begin{split}K = U \\\\Lambda U^T\\n= \\\\begin{bmatrix} U_1 \\\\ U_2\\\\end{bmatrix} \\\\Lambda \\\\begin{bmatrix} U_1 \\\\ U_2 \\\\end{bmatrix}^T\\n= \\\\begin{bmatrix} U_1 \\\\Lambda U_1^T & U_1 \\\\Lambda U_2^T \\\\ U_2 \\\\Lambda U_1^T & U_2 \\\\Lambda U_2^T \\\\end{bmatrix}\\n\\\\equiv \\\\begin{bmatrix} K_{11} & K_{12} \\\\ K_{21} & K_{22} \\\\end{bmatrix}\\\\end{split}]\\nwhere:\\n(U) is orthonormal\\n(\\\\Lambda) is diagonal matrix of eigenvalues\\n(U_1) is orthonormal matrix of samples that were chosen\\n(U_2) is orthonormal matrix of samples that were not chosen\\nGiven that (U_1 \\\\Lambda U_1^T) can be obtained by orthonormalization of\\nthe matrix (K_{11}), and (U_2 \\\\Lambda U_1^T) can be evaluated (as\\nwell as its transpose), the only remaining term to elucidate is\\n(U_2 \\\\Lambda U_2^T). To do this we can express it in terms of the already\\nevaluated matrices:\\n[\\\\begin{split}\\\\begin{align} U_2 \\\\Lambda U_2^T &= \\\\left(K_{21} U_1 \\\\Lambda^{-1}\\\\right) \\\\Lambda \\\\left(K_{21} U_1 \\\\Lambda^{-1}\\\\right)^T\\n\\\\&= K_{21} U_1 (\\\\Lambda^{-1} \\\\Lambda) \\\\Lambda^{-1} U_1^T K_{21}^T\\n\\\\&= K_{21} U_1 \\\\Lambda^{-1} U_1^T K_{21}^T\\n\\\\&= K_{21} K_{11}^{-1} K_{21}^T\\n\\\\&= \\\\left( K_{21} K_{11}^{-\\\\frac12} \\\\right) \\\\left( K_{21} K_{11}^{-\\\\frac12} \\\\right)^T\\n.\\\\end{align}\\\\end{split}]\\nDuring fit, the class Nystroem evaluates the basis (U_1), and\\ncomputes the normalization constant, (K_{11}^{-\\\\frac12}). Later, during\\ntransform, the kernel matrix is determined between the basis (given by the\\ncomponents_ attribute) and the new data points, X. This matrix is then\\nmultiplied by the normalization_ matrix for the final result.\\nBy default Nystroem uses the rbf kernel, but it can use any kernel\\nfunction or a precomputed kernel matrix. The number of samples used - which is\\nalso the dimensionality of the features computed - is given by the parameter\\nn_components.\\nExamples:\\nSee the example entitled\\nTime-related feature engineering,\\nthat shows an efficient machine learning pipeline that uses a\\nNystroem kernel.\\n- 6.7.2. Radial Basis Function Kernel¶\\nThe RBFSampler constructs an approximate mapping for the radial basis\\nfunction kernel, also known as Random Kitchen Sinks [RR2007]. This\\ntransformation can be used to explicitly model a kernel map, prior to applying\\na linear algorithm, for example a linear SVM:\\n\\nfrom sklearn.kernel_approximation import RBFSampler\\nfrom sklearn.linear_model import SGDClassifier\\nX = [[0, 0], [1, 1], [1, 0], [0, 1]]\\ny = [0, 0, 1, 1]\\nrbf_feature = RBFSampler(gamma=1, random_state=1)\\nX_features = rbf_feature.fit_transform(X)\\nclf = SGDClassifier(max_iter=5)\\nclf.fit(X_features, y)\\nSGDClassifier(max_iter=5)\\nclf.score(X_features, y)\\n1.0\\nThe mapping relies on a Monte Carlo approximation to the\\nkernel values. The fit function performs the Monte Carlo sampling, whereas\\nthe transform method performs the mapping of the data.  Because of the\\ninherent randomness of the process, results may vary between different calls to\\nthe fit function.\\nThe fit function takes two arguments:\\nn_components, which is the target dimensionality of the feature transform,\\nand gamma, the parameter of the RBF-kernel.  A higher n_components will\\nresult in a better approximation of the kernel and will yield results more\\nsimilar to those produced by a kernel SVM. Note that “fitting” the feature\\nfunction does not actually depend on the data given to the fit function.\\nOnly the dimensionality of the data is used.\\nDetails on the method can be found in [RR2007].\\nFor a given value of n_components RBFSampler is often less accurate\\nas Nystroem. RBFSampler is cheaper to compute, though, making\\nuse of larger feature spaces more efficient.\\nComparing an exact RBF kernel (left) with the approximation (right)¶\\nExamples:\\nExplicit feature map approximation for RBF kernels\\n- 6.7.3. Additive Chi Squared Kernel¶\\nThe additive chi squared kernel is a kernel on histograms, often used in computer vision.\\nThe additive chi squared kernel as used here is given by\\n[k(x, y) = \\\\sum_i \\\\frac{2x_iy_i}{x_i+y_i}]\\nThis is not exactly the same as sklearn.metrics.pairwise.additive_chi2_kernel.\\nThe authors of [VZ2010] prefer the version above as it is always positive\\ndefinite.\\nSince the kernel is additive, it is possible to treat all components\\n(x_i) separately for embedding. This makes it possible to sample\\nthe Fourier transform in regular intervals, instead of approximating\\nusing Monte Carlo sampling.\\nThe class AdditiveChi2Sampler implements this component wise\\ndeterministic sampling. Each component is sampled (n) times, yielding\\n(2n+1) dimensions per input dimension (the multiple of two stems\\nfrom the real and complex part of the Fourier transform).\\nIn the literature, (n) is usually chosen to be 1 or 2, transforming\\nthe dataset to size n_samples * 5 * n_features (in the case of (n=2)).\\nThe approximate feature map provided by AdditiveChi2Sampler can be combined\\nwith the approximate feature map provided by RBFSampler to yield an approximate\\nfeature map for the exponentiated chi squared kernel.\\nSee the [VZ2010] for details and [VVZ2010] for combination with the RBFSampler.\\n- 6.7.4. Skewed Chi Squared Kernel¶\\nThe skewed chi squared kernel is given by:\\n[k(x,y) = \\\\prod_i \\\\frac{2\\\\sqrt{x_i+c}\\\\sqrt{y_i+c}}{x_i + y_i + 2c}]\\nIt has properties that are similar to the exponentiated chi squared kernel\\noften used in computer vision, but allows for a simple Monte Carlo\\napproximation of the feature map.\\nThe usage of the SkewedChi2Sampler is the same as the usage described\\nabove for the RBFSampler. The only difference is in the free\\nparameter, that is called (c).\\nFor a motivation for this mapping and the mathematical details see [LS2010].\\n- 6.7.5. Polynomial Kernel Approximation via Tensor Sketch¶\\nThe polynomial kernel is a popular type of kernel\\nfunction given by:\\n[k(x, y) = (\\\\gamma x^\\\\top y +c_0)^d]\\nwhere:\\nx, y are the input vectors\\nd is the kernel degree\\nIntuitively, the feature space of the polynomial kernel of degree d\\nconsists of all possible degree-d products among input features, which enables\\nlearning algorithms using this kernel to account for interactions between features.\\nThe TensorSketch [PP2013] method, as implemented in PolynomialCountSketch, is a\\nscalable, input data independent method for polynomial kernel approximation.\\nIt is based on the concept of Count sketch [WIKICS] [CCF2002] , a dimensionality\\nreduction technique similar to feature hashing, which instead uses several\\nindependent hash functions. TensorSketch obtains a Count Sketch of the outer product\\nof two vectors (or a vector with itself), which can be used as an approximation of the\\npolynomial kernel feature space. In particular, instead of explicitly computing\\nthe outer product, TensorSketch computes the Count Sketch of the vectors and then\\nuses polynomial multiplication via the Fast Fourier Transform to compute the\\nCount Sketch of their outer product.\\nConveniently, the training phase of TensorSketch simply consists of initializing\\nsome random variables. It is thus independent of the input data, i.e. it only\\ndepends on the number of input features, but not the data values.\\nIn addition, this method can transform samples in\\n(\\\\mathcal{O}(n_{\\\\text{samples}}(n_{\\\\text{features}} + n_{\\\\text{components}} \\\\log(n_{\\\\text{components}}))))\\ntime, where (n_{\\\\text{components}}) is the desired output dimension,\\ndetermined by n_components.\\nExamples:\\nScalable learning with polynomial kernel approximation\\n- 6.7.6. Mathematical Details¶\\nKernel methods like support vector machines or kernelized\\nPCA rely on a property of reproducing kernel Hilbert spaces.\\nFor any positive definite kernel function (k) (a so called Mercer kernel),\\nit is guaranteed that there exists a mapping (\\\\phi)\\ninto a Hilbert space (\\\\mathcal{H}), such that\\n[k(x,y) = \\\\langle \\\\phi(x), \\\\phi(y) \\\\rangle]\\nWhere (\\\\langle \\\\cdot, \\\\cdot \\\\rangle) denotes the inner product in the\\nHilbert space.\\nIf an algorithm, such as a linear support vector machine or PCA,\\nrelies only on the scalar product of data points (x_i), one may use\\nthe value of (k(x_i, x_j)), which corresponds to applying the algorithm\\nto the mapped data points (\\\\phi(x_i)).\\nThe advantage of using (k) is that the mapping (\\\\phi) never has\\nto be calculated explicitly, allowing for arbitrary large\\nfeatures (even infinite).\\nOne drawback of kernel methods is, that it might be necessary\\nto store many kernel values (k(x_i, x_j)) during optimization.\\nIf a kernelized classifier is applied to new data (y_j),\\n(k(x_i, y_j)) needs to be computed to make predictions,\\npossibly for many different (x_i) in the training set.\\nThe classes in this submodule allow to approximate the embedding\\n(\\\\phi), thereby working explicitly with the representations\\n(\\\\phi(x_i)), which obviates the need to apply the kernel\\nor store training examples.\\nReferences:\\n[WS2001]\\n“Using the Nyström method to speed up kernel machines”\\nWilliams, C.K.I.; Seeger, M. - 2001.\\n[RR2007]\\n(1,2)\\n“Random features for large-scale kernel machines”\\nRahimi, A. and Recht, B. - Advances in neural information processing 2007,\\n[LS2010]\\n“Random Fourier approximations for skewed multiplicative histogram kernels”\\nLi, F., Ionescu, C., and Sminchisescu, C.\\n- Pattern Recognition,  DAGM 2010, Lecture Notes in Computer Science.\\n[VZ2010]\\n(1,2)\\n“Efficient additive kernels via explicit feature maps”\\nVedaldi, A. and Zisserman, A. - Computer Vision and Pattern Recognition 2010\\n[VVZ2010]\\n“Generalized RBF feature maps for Efficient Detection”\\nVempati, S. and Vedaldi, A. and Zisserman, A. and Jawahar, CV - 2010\\n[PP2013]\\n“Fast and scalable polynomial kernels via explicit feature maps”\\nPham, N., & Pagh, R. - 2013\\n[CCF2002]\\n“Finding frequent items in data streams”\\nCharikar, M., Chen, K., & Farach-Colton - 2002\\n[WIKICS]\\n“Wikipedia: Count sketch”\\n\\n6.8. Pairwise metrics, Affinities and Kernels — scikit-learn 1.4.2 documentation\\n\\nInstall\\nUser Guide\\nAPI\\nExamples\\nCommunity\\nGetting Started\\nTutorial\\nWhat\\'s new\\nGlossary\\nDevelopment\\nFAQ\\nSupport\\nRelated packages\\nRoadmap\\nGovernance\\nAbout us\\nGitHub\\nOther Versions and Download\\nMore\\nGetting Started\\nTutorial\\nWhat\\'s new\\nGlossary\\nDevelopment\\nFAQ\\nSupport\\nRelated packages\\nRoadmap\\nGovernance\\nAbout us\\nGitHub\\nOther Versions and Download\\nToggle Menu\\nPrevUp\\nNext\\nscikit-learn 1.4.2\\nOther versions\\nPlease cite us if you use the software.\\n\\n6.8. Pairwise metrics, Affinities and Kernels\\n\\n6.8.1. Cosine similarity\\n\\n6.8.2. Linear kernel\\n\\n6.8.3. Polynomial kernel\\n\\n6.8.4. Sigmoid kernel\\n\\n6.8.5. RBF kernel\\n\\n6.8.6. Laplacian kernel\\n\\n6.8.7. Chi-squared kernel\\n\\n6.8. Pairwise metrics, Affinities and Kernels¶\\n\\nThe sklearn.metrics.pairwise submodule implements utilities to evaluate\\npairwise distances or affinity of sets of samples.\\nThis module contains both distance metrics and kernels. A brief summary is\\ngiven on the two here.\\nDistance metrics are functions d(a, b) such that d(a, b) < d(a, c)\\nif objects a and b are considered “more similar” than objects a\\nand c. Two objects exactly alike would have a distance of zero.\\nOne of the most popular examples is Euclidean distance.\\nTo be a ‘true’ metric, it must obey the following four conditions:\\n\\n1. d(a, b) >= 0, for all a and b\\n\\n2. d(a, b) == 0, if and only if a = b, positive definiteness\\n\\n3. d(a, b) == d(b, a), symmetry\\n\\n4. d(a, c) <= d(a, b) + d(b, c), the triangle inequality\\n\\nKernels are measures of similarity, i.e. s(a, b) > s(a, c)\\nif objects a and b are considered “more similar” than objects\\na and c. A kernel must also be positive semi-definite.\\nThere are a number of ways to convert between a distance metric and a\\nsimilarity measure, such as a kernel. Let D be the distance, and S be\\nthe kernel:\\nS = np.exp(-D * gamma), where one heuristic for choosinggamma is 1 / num_features\\nS = 1. / (D / np.max(D))\\nThe distances between the row vectors of X and the row vectors of Y\\ncan be evaluated using pairwise_distances. If Y is omitted the\\npairwise distances of the row vectors of X are calculated. Similarly,\\npairwise.pairwise_kernels can be used to calculate the kernel between X\\nand Y using different kernel functions. See the API reference for more\\ndetails.\\n\\nimport numpy as np\\nfrom sklearn.metrics import pairwise_distances\\nfrom sklearn.metrics.pairwise import pairwise_kernels\\nX = np.array([[2, 3], [3, 5], [5, 8]])\\nY = np.array([[1, 0], [2, 1]])\\npairwise_distances(X, Y, metric=\\'manhattan\\')\\narray([[ 4.,  2.],\\n[ 7.,  5.],\\n[12., 10.]])\\npairwise_distances(X, metric=\\'manhattan\\')\\narray([[0., 3., 8.],\\n[3., 0., 5.],\\n[8., 5., 0.]])\\npairwise_kernels(X, Y, metric=\\'linear\\')\\narray([[ 2.,  7.],\\n[ 3., 11.],\\n[ 5., 18.]])\\n- 6.8.1. Cosine similarity¶\\ncosine_similarity computes the L2-normalized dot product of vectors.\\nThat is, if (x) and (y) are row vectors,\\ntheir cosine similarity (k) is defined as:\\n[k(x, y) = \\\\frac{x y^\\\\top}{|x| |y|}]\\nThis is called cosine similarity, because Euclidean (L2) normalization\\nprojects the vectors onto the unit sphere,\\nand their dot product is then the cosine of the angle between the points\\ndenoted by the vectors.\\nThis kernel is a popular choice for computing the similarity of documents\\nrepresented as tf-idf vectors.\\ncosine_similarity accepts scipy.sparse matrices.\\n(Note that the tf-idf functionality in sklearn.feature_extraction.text\\ncan produce normalized vectors, in which case cosine_similarity\\nis equivalent to linear_kernel, only slower.)\\nReferences:\\nC.D. Manning, P. Raghavan and H. Schütze (2008). Introduction to\\nInformation Retrieval. Cambridge University Press.\\nhttps://nlp.stanford.edu/IR-book/html/htmledition/the-vector-space-model-for-scoring-1.html\\n- 6.8.2. Linear kernel¶\\nThe function linear_kernel computes the linear kernel, that is, a\\nspecial case of polynomial_kernel with degree=1 and coef0=0 (homogeneous).\\nIf x and y are column vectors, their linear kernel is:\\n[k(x, y) = x^\\\\top y]\\n- 6.8.3. Polynomial kernel¶\\nThe function polynomial_kernel computes the degree-d polynomial kernel\\nbetween two vectors. The polynomial kernel represents the similarity between two\\nvectors. Conceptually, the polynomial kernels considers not only the similarity\\nbetween vectors under the same dimension, but also across dimensions. When used\\nin machine learning algorithms, this allows to account for feature interaction.\\nThe polynomial kernel is defined as:\\n[k(x, y) = (\\\\gamma x^\\\\top y +c_0)^d]\\nwhere:\\nx, y are the input vectors\\nd is the kernel degree\\nIf (c_0 = 0) the kernel is said to be homogeneous.\\n- 6.8.4. Sigmoid kernel¶\\nThe function sigmoid_kernel computes the sigmoid kernel between two\\nvectors. The sigmoid kernel is also known as hyperbolic tangent, or Multilayer\\nPerceptron (because, in the neural network field, it is often used as neuron\\nactivation function). It is defined as:\\n[k(x, y) = \\\\tanh( \\\\gamma x^\\\\top y + c_0)]\\nwhere:\\nx, y are the input vectors\\n(\\\\gamma) is known as slope\\n(c_0) is known as intercept\\n- 6.8.5. RBF kernel¶\\nThe function rbf_kernel computes the radial basis function (RBF) kernel\\nbetween two vectors. This kernel is defined as:\\n[k(x, y) = \\\\exp( -\\\\gamma | x-y |^2)]\\nwhere x and y are the input vectors. If (\\\\gamma = \\\\sigma^{-2})\\nthe kernel is known as the Gaussian kernel of variance (\\\\sigma^2).\\n- 6.8.6. Laplacian kernel¶\\nThe function laplacian_kernel is a variant on the radial basis\\nfunction kernel defined as:\\n[k(x, y) = \\\\exp( -\\\\gamma | x-y |_1)]\\nwhere x and y are the input vectors and (|x-y|_1) is the\\nManhattan distance between the input vectors.\\nIt has proven useful in ML applied to noiseless data.\\nSee e.g. Machine learning for quantum mechanics in a nutshell.\\n- 6.8.7. Chi-squared kernel¶\\nThe chi-squared kernel is a very popular choice for training non-linear SVMs in\\ncomputer vision applications.\\nIt can be computed using chi2_kernel and then passed to an\\nSVC with kernel=\"precomputed\":\\nfrom sklearn.svm import SVC\\nfrom sklearn.metrics.pairwise import chi2_kernel\\nX = [[0, 1], [1, 0], [.2, .8], [.7, .3]]\\ny = [0, 1, 0, 1]\\nK = chi2_kernel(X, gamma=.5)\\nK\\narray([[1.        , 0.36787944, 0.89483932, 0.58364548],\\n[0.36787944, 1.        , 0.51341712, 0.83822343],\\n[0.89483932, 0.51341712, 1.        , 0.7768366 ],\\n[0.58364548, 0.83822343, 0.7768366 , 1.        ]])\\nsvm = SVC(kernel=\\'precomputed\\').fit(K, y)\\nsvm.predict(K)\\narray([0, 1, 0, 1])\\nIt can also be directly used as the kernel argument:\\nsvm = SVC(kernel=chi2_kernel).fit(X, y)\\nsvm.predict(X)\\narray([0, 1, 0, 1])\\nThe chi squared kernel is given by\\n[k(x, y) = \\\\exp \\\\left (-\\\\gamma \\\\sum_i \\\\frac{(x[i] - y[i]) ^ 2}{x[i] + y[i]} \\\\right )]\\nThe data is assumed to be non-negative, and is often normalized to have an L1-norm of one.\\nThe normalization is rationalized with the connection to the chi squared distance,\\nwhich is a distance between discrete probability distributions.\\nThe chi squared kernel is most commonly used on histograms (bags) of visual words.\\nReferences:\\nZhang, J. and Marszalek, M. and Lazebnik, S. and Schmid, C.\\nLocal features and kernels for classification of texture and object\\ncategories: A comprehensive study\\nInternational Journal of Computer Vision 2007\\nhttps://hal.archives-ouvertes.fr/hal-00171412/document\\n\\n6.9. Transforming the prediction target (y) — scikit-learn 1.4.2 documentation\\n\\nInstall\\nUser Guide\\nAPI\\nExamples\\nCommunity\\nGetting Started\\nTutorial\\nWhat\\'s new\\nGlossary\\nDevelopment\\nFAQ\\nSupport\\nRelated packages\\nRoadmap\\nGovernance\\nAbout us\\nGitHub\\nOther Versions and Download\\nMore\\nGetting Started\\nTutorial\\nWhat\\'s new\\nGlossary\\nDevelopment\\nFAQ\\nSupport\\nRelated packages\\nRoadmap\\nGovernance\\nAbout us\\nGitHub\\nOther Versions and Download\\nToggle Menu\\nPrevUp\\nNext\\nscikit-learn 1.4.2\\nOther versions\\nPlease cite us if you use the software.\\n\\n6.9. Transforming the prediction target (y)\\n\\n6.9.1. Label binarization\\n6.9.1.1. LabelBinarizer\\n6.9.1.2. MultiLabelBinarizer\\n\\n6.9.2. Label encoding\\n\\n6.9. Transforming the prediction target (y)¶\\n\\nThese are transformers that are not intended to be used on features, only on\\nsupervised learning targets. See also Transforming target in regression if\\nyou want to transform the prediction target for learning, but evaluate the\\nmodel in the original (untransformed) space.\\n- 6.9.1. Label binarization¶\\n6.9.1.1. LabelBinarizer¶\\nLabelBinarizer is a utility class to help create a label\\nindicator matrix from a list of multiclass labels:\\n\\nfrom sklearn import preprocessing\\nlb = preprocessing.LabelBinarizer()\\nlb.fit([1, 2, 6, 4, 2])\\nLabelBinarizer()\\nlb.classes_\\narray([1, 2, 4, 6])\\nlb.transform([1, 6])\\narray([[1, 0, 0, 0],\\n[0, 0, 0, 1]])\\nUsing this format can enable multiclass classification in estimators\\nthat support the label indicator matrix format.\\nWarning\\nLabelBinarizer is not needed if you are using an estimator that\\nalready supports multiclass data.\\nFor more information about multiclass classification, refer to\\nMulticlass classification.\\n6.9.1.2. MultiLabelBinarizer¶\\nIn multilabel learning, the joint set of binary classification tasks is\\nexpressed with a label binary indicator array: each sample is one row of a 2d\\narray of shape (n_samples, n_classes) with binary values where the one, i.e. the\\nnon zero elements, corresponds to the subset of labels for that sample. An array\\nsuch as np.array([[1, 0, 0], [0, 1, 1], [0, 0, 0]]) represents label 0 in the\\nfirst sample, labels 1 and 2 in the second sample, and no labels in the third\\nsample.\\nProducing multilabel data as a list of sets of labels may be more intuitive.\\nThe MultiLabelBinarizer\\ntransformer can be used to convert between a collection of collections of\\nlabels and the indicator format:\\nfrom sklearn.preprocessing import MultiLabelBinarizer\\ny = [[2, 3, 4], [2], [0, 1, 3], [0, 1, 2, 3, 4], [0, 1, 2]]\\nMultiLabelBinarizer().fit_transform(y)\\narray([[0, 0, 1, 1, 1],\\n[0, 0, 1, 0, 0],\\n[1, 1, 0, 1, 0],\\n[1, 1, 1, 1, 1],\\n[1, 1, 1, 0, 0]])\\nFor more information about multilabel classification, refer to\\nMultilabel classification.\\n- 6.9.2. Label encoding¶\\nLabelEncoder is a utility class to help normalize labels such that\\nthey contain only values between 0 and n_classes-1. This is sometimes useful\\nfor writing efficient Cython routines. LabelEncoder can be used as\\nfollows:\\nfrom sklearn import preprocessing\\nle = preprocessing.LabelEncoder()\\nle.fit([1, 2, 2, 6])\\nLabelEncoder()\\nle.classes_\\narray([1, 2, 6])\\nle.transform([1, 1, 2, 6])\\narray([0, 0, 1, 2])\\nle.inverse_transform([0, 0, 1, 2])\\narray([1, 1, 2, 6])\\nIt can also be used to transform non-numerical labels (as long as they are\\nhashable and comparable) to numerical labels:\\nle = preprocessing.LabelEncoder()\\nle.fit([\"paris\", \"paris\", \"tokyo\", \"amsterdam\"])\\nLabelEncoder()\\nlist(le.classes_)\\n[\\'amsterdam\\', \\'paris\\', \\'tokyo\\']\\nle.transform([\"tokyo\", \"tokyo\", \"paris\"])\\narray([2, 2, 1])\\nlist(le.inverse_transform([2, 2, 1]))\\n[\\'tokyo\\', \\'tokyo\\', \\'paris\\']\\n\\n7.1. Toy datasets — scikit-learn 1.4.2 documentation\\n\\nInstall\\nUser Guide\\nAPI\\nExamples\\nCommunity\\nGetting Started\\nTutorial\\nWhat\\'s new\\nGlossary\\nDevelopment\\nFAQ\\nSupport\\nRelated packages\\nRoadmap\\nGovernance\\nAbout us\\nGitHub\\nOther Versions and Download\\nMore\\nGetting Started\\nTutorial\\nWhat\\'s new\\nGlossary\\nDevelopment\\nFAQ\\nSupport\\nRelated packages\\nRoadmap\\nGovernance\\nAbout us\\nGitHub\\nOther Versions and Download\\nToggle Menu\\nPrevUp\\nNext\\nscikit-learn 1.4.2\\nOther versions\\nPlease cite us if you use the software.\\nUser Guide\\n\\n1. Supervised learning\\n\\n2. Unsupervised learning\\n\\n3. Model selection and evaluation\\n\\n4. Inspection\\n\\n5. Visualizations\\n\\n6. Dataset transformations\\n\\n7. Dataset loading utilities\\n\\n7.1. Toy datasets\\n\\n7.2. Real world datasets\\n\\n7.3. Generated datasets\\n\\n7.4. Loading other datasets\\n\\n8. Computing with scikit-learn\\n\\n9. Model persistence\\n\\n10. Common pitfalls and recommended practices\\n\\n11. Dispatching\\n\\n7.1. Toy datasets¶\\n\\nscikit-learn comes with a few small standard datasets that do not require to\\ndownload any file from some external website.\\nThey can be loaded using the following functions:\\nload_iris([, return_X_y, as_frame])\\nLoad and return the iris dataset (classification).\\nload_diabetes([, return_X_y, as_frame, scaled])\\nLoad and return the diabetes dataset (regression).\\nload_digits([, n_class, return_X_y, as_frame])\\nLoad and return the digits dataset (classification).\\nload_linnerud([, return_X_y, as_frame])\\nLoad and return the physical exercise Linnerud dataset.\\nload_wine([, return_X_y, as_frame])\\nLoad and return the wine dataset (classification).\\nload_breast_cancer([, return_X_y, as_frame])\\nLoad and return the breast cancer wisconsin dataset (classification).\\nThese datasets are useful to quickly illustrate the behavior of the\\nvarious algorithms implemented in scikit-learn. They are however often too\\nsmall to be representative of real world machine learning tasks.\\n- 7.1.1. Iris plants dataset¶\\nData Set Characteristics:\\nNumber of Instances:\\n150 (50 in each of three classes)\\nNumber of Attributes:\\n4 numeric, predictive attributes and the class\\nAttribute Information:\\nsepal length in cm\\nsepal width in cm\\npetal length in cm\\npetal width in cm\\nclass:\\nIris-Setosa\\nIris-Versicolour\\nIris-Virginica\\nSummary Statistics:\\nsepal length:\\n4.3\\n7.9\\n5.84\\n0.83\\n0.7826\\nsepal width:\\n2.0\\n4.4\\n3.05\\n0.43\\n-0.4194\\npetal length:\\n1.0\\n6.9\\n3.76\\n1.76\\n0.9490  (high!)\\npetal width:\\n0.1\\n2.5\\n1.20\\n0.76\\n0.9565  (high!)\\nMissing Attribute Values:\\nNone\\nClass Distribution:\\n33.3% for each of 3 classes.\\nCreator:\\nR.A. Fisher\\nDonor:\\nMichael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\\nDate:\\nJuly, 1988\\nThe famous Iris database, first used by Sir R.A. Fisher. The dataset is taken\\nfrom Fisher’s paper. Note that it’s the same as in R, but not as in the UCI\\nMachine Learning Repository, which has two wrong data points.\\nThis is perhaps the best known database to be found in the\\npattern recognition literature.  Fisher’s paper is a classic in the field and\\nis referenced frequently to this day.  (See Duda & Hart, for example.)  The\\ndata set contains 3 classes of 50 instances each, where each class refers to a\\ntype of iris plant.  One class is linearly separable from the other 2; the\\nlatter are NOT linearly separable from each other.\\nReferences\\nClick for more details\\n¶\\nFisher, R.A. “The use of multiple measurements in taxonomic problems”\\nAnnual Eugenics, 7, Part II, 179-188 (1936); also in “Contributions to\\nMathematical Statistics” (John Wiley, NY, 1950).\\nDuda, R.O., & Hart, P.E. (1973) Pattern Classification and Scene Analysis.\\n(Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\\nDasarathy, B.V. (1980) “Nosing Around the Neighborhood: A New System\\nStructure and Classification Rule for Recognition in Partially Exposed\\nEnvironments”.  IEEE Transactions on Pattern Analysis and Machine\\nIntelligence, Vol. PAMI-2, No. 1, 67-71.\\nGates, G.W. (1972) “The Reduced Nearest Neighbor Rule”.  IEEE Transactions\\non Information Theory, May 1972, 431-433.\\nSee also: 1988 MLC Proceedings, 54-64.  Cheeseman et al”s AUTOCLASS II\\nconceptual clustering system finds 3 classes in the data.\\nMany, many more …\\n- 7.1.2. Diabetes dataset¶\\nTen baseline variables, age, sex, body mass index, average blood\\npressure, and six blood serum measurements were obtained for each of n =\\n442 diabetes patients, as well as the response of interest, a\\nquantitative measure of disease progression one year after baseline.\\nData Set Characteristics:\\nNumber of Instances:\\n442\\nNumber of Attributes:\\nFirst 10 columns are numeric predictive values\\nTarget:\\nColumn 11 is a quantitative measure of disease progression one year after baseline\\nAttribute Information:\\nage     age in years\\nsex\\nbmi     body mass index\\nbp      average blood pressure\\ns1      tc, total serum cholesterol\\ns2      ldl, low-density lipoproteins\\ns3      hdl, high-density lipoproteins\\ns4      tch, total cholesterol / HDL\\ns5      ltg, possibly log of serum triglycerides level\\ns6      glu, blood sugar level\\nNote: Each of these 10 feature variables have been mean centered and scaled by the standard deviation times the square root of n_samples (i.e. the sum of squares of each column totals 1).\\nSource URL:\\nhttps://www4.stat.ncsu.edu/~boos/var.select/diabetes.html\\nFor more information see:\\nBradley Efron, Trevor Hastie, Iain Johnstone and Robert Tibshirani (2004) “Least Angle Regression,” Annals of Statistics (with discussion), 407-499.\\n(https://web.stanford.edu/~hastie/Papers/LARS/LeastAngle_2002.pdf)\\n- 7.1.3. Optical recognition of handwritten digits dataset¶\\nData Set Characteristics:\\nNumber of Instances:\\n1797\\nNumber of Attributes:\\n64\\nAttribute Information:\\n8x8 image of integer pixels in the range 0..16.\\nMissing Attribute Values:\\nNone\\nCreator:\\nAlpaydin (alpaydin ‘@’ boun.edu.tr)\\nDate:\\nJuly; 1998\\nThis is a copy of the test set of the UCI ML hand-written digits datasets\\nhttps://archive.ics.uci.edu/ml/datasets/Optical+Recognition+of+Handwritten+Digits\\nThe data set contains images of hand-written digits: 10 classes where\\neach class refers to a digit.\\nPreprocessing programs made available by NIST were used to extract\\nnormalized bitmaps of handwritten digits from a preprinted form. From a\\ntotal of 43 people, 30 contributed to the training set and different 13\\nto the test set. 32x32 bitmaps are divided into nonoverlapping blocks of\\n4x4 and the number of on pixels are counted in each block. This generates\\nan input matrix of 8x8 where each element is an integer in the range\\n0..16. This reduces dimensionality and gives invariance to small\\ndistortions.\\nFor info on NIST preprocessing routines, see M. D. Garris, J. L. Blue, G.\\nT. Candela, D. L. Dimmick, J. Geist, P. J. Grother, S. A. Janet, and C.\\nL. Wilson, NIST Form-Based Handprint Recognition System, NISTIR 5469,\\n1994.\\nReferences\\nClick for more details\\n¶\\nC. Kaynak (1995) Methods of Combining Multiple Classifiers and Their\\nApplications to Handwritten Digit Recognition, MSc Thesis, Institute of\\nGraduate Studies in Science and Engineering, Bogazici University.\\nAlpaydin, C. Kaynak (1998) Cascading Classifiers, Kybernetika.\\nKen Tang and Ponnuthurai N. Suganthan and Xi Yao and A. Kai Qin.\\nLinear dimensionalityreduction using relevance weighted LDA. School of\\nElectrical and Electronic Engineering Nanyang Technological University.\\n2005.\\nClaudio Gentile. A New Approximate Maximal Margin Classification\\nAlgorithm. NIPS. 2000.\\n- 7.1.4. Linnerrud dataset¶\\nData Set Characteristics:\\nNumber of Instances:\\n20\\nNumber of Attributes:\\n3\\nMissing Attribute Values:\\nNone\\nThe Linnerud dataset is a multi-output regression dataset. It consists of three\\nexercise (data) and three physiological (target) variables collected from\\ntwenty middle-aged men in a fitness club:\\nphysiological - CSV containing 20 observations on 3 physiological variables:Weight, Waist and Pulse.\\nexercise - CSV containing 20 observations on 3 exercise variables:Chins, Situps and Jumps.\\nReferences\\nClick for more details\\n¶\\nTenenhaus, M. (1998). La regression PLS: theorie et pratique. Paris:\\nEditions Technic.\\n- 7.1.5. Wine recognition dataset¶\\nData Set Characteristics:\\nNumber of Instances:\\n178\\nNumber of Attributes:\\n13 numeric, predictive attributes and the class\\nAttribute Information:\\nAlcohol\\nMalic acid\\nAsh\\nAlcalinity of ash\\nMagnesium\\nTotal phenols\\nFlavanoids\\nNonflavanoid phenols\\nProanthocyanins\\nColor intensity\\nHue\\nOD280/OD315 of diluted wines\\nProline\\nclass:\\nclass_0\\nclass_1\\nclass_2\\nSummary Statistics:\\nAlcohol:\\n11.0\\n14.8\\n13.0\\n0.8\\nMalic Acid:\\n0.74\\n5.80\\n2.34\\n1.12\\nAsh:\\n1.36\\n3.23\\n2.36\\n0.27\\nAlcalinity of Ash:\\n10.6\\n30.0\\n19.5\\n3.3\\nMagnesium:\\n70.0\\n162.0\\n99.7\\n14.3\\nTotal Phenols:\\n0.98\\n3.88\\n2.29\\n0.63\\nFlavanoids:\\n0.34\\n5.08\\n2.03\\n1.00\\nNonflavanoid Phenols:\\n0.13\\n0.66\\n0.36\\n0.12\\nProanthocyanins:\\n0.41\\n3.58\\n1.59\\n0.57\\nColour Intensity:\\n1.3\\n13.0\\n5.1\\n2.3\\nHue:\\n0.48\\n1.71\\n0.96\\n0.23\\nOD280/OD315 of diluted wines:\\n1.27\\n4.00\\n2.61\\n0.71\\nProline:\\n278\\n1680\\n746\\n315\\nMissing Attribute Values:\\nNone\\nClass Distribution:\\nclass_0 (59), class_1 (71), class_2 (48)\\nCreator:\\nR.A. Fisher\\nDonor:\\nMichael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\\nDate:\\nJuly, 1988\\nThis is a copy of UCI ML Wine recognition datasets.\\nhttps://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data\\nThe data is the results of a chemical analysis of wines grown in the same\\nregion in Italy by three different cultivators. There are thirteen different\\nmeasurements taken for different constituents found in the three types of\\nwine.\\nOriginal Owners:\\nForina, M. et al, PARVUS -\\nAn Extendible Package for Data Exploration, Classification and Correlation.\\nInstitute of Pharmaceutical and Food Analysis and Technologies,\\nVia Brigata Salerno, 16147 Genoa, Italy.\\nCitation:\\nLichman, M. (2013). UCI Machine Learning Repository\\n[https://archive.ics.uci.edu/ml]. Irvine, CA: University of California,\\nSchool of Information and Computer Science.\\nReferences\\nClick for more details\\n¶\\n(1) S. Aeberhard, D. Coomans and O. de Vel,\\nComparison of Classifiers in High Dimensional Settings,\\nTech. Rep. no. 92-02, (1992), Dept. of Computer Science and Dept. of\\nMathematics and Statistics, James Cook University of North Queensland.\\n(Also submitted to Technometrics).\\nThe data was used with many others for comparing various\\nclassifiers. The classes are separable, though only RDA\\nhas achieved 100% correct classification.\\n(RDA : 100%, QDA 99.4%, LDA 98.9%, 1NN 96.1% (z-transformed data))\\n(All results using the leave-one-out technique)\\n(2) S. Aeberhard, D. Coomans and O. de Vel,\\n“THE CLASSIFICATION PERFORMANCE OF RDA”\\nTech. Rep. no. 92-01, (1992), Dept. of Computer Science and Dept. of\\nMathematics and Statistics, James Cook University of North Queensland.\\n(Also submitted to Journal of Chemometrics).\\n- 7.1.6. Breast cancer wisconsin (diagnostic) dataset¶\\nData Set Characteristics:\\nNumber of Instances:\\n569\\nNumber of Attributes:\\n30 numeric, predictive attributes and the class\\nAttribute Information:\\nradius (mean of distances from center to points on the perimeter)\\ntexture (standard deviation of gray-scale values)\\nperimeter\\narea\\nsmoothness (local variation in radius lengths)\\ncompactness (perimeter^2 / area - 1.0)\\nconcavity (severity of concave portions of the contour)\\nconcave points (number of concave portions of the contour)\\nsymmetry\\nfractal dimension (“coastline approximation” - 1)\\nThe mean, standard error, and “worst” or largest (mean of the three\\nworst/largest values) of these features were computed for each image,\\nresulting in 30 features.  For instance, field 0 is Mean Radius, field\\n10 is Radius SE, field 20 is Worst Radius.\\nclass:\\nWDBC-Malignant\\nWDBC-Benign\\nSummary Statistics:\\nradius (mean):\\n6.981\\n28.11\\ntexture (mean):\\n9.71\\n39.28\\nperimeter (mean):\\n43.79\\n188.5\\narea (mean):\\n143.5\\n2501.0\\nsmoothness (mean):\\n0.053\\n0.163\\ncompactness (mean):\\n0.019\\n0.345\\nconcavity (mean):\\n0.0\\n0.427\\nconcave points (mean):\\n0.0\\n0.201\\nsymmetry (mean):\\n0.106\\n0.304\\nfractal dimension (mean):\\n0.05\\n0.097\\nradius (standard error):\\n0.112\\n2.873\\ntexture (standard error):\\n0.36\\n4.885\\nperimeter (standard error):\\n0.757\\n21.98\\narea (standard error):\\n6.802\\n542.2\\nsmoothness (standard error):\\n0.002\\n0.031\\ncompactness (standard error):\\n0.002\\n0.135\\nconcavity (standard error):\\n0.0\\n0.396\\nconcave points (standard error):\\n0.0\\n0.053\\nsymmetry (standard error):\\n0.008\\n0.079\\nfractal dimension (standard error):\\n0.001\\n0.03\\nradius (worst):\\n7.93\\n36.04\\ntexture (worst):\\n12.02\\n49.54\\nperimeter (worst):\\n50.41\\n251.2\\narea (worst):\\n185.2\\n4254.0\\nsmoothness (worst):\\n0.071\\n0.223\\ncompactness (worst):\\n0.027\\n1.058\\nconcavity (worst):\\n0.0\\n1.252\\nconcave points (worst):\\n0.0\\n0.291\\nsymmetry (worst):\\n0.156\\n0.664\\nfractal dimension (worst):\\n0.055\\n0.208\\nMissing Attribute Values:\\nNone\\nClass Distribution:\\n212 - Malignant, 357 - Benign\\nCreator:\\nDr. William H. Wolberg, W. Nick Street, Olvi L. Mangasarian\\nDonor:\\nNick Street\\nDate:\\nNovember, 1995\\nThis is a copy of UCI ML Breast Cancer Wisconsin (Diagnostic) datasets.\\nhttps://goo.gl/U2Uwz2\\nFeatures are computed from a digitized image of a fine needle\\naspirate (FNA) of a breast mass.  They describe\\ncharacteristics of the cell nuclei present in the image.\\nSeparating plane described above was obtained using\\nMultisurface Method-Tree (MSM-T) [K. P. Bennett, “Decision Tree\\nConstruction Via Linear Programming.” Proceedings of the 4th\\nMidwest Artificial Intelligence and Cognitive Science Society,\\npp. 97-101, 1992], a classification method which uses linear\\nprogramming to construct a decision tree.  Relevant features\\nwere selected using an exhaustive search in the space of 1-4\\nfeatures and 1-3 separating planes.\\nThe actual linear program used to obtain the separating plane\\nin the 3-dimensional space is that described in:\\n[K. P. Bennett and O. L. Mangasarian: “Robust Linear\\nProgramming Discrimination of Two Linearly Inseparable Sets”,\\nOptimization Methods and Software 1, 1992, 23-34].\\nThis database is also available through the UW CS ftp server:\\nftp ftp.cs.wisc.edu\\ncd math-prog/cpo-dataset/machine-learn/WDBC/\\nReferences\\nClick for more details\\n¶\\nW.N. Street, W.H. Wolberg and O.L. Mangasarian. Nuclear feature extraction\\nfor breast tumor diagnosis. IS&T/SPIE 1993 International Symposium on\\nElectronic Imaging: Science and Technology, volume 1905, pages 861-870,\\nSan Jose, CA, 1993.\\nO.L. Mangasarian, W.N. Street and W.H. Wolberg. Breast cancer diagnosis and\\nprognosis via linear programming. Operations Research, 43(4), pages 570-577,\\nJuly-August 1995.\\nW.H. Wolberg, W.N. Street, and O.L. Mangasarian. Machine learning techniques\\nto diagnose breast cancer from fine-needle aspirates. Cancer Letters 77 (1994)\\n163-171.\\n\\n7.2. Real world datasets — scikit-learn 1.4.2 documentation\\n\\nInstall\\nUser Guide\\nAPI\\nExamples\\nCommunity\\nGetting Started\\nTutorial\\nWhat\\'s new\\nGlossary\\nDevelopment\\nFAQ\\nSupport\\nRelated packages\\nRoadmap\\nGovernance\\nAbout us\\nGitHub\\nOther Versions and Download\\nMore\\nGetting Started\\nTutorial\\nWhat\\'s new\\nGlossary\\nDevelopment\\nFAQ\\nSupport\\nRelated packages\\nRoadmap\\nGovernance\\nAbout us\\nGitHub\\nOther Versions and Download\\nToggle Menu\\nPrevUp\\nNext\\nscikit-learn 1.4.2\\nOther versions\\nPlease cite us if you use the software.\\nUser Guide\\n\\n1. Supervised learning\\n\\n2. Unsupervised learning\\n\\n3. Model selection and evaluation\\n\\n4. Inspection\\n\\n5. Visualizations\\n\\n6. Dataset transformations\\n\\n7. Dataset loading utilities\\n\\n7.1. Toy datasets\\n\\n7.2. Real world datasets\\n\\n7.3. Generated datasets\\n\\n7.4. Loading other datasets\\n\\n8. Computing with scikit-learn\\n\\n9. Model persistence\\n\\n10. Common pitfalls and recommended practices\\n\\n11. Dispatching\\n\\n7.2. Real world datasets¶\\n\\nscikit-learn provides tools to load larger datasets, downloading them if\\nnecessary.\\nThey can be loaded using the following functions:\\nfetch_olivetti_faces([, data_home, ...])\\nLoad the Olivetti faces data-set from AT&T (classification).\\nfetch_20newsgroups([, data_home, subset, ...])\\nLoad the filenames and data from the 20 newsgroups dataset (classification).\\nfetch_20newsgroups_vectorized([, subset, ...])\\nLoad and vectorize the 20 newsgroups dataset (classification).\\nfetch_lfw_people([, data_home, funneled, ...])\\nLoad the Labeled Faces in the Wild (LFW) people dataset (classification).\\nfetch_lfw_pairs([, subset, data_home, ...])\\nLoad the Labeled Faces in the Wild (LFW) pairs dataset (classification).\\nfetch_covtype([, data_home, ...])\\nLoad the covertype dataset (classification).\\nfetch_rcv1([, data_home, subset, ...])\\nLoad the RCV1 multilabel dataset (classification).\\nfetch_kddcup99([, subset, data_home, ...])\\nLoad the kddcup99 dataset (classification).\\nfetch_california_housing([, data_home, ...])\\nLoad the California housing dataset (regression).\\nfetch_species_distributions([, data_home, ...])\\nLoader for species distribution dataset from Phillips et.\\n- 7.2.1. The Olivetti faces dataset¶\\nThis dataset contains a set of face images taken between April 1992 and\\nApril 1994 at AT&T Laboratories Cambridge. The\\nsklearn.datasets.fetch_olivetti_faces function is the data\\nfetching / caching function that downloads the data\\narchive from AT&T.\\nAs described on the original website:\\nThere are ten different images of each of 40 distinct subjects. For some\\nsubjects, the images were taken at different times, varying the lighting,\\nfacial expressions (open / closed eyes, smiling / not smiling) and facial\\ndetails (glasses / no glasses). All the images were taken against a dark\\nhomogeneous background with the subjects in an upright, frontal position\\n(with tolerance for some side movement).\\nData Set Characteristics:\\nClasses\\n40\\nSamples total\\n400\\nDimensionality\\n4096\\nFeatures\\nreal, between 0 and 1\\nThe image is quantized to 256 grey levels and stored as unsigned 8-bit\\nintegers; the loader will convert these to floating point values on the\\ninterval [0, 1], which are easier to work with for many algorithms.\\nThe “target” for this database is an integer from 0 to 39 indicating the\\nidentity of the person pictured; however, with only 10 examples per class, this\\nrelatively small dataset is more interesting from an unsupervised or\\nsemi-supervised perspective.\\nThe original dataset consisted of 92 x 112, while the version available here\\nconsists of 64x64 images.\\nWhen using these images, please give credit to AT&T Laboratories Cambridge.\\n- 7.2.2. The 20 newsgroups text dataset¶\\nThe 20 newsgroups dataset comprises around 18000 newsgroups posts on\\n20 topics split in two subsets: one for training (or development)\\nand the other one for testing (or for performance evaluation). The split\\nbetween the train and test set is based upon a messages posted before\\nand after a specific date.\\nThis module contains two loaders. The first one,\\nsklearn.datasets.fetch_20newsgroups,\\nreturns a list of the raw texts that can be fed to text feature\\nextractors such as CountVectorizer\\nwith custom parameters so as to extract feature vectors.\\nThe second one, sklearn.datasets.fetch_20newsgroups_vectorized,\\nreturns ready-to-use features, i.e., it is not necessary to use a feature\\nextractor.\\nData Set Characteristics:\\nClasses\\n20\\nSamples total\\n18846\\nDimensionality\\n1\\nFeatures\\ntext\\nUsage\\nClick for more details\\n¶\\nThe sklearn.datasets.fetch_20newsgroups function is a data\\nfetching / caching functions that downloads the data archive from\\nthe original 20 newsgroups website, extracts the archive contents\\nin the ~/scikit_learn_data/20news_home folder and calls the\\nsklearn.datasets.load_files on either the training or\\ntesting set folder, or both of them:\\n\\nfrom sklearn.datasets import fetch_20newsgroups\\nnewsgroups_train = fetch_20newsgroups(subset=\\'train\\')\\nfrom pprint import pprint\\npprint(list(newsgroups_train.target_names))\\n[\\'alt.atheism\\',\\n\\'comp.graphics\\',\\n\\'comp.os.ms-windows.misc\\',\\n\\'comp.sys.ibm.pc.hardware\\',\\n\\'comp.sys.mac.hardware\\',\\n\\'comp.windows.x\\',\\n\\'misc.forsale\\',\\n\\'rec.autos\\',\\n\\'rec.motorcycles\\',\\n\\'rec.sport.baseball\\',\\n\\'rec.sport.hockey\\',\\n\\'sci.crypt\\',\\n\\'sci.electronics\\',\\n\\'sci.med\\',\\n\\'sci.space\\',\\n\\'soc.religion.christian\\',\\n\\'talk.politics.guns\\',\\n\\'talk.politics.mideast\\',\\n\\'talk.politics.misc\\',\\n\\'talk.religion.misc\\']\\nThe real data lies in the filenames and target attributes. The target\\nattribute is the integer index of the category:\\nnewsgroups_train.filenames.shape\\n(11314,)\\nnewsgroups_train.target.shape\\n(11314,)\\nnewsgroups_train.target[:10]\\narray([ 7,  4,  4,  1, 14, 16, 13,  3,  2,  4])\\nIt is possible to load only a sub-selection of the categories by passing the\\nlist of the categories to load to the\\nsklearn.datasets.fetch_20newsgroups function:\\ncats = [\\'alt.atheism\\', \\'sci.space\\']\\nnewsgroups_train = fetch_20newsgroups(subset=\\'train\\', categories=cats)\\nlist(newsgroups_train.target_names)\\n[\\'alt.atheism\\', \\'sci.space\\']\\nnewsgroups_train.filenames.shape\\n(1073,)\\nnewsgroups_train.target.shape\\n(1073,)\\nnewsgroups_train.target[:10]\\narray([0, 1, 1, 1, 0, 1, 1, 0, 0, 0])\\nConverting text to vectors\\nClick for more details\\n¶\\nIn order to feed predictive or clustering models with the text data,\\none first need to turn the text into vectors of numerical values suitable\\nfor statistical analysis. This can be achieved with the utilities of the\\nsklearn.feature_extraction.text as demonstrated in the following\\nexample that extract TF-IDF vectors of unigram tokens\\nfrom a subset of 20news:\\nfrom sklearn.feature_extraction.text import TfidfVectorizer\\ncategories = [\\'alt.atheism\\', \\'talk.religion.misc\\',\\n...               \\'comp.graphics\\', \\'sci.space\\']\\nnewsgroups_train = fetch_20newsgroups(subset=\\'train\\',\\n...                                       categories=categories)\\nvectorizer = TfidfVectorizer()\\nvectors = vectorizer.fit_transform(newsgroups_train.data)\\nvectors.shape\\n(2034, 34118)\\nThe extracted TF-IDF vectors are very sparse, with an average of 159 non-zero\\ncomponents by sample in a more than 30000-dimensional space\\n(less than .5% non-zero features):\\nvectors.nnz / float(vectors.shape[0])\\n159.01327...\\nsklearn.datasets.fetch_20newsgroups_vectorized is a function which\\nreturns ready-to-use token counts features instead of file names.\\nFiltering text for more realistic training\\nClick for more details\\n¶\\nIt is easy for a classifier to overfit on particular things that appear in the\\n20 Newsgroups data, such as newsgroup headers. Many classifiers achieve very\\nhigh F-scores, but their results would not generalize to other documents that\\naren’t from this window of time.\\nFor example, let’s look at the results of a multinomial Naive Bayes classifier,\\nwhich is fast to train and achieves a decent F-score:\\nfrom sklearn.naive_bayes import MultinomialNB\\nfrom sklearn import metrics\\nnewsgroups_test = fetch_20newsgroups(subset=\\'test\\',\\n...                                      categories=categories)\\nvectors_test = vectorizer.transform(newsgroups_test.data)\\nclf = MultinomialNB(alpha=.01)\\nclf.fit(vectors, newsgroups_train.target)\\nMultinomialNB(alpha=0.01, class_prior=None, fit_prior=True)\\npred = clf.predict(vectors_test)\\nmetrics.f1_score(newsgroups_test.target, pred, average=\\'macro\\')\\n0.88213...\\n(The example Classification of text documents using sparse features shuffles\\nthe training and test data, instead of segmenting by time, and in that case\\nmultinomial Naive Bayes gets a much higher F-score of 0.88. Are you suspicious\\nyet of what’s going on inside this classifier?)\\nLet’s take a look at what the most informative features are:\\nimport numpy as np\\ndef show_top10(classifier, vectorizer, categories):\\n...     feature_names = vectorizer.get_feature_names_out()\\n...     for i, category in enumerate(categories):\\n...         top10 = np.argsort(classifier.coef_[i])[-10:]\\n...         print(\"%s: %s\" % (category, \" \".join(feature_names[top10])))\\n...\\nshow_top10(clf, vectorizer, newsgroups_train.target_names)\\nalt.atheism: edu it and in you that is of to the\\ncomp.graphics: edu in graphics it is for and of to the\\nsci.space: edu it that is in and space to of the\\ntalk.religion.misc: not it you in is that and to of the\\nYou can now see many things that these features have overfit to:\\nAlmost every group is distinguished by whether headers such as\\nNNTP-Posting-Host: and Distribution: appear more or less often.\\nAnother significant feature involves whether the sender is affiliated with\\na university, as indicated either by their headers or their signature.\\nThe word “article” is a significant feature, based on how often people quote\\nprevious posts like this: “In article [article ID], [name] <[e-mail address]>\\nwrote:”\\nOther features match the names and e-mail addresses of particular people who\\nwere posting at the time.\\nWith such an abundance of clues that distinguish newsgroups, the classifiers\\nbarely have to identify topics from text at all, and they all perform at the\\nsame high level.\\nFor this reason, the functions that load 20 Newsgroups data provide a\\nparameter called remove, telling it what kinds of information to strip out\\nof each file. remove should be a tuple containing any subset of\\n(\\'headers\\', \\'footers\\', \\'quotes\\'), telling it to remove headers, signature\\nblocks, and quotation blocks respectively.\\nnewsgroups_test = fetch_20newsgroups(subset=\\'test\\',\\n...                                      remove=(\\'headers\\', \\'footers\\', \\'quotes\\'),\\n...                                      categories=categories)\\nvectors_test = vectorizer.transform(newsgroups_test.data)\\npred = clf.predict(vectors_test)\\nmetrics.f1_score(pred, newsgroups_test.target, average=\\'macro\\')\\n0.77310...\\nThis classifier lost over a lot of its F-score, just because we removed\\nmetadata that has little to do with topic classification.\\nIt loses even more if we also strip this metadata from the training data:\\nnewsgroups_train = fetch_20newsgroups(subset=\\'train\\',\\n...                                       remove=(\\'headers\\', \\'footers\\', \\'quotes\\'),\\n...                                       categories=categories)\\nvectors = vectorizer.fit_transform(newsgroups_train.data)\\nclf = MultinomialNB(alpha=.01)\\nclf.fit(vectors, newsgroups_train.target)\\nMultinomialNB(alpha=0.01, class_prior=None, fit_prior=True)\\nvectors_test = vectorizer.transform(newsgroups_test.data)\\npred = clf.predict(vectors_test)\\nmetrics.f1_score(newsgroups_test.target, pred, average=\\'macro\\')\\n0.76995...\\nSome other classifiers cope better with this harder version of the task. Try the\\nSample pipeline for text feature extraction and evaluation\\nexample with and without the remove option to compare the results.\\nData Considerations\\nThe Cleveland Indians is a major league baseball team based in Cleveland,\\nOhio, USA. In December 2020, it was reported that “After several months of\\ndiscussion sparked by the death of George Floyd and a national reckoning over\\nrace and colonialism, the Cleveland Indians have decided to change their\\nname.” Team owner Paul Dolan “did make it clear that the team will not make\\nits informal nickname – the Tribe – its new team name.” “It’s not going to\\nbe a half-step away from the Indians,” Dolan said.”We will not have a Native\\nAmerican-themed name.”\\nhttps://www.mlb.com/news/cleveland-indians-team-name-change\\nRecommendation\\nWhen evaluating text classifiers on the 20 Newsgroups data, you\\nshould strip newsgroup-related metadata. In scikit-learn, you can do this\\nby setting remove=(\\'headers\\', \\'footers\\', \\'quotes\\'). The F-score will be\\nlower because it is more realistic.\\nThis text dataset contains data which may be inappropriate for certain NLP\\napplications. An example is listed in the “Data Considerations” section\\nabove. The challenge with using current text datasets in NLP for tasks such\\nas sentence completion, clustering, and other applications is that text\\nthat is culturally biased and inflammatory will propagate biases. This\\nshould be taken into consideration when using the dataset, reviewing the\\noutput, and the bias should be documented.\\nExamples\\nSample pipeline for text feature extraction and evaluation\\nClassification of text documents using sparse features\\nFeatureHasher and DictVectorizer Comparison\\nClustering text documents using k-means\\n- 7.2.3. The Labeled Faces in the Wild face recognition dataset¶\\nThis dataset is a collection of JPEG pictures of famous people collected\\nover the internet, all details are available on the official website:\\nhttp://vis-www.cs.umass.edu/lfw/\\nEach picture is centered on a single face. The typical task is called\\nFace Verification: given a pair of two pictures, a binary classifier\\nmust predict whether the two images are from the same person.\\nAn alternative task, Face Recognition or Face Identification is:\\ngiven the picture of the face of an unknown person, identify the name\\nof the person by referring to a gallery of previously seen pictures of\\nidentified persons.\\nBoth Face Verification and Face Recognition are tasks that are typically\\nperformed on the output of a model trained to perform Face Detection. The\\nmost popular model for Face Detection is called Viola-Jones and is\\nimplemented in the OpenCV library. The LFW faces were extracted by this\\nface detector from various online websites.\\nData Set Characteristics:\\nClasses\\n5749\\nSamples total\\n13233\\nDimensionality\\n5828\\nFeatures\\nreal, between 0 and 255\\nUsage\\nClick for more details\\n¶\\nscikit-learn provides two loaders that will automatically download,\\ncache, parse the metadata files, decode the jpeg and convert the\\ninteresting slices into memmapped numpy arrays. This dataset size is more\\nthan 200 MB. The first load typically takes more than a couple of minutes\\nto fully decode the relevant part of the JPEG files into numpy arrays. If\\nthe dataset has  been loaded once, the following times the loading times\\nless than 200ms by using a memmapped version memoized on the disk in the\\n~/scikit_learn_data/lfw_home/ folder using joblib.\\nThe first loader is used for the Face Identification task: a multi-class\\nclassification task (hence supervised learning):\\nfrom sklearn.datasets import fetch_lfw_people\\nlfw_people = fetch_lfw_people(min_faces_per_person=70, resize=0.4)\\nfor name in lfw_people.target_names:\\n...     print(name)\\n...\\nAriel Sharon\\nColin Powell\\nDonald Rumsfeld\\nGeorge W Bush\\nGerhard Schroeder\\nHugo Chavez\\nTony Blair\\nThe default slice is a rectangular shape around the face, removing\\nmost of the background:\\nlfw_people.data.dtype\\ndtype(\\'float32\\')\\nlfw_people.data.shape\\n(1288, 1850)\\nlfw_people.images.shape\\n(1288, 50, 37)\\nEach of the 1140 faces is assigned to a single person id in the target\\narray:\\nlfw_people.target.shape\\n(1288,)\\nlist(lfw_people.target[:10])\\n[5, 6, 3, 1, 0, 1, 3, 4, 3, 0]\\nThe second loader is typically used for the face verification task: each sample\\nis a pair of two picture belonging or not to the same person:\\nfrom sklearn.datasets import fetch_lfw_pairs\\nlfw_pairs_train = fetch_lfw_pairs(subset=\\'train\\')\\nlist(lfw_pairs_train.target_names)\\n[\\'Different persons\\', \\'Same person\\']\\nlfw_pairs_train.pairs.shape\\n(2200, 2, 62, 47)\\nlfw_pairs_train.data.shape\\n(2200, 5828)\\nlfw_pairs_train.target.shape\\n(2200,)\\nBoth for the sklearn.datasets.fetch_lfw_people and\\nsklearn.datasets.fetch_lfw_pairs function it is\\npossible to get an additional dimension with the RGB color channels by\\npassing color=True, in that case the shape will be\\n(2200, 2, 62, 47, 3).\\nThe sklearn.datasets.fetch_lfw_pairs datasets is subdivided into\\n3 subsets: the development train set, the development test set and\\nan evaluation 10_folds set meant to compute performance metrics using a\\n10-folds cross validation scheme.\\nReferences:\\nLabeled Faces in the Wild: A Database for Studying Face Recognition\\nin Unconstrained Environments.\\nGary B. Huang, Manu Ramesh, Tamara Berg, and Erik Learned-Miller.\\nUniversity of Massachusetts, Amherst, Technical Report 07-49, October, 2007.\\nExamples:\\nFaces recognition example using eigenfaces and SVMs\\n- 7.2.4. Forest covertypes¶\\nThe samples in this dataset correspond to 30×30m patches of forest in the US,\\ncollected for the task of predicting each patch’s cover type,\\ni.e. the dominant species of tree.\\nThere are seven covertypes, making this a multiclass classification problem.\\nEach sample has 54 features, described on the\\ndataset’s homepage.\\nSome of the features are boolean indicators,\\nwhile others are discrete or continuous measurements.\\nData Set Characteristics:\\nClasses\\n7\\nSamples total\\n581012\\nDimensionality\\n54\\nFeatures\\nint\\nsklearn.datasets.fetch_covtype will load the covertype dataset;\\nit returns a dictionary-like ‘Bunch’ object\\nwith the feature matrix in the data member\\nand the target values in target. If optional argument ‘as_frame’ is\\nset to ‘True’, it will return data and target as pandas\\ndata frame, and there will be an additional member frame as well.\\nThe dataset will be downloaded from the web if necessary.\\n- 7.2.5. RCV1 dataset¶\\nReuters Corpus Volume I (RCV1) is an archive of over 800,000 manually\\ncategorized newswire stories made available by Reuters, Ltd. for research\\npurposes. The dataset is extensively described in [1].\\nData Set Characteristics:\\nClasses\\n103\\nSamples total\\n804414\\nDimensionality\\n47236\\nFeatures\\nreal, between 0 and 1\\nsklearn.datasets.fetch_rcv1 will load the following\\nversion: RCV1-v2, vectors, full sets, topics multilabels:\\nfrom sklearn.datasets import fetch_rcv1\\nrcv1 = fetch_rcv1()\\nIt returns a dictionary-like object, with the following attributes:\\ndata:\\nThe feature matrix is a scipy CSR sparse matrix, with 804414 samples and\\n47236 features. Non-zero values contains cosine-normalized, log TF-IDF vectors.\\nA nearly chronological split is proposed in [1]: The first 23149 samples are\\nthe training set. The last 781265 samples are the testing set. This follows\\nthe official LYRL2004 chronological split. The array has 0.16% of non zero\\nvalues:\\nrcv1.data.shape\\n(804414, 47236)\\ntarget:\\nThe target values are stored in a scipy CSR sparse matrix, with 804414 samples\\nand 103 categories. Each sample has a value of 1 in its categories, and 0 in\\nothers. The array has 3.15% of non zero values:\\nrcv1.target.shape\\n(804414, 103)\\nsample_id:\\nEach sample can be identified by its ID, ranging (with gaps) from 2286\\nto 810596:\\nrcv1.sample_id[:3]\\narray([2286, 2287, 2288], dtype=uint32)\\ntarget_names:\\nThe target values are the topics of each sample. Each sample belongs to at\\nleast one topic, and to up to 17 topics. There are 103 topics, each\\nrepresented by a string. Their corpus frequencies span five orders of\\nmagnitude, from 5 occurrences for ‘GMIL’, to 381327 for ‘CCAT’:\\nrcv1.target_names[:3].tolist()\\n[\\'E11\\', \\'ECAT\\', \\'M11\\']\\nThe dataset will be downloaded from the rcv1 homepage if necessary.\\nThe compressed size is about 656 MB.\\nReferences\\n[1]\\n(1,2)\\nLewis, D. D., Yang, Y., Rose, T. G., & Li, F. (2004).\\nRCV1: A new benchmark collection for text categorization research.\\nThe Journal of Machine Learning Research, 5, 361-397.\\n- 7.2.6. Kddcup 99 dataset¶\\nThe KDD Cup ‘99 dataset was created by processing the tcpdump portions\\nof the 1998 DARPA Intrusion Detection System (IDS) Evaluation dataset,\\ncreated by MIT Lincoln Lab [2]. The artificial data (described on the dataset’s\\nhomepage) was\\ngenerated using a closed network and hand-injected attacks to produce a\\nlarge number of different types of attack with normal activity in the\\nbackground. As the initial goal was to produce a large training set for\\nsupervised learning algorithms, there is a large proportion (80.1%) of\\nabnormal data which is unrealistic in real world, and inappropriate for\\nunsupervised anomaly detection which aims at detecting ‘abnormal’ data, i.e.:\\nqualitatively different from normal data\\nin large minority among the observations.\\nWe thus transform the KDD Data set into two different data sets: SA and SF.\\nSA is obtained by simply selecting all the normal data, and a small\\nproportion of abnormal data to gives an anomaly proportion of 1%.\\nSF is obtained as in [3]\\nby simply picking up the data whose attribute logged_in is positive, thus\\nfocusing on the intrusion attack, which gives a proportion of 0.3% of\\nattack.\\nhttp and smtp are two subsets of SF corresponding with third feature\\nequal to ‘http’ (resp. to ‘smtp’).\\nGeneral KDD structure:\\nSamples total\\n4898431\\nDimensionality\\n41\\nFeatures\\ndiscrete (int) or continuous (float)\\nTargets\\nstr, ‘normal.’ or name of the anomaly type\\nSA structure:\\nSamples total\\n976158\\nDimensionality\\n41\\nFeatures\\ndiscrete (int) or continuous (float)\\nTargets\\nstr, ‘normal.’ or name of the anomaly type\\nSF structure:\\nSamples total\\n699691\\nDimensionality\\n4\\nFeatures\\ndiscrete (int) or continuous (float)\\nTargets\\nstr, ‘normal.’ or name of the anomaly type\\nhttp structure:\\nSamples total\\n619052\\nDimensionality\\n3\\nFeatures\\ndiscrete (int) or continuous (float)\\nTargets\\nstr, ‘normal.’ or name of the anomaly type\\nsmtp structure:\\nSamples total\\n95373\\nDimensionality\\n3\\nFeatures\\ndiscrete (int) or continuous (float)\\nTargets\\nstr, ‘normal.’ or name of the anomaly type\\nsklearn.datasets.fetch_kddcup99 will load the kddcup99 dataset; it\\nreturns a dictionary-like object with the feature matrix in the data member\\nand the target values in target. The “as_frame” optional argument converts\\ndata into a pandas DataFrame and target into a pandas Series. The\\ndataset will be downloaded from the web if necessary.\\nReferences\\n[2]\\nAnalysis and Results of the 1999 DARPA Off-Line Intrusion\\nDetection Evaluation, Richard Lippmann, Joshua W. Haines,\\nDavid J. Fried, Jonathan Korba, Kumar Das.\\n[3]\\nK. Yamanishi, J.-I. Takeuchi, G. Williams, and P. Milne. Online\\nunsupervised outlier detection using finite mixtures with\\ndiscounting learning algorithms. In Proceedings of the sixth\\nACM SIGKDD international conference on Knowledge discovery\\nand data mining, pages 320-324. ACM Press, 2000.\\n- 7.2.7. California Housing dataset¶\\nData Set Characteristics:\\nNumber of Instances:\\n20640\\nNumber of Attributes:\\n8 numeric, predictive attributes and the target\\nAttribute Information:\\nMedInc        median income in block group\\nHouseAge      median house age in block group\\nAveRooms      average number of rooms per household\\nAveBedrms     average number of bedrooms per household\\nPopulation    block group population\\nAveOccup      average number of household members\\nLatitude      block group latitude\\nLongitude     block group longitude\\nMissing Attribute Values:\\nNone\\nThis dataset was obtained from the StatLib repository.\\nhttps://www.dcc.fc.up.pt/~ltorgo/Regression/cal_housing.html\\nThe target variable is the median house value for California districts,\\nexpressed in hundreds of thousands of dollars ($100,000).\\nThis dataset was derived from the 1990 U.S. census, using one row per census\\nblock group. A block group is the smallest geographical unit for which the U.S.\\nCensus Bureau publishes sample data (a block group typically has a population\\nof 600 to 3,000 people).\\nA household is a group of people residing within a home. Since the average\\nnumber of rooms and bedrooms in this dataset are provided per household, these\\ncolumns may take surprisingly large values for block groups with few households\\nand many empty houses, such as vacation resorts.\\nIt can be downloaded/loaded using the\\nsklearn.datasets.fetch_california_housing function.\\nReferences\\nPace, R. Kelley and Ronald Barry, Sparse Spatial Autoregressions,\\nStatistics and Probability Letters, 33 (1997) 291-297\\n- 7.2.8. Species distribution dataset¶\\nThis dataset represents the geographic distribution of two species in Central and\\nSouth America. The two species are:\\n“Bradypus variegatus” ,\\nthe Brown-throated Sloth.\\n“Microryzomys minutus” ,\\nalso known as the Forest Small Rice Rat, a rodent that lives in Peru,\\nColombia, Ecuador, Peru, and Venezuela.\\nThe dataset is not a typical dataset since a Bunch\\ncontaining the attributes data and target is not returned. Instead, we have\\ninformation allowing to create a “density” map of the different species.\\nThe grid for the map can be built using the attributes x_left_lower_corner,\\ny_left_lower_corner, Nx, Ny and grid_size, which respectively correspond\\nto the x and y coordinates of the lower left corner of the grid, the number of\\npoints along the x- and y-axis and the size of the step on the grid.\\nThe density at each location of the grid is contained in the coverage attribute.\\nFinally, the train and test attributes contain information regarding the location\\nof a species at a specific location.\\nThe dataset is provided by Phillips et. al. (2006).\\nReferences\\n“Maximum entropy modeling of species geographic distributions” S. J. Phillips,\\nR. P. Anderson, R. E. Schapire - Ecological Modelling, 190:231-259, 2006.\\n\\n7.3. Generated datasets — scikit-learn 1.4.2 documentation\\n\\nInstall\\nUser Guide\\nAPI\\nExamples\\nCommunity\\nGetting Started\\nTutorial\\nWhat\\'s new\\nGlossary\\nDevelopment\\nFAQ\\nSupport\\nRelated packages\\nRoadmap\\nGovernance\\nAbout us\\nGitHub\\nOther Versions and Download\\nMore\\nGetting Started\\nTutorial\\nWhat\\'s new\\nGlossary\\nDevelopment\\nFAQ\\nSupport\\nRelated packages\\nRoadmap\\nGovernance\\nAbout us\\nGitHub\\nOther Versions and Download\\nToggle Menu\\nPrevUp\\nNext\\nscikit-learn 1.4.2\\nOther versions\\nPlease cite us if you use the software.\\nUser Guide\\n\\n1. Supervised learning\\n\\n2. Unsupervised learning\\n\\n3. Model selection and evaluation\\n\\n4. Inspection\\n\\n5. Visualizations\\n\\n6. Dataset transformations\\n\\n7. Dataset loading utilities\\n\\n7.1. Toy datasets\\n\\n7.2. Real world datasets\\n\\n7.3. Generated datasets\\n\\n7.4. Loading other datasets\\n\\n8. Computing with scikit-learn\\n\\n9. Model persistence\\n\\n10. Common pitfalls and recommended practices\\n\\n11. Dispatching\\n\\n7.3. Generated datasets¶\\n\\nIn addition, scikit-learn includes various random sample generators that\\ncan be used to build artificial datasets of controlled size and complexity.\\n- 7.3.1. Generators for classification and clustering¶\\nThese generators produce a matrix of features and corresponding discrete\\ntargets.\\n7.3.1.1. Single label¶\\nBoth make_blobs and make_classification create multiclass\\ndatasets by allocating each class one or more normally-distributed clusters of\\npoints.  make_blobs provides greater control regarding the centers and\\nstandard deviations of each cluster, and is used to demonstrate clustering.\\nmake_classification specializes in introducing noise by way of:\\ncorrelated, redundant and uninformative features; multiple Gaussian clusters\\nper class; and linear transformations of the feature space.\\nmake_gaussian_quantiles divides a single Gaussian cluster into\\nnear-equal-size classes separated by concentric hyperspheres.\\nmake_hastie_10_2 generates a similar binary, 10-dimensional problem.\\nmake_circles and make_moons generate 2d binary classification\\ndatasets that are challenging to certain algorithms (e.g. centroid-based\\nclustering or linear classification), including optional Gaussian noise.\\nThey are useful for visualization. make_circles produces Gaussian data\\nwith a spherical decision boundary for binary classification, while\\nmake_moons produces two interleaving half circles.\\n7.3.1.2. Multilabel¶\\nmake_multilabel_classification generates random samples with multiple\\nlabels, reflecting a bag of words drawn from a mixture of topics. The number of\\ntopics for each document is drawn from a Poisson distribution, and the topics\\nthemselves are drawn from a fixed random distribution. Similarly, the number of\\nwords is drawn from Poisson, with words drawn from a multinomial, where each\\ntopic defines a probability distribution over words. Simplifications with\\nrespect to true bag-of-words mixtures include:\\nPer-topic word distributions are independently drawn, where in reality all\\nwould be affected by a sparse base distribution, and would be correlated.\\nFor a document generated from multiple topics, all topics are weighted\\nequally in generating its bag of words.\\nDocuments without labels words at random, rather than from a base\\ndistribution.\\n7.3.1.3. Biclustering¶\\nmake_biclusters(shape, n_clusters, [, ...])\\nGenerate a constant block diagonal structure array for biclustering.\\nmake_checkerboard(shape, n_clusters, [, ...])\\nGenerate an array with block checkerboard structure for biclustering.\\n- 7.3.2. Generators for regression¶\\nmake_regression produces regression targets as an optionally-sparse\\nrandom linear combination of random features, with noise. Its informative\\nfeatures may be uncorrelated, or low rank (few features account for most of the\\nvariance).\\nOther regression generators generate functions deterministically from\\nrandomized features.  make_sparse_uncorrelated produces a target as a\\nlinear combination of four features with fixed coefficients.\\nOthers encode explicitly non-linear relations:\\nmake_friedman1 is related by polynomial and sine transforms;\\nmake_friedman2 includes feature multiplication and reciprocation; and\\nmake_friedman3 is similar with an arctan transformation on the target.\\n- 7.3.3. Generators for manifold learning¶\\nmake_s_curve([n_samples, noise, random_state])\\nGenerate an S curve dataset.\\nmake_swiss_roll([n_samples, noise, ...])\\nGenerate a swiss roll dataset.\\n- 7.3.4. Generators for decomposition¶\\nmake_low_rank_matrix([n_samples, ...])\\nGenerate a mostly low rank matrix with bell-shaped singular values.\\nmake_sparse_coded_signal(n_samples, , ...)\\nGenerate a signal as a sparse combination of dictionary elements.\\nmake_spd_matrix(n_dim, [, random_state])\\nGenerate a random symmetric, positive-definite matrix.\\nmake_sparse_spd_matrix([n_dim, alpha, ...])\\nGenerate a sparse symmetric definite positive matrix.\\n\\n7.4. Loading other datasets — scikit-learn 1.4.2 documentation\\n\\nInstall\\nUser Guide\\nAPI\\nExamples\\nCommunity\\nGetting Started\\nTutorial\\nWhat\\'s new\\nGlossary\\nDevelopment\\nFAQ\\nSupport\\nRelated packages\\nRoadmap\\nGovernance\\nAbout us\\nGitHub\\nOther Versions and Download\\nMore\\nGetting Started\\nTutorial\\nWhat\\'s new\\nGlossary\\nDevelopment\\nFAQ\\nSupport\\nRelated packages\\nRoadmap\\nGovernance\\nAbout us\\nGitHub\\nOther Versions and Download\\nToggle Menu\\nPrevUp\\nNext\\nscikit-learn 1.4.2\\nOther versions\\nPlease cite us if you use the software.\\nUser Guide\\n\\n1. Supervised learning\\n\\n2. Unsupervised learning\\n\\n3. Model selection and evaluation\\n\\n4. Inspection\\n\\n5. Visualizations\\n\\n6. Dataset transformations\\n\\n7. Dataset loading utilities\\n\\n7.1. Toy datasets\\n\\n7.2. Real world datasets\\n\\n7.3. Generated datasets\\n\\n7.4. Loading other datasets\\n\\n8. Computing with scikit-learn\\n\\n9. Model persistence\\n\\n10. Common pitfalls and recommended practices\\n\\n11. Dispatching\\n\\n7.4. Loading other datasets¶\\n\\n7.4.1. Sample images¶\\nScikit-learn also embeds a couple of sample JPEG images published under Creative\\nCommons license by their authors. Those images can be useful to test algorithms\\nand pipelines on 2D data.\\nload_sample_images()\\nLoad sample images for image manipulation.\\nload_sample_image(image_name)\\nLoad the numpy array of a single sample image.\\nWarning\\nThe default coding of images is based on the uint8 dtype to\\nspare memory. Often machine learning algorithms work best if the\\ninput is converted to a floating point representation first. Also,\\nif you plan to use matplotlib.pyplpt.imshow, don’t forget to scale to the range\\n0 - 1 as done in the following example.\\nExamples:\\nColor Quantization using K-Means\\n\\n7.4.2. Datasets in svmlight / libsvm format¶\\nscikit-learn includes utility functions for loading\\ndatasets in the svmlight / libsvm format. In this format, each line\\ntakes the form  :\\n: .... This format is especially suitable for sparse datasets.\\nIn this module, scipy sparse CSR matrices are used for X and numpy arrays are used for y.\\nYou may load a dataset like as follows:\\n\\n\\nfrom sklearn.datasets import load_svmlight_file\\nX_train, y_train = load_svmlight_file(\"/path/to/train_dataset.txt\")\\n...\\nYou may also load two (or more) datasets at once:\\nX_train, y_train, X_test, y_test = load_svmlight_files(\\n...     (\"/path/to/train_dataset.txt\", \"/path/to/test_dataset.txt\"))\\n...\\nIn this case, X_train and X_test are guaranteed to have the same number\\nof features. Another way to achieve the same result is to fix the number of\\nfeatures:\\nX_test, y_test = load_svmlight_file(\\n...     \"/path/to/test_dataset.txt\", n_features=X_train.shape[1])\\n...\\nRelated links:\\nPublic datasets in svmlight / libsvm format: https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets\\nFaster API-compatible implementation: https://github.com/mblondel/svmlight-loader\\n\\n7.4.3. Downloading datasets from the openml.org repository¶\\nopenml.org is a public repository for machine learning\\ndata and experiments, that allows everybody to upload open datasets.\\nThe sklearn.datasets package is able to download datasets\\nfrom the repository using the function\\nsklearn.datasets.fetch_openml.\\nFor example, to download a dataset of gene expressions in mice brains:\\n\\n\\nfrom sklearn.datasets import fetch_openml\\nmice = fetch_openml(name=\\'miceprotein\\', version=4)\\nTo fully specify a dataset, you need to provide a name and a version, though\\nthe version is optional, see Dataset Versions below.\\nThe dataset contains a total of 1080 examples belonging to 8 different\\nclasses:\\nmice.data.shape\\n(1080, 77)\\nmice.target.shape\\n(1080,)\\nnp.unique(mice.target)\\narray([\\'c-CS-m\\', \\'c-CS-s\\', \\'c-SC-m\\', \\'c-SC-s\\', \\'t-CS-m\\', \\'t-CS-s\\', \\'t-SC-m\\', \\'t-SC-s\\'], dtype=object)\\nYou can get more information on the dataset by looking at the DESCR\\nand details attributes:\\nprint(mice.DESCR)\\nAuthor: Clara Higuera, Katheleen J. Gardiner, Krzysztof J. Cios\\nSource: UCI - 2015\\nPlease cite: Higuera C, Gardiner KJ, Cios KJ (2015) Self-Organizing\\nFeature Maps Identify Proteins Critical to Learning in a Mouse Model of Down\\nSyndrome. PLoS ONE 10(6): e0129126...\\nmice.details\\n{\\'id\\': \\'40966\\', \\'name\\': \\'MiceProtein\\', \\'version\\': \\'4\\', \\'format\\': \\'ARFF\\',\\n\\'upload_date\\': \\'2017-11-08T16:00:15\\', \\'licence\\': \\'Public\\',\\n\\'url\\': \\'https://www.openml.org/data/v1/download/17928620/MiceProtein.arff\\',\\n\\'file_id\\': \\'17928620\\', \\'default_target_attribute\\': \\'class\\',\\n\\'row_id_attribute\\': \\'MouseID\\',\\n\\'ignore_attribute\\': [\\'Genotype\\', \\'Treatment\\', \\'Behavior\\'],\\n\\'tag\\': [\\'OpenML-CC18\\', \\'study_135\\', \\'study_98\\', \\'study_99\\'],\\n\\'visibility\\': \\'public\\', \\'status\\': \\'active\\',\\n\\'md5_checksum\\': \\'3c479a6885bfa0438971388283a1ce32\\'}\\nThe DESCR contains a free-text description of the data, while details\\ncontains a dictionary of meta-data stored by openml, like the dataset id.\\nFor more details, see the OpenML documentation The data_id of the mice protein dataset\\nis 40966, and you can use this (or the name) to get more information on the\\ndataset on the openml website:\\nmice.url\\n\\'https://www.openml.org/d/40966\\'\\nThe data_id also uniquely identifies a dataset from OpenML:\\nmice = fetch_openml(data_id=40966)\\nmice.details\\n{\\'id\\': \\'4550\\', \\'name\\': \\'MiceProtein\\', \\'version\\': \\'1\\', \\'format\\': \\'ARFF\\',\\n\\'creator\\': ...,\\n\\'upload_date\\': \\'2016-02-17T14:32:49\\', \\'licence\\': \\'Public\\', \\'url\\':\\n\\'https://www.openml.org/data/v1/download/1804243/MiceProtein.ARFF\\', \\'file_id\\':\\n\\'1804243\\', \\'default_target_attribute\\': \\'class\\', \\'citation\\': \\'Higuera C,\\nGardiner KJ, Cios KJ (2015) Self-Organizing Feature Maps Identify Proteins\\nCritical to Learning in a Mouse Model of Down Syndrome. PLoS ONE 10(6):\\ne0129126. [Web Link] journal.pone.0129126\\', \\'tag\\': [\\'OpenML100\\', \\'study_14\\',\\n\\'study_34\\'], \\'visibility\\': \\'public\\', \\'status\\': \\'active\\', \\'md5_checksum\\':\\n\\'3c479a6885bfa0438971388283a1ce32\\'}\\n7.4.3.1. Dataset Versions¶\\nA dataset is uniquely specified by its data_id, but not necessarily by its\\nname. Several different “versions” of a dataset with the same name can exist\\nwhich can contain entirely different datasets.\\nIf a particular version of a dataset has been found to contain significant\\nissues, it might be deactivated. Using a name to specify a dataset will yield\\nthe earliest version of a dataset that is still active. That means that\\nfetch_openml(name=\"miceprotein\") can yield different results\\nat different times if earlier versions become inactive.\\nYou can see that the dataset with data_id 40966 that we fetched above is\\nthe first version of the “miceprotein” dataset:\\nmice.details[\\'version\\']\\n\\'1\\'\\nIn fact, this dataset only has one version. The iris dataset on the other hand\\nhas multiple versions:\\niris = fetch_openml(name=\"iris\")\\niris.details[\\'version\\']\\n\\'1\\'\\niris.details[\\'id\\']\\n\\'61\\'\\niris_61 = fetch_openml(data_id=61)\\niris_61.details[\\'version\\']\\n\\'1\\'\\niris_61.details[\\'id\\']\\n\\'61\\'\\niris_969 = fetch_openml(data_id=969)\\niris_969.details[\\'version\\']\\n\\'3\\'\\niris_969.details[\\'id\\']\\n\\'969\\'\\nSpecifying the dataset by the name “iris” yields the lowest version, version 1,\\nwith the data_id 61. To make sure you always get this exact dataset, it is\\nsafest to specify it by the dataset data_id. The other dataset, with\\ndata_id 969, is version 3 (version 2 has become inactive), and contains a\\nbinarized version of the data:\\nnp.unique(iris_969.target)\\narray([\\'N\\', \\'P\\'], dtype=object)\\nYou can also specify both the name and the version, which also uniquely\\nidentifies the dataset:\\niris_version_3 = fetch_openml(name=\"iris\", version=3)\\niris_version_3.details[\\'version\\']\\n\\'3\\'\\niris_version_3.details[\\'id\\']\\n\\'969\\'\\nReferences:\\nVanschoren, van Rijn, Bischl and Torgo. “OpenML: networked science in\\nmachine learning” ACM SIGKDD Explorations Newsletter, 15(2), 49-60, 2014.\\n7.4.3.2. ARFF parser¶\\nFrom version 1.2, scikit-learn provides a new keyword argument parser that\\nprovides several options to parse the ARFF files provided by OpenML. The legacy\\nparser (i.e. parser=\"liac-arff\") is based on the project\\nLIAC-ARFF. This parser is however\\nslow and consume more memory than required. A new parser based on pandas\\n(i.e. parser=\"pandas\") is both faster and more memory efficient.\\nHowever, this parser does not support sparse data.\\nTherefore, we recommend using parser=\"auto\" which will use the best parser\\navailable for the requested dataset.\\nThe \"pandas\" and \"liac-arff\" parsers can lead to different data types in\\nthe output. The notable differences are the following:\\nThe \"liac-arff\" parser always encodes categorical features as str\\nobjects. To the contrary, the \"pandas\" parser instead infers the type while\\nreading and numerical categories will be casted into integers whenever\\npossible.\\nThe \"liac-arff\" parser uses float64 to encode numerical features tagged as\\n‘REAL’ and ‘NUMERICAL’ in the metadata. The \"pandas\" parser instead infers\\nif these numerical features corresponds to integers and uses panda’s Integer\\nextension dtype.\\nIn particular, classification datasets with integer categories are typically\\nloaded as such (0, 1, ...) with the \"pandas\" parser while \"liac-arff\"\\nwill force the use of string encoded class labels such as \"0\", \"1\" and so\\non.\\nThe \"pandas\" parser will not strip single quotes - i.e. \\' - from string\\ncolumns. For instance, a string \\'my string\\' will be kept as is while the\\n\"liac-arff\" parser will strip the single quotes. For categorical columns,\\nthe single quotes are stripped from the values.\\nIn addition, when as_frame=False is used, the \"liac-arff\" parser returns\\nordinally encoded data where the categories are provided in the attribute\\ncategories of the Bunch instance. Instead, \"pandas\" returns a NumPy array\\nwere the categories. Then it’s up to the user to design a feature\\nengineering pipeline with an instance of  OneHotEncoder or\\nOrdinalEncoder typically wrapped in a ColumnTransformer to\\npreprocess the categorical columns explicitly. See for instance: Column Transformer with Mixed Types.\\n\\n7.4.4. Loading from external datasets¶\\nscikit-learn works on any numeric data stored as numpy arrays or scipy sparse\\nmatrices. Other types that are convertible to numeric arrays such as pandas\\nDataFrame are also acceptable.\\nHere are some recommended ways to load standard columnar data into a\\nformat usable by scikit-learn:\\npandas.io\\nprovides tools to read data from common formats including CSV, Excel, JSON\\nand SQL. DataFrames may also be constructed from lists of tuples or dicts.\\nPandas handles heterogeneous data smoothly and provides tools for\\nmanipulation and conversion into a numeric array suitable for scikit-learn.\\nscipy.io\\nspecializes in binary formats often used in scientific computing\\ncontext such as .mat and .arff\\nnumpy/routines.io\\nfor standard loading of columnar data into numpy arrays\\nscikit-learn’s load_svmlight_file for the svmlight or libSVM\\nsparse format\\nscikit-learn’s load_files for directories of text files where\\nthe name of each directory is the name of each category and each file inside\\nof each directory corresponds to one sample from that category\\nFor some miscellaneous data such as images, videos, and audio, you may wish to\\nrefer to:\\nskimage.io or\\nImageio\\nfor loading images and videos into numpy arrays\\nscipy.io.wavfile.read\\nfor reading WAV files into a numpy array\\nCategorical (or nominal) features stored as strings (common in pandas DataFrames)\\nwill need converting to numerical features using OneHotEncoder\\nor OrdinalEncoder or similar.\\nSee Preprocessing data.\\nNote: if you manage your own numerical data it is recommended to use an\\noptimized file format such as HDF5 to reduce data load times. Various libraries\\nsuch as H5Py, PyTables and pandas provides a Python interface for reading and\\nwriting data in that format.\\n\\n8.1. Strategies to scale computationally: bigger data — scikit-learn 1.4.2 documentation\\n\\nInstall\\nUser Guide\\nAPI\\nExamples\\nCommunity\\nGetting Started\\nTutorial\\nWhat\\'s new\\nGlossary\\nDevelopment\\nFAQ\\nSupport\\nRelated packages\\nRoadmap\\nGovernance\\nAbout us\\nGitHub\\nOther Versions and Download\\nMore\\nGetting Started\\nTutorial\\nWhat\\'s new\\nGlossary\\nDevelopment\\nFAQ\\nSupport\\nRelated packages\\nRoadmap\\nGovernance\\nAbout us\\nGitHub\\nOther Versions and Download\\nToggle Menu\\nPrevUp\\nNext\\nscikit-learn 1.4.2\\nOther versions\\nPlease cite us if you use the software.\\nUser Guide\\n\\n1. Supervised learning\\n\\n2. Unsupervised learning\\n\\n3. Model selection and evaluation\\n\\n4. Inspection\\n\\n5. Visualizations\\n\\n6. Dataset transformations\\n\\n7. Dataset loading utilities\\n\\n8. Computing with scikit-learn\\n\\n8.1. Strategies to scale computationally: bigger data\\n\\n8.2. Computational Performance\\n\\n8.3. Parallelism, resource management, and configuration\\n\\n9. Model persistence\\n\\n10. Common pitfalls and recommended practices\\n\\n11. Dispatching\\n\\n8.1. Strategies to scale computationally: bigger data¶\\n\\nFor some applications the amount of examples, features (or both) and/or the\\nspeed at which they need to be processed are challenging for traditional\\napproaches. In these cases scikit-learn has a number of options you can\\nconsider to make your system scale.\\n- 8.1.1. Scaling with instances using out-of-core learning¶\\nOut-of-core (or “external memory”) learning is a technique used to learn from\\ndata that cannot fit in a computer’s main memory (RAM).\\nHere is a sketch of a system designed to achieve this goal:\\na way to stream instances\\na way to extract features from instances\\nan incremental algorithm\\n8.1.1.1. Streaming instances¶\\nBasically, 1. may be a reader that yields instances from files on a\\nhard drive, a database, from a network stream etc. However,\\ndetails on how to achieve this are beyond the scope of this documentation.\\n8.1.1.2. Extracting features¶\\n\\n2. could be any relevant way to extract features among the\\n\\ndifferent feature extraction methods supported by\\nscikit-learn. However, when working with data that needs vectorization and\\nwhere the set of features or values is not known in advance one should take\\nexplicit care. A good example is text classification where unknown terms are\\nlikely to be found during training. It is possible to use a stateful\\nvectorizer if making multiple passes over the data is reasonable from an\\napplication point of view. Otherwise, one can turn up the difficulty by using\\na stateless feature extractor. Currently the preferred way to do this is to\\nuse the so-called hashing trick as implemented by\\nsklearn.feature_extraction.FeatureHasher for datasets with categorical\\nvariables represented as list of Python dicts or\\nsklearn.feature_extraction.text.HashingVectorizer for text documents.\\n8.1.1.3. Incremental learning¶\\nFinally, for 3. we have a number of options inside scikit-learn. Although not\\nall algorithms can learn incrementally (i.e. without seeing all the instances\\nat once), all estimators implementing the partial_fit API are candidates.\\nActually, the ability to learn incrementally from a mini-batch of instances\\n(sometimes called “online learning”) is key to out-of-core learning as it\\nguarantees that at any given time there will be only a small amount of\\ninstances in the main memory. Choosing a good size for the mini-batch that\\nbalances relevancy and memory footprint could involve some tuning [1].\\nHere is a list of incremental estimators for different tasks:\\nClassification\\nsklearn.naive_bayes.MultinomialNB\\nsklearn.naive_bayes.BernoulliNB\\nsklearn.linear_model.Perceptron\\nsklearn.linear_model.SGDClassifier\\nsklearn.linear_model.PassiveAggressiveClassifier\\nsklearn.neural_network.MLPClassifier\\nRegression\\nsklearn.linear_model.SGDRegressor\\nsklearn.linear_model.PassiveAggressiveRegressor\\nsklearn.neural_network.MLPRegressor\\nClustering\\nsklearn.cluster.MiniBatchKMeans\\nsklearn.cluster.Birch\\nDecomposition / feature Extraction\\nsklearn.decomposition.MiniBatchDictionaryLearning\\nsklearn.decomposition.IncrementalPCA\\nsklearn.decomposition.LatentDirichletAllocation\\nsklearn.decomposition.MiniBatchNMF\\nPreprocessing\\nsklearn.preprocessing.StandardScaler\\nsklearn.preprocessing.MinMaxScaler\\nsklearn.preprocessing.MaxAbsScaler\\nFor classification, a somewhat important thing to note is that although a\\nstateless feature extraction routine may be able to cope with new/unseen\\nattributes, the incremental learner itself may be unable to cope with\\nnew/unseen targets classes. In this case you have to pass all the possible\\nclasses to the first partial_fit call using the classes= parameter.\\nAnother aspect to consider when choosing a proper algorithm is that not all of\\nthem put the same importance on each example over time. Namely, the\\nPerceptron is still sensitive to badly labeled examples even after many\\nexamples whereas the SGD and PassiveAggressive families are more\\nrobust to this kind of artifacts. Conversely, the latter also tend to give less\\nimportance to remarkably different, yet properly labeled examples when they\\ncome late in the stream as their learning rate decreases over time.\\n8.1.1.4. Examples¶\\nFinally, we have a full-fledged example of\\nOut-of-core classification of text documents. It is aimed at\\nproviding a starting point for people wanting to build out-of-core learning\\nsystems and demonstrates most of the notions discussed above.\\nFurthermore, it also shows the evolution of the performance of different\\nalgorithms with the number of processed examples.\\nNow looking at the computation time of the different parts, we see that the\\nvectorization is much more expensive than learning itself. From the different\\nalgorithms, MultinomialNB is the most expensive, but its overhead can be\\nmitigated by increasing the size of the mini-batches (exercise: change\\nminibatch_size to 100 and 10000 in the program and compare).\\n8.1.1.5. Notes¶\\n[1]\\nDepending on the algorithm the mini-batch size can influence results or\\nnot. SGD, PassiveAggressive, and discrete NaiveBayes are truly online\\nand are not affected by batch size. Conversely, MiniBatchKMeans\\nconvergence rate is affected by the batch size. Also, its memory\\nfootprint can vary dramatically with batch size.\\n\\n8.2. Computational Performance — scikit-learn 1.4.2 documentation\\n\\nInstall\\nUser Guide\\nAPI\\nExamples\\nCommunity\\nGetting Started\\nTutorial\\nWhat\\'s new\\nGlossary\\nDevelopment\\nFAQ\\nSupport\\nRelated packages\\nRoadmap\\nGovernance\\nAbout us\\nGitHub\\nOther Versions and Download\\nMore\\nGetting Started\\nTutorial\\nWhat\\'s new\\nGlossary\\nDevelopment\\nFAQ\\nSupport\\nRelated packages\\nRoadmap\\nGovernance\\nAbout us\\nGitHub\\nOther Versions and Download\\nToggle Menu\\nPrevUp\\nNext\\nscikit-learn 1.4.2\\nOther versions\\nPlease cite us if you use the software.\\nUser Guide\\n\\n1. Supervised learning\\n\\n2. Unsupervised learning\\n\\n3. Model selection and evaluation\\n\\n4. Inspection\\n\\n5. Visualizations\\n\\n6. Dataset transformations\\n\\n7. Dataset loading utilities\\n\\n8. Computing with scikit-learn\\n\\n8.1. Strategies to scale computationally: bigger data\\n\\n8.2. Computational Performance\\n\\n8.3. Parallelism, resource management, and configuration\\n\\n9. Model persistence\\n\\n10. Common pitfalls and recommended practices\\n\\n11. Dispatching\\n\\n8.2. Computational Performance¶\\n\\nFor some applications the performance (mainly latency and throughput at\\nprediction time) of estimators is crucial. It may also be of interest to\\nconsider the training throughput but this is often less important in a\\nproduction setup (where it often takes place offline).\\nWe will review here the orders of magnitude you can expect from a number of\\nscikit-learn estimators in different contexts and provide some tips and\\ntricks for overcoming performance bottlenecks.\\nPrediction latency is measured as the elapsed time necessary to make a\\nprediction (e.g. in micro-seconds). Latency is often viewed as a distribution\\nand operations engineers often focus on the latency at a given percentile of\\nthis distribution (e.g. the 90 percentile).\\nPrediction throughput is defined as the number of predictions the software can\\ndeliver in a given amount of time (e.g. in predictions per second).\\nAn important aspect of performance optimization is also that it can hurt\\nprediction accuracy. Indeed, simpler models (e.g. linear instead of\\nnon-linear, or with fewer parameters) often run faster but are not always able\\nto take into account the same exact properties of the data as more complex ones.\\n- 8.2.1. Prediction Latency¶\\nOne of the most straight-forward concerns one may have when using/choosing a\\nmachine learning toolkit is the latency at which predictions can be made in a\\nproduction environment.\\nThe main factors that influence the prediction latency are\\nNumber of features\\nInput data representation and sparsity\\nModel complexity\\nFeature extraction\\nA last major parameter is also the possibility to do predictions in bulk or\\none-at-a-time mode.\\n8.2.1.1. Bulk versus Atomic mode¶\\nIn general doing predictions in bulk (many instances at the same time) is\\nmore efficient for a number of reasons (branching predictability, CPU cache,\\nlinear algebra libraries optimizations etc.). Here we see on a setting\\nwith few features that independently of estimator choice the bulk mode is\\nalways faster, and for some of them by 1 to 2 orders of magnitude:\\nTo benchmark different estimators for your case you can simply change the\\nn_features parameter in this example:\\nPrediction Latency. This should give\\nyou an estimate of the order of magnitude of the prediction latency.\\n8.2.1.2. Configuring Scikit-learn for reduced validation overhead¶\\nScikit-learn does some validation on data that increases the overhead per\\ncall to predict and similar functions. In particular, checking that\\nfeatures are finite (not NaN or infinite) involves a full pass over the\\ndata. If you ensure that your data is acceptable, you may suppress\\nchecking for finiteness by setting the environment variable\\nSKLEARN_ASSUME_FINITE to a non-empty string before importing\\nscikit-learn, or configure it in Python with set_config.\\nFor more control than these global settings, a config_context\\nallows you to set this configuration within a specified context:\\n\\nimport sklearn\\nwith sklearn.config_context(assume_finite=True):\\n...     pass  # do learning/prediction here with reduced validation\\nNote that this will affect all uses of\\nassert_all_finite within the context.\\n8.2.1.3. Influence of the Number of Features¶\\nObviously when the number of features increases so does the memory\\nconsumption of each example. Indeed, for a matrix of (M) instances\\nwith (N) features, the space complexity is in (O(NM)).\\nFrom a computing perspective it also means that the number of basic operations\\n(e.g., multiplications for vector-matrix products in linear models) increases\\ntoo. Here is a graph of the evolution of the prediction latency with the\\nnumber of features:\\nOverall you can expect the prediction time to increase at least linearly with\\nthe number of features (non-linear cases can happen depending on the global\\nmemory footprint and estimator).\\n8.2.1.4. Influence of the Input Data Representation¶\\nScipy provides sparse matrix data structures which are optimized for storing\\nsparse data. The main feature of sparse formats is that you don’t store zeros\\nso if your data is sparse then you use much less memory. A non-zero value in\\na sparse (CSR or CSC)\\nrepresentation will only take on average one 32bit integer position + the 64\\nbit floating point value + an additional 32bit per row or column in the matrix.\\nUsing sparse input on a dense (or sparse) linear model can speedup prediction\\nby quite a bit as only the non zero valued features impact the dot product\\nand thus the model predictions. Hence if you have 100 non zeros in 1e6\\ndimensional space, you only need 100 multiply and add operation instead of 1e6.\\nCalculation over a dense representation, however, may leverage highly optimized\\nvector operations and multithreading in BLAS, and tends to result in fewer CPU\\ncache misses. So the sparsity should typically be quite high (10% non-zeros\\nmax, to be checked depending on the hardware) for the sparse input\\nrepresentation to be faster than the dense input representation on a machine\\nwith many CPUs and an optimized BLAS implementation.\\nHere is sample code to test the sparsity of your input:\\ndef sparsity_ratio(X):\\nreturn 1.0 - np.count_nonzero(X) / float(X.shape[0] * X.shape[1])\\nprint(\"input sparsity ratio:\", sparsity_ratio(X))\\nAs a rule of thumb you can consider that if the sparsity ratio is greater\\nthan 90% you can probably benefit from sparse formats. Check Scipy’s sparse\\nmatrix formats documentation\\nfor more information on how to build (or convert your data to) sparse matrix\\nformats. Most of the time the CSR and CSC formats work best.\\n8.2.1.5. Influence of the Model Complexity¶\\nGenerally speaking, when model complexity increases, predictive power and\\nlatency are supposed to increase. Increasing predictive power is usually\\ninteresting, but for many applications we would better not increase\\nprediction latency too much. We will now review this idea for different\\nfamilies of supervised models.\\nFor sklearn.linear_model (e.g. Lasso, ElasticNet,\\nSGDClassifier/Regressor, Ridge & RidgeClassifier,\\nPassiveAggressiveClassifier/Regressor, LinearSVC, LogisticRegression…) the\\ndecision function that is applied at prediction time is the same (a dot product)\\n, so latency should be equivalent.\\nHere is an example using\\nSGDClassifier with the\\nelasticnet penalty. The regularization strength is globally controlled by\\nthe alpha parameter. With a sufficiently high alpha,\\none can then increase the l1_ratio parameter of elasticnet to\\nenforce various levels of sparsity in the model coefficients. Higher sparsity\\nhere is interpreted as less model complexity as we need fewer coefficients to\\ndescribe it fully. Of course sparsity influences in turn the prediction time\\nas the sparse dot-product takes time roughly proportional to the number of\\nnon-zero coefficients.\\nFor the sklearn.svm family of algorithms with a non-linear kernel,\\nthe latency is tied to the number of support vectors (the fewer the faster).\\nLatency and throughput should (asymptotically) grow linearly with the number\\nof support vectors in a SVC or SVR model. The kernel will also influence the\\nlatency as it is used to compute the projection of the input vector once per\\nsupport vector. In the following graph the nu parameter of\\nNuSVR was used to influence the number of\\nsupport vectors.\\nFor sklearn.ensemble of trees (e.g. RandomForest, GBT,\\nExtraTrees, etc.) the number of trees and their depth play the most\\nimportant role. Latency and throughput should scale linearly with the number\\nof trees. In this case we used directly the n_estimators parameter of\\nGradientBoostingRegressor.\\nIn any case be warned that decreasing model complexity can hurt accuracy as\\nmentioned above. For instance a non-linearly separable problem can be handled\\nwith a speedy linear model but prediction power will very likely suffer in\\nthe process.\\n8.2.1.6. Feature Extraction Latency¶\\nMost scikit-learn models are usually pretty fast as they are implemented\\neither with compiled Cython extensions or optimized computing libraries.\\nOn the other hand, in many real world applications the feature extraction\\nprocess (i.e. turning raw data like database rows or network packets into\\nnumpy arrays) governs the overall prediction time. For example on the Reuters\\ntext classification task the whole preparation (reading and parsing SGML\\nfiles, tokenizing the text and hashing it into a common vector space) is\\ntaking 100 to 500 times more time than the actual prediction code, depending on\\nthe chosen model.\\nIn many cases it is thus recommended to carefully time and profile your\\nfeature extraction code as it may be a good place to start optimizing when\\nyour overall latency is too slow for your application.\\n- 8.2.2. Prediction Throughput¶\\nAnother important metric to care about when sizing production systems is the\\nthroughput i.e. the number of predictions you can make in a given amount of\\ntime. Here is a benchmark from the\\nPrediction Latency example that measures\\nthis quantity for a number of estimators on synthetic data:\\nThese throughputs are achieved on a single process. An obvious way to\\nincrease the throughput of your application is to spawn additional instances\\n(usually processes in Python because of the\\nGIL) that share the\\nsame model. One might also add machines to spread the load. A detailed\\nexplanation on how to achieve this is beyond the scope of this documentation\\nthough.\\n- 8.2.3. Tips and Tricks¶\\n8.2.3.1. Linear algebra libraries¶\\nAs scikit-learn relies heavily on Numpy/Scipy and linear algebra in general it\\nmakes sense to take explicit care of the versions of these libraries.\\nBasically, you ought to make sure that Numpy is built using an optimized BLAS /\\nLAPACK library.\\nNot all models benefit from optimized BLAS and Lapack implementations. For\\ninstance models based on (randomized) decision trees typically do not rely on\\nBLAS calls in their inner loops, nor do kernel SVMs (SVC, SVR,\\nNuSVC, NuSVR).  On the other hand a linear model implemented with a\\nBLAS DGEMM call (via numpy.dot) will typically benefit hugely from a tuned\\nBLAS implementation and lead to orders of magnitude speedup over a\\nnon-optimized BLAS.\\nYou can display the BLAS / LAPACK implementation used by your NumPy / SciPy /\\nscikit-learn install with the following command:\\npython -c \"import sklearn; sklearn.show_versions()\"\\nOptimized BLAS / LAPACK implementations include:\\nAtlas (need hardware specific tuning by rebuilding on the target machine)\\nOpenBLAS\\nMKL\\nApple Accelerate and vecLib frameworks (OSX only)\\nMore information can be found on the NumPy install page\\nand in this\\nblog post\\nfrom Daniel Nouri which has some nice step by step install instructions for\\nDebian / Ubuntu.\\n8.2.3.2. Limiting Working Memory¶\\nSome calculations when implemented using standard numpy vectorized operations\\ninvolve using a large amount of temporary memory.  This may potentially exhaust\\nsystem memory.  Where computations can be performed in fixed-memory chunks, we\\nattempt to do so, and allow the user to hint at the maximum size of this\\nworking memory (defaulting to 1GB) using set_config or\\nconfig_context.  The following suggests to limit temporary working\\nmemory to 128 MiB:\\nimport sklearn\\nwith sklearn.config_context(working_memory=128):\\n...     pass  # do chunked work here\\nAn example of a chunked operation adhering to this setting is\\npairwise_distances_chunked, which facilitates computing\\nrow-wise reductions of a pairwise distance matrix.\\n8.2.3.3. Model Compression¶\\nModel compression in scikit-learn only concerns linear models for the moment.\\nIn this context it means that we want to control the model sparsity (i.e. the\\nnumber of non-zero coordinates in the model vectors). It is generally a good\\nidea to combine model sparsity with sparse input data representation.\\nHere is sample code that illustrates the use of the sparsify() method:\\nclf = SGDRegressor(penalty=\\'elasticnet\\', l1_ratio=0.25)\\nclf.fit(X_train, y_train).sparsify()\\nclf.predict(X_test)\\nIn this example we prefer the elasticnet penalty as it is often a good\\ncompromise between model compactness and prediction power. One can also\\nfurther tune the l1_ratio parameter (in combination with the\\nregularization strength alpha) to control this tradeoff.\\nA typical benchmark\\non synthetic data yields a >30% decrease in latency when both the model and\\ninput are sparse (with 0.000024 and 0.027400 non-zero coefficients ratio\\nrespectively). Your mileage may vary depending on the sparsity and size of\\nyour data and model.\\nFurthermore, sparsifying can be very useful to reduce the memory usage of\\npredictive models deployed on production servers.\\n8.2.3.4. Model Reshaping¶\\nModel reshaping consists in selecting only a portion of the available features\\nto fit a model. In other words, if a model discards features during the\\nlearning phase we can then strip those from the input. This has several\\nbenefits. Firstly it reduces memory (and therefore time) overhead of the\\nmodel itself. It also allows to discard explicit\\nfeature selection components in a pipeline once we know which features to\\nkeep from a previous run. Finally, it can help reduce processing time and I/O\\nusage upstream in the data access and feature extraction layers by not\\ncollecting and building features that are discarded by the model. For instance\\nif the raw data come from a database, it can make it possible to write simpler\\nand faster queries or reduce I/O usage by making the queries return lighter\\nrecords.\\nAt the moment, reshaping needs to be performed manually in scikit-learn.\\nIn the case of sparse input (particularly in CSR format), it is generally\\nsufficient to not generate the relevant features, leaving their columns empty.\\n8.2.3.5. Links¶\\nscikit-learn developer performance documentation\\nScipy sparse matrix formats documentation\\n\\n8.3. Parallelism, resource management, and configuration — scikit-learn 1.4.2 documentation\\n\\nInstall\\nUser Guide\\nAPI\\nExamples\\nCommunity\\nGetting Started\\nTutorial\\nWhat\\'s new\\nGlossary\\nDevelopment\\nFAQ\\nSupport\\nRelated packages\\nRoadmap\\nGovernance\\nAbout us\\nGitHub\\nOther Versions and Download\\nMore\\nGetting Started\\nTutorial\\nWhat\\'s new\\nGlossary\\nDevelopment\\nFAQ\\nSupport\\nRelated packages\\nRoadmap\\nGovernance\\nAbout us\\nGitHub\\nOther Versions and Download\\nToggle Menu\\nPrevUp\\nNext\\nscikit-learn 1.4.2\\nOther versions\\nPlease cite us if you use the software.\\nUser Guide\\n\\n1. Supervised learning\\n\\n2. Unsupervised learning\\n\\n3. Model selection and evaluation\\n\\n4. Inspection\\n\\n5. Visualizations\\n\\n6. Dataset transformations\\n\\n7. Dataset loading utilities\\n\\n8. Computing with scikit-learn\\n\\n8.1. Strategies to scale computationally: bigger data\\n\\n8.2. Computational Performance\\n\\n8.3. Parallelism, resource management, and configuration\\n\\n9. Model persistence\\n\\n10. Common pitfalls and recommended practices\\n\\n11. Dispatching\\n\\n8.3. Parallelism, resource management, and configuration¶\\n\\n8.3.1. Parallelism¶\\nSome scikit-learn estimators and utilities parallelize costly operations\\nusing multiple CPU cores.\\nDepending on the type of estimator and sometimes the values of the\\nconstructor parameters, this is either done:\\nwith higher-level parallelism via joblib.\\nwith lower-level parallelism via OpenMP, used in C or Cython code.\\nwith lower-level parallelism via BLAS, used by NumPy and SciPy for generic operations\\non arrays.\\nThe n_jobs parameters of estimators always controls the amount of parallelism\\nmanaged by joblib (processes or threads depending on the joblib backend).\\nThe thread-level parallelism managed by OpenMP in scikit-learn’s own Cython code\\nor by BLAS & LAPACK libraries used by NumPy and SciPy operations used in scikit-learn\\nis always controlled by environment variables or threadpoolctl as explained below.\\nNote that some estimators can leverage all three kinds of parallelism at different\\npoints of their training and prediction methods.\\nWe describe these 3 types of parallelism in the following subsections in more details.\\n8.3.1.1. Higher-level parallelism with joblib¶\\nWhen the underlying implementation uses joblib, the number of workers\\n(threads or processes) that are spawned in parallel can be controlled via the\\nn_jobs parameter.\\nNote\\nWhere (and how) parallelization happens in the estimators using joblib by\\nspecifying n_jobs is currently poorly documented.\\nPlease help us by improving our docs and tackle issue 14228!\\nJoblib is able to support both multi-processing and multi-threading. Whether\\njoblib chooses to spawn a thread or a process depends on the backend\\nthat it’s using.\\nscikit-learn generally relies on the loky backend, which is joblib’s\\ndefault backend. Loky is a multi-processing backend. When doing\\nmulti-processing, in order to avoid duplicating the memory in each process\\n(which isn’t reasonable with big datasets), joblib will create a memmap\\nthat all processes can share, when the data is bigger than 1MB.\\nIn some specific cases (when the code that is run in parallel releases the\\nGIL), scikit-learn will indicate to joblib that a multi-threading\\nbackend is preferable.\\nAs a user, you may control the backend that joblib will use (regardless of\\nwhat scikit-learn recommends) by using a context manager:\\nfrom joblib import parallel_backend\\nwith parallel_backend(\\'threading\\', n_jobs=2):\\n\\nYour scikit-learn code here\\n\\nPlease refer to the joblib’s docs\\nfor more details.\\nIn practice, whether parallelism is helpful at improving runtime depends on\\nmany factors. It is usually a good idea to experiment rather than assuming\\nthat increasing the number of workers is always a good thing. In some cases\\nit can be highly detrimental to performance to run multiple copies of some\\nestimators or functions in parallel (see oversubscription below).\\n8.3.1.2. Lower-level parallelism with OpenMP¶\\nOpenMP is used to parallelize code written in Cython or C, relying on\\nmulti-threading exclusively. By default, the implementations using OpenMP\\nwill use as many threads as possible, i.e. as many threads as logical cores.\\nYou can control the exact number of threads that are used either:\\nvia the OMP_NUM_THREADS environment variable, for instance when:\\nrunning a python script:\\nOMP_NUM_THREADS=4 python my_script.py\\nor via threadpoolctl as explained by this piece of documentation.\\n8.3.1.3. Parallel NumPy and SciPy routines from numerical libraries¶\\nscikit-learn relies heavily on NumPy and SciPy, which internally call\\nmulti-threaded linear algebra routines (BLAS & LAPACK) implemented in libraries\\nsuch as MKL, OpenBLAS or BLIS.\\nYou can control the exact number of threads used by BLAS for each library\\nusing environment variables, namely:\\nMKL_NUM_THREADS sets the number of thread MKL uses,\\nOPENBLAS_NUM_THREADS sets the number of threads OpenBLAS uses\\nBLIS_NUM_THREADS sets the number of threads BLIS uses\\nNote that BLAS & LAPACK implementations can also be impacted by\\nOMP_NUM_THREADS. To check whether this is the case in your environment,\\nyou can inspect how the number of threads effectively used by those libraries\\nis affected when running the following command in a bash or zsh terminal\\nfor different values of OMP_NUM_THREADS:\\nOMP_NUM_THREADS=2 python -m threadpoolctl -i numpy scipy\\nNote\\nAt the time of writing (2022), NumPy and SciPy packages which are\\ndistributed on pypi.org (i.e. the ones installed via pip install)\\nand on the conda-forge channel (i.e. the ones installed via\\nconda install --channel conda-forge) are linked with OpenBLAS, while\\nNumPy and SciPy packages packages shipped on the defaults conda\\nchannel from Anaconda.org (i.e. the ones installed via conda install)\\nare linked by default with MKL.\\n8.3.1.4. Oversubscription: spawning too many threads¶\\nIt is generally recommended to avoid using significantly more processes or\\nthreads than the number of CPUs on a machine. Over-subscription happens when\\na program is running too many threads at the same time.\\nSuppose you have a machine with 8 CPUs. Consider a case where you’re running\\na GridSearchCV (parallelized with joblib)\\nwith n_jobs=8 over a\\nHistGradientBoostingClassifier (parallelized with\\nOpenMP). Each instance of\\nHistGradientBoostingClassifier will spawn 8 threads\\n(since you have 8 CPUs). That’s a total of 8 * 8 = 64 threads, which\\nleads to oversubscription of threads for physical CPU resources and thus\\nto scheduling overhead.\\nOversubscription can arise in the exact same fashion with parallelized\\nroutines from MKL, OpenBLAS or BLIS that are nested in joblib calls.\\nStarting from joblib >= 0.14, when the loky backend is used (which\\nis the default), joblib will tell its child processes to limit the\\nnumber of threads they can use, so as to avoid oversubscription. In practice\\nthe heuristic that joblib uses is to tell the processes to use max_threads\\n= n_cpus // n_jobs, via their corresponding environment variable. Back to\\nour example from above, since the joblib backend of\\nGridSearchCV is loky, each process will\\nonly be able to use 1 thread instead of 8, thus mitigating the\\noversubscription issue.\\nNote that:\\nManually setting one of the environment variables (OMP_NUM_THREADS,\\nMKL_NUM_THREADS, OPENBLAS_NUM_THREADS, or BLIS_NUM_THREADS)\\nwill take precedence over what joblib tries to do. The total number of\\nthreads will be n_jobs * _NUM_THREADS. Note that setting this\\nlimit will also impact your computations in the main process, which will\\nonly use _NUM_THREADS. Joblib exposes a context manager for\\nfiner control over the number of threads in its workers (see joblib docs\\nlinked below).\\nWhen joblib is configured to use the threading backend, there is no\\nmechanism to avoid oversubscriptions when calling into parallel native\\nlibraries in the joblib-managed threads.\\nAll scikit-learn estimators that explicitly rely on OpenMP in their Cython code\\nalways use threadpoolctl internally to automatically adapt the numbers of\\nthreads used by OpenMP and potentially nested BLAS calls so as to avoid\\noversubscription.\\nYou will find additional details about joblib mitigation of oversubscription\\nin joblib documentation.\\nYou will find additional details about parallelism in numerical python libraries\\nin this document from Thomas J. Fan.\\n- 8.3.2. Configuration switches¶\\n8.3.2.1. Python API¶\\nsklearn.set_config and sklearn.config_context can be used to change\\nparameters of the configuration which control aspect of parallelism.\\n8.3.2.2. Environment variables¶\\nThese environment variables should be set before importing scikit-learn.\\n8.3.2.2.1. SKLEARN_ASSUME_FINITE¶\\nSets the default value for the assume_finite argument of\\nsklearn.set_config.\\n8.3.2.2.2. SKLEARN_WORKING_MEMORY¶\\nSets the default value for the working_memory argument of\\nsklearn.set_config.\\n8.3.2.2.3. SKLEARN_SEED¶\\nSets the seed of the global random generator when running the tests, for\\nreproducibility.\\nNote that scikit-learn tests are expected to run deterministically with\\nexplicit seeding of their own independent RNG instances instead of relying on\\nthe numpy or Python standard library RNG singletons to make sure that test\\nresults are independent of the test execution order. However some tests might\\nforget to use explicit seeding and this variable is a way to control the initial\\nstate of the aforementioned singletons.\\n8.3.2.2.4. SKLEARN_TESTS_GLOBAL_RANDOM_SEED¶\\nControls the seeding of the random number generator used in tests that rely on\\nthe global_random_seed` fixture.\\nAll tests that use this fixture accept the contract that they should\\ndeterministically pass for any seed value from 0 to 99 included.\\nIf the SKLEARN_TESTS_GLOBAL_RANDOM_SEED environment variable is set to\\n\"any\" (which should be the case on nightly builds on the CI), the fixture\\nwill choose an arbitrary seed in the above range (based on the BUILD_NUMBER or\\nthe current day) and all fixtured tests will run for that specific seed. The\\ngoal is to ensure that, over time, our CI will run all tests with different\\nseeds while keeping the test duration of a single run of the full test suite\\nlimited. This will check that the assertions of tests written to use this\\nfixture are not dependent on a specific seed value.\\nThe range of admissible seed values is limited to [0, 99] because it is often\\nnot possible to write a test that can work for any possible seed and we want to\\navoid having tests that randomly fail on the CI.\\nValid values for SKLEARN_TESTS_GLOBAL_RANDOM_SEED:\\nSKLEARN_TESTS_GLOBAL_RANDOM_SEED=\"42\": run tests with a fixed seed of 42\\nSKLEARN_TESTS_GLOBAL_RANDOM_SEED=\"40-42\": run the tests with all seeds\\nbetween 40 and 42 included\\nSKLEARN_TESTS_GLOBAL_RANDOM_SEED=\"any\": run the tests with an arbitrary\\nseed selected between 0 and 99 included\\nSKLEARN_TESTS_GLOBAL_RANDOM_SEED=\"all\": run the tests with all seeds\\nbetween 0 and 99 included. This can take a long time: only use for individual\\ntests, not the full test suite!\\nIf the variable is not set, then 42 is used as the global seed in a\\ndeterministic manner. This ensures that, by default, the scikit-learn test\\nsuite is as deterministic as possible to avoid disrupting our friendly\\nthird-party package maintainers. Similarly, this variable should not be set in\\nthe CI config of pull-requests to make sure that our friendly contributors are\\nnot the first people to encounter a seed-sensitivity regression in a test\\nunrelated to the changes of their own PR. Only the scikit-learn maintainers who\\nwatch the results of the nightly builds are expected to be annoyed by this.\\nWhen writing a new test function that uses this fixture, please use the\\nfollowing command to make sure that it passes deterministically for all\\nadmissible seeds on your local machine:\\nSKLEARN_TESTS_GLOBAL_RANDOM_SEED=\"all\" pytest -v -k test_your_test_name\\n8.3.2.2.5. SKLEARN_SKIP_NETWORK_TESTS¶\\nWhen this environment variable is set to a non zero value, the tests that need\\nnetwork access are skipped. When this environment variable is not set then\\nnetwork tests are skipped.\\n8.3.2.2.6. SKLEARN_RUN_FLOAT32_TESTS¶\\nWhen this environment variable is set to ‘1’, the tests using the\\nglobal_dtype fixture are also run on float32 data.\\nWhen this environment variable is not set, the tests are only run on\\nfloat64 data.\\n8.3.2.2.7. SKLEARN_ENABLE_DEBUG_CYTHON_DIRECTIVES¶\\nWhen this environment variable is set to a non zero value, the Cython\\nderivative, boundscheck is set to True. This is useful for finding\\nsegfaults.\\n8.3.2.2.8. SKLEARN_BUILD_ENABLE_DEBUG_SYMBOLS¶\\nWhen this environment variable is set to a non zero value, the debug symbols\\nwill be included in the compiled C extensions. Only debug symbols for POSIX\\nsystems is configured.\\n8.3.2.2.9. SKLEARN_PAIRWISE_DIST_CHUNK_SIZE¶\\nThis sets the size of chunk to be used by the underlying PairwiseDistancesReductions\\nimplementations. The default value is 256 which has been showed to be adequate on\\nmost machines.\\nUsers looking for the best performance might want to tune this variable using\\npowers of 2 so as to get the best parallelism behavior for their hardware,\\nespecially with respect to their caches’ sizes.\\n8.3.2.2.10. SKLEARN_WARNINGS_AS_ERRORS¶\\nThis environment variable is used to turn warnings into errors in tests and\\ndocumentation build.\\nSome CI (Continuous Integration) builds set SKLEARN_WARNINGS_AS_ERRORS=1, for\\nexample to make sure that we catch deprecation warnings from our dependencies\\nand that we adapt our code.\\nTo locally run with the same “warnings as errors” setting as in these CI builds\\nyou can set SKLEARN_WARNINGS_AS_ERRORS=1.\\nBy default, warnings are not turned into errors. This is the case if\\nSKLEARN_WARNINGS_AS_ERRORS is unset, or SKLEARN_WARNINGS_AS_ERRORS=0.\\nThis environment variable use specific warning filters to ignore some warnings,\\nsince sometimes warnings originate from third-party libraries and there is not\\nmuch we can do about it. You can see the warning filters in the\\n_get_warnings_filters_info_list function in sklearn/utils/_testing.py.\\nNote that for documentation build, SKLEARN_WARNING_AS_ERRORS=1 is checking\\nthat the documentation build, in particular running examples, does not produce\\nany warnings. This is different from the -W sphinx-build argument that\\ncatches syntax warnings in the rst files.\\n\\n9. Model persistence — scikit-learn 1.4.2 documentation\\n\\nInstall\\nUser Guide\\nAPI\\nExamples\\nCommunity\\nGetting Started\\nTutorial\\nWhat\\'s new\\nGlossary\\nDevelopment\\nFAQ\\nSupport\\nRelated packages\\nRoadmap\\nGovernance\\nAbout us\\nGitHub\\nOther Versions and Download\\nMore\\nGetting Started\\nTutorial\\nWhat\\'s new\\nGlossary\\nDevelopment\\nFAQ\\nSupport\\nRelated packages\\nRoadmap\\nGovernance\\nAbout us\\nGitHub\\nOther Versions and Download\\nToggle Menu\\nPrevUp\\nNext\\nscikit-learn 1.4.2\\nOther versions\\nPlease cite us if you use the software.\\nUser Guide\\n\\n1. Supervised learning\\n\\n2. Unsupervised learning\\n\\n3. Model selection and evaluation\\n\\n4. Inspection\\n\\n5. Visualizations\\n\\n6. Dataset transformations\\n\\n7. Dataset loading utilities\\n\\n8. Computing with scikit-learn\\n\\n9. Model persistence\\n\\n10. Common pitfalls and recommended practices\\n\\n11. Dispatching\\n\\n9. Model persistence¶\\n\\nAfter training a scikit-learn model, it is desirable to have a way to persist\\nthe model for future use without having to retrain. The following sections give\\nyou some hints on how to persist a scikit-learn model.\\n\\n9.1. Python specific serialization¶\\n\\nIt is possible to save a model in scikit-learn by using Python’s built-in\\npersistence model, namely pickle:\\n\\nfrom sklearn import svm\\nfrom sklearn import datasets\\nclf = svm.SVC()\\nX, y= datasets.load_iris(return_X_y=True)\\nclf.fit(X, y)\\nSVC()\\nimport pickle\\ns = pickle.dumps(clf)\\nclf2 = pickle.loads(s)\\nclf2.predict(X[0:1])\\narray([0])\\ny[0]\\n0\\nIn the specific case of scikit-learn, it may be better to use joblib’s\\nreplacement of pickle (dump & load), which is more efficient on\\nobjects that carry large numpy arrays internally as is often the case for\\nfitted scikit-learn estimators, but can only pickle to the disk and not to a\\nstring:\\nfrom joblib import dump, load\\ndump(clf, \\'filename.joblib\\')\\nLater you can load back the pickled model (possibly in another Python process)\\nwith:\\nclf = load(\\'filename.joblib\\')\\nNote\\ndump and load functions also accept file-like object\\ninstead of filenames. More information on data persistence with Joblib is\\navailable here.\\nInconsistentVersionWarning\\nClick for more details\\n¶\\nWhen an estimator is unpickled with a scikit-learn version that is inconsistent\\nwith the version the estimator was pickled with, a\\nInconsistentVersionWarning is raised. This warning\\ncan be caught to obtain the original version the estimator was pickled with:\\nfrom sklearn.exceptions import InconsistentVersionWarning\\nwarnings.simplefilter(\"error\", InconsistentVersionWarning)\\ntry:\\nest = pickle.loads(\"model_from_prevision_version.pickle\")\\nexcept InconsistentVersionWarning as w:\\nprint(w.original_sklearn_version)\\n- 9.1.1. Security & maintainability limitations¶\\npickle (and joblib by extension), has some issues regarding maintainability\\nand security. Because of this,\\nNever unpickle untrusted data as it could lead to malicious code being\\nexecuted upon loading.\\nWhile models saved using one version of scikit-learn might load in\\nother versions, this is entirely unsupported and inadvisable. It should\\nalso be kept in mind that operations performed on such data could give\\ndifferent and unexpected results.\\nIn order to rebuild a similar model with future versions of scikit-learn,\\nadditional metadata should be saved along the pickled model:\\nThe training data, e.g. a reference to an immutable snapshot\\nThe python source code used to generate the model\\nThe versions of scikit-learn and its dependencies\\nThe cross validation score obtained on the training data\\nThis should make it possible to check that the cross-validation score is in the\\nsame range as before.\\nAside for a few exceptions, pickled models should be portable across\\narchitectures assuming the same versions of dependencies and Python are used.\\nIf you encounter an estimator that is not portable please open an issue on\\nGitHub. Pickled models are often deployed in production using containers, like\\nDocker, in order to freeze the environment and dependencies.\\nIf you want to know more about these issues and explore other possible\\nserialization methods, please refer to this\\ntalk by Alex Gaynor.\\n- 9.1.2. A more secure format: skops¶\\nskops provides a more secure\\nformat via the skops.io module. It avoids using pickle and only\\nloads files which have types and references to functions which are trusted\\neither by default or by the user.\\nUsing skops\\nClick for more details\\n¶\\nThe API is very similar to pickle, and\\nyou can persist your models as explain in the docs using\\nskops.io.dump and skops.io.dumps:\\nimport skops.io as sio\\nobj = sio.dumps(clf)\\nAnd you can load them back using skops.io.load and\\nskops.io.loads. However, you need to specify the types which are\\ntrusted by you. You can get existing unknown types in a dumped object / file\\nusing skops.io.get_untrusted_types, and after checking its contents,\\npass it to the load function:\\nunknown_types = sio.get_untrusted_types(data=obj)\\nclf = sio.loads(obj, trusted=unknown_types)\\nIf you trust the source of the file / object, you can pass trusted=True:\\nclf = sio.loads(obj, trusted=True)\\nPlease report issues and feature requests related to this format on the skops\\nissue tracker.\\n\\n9.2. Interoperable formats¶\\n\\nFor reproducibility and quality control needs, when different architectures\\nand environments should be taken into account, exporting the model in\\nOpen Neural Network\\nExchange format or Predictive Model Markup Language\\n(PMML) format\\nmight be a better approach than using pickle alone.\\nThese are helpful where you may want to use your model for prediction in a\\ndifferent environment from where the model was trained.\\nONNX is a binary serialization of the model. It has been developed to improve\\nthe usability of the interoperable representation of data models.\\nIt aims to facilitate the conversion of the data\\nmodels between different machine learning frameworks, and to improve their\\nportability on different computing architectures. More details are available\\nfrom the ONNX tutorial.\\nTo convert scikit-learn model to ONNX a specific tool sklearn-onnx has been developed.\\nPMML is an implementation of the XML document standard\\ndefined to represent data models together with the data used to generate them.\\nBeing human and machine readable,\\nPMML is a good option for model validation on different platforms and\\nlong term archiving. On the other hand, as XML in general, its verbosity does\\nnot help in production when performance is critical.\\nTo convert scikit-learn model to PMML you can use for example sklearn2pmml distributed under the Affero GPLv3\\nlicense.\\n\\n9. Model persistence — scikit-learn 1.4.2 documentation\\n\\nInstall\\nUser Guide\\nAPI\\nExamples\\nCommunity\\nGetting Started\\nTutorial\\nWhat\\'s new\\nGlossary\\nDevelopment\\nFAQ\\nSupport\\nRelated packages\\nRoadmap\\nGovernance\\nAbout us\\nGitHub\\nOther Versions and Download\\nMore\\nGetting Started\\nTutorial\\nWhat\\'s new\\nGlossary\\nDevelopment\\nFAQ\\nSupport\\nRelated packages\\nRoadmap\\nGovernance\\nAbout us\\nGitHub\\nOther Versions and Download\\nToggle Menu\\nPrevUp\\nNext\\nscikit-learn 1.4.2\\nOther versions\\nPlease cite us if you use the software.\\nUser Guide\\n\\n1. Supervised learning\\n\\n2. Unsupervised learning\\n\\n3. Model selection and evaluation\\n\\n4. Inspection\\n\\n5. Visualizations\\n\\n6. Dataset transformations\\n\\n7. Dataset loading utilities\\n\\n8. Computing with scikit-learn\\n\\n9. Model persistence\\n\\n10. Common pitfalls and recommended practices\\n\\n11. Dispatching\\n\\n9. Model persistence¶\\n\\nAfter training a scikit-learn model, it is desirable to have a way to persist\\nthe model for future use without having to retrain. The following sections give\\nyou some hints on how to persist a scikit-learn model.\\n\\n9.1. Python specific serialization¶\\n\\nIt is possible to save a model in scikit-learn by using Python’s built-in\\npersistence model, namely pickle:\\n\\nfrom sklearn import svm\\nfrom sklearn import datasets\\nclf = svm.SVC()\\nX, y= datasets.load_iris(return_X_y=True)\\nclf.fit(X, y)\\nSVC()\\nimport pickle\\ns = pickle.dumps(clf)\\nclf2 = pickle.loads(s)\\nclf2.predict(X[0:1])\\narray([0])\\ny[0]\\n0\\nIn the specific case of scikit-learn, it may be better to use joblib’s\\nreplacement of pickle (dump & load), which is more efficient on\\nobjects that carry large numpy arrays internally as is often the case for\\nfitted scikit-learn estimators, but can only pickle to the disk and not to a\\nstring:\\nfrom joblib import dump, load\\ndump(clf, \\'filename.joblib\\')\\nLater you can load back the pickled model (possibly in another Python process)\\nwith:\\nclf = load(\\'filename.joblib\\')\\nNote\\ndump and load functions also accept file-like object\\ninstead of filenames. More information on data persistence with Joblib is\\navailable here.\\nInconsistentVersionWarning\\nClick for more details\\n¶\\nWhen an estimator is unpickled with a scikit-learn version that is inconsistent\\nwith the version the estimator was pickled with, a\\nInconsistentVersionWarning is raised. This warning\\ncan be caught to obtain the original version the estimator was pickled with:\\nfrom sklearn.exceptions import InconsistentVersionWarning\\nwarnings.simplefilter(\"error\", InconsistentVersionWarning)\\ntry:\\nest = pickle.loads(\"model_from_prevision_version.pickle\")\\nexcept InconsistentVersionWarning as w:\\nprint(w.original_sklearn_version)\\n- 9.1.1. Security & maintainability limitations¶\\npickle (and joblib by extension), has some issues regarding maintainability\\nand security. Because of this,\\nNever unpickle untrusted data as it could lead to malicious code being\\nexecuted upon loading.\\nWhile models saved using one version of scikit-learn might load in\\nother versions, this is entirely unsupported and inadvisable. It should\\nalso be kept in mind that operations performed on such data could give\\ndifferent and unexpected results.\\nIn order to rebuild a similar model with future versions of scikit-learn,\\nadditional metadata should be saved along the pickled model:\\nThe training data, e.g. a reference to an immutable snapshot\\nThe python source code used to generate the model\\nThe versions of scikit-learn and its dependencies\\nThe cross validation score obtained on the training data\\nThis should make it possible to check that the cross-validation score is in the\\nsame range as before.\\nAside for a few exceptions, pickled models should be portable across\\narchitectures assuming the same versions of dependencies and Python are used.\\nIf you encounter an estimator that is not portable please open an issue on\\nGitHub. Pickled models are often deployed in production using containers, like\\nDocker, in order to freeze the environment and dependencies.\\nIf you want to know more about these issues and explore other possible\\nserialization methods, please refer to this\\ntalk by Alex Gaynor.\\n- 9.1.2. A more secure format: skops¶\\nskops provides a more secure\\nformat via the skops.io module. It avoids using pickle and only\\nloads files which have types and references to functions which are trusted\\neither by default or by the user.\\nUsing skops\\nClick for more details\\n¶\\nThe API is very similar to pickle, and\\nyou can persist your models as explain in the docs using\\nskops.io.dump and skops.io.dumps:\\nimport skops.io as sio\\nobj = sio.dumps(clf)\\nAnd you can load them back using skops.io.load and\\nskops.io.loads. However, you need to specify the types which are\\ntrusted by you. You can get existing unknown types in a dumped object / file\\nusing skops.io.get_untrusted_types, and after checking its contents,\\npass it to the load function:\\nunknown_types = sio.get_untrusted_types(data=obj)\\nclf = sio.loads(obj, trusted=unknown_types)\\nIf you trust the source of the file / object, you can pass trusted=True:\\nclf = sio.loads(obj, trusted=True)\\nPlease report issues and feature requests related to this format on the skops\\nissue tracker.\\n\\n9.2. Interoperable formats¶\\n\\nFor reproducibility and quality control needs, when different architectures\\nand environments should be taken into account, exporting the model in\\nOpen Neural Network\\nExchange format or Predictive Model Markup Language\\n(PMML) format\\nmight be a better approach than using pickle alone.\\nThese are helpful where you may want to use your model for prediction in a\\ndifferent environment from where the model was trained.\\nONNX is a binary serialization of the model. It has been developed to improve\\nthe usability of the interoperable representation of data models.\\nIt aims to facilitate the conversion of the data\\nmodels between different machine learning frameworks, and to improve their\\nportability on different computing architectures. More details are available\\nfrom the ONNX tutorial.\\nTo convert scikit-learn model to ONNX a specific tool sklearn-onnx has been developed.\\nPMML is an implementation of the XML document standard\\ndefined to represent data models together with the data used to generate them.\\nBeing human and machine readable,\\nPMML is a good option for model validation on different platforms and\\nlong term archiving. On the other hand, as XML in general, its verbosity does\\nnot help in production when performance is critical.\\nTo convert scikit-learn model to PMML you can use for example sklearn2pmml distributed under the Affero GPLv3\\nlicense.\\n\\n1. Metadata Routing — scikit-learn 1.4.2 documentation\\n\\nInstall\\nUser Guide\\nAPI\\nExamples\\nCommunity\\nGetting Started\\nTutorial\\nWhat\\'s new\\nGlossary\\nDevelopment\\nFAQ\\nSupport\\nRelated packages\\nRoadmap\\nGovernance\\nAbout us\\nGitHub\\nOther Versions and Download\\nMore\\nGetting Started\\nTutorial\\nWhat\\'s new\\nGlossary\\nDevelopment\\nFAQ\\nSupport\\nRelated packages\\nRoadmap\\nGovernance\\nAbout us\\nGitHub\\nOther Versions and Download\\nToggle Menu\\nPrevUp\\nNext\\nscikit-learn 1.4.2\\nOther versions\\nPlease cite us if you use the software.\\n\\n1. Metadata Routing\\n\\n1.1. Usage Examples\\n\\n1.1.1. Weighted scoring and fitting\\n\\n1.1.2. Weighted scoring and unweighted fitting\\n\\n1.1.3. Unweighted feature selection\\n\\n1.1.4. Advanced: Different scoring and fitting weights\\n\\n1.2. API Interface\\n\\n1.3. Metadata Routing Support Status\\n\\n1. Metadata Routing¶\\n\\nNote\\nThe Metadata Routing API is experimental, and is not implemented yet for many\\nestimators. Please refer to the list of supported and unsupported\\nmodels for more information. It may change without\\nthe usual deprecation cycle. By default this feature is not enabled. You can\\nenable this feature  by setting the enable_metadata_routing flag to\\nTrue:\\n\\nimport sklearn\\nsklearn.set_config(enable_metadata_routing=True)\\nThis guide demonstrates how metadata such as sample_weight can be routed\\nand passed along to estimators, scorers, and CV splitters through\\nmeta-estimators such as Pipeline and\\nGridSearchCV. In order to pass metadata to a method\\nsuch as fit or score, the object consuming the metadata, must request\\nit. For estimators and splitters, this is done via set_*_request methods,\\ne.g. set_fit_request(...), and for scorers this is done via the\\nset_score_request method. For grouped splitters such as\\nGroupKFold, a groups parameter is requested by\\ndefault. This is best demonstrated by the following examples.\\nIf you are developing a scikit-learn compatible estimator or meta-estimator,\\nyou can check our related developer guide:\\nMetadata Routing.\\nNote\\nNote that the methods and requirements introduced in this document are only\\nrelevant if you want to pass metadata (e.g. sample_weight) to a method.\\nIf you’re only passing X and y and no other parameter / metadata to\\nmethods such as fit, transform, etc, then you don’t need to set\\nanything.\\n\\n1.1. Usage Examples¶\\n\\nHere we present a few examples to show different common use-cases. The examples\\nin this section require the following imports and data:\\n\\nimport numpy as np\\nfrom sklearn.metrics import make_scorer, accuracy_score\\nfrom sklearn.linear_model import LogisticRegressionCV, LogisticRegression\\nfrom sklearn.model_selection import cross_validate, GridSearchCV, GroupKFold\\nfrom sklearn.feature_selection import SelectKBest\\nfrom sklearn.pipeline import make_pipeline\\nn_samples, n_features = 100, 4\\nrng = np.random.RandomState(42)\\nX = rng.rand(n_samples, n_features)\\ny = rng.randint(0, 2, size=n_samples)\\nmy_groups = rng.randint(0, 10, size=n_samples)\\nmy_weights = rng.rand(n_samples)\\nmy_other_weights = rng.rand(n_samples)\\n- 1.1.1. Weighted scoring and fitting¶\\nHere GroupKFold requests groups by default. However, we\\nneed to explicitly request weights for our scorer and the internal cross validation of\\nLogisticRegressionCV. Both of these consumers know how to use\\nmetadata called sample_weight:\\nweighted_acc = make_scorer(accuracy_score).set_score_request(\\n...     sample_weight=True\\n... )\\nlr = LogisticRegressionCV(\\n...     cv=GroupKFold(), scoring=weighted_acc,\\n... ).set_fit_request(sample_weight=True)\\ncv_results = cross_validate(\\n...     lr,\\n...     X,\\n...     y,\\n...     params={\"sample_weight\": my_weights, \"groups\": my_groups},\\n...     cv=GroupKFold(),\\n...     scoring=weighted_acc,\\n... )\\nNote that in this example, my_weights is passed to both the scorer and\\nLogisticRegressionCV.\\nError handling: if params={\"sample_weigh\": my_weights, ...} were passed\\n(note the typo), cross_validate would raise an error,\\nsince sample_weigh was not requested by any of its underlying objects.\\n- 1.1.2. Weighted scoring and unweighted fitting¶\\nWhen passing metadata such as sample_weight around, all sample_weight\\nconsumers require weights to be either explicitly requested\\nor not requested (i.e. True or False) when used in another\\nrouter such as a Pipeline or a *GridSearchCV. To\\nperform an unweighted fit, we need to configure\\nLogisticRegressionCV to not request sample weights, so\\nthat cross_validate does not pass the weights along:\\nweighted_acc = make_scorer(accuracy_score).set_score_request(\\n...     sample_weight=True\\n... )\\nlr = LogisticRegressionCV(\\n...     cv=GroupKFold(), scoring=weighted_acc,\\n... ).set_fit_request(sample_weight=False)\\ncv_results = cross_validate(\\n...     lr,\\n...     X,\\n...     y,\\n...     cv=GroupKFold(),\\n...     params={\"sample_weight\": my_weights, \"groups\": my_groups},\\n...     scoring=weighted_acc,\\n... )\\nIf linear_model.LogisticRegressionCV.set_fit_request has not\\nbeen called, cross_validate will raise an\\nerror because sample_weight is passed in but\\nLogisticRegressionCV would not be explicitly configured\\nto recognize the weights.\\n- 1.1.3. Unweighted feature selection¶\\nSetting request values for metadata are only required if the object, e.g. estimator,\\nscorer, etc., is a consumer of that metadata Unlike\\nLogisticRegressionCV, SelectKBest\\ndoesn’t consume weights and therefore no request value for sample_weight on its\\ninstance is set and sample_weight is not routed to it:\\nweighted_acc = make_scorer(accuracy_score).set_score_request(\\n...     sample_weight=True\\n... )\\nlr = LogisticRegressionCV(\\n...     cv=GroupKFold(), scoring=weighted_acc,\\n... ).set_fit_request(sample_weight=True)\\nsel = SelectKBest(k=2)\\npipe = make_pipeline(sel, lr)\\ncv_results = cross_validate(\\n...     pipe,\\n...     X,\\n...     y,\\n...     cv=GroupKFold(),\\n...     params={\"sample_weight\": my_weights, \"groups\": my_groups},\\n...     scoring=weighted_acc,\\n... )\\n- 1.1.4. Advanced: Different scoring and fitting weights¶\\nDespite make_scorer and\\nLogisticRegressionCV both expecting the key\\nsample_weight, we can use aliases to pass different weights to different\\nconsumers. In this example, we pass scoring_weight to the scorer, and\\nfitting_weight to LogisticRegressionCV:\\nweighted_acc = make_scorer(accuracy_score).set_score_request(\\n...    sample_weight=\"scoring_weight\"\\n... )\\nlr = LogisticRegressionCV(\\n...     cv=GroupKFold(), scoring=weighted_acc,\\n... ).set_fit_request(sample_weight=\"fitting_weight\")\\ncv_results = cross_validate(\\n...     lr,\\n...     X,\\n...     y,\\n...     cv=GroupKFold(),\\n...     params={\\n...         \"scoring_weight\": my_weights,\\n...         \"fitting_weight\": my_other_weights,\\n...         \"groups\": my_groups,\\n...     },\\n...     scoring=weighted_acc,\\n... )\\n\\n1.2. API Interface¶\\n\\nA consumer is an object (estimator, meta-estimator, scorer, splitter)\\nwhich accepts and uses some metadata in at least one of its methods\\n(fit, predict, inverse_transform, transform, score,\\nsplit). Meta-estimators which only forward the metadata to other objects\\n(the child estimator, scorers, or splitters) and don’t use the metadata\\nthemselves are not consumers. (Meta-)Estimators which route metadata to other\\nobjects are routers. A(n) (meta-)estimator can be a\\nconsumer and a router at the same time. (Meta-)Estimators and\\nsplitters expose a set__request method for each method which accepts at\\nleast one metadata. For instance, if an estimator supports sample_weight in\\nfit and score, it exposes\\nestimator.set_fit_request(sample_weight=value) and\\nestimator.set_score_request(sample_weight=value). Here value can be:\\nTrue: method requests a sample_weight. This means if the metadata is\\nprovided, it will be used, otherwise no error is raised.\\nFalse: method does not request a sample_weight.\\nNone: router will raise an error if sample_weight is passed. This is\\nin almost all cases the default value when an object is instantiated and\\nensures the user sets the metadata requests explicitly when a metadata is\\npassed. The only exception are GroupFold splitters.\\n\"param_name\": if this estimator is used in a meta-estimator, the\\nmeta-estimator should forward \"param_name\" as sample_weight to this\\nestimator. This means the mapping between the metadata required by the\\nobject, e.g. sample_weight and what is provided by the user, e.g.\\nmy_weights is done at the router level, and not by the object, e.g.\\nestimator, itself.\\nMetadata are requested in the same way for scorers using set_score_request.\\nIf a metadata, e.g. sample_weight, is passed by the user, the metadata\\nrequest for all objects which potentially can consume sample_weight should\\nbe set by the user, otherwise an error is raised by the router object. For\\nexample, the following code raises an error, since it hasn’t been explicitly\\nspecified whether sample_weight should be passed to the estimator’s scorer\\nor not:\\n\\nparam_grid = {\"C\": [0.1, 1]}\\nlr = LogisticRegression().set_fit_request(sample_weight=True)\\ntry:\\n...     GridSearchCV(\\n...         estimator=lr, param_grid=param_grid\\n...     ).fit(X, y, sample_weight=my_weights)\\n... except ValueError as e:\\n...     print(e)\\n[sample_weight] are passed but are not explicitly set as requested or not for\\nLogisticRegression.score\\nThe issue can be fixed by explicitly setting the request value:\\nlr = LogisticRegression().set_fit_request(\\n...     sample_weight=True\\n... ).set_score_request(sample_weight=False)\\nAt the end we disable the configuration flag for metadata routing:\\nsklearn.set_config(enable_metadata_routing=False)\\n\\n1.3. Metadata Routing Support Status¶\\n\\nAll consumers (i.e. simple estimators which only consume metadata and don’t\\nroute them) support metadata routing, meaning they can be used inside\\nmeta-estimators which support metadata routing. However, development of support\\nfor metadata routing for meta-estimators is in progress, and here is a list of\\nmeta-estimators and tools which support and don’t yet support metadata routing.\\nMeta-estimators and functions supporting metadata routing:\\nsklearn.calibration.CalibratedClassifierCV\\nsklearn.compose.ColumnTransformer\\nsklearn.feature_selection.SelectFromModel\\nsklearn.linear_model.ElasticNetCV\\nsklearn.linear_model.LarsCV\\nsklearn.linear_model.LassoCV\\nsklearn.linear_model.LassoLarsCV\\nsklearn.linear_model.LogisticRegressionCV\\nsklearn.linear_model.MultiTaskElasticNetCV\\nsklearn.linear_model.MultiTaskLassoCV\\nsklearn.model_selection.GridSearchCV\\nsklearn.model_selection.HalvingGridSearchCV\\nsklearn.model_selection.HalvingRandomSearchCV\\nsklearn.model_selection.RandomizedSearchCV\\nsklearn.model_selection.cross_validate\\nsklearn.model_selection.cross_val_score\\nsklearn.model_selection.cross_val_predict\\nsklearn.multiclass.OneVsOneClassifier\\nsklearn.multiclass.OneVsRestClassifier\\nsklearn.multiclass.OutputCodeClassifier\\nsklearn.multioutput.ClassifierChain\\nsklearn.multioutput.MultiOutputClassifier\\nsklearn.multioutput.MultiOutputRegressor\\nsklearn.linear_model.OrthogonalMatchingPursuitCV\\nsklearn.multioutput.RegressorChain\\nsklearn.pipeline.Pipeline\\nMeta-estimators and tools not supporting metadata routing yet:\\nsklearn.compose.TransformedTargetRegressor\\nsklearn.covariance.GraphicalLassoCV\\nsklearn.ensemble.AdaBoostClassifier\\nsklearn.ensemble.AdaBoostRegressor\\nsklearn.ensemble.BaggingClassifier\\nsklearn.ensemble.BaggingRegressor\\nsklearn.ensemble.StackingClassifier\\nsklearn.ensemble.StackingRegressor\\nsklearn.ensemble.VotingClassifier\\nsklearn.ensemble.VotingRegressor\\nsklearn.feature_selection.RFE\\nsklearn.feature_selection.RFECV\\nsklearn.feature_selection.SequentialFeatureSelector\\nsklearn.impute.IterativeImputer\\nsklearn.linear_model.RANSACRegressor\\nsklearn.linear_model.RidgeClassifierCV\\nsklearn.linear_model.RidgeCV\\nsklearn.model_selection.learning_curve\\nsklearn.model_selection.permutation_test_score\\nsklearn.model_selection.validation_curve\\nsklearn.pipeline.FeatureUnion\\nsklearn.semi_supervised.SelfTrainingClassifier\\n© 2007 - 2024, scikit-learn developers (BSD License).\\nShow this page source', metadata={'source': '/content/sklearn_user_guide.md'})]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Splitting & chunking**"
      ],
      "metadata": {
        "id": "6xWJFbrszMwH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "splits = text_splitter.split_documents(docs)"
      ],
      "metadata": {
        "id": "uNHf8OgZx8US"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Vector Embedding: HuggingFaceEmbeddings**"
      ],
      "metadata": {
        "id": "X_O0PY_e0FBi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "\n",
        "# create an instance of embedding to use later while creating vectorstore\n",
        "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "# or\n",
        "# embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-V2\")"
      ],
      "metadata": {
        "id": "nABBXKRH0R5J",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 571,
          "referenced_widgets": [
            "9422852399274b0ab3f88afc09191ab0",
            "045096e350284c388be71245942d41ca",
            "33f10b1ac46e44a4b8378cf978c2236d",
            "5c9ae65d92694bf29180c083d31e5a86",
            "8cf7aa130e8b4c4c80e354e948916cca",
            "5f2b30a5d8f04f9faa47393089573690",
            "1e2f12b3e87b4c5e9d313b7243dc1ed6",
            "a5661a1dd2214d47b666796534571305",
            "11b3603c6d4a49908e4eca9ee195db76",
            "df130ab37c9d4a44b82f299010aba689",
            "7e69d70ef12742358dd9dd913c12759c",
            "4759a41091b3482e8f658290d879d2b1",
            "70a9548f2b504dc0bf865c117576a420",
            "01815fd0763c471a943e99703fc179ff",
            "ba045ec626134d69a7b1d0f55c66032e",
            "557abb5bb1394efe8556b9baaa468564",
            "fc47e61786714ef2bcdad497f4d8c804",
            "49036fc5d39e45e59aa9d89cca64244d",
            "ea3bd93d49b24523b9aa6504d2e17e65",
            "32704d828ed54d7ba12f51275d64146f",
            "d65131df864a4a318b829541a88e0234",
            "e8621e6ef355424ca23e78e8bbc7078c",
            "7a4abc76695c411d9f608d1ce57c878f",
            "a598cd188aff4e1da09238269ab9af3a",
            "8416306021fd4116b5348afffa1e42a5",
            "14a73de9dfcd4c19a21889df03142e29",
            "2b6ae5e793d44a539e641321319e963e",
            "830513963e294d43880736f89f035c55",
            "787caffddbf043efa19e28e20b5583e8",
            "59c1658884594a4ea34e73d1930dbfd0",
            "a8aef2cf48fe4867a6fc976d6291a244",
            "e0869e6d8a67450f9fbb508f0e3fe853",
            "c91a86233e1f410d8b38da1f9af8d866",
            "ff3def560e034184850d354ae0799feb",
            "71d5bf03568146b7ac9d4eae820e6df6",
            "cf7e80d4cab94249bd10958d7ae0399b",
            "026ae44220a643d0aa2cde1264dee832",
            "37f00bfb116a4c7e9d4ae45b1d1b36dc",
            "a0a8b60b43bd4d82828e0606644b34a1",
            "2df34c9b387a47529d940540222036a7",
            "153a35e905174de887c869b394065649",
            "3a678dfdb2e0438ba10148f0a244493e",
            "143cd328d10a4662a8cf31be11c0bbc8",
            "66c449a1f3c34852847f32167e46285b",
            "5ff989c9349b4ad3a2e3db9e6483c8d0",
            "734a2e96ab6f4883974bd7ca7a7d17f6",
            "94cf1170c0f64755947be9b99fb87bb8",
            "7ff7bc1486aa486284fce7b20a1e0b8d",
            "153b05f1621445b7904bd0de6156f886",
            "fbbeef11473b4306a5751326a15ae3e1",
            "31c7116aaf58461f8381dd6d59d7a7fc",
            "79e6f6690d3f480084d70a90f9ed13a9",
            "378c23a271b240e1a0e115eb9a2f8865",
            "4a6f0c76eef744b3844993b6599d780a",
            "98c018357dd74ed0b8046da1bde1c206",
            "d928c5dbf1c64b838602c5aff0b43f3e",
            "a51e4c50b0b4491984ea5b25fbc06a2e",
            "5e435c2cab2849c097d9d22dc7933330",
            "8a1d173c73914106b737fda2cbf00c7c",
            "d3efb3de822c428b9c8d491b865c3a2f",
            "7c649407cfdb44b496d42fb880713c44",
            "81fbfc49ef244086a539e19533c5b4eb",
            "4f69eedf70ee4c88aaef12b977b63239",
            "18a45983efbe4281bf6eacacbc553281",
            "8ae899bd6f8d4c8698587b4cacf1fd7c",
            "50247d0985044ac2896e9e4d3030dd9b",
            "7b5d38b25802462eb227ecfb71c1f4dd",
            "db04bd7f08f142bf9da562740d078552",
            "44aafa4eae264500ba4294cc493ad4d3",
            "630d3cc4b759497991212f6ba32263c2",
            "91a29a80885c4949a9fa47aa2a3a4f45",
            "78409fbc6b824ef8b5dcfb46a61dbf5c",
            "2d4372f0d2ea475eb3798e5f6a6575cd",
            "c97174c06e41447da48ad90e9cfc353a",
            "cba8d805f22f47839f24f4b7e41cfeef",
            "8e1e4a90ca754a22ac438e5dba5be0b5",
            "5cfaef55e82f47d3a1dc4b2503fdd7b5",
            "2c4cee6d1e0648d4923eae9d20eaa6b5",
            "8e51bc0701d24e8f9cb0a49d9ffc7609",
            "32c071f25d7d4857b1e057e31deedcf5",
            "c572d50efeb54dbd9baad56b3d3f8805",
            "e708ec7e8a9743c8b024af1e73dc9c7e",
            "332aa522df834523addd91bcb53d95b0",
            "b7aff9c2fc5e4e3a971f350f28e8836b",
            "ac1f25287db642f5910c4328fb291502",
            "c5dbe2e501c34a03ad84e12943983b12",
            "785a376ec50d42e7a0d074761163d45d",
            "ef889e8d50be47b1807d54b8e858a4e4",
            "d1a2bc2820cb400eb73a7135d6fecc87",
            "985fc03c885b4741b488933edf870d71",
            "a3ab0162e401455790a5e637f2b32efb",
            "29233d16a5484357869f80ae6da46971",
            "039ae667277d404e927831cc38dac525",
            "7b26c27bffcd49279c272fac96a3d0ec",
            "9b70409d3ada4c2ba7593f4077ffdf46",
            "29ad6ee6914e4430807dba947ce54fac",
            "3dd674deed71418b9f42ecf392b1b13c",
            "727b520c38d74abda6d97fe621da4169",
            "7562580557db4751870ebb687f651b80",
            "55d0c55407914b53a2cb5c09463e45db",
            "375cb2731f184e09a8595ce7c7d8e101",
            "4240dc265886403997c29389b1fa9b72",
            "72a641755ceb4726aa718cceb43e891a",
            "85908de9d0d04570a8d66713c68e7d6a",
            "357603c901364107beac27a22869620c",
            "e02c2dabe6cc4267963d23e9d296b215",
            "b2d2c1d4ca004772aabbb5e8bccf7f6d",
            "a4088effa87e49d78baedc17c68de025",
            "afabba6d15814d9d991e51c974a19c5a",
            "57999e243240438987df3a2f85b7b7cf",
            "fa107784588f4450bd109042bd8711c2",
            "c469621996c34b15b5b32234c704953d",
            "bca463af7cd54287a0fa85da88aae812",
            "b1c4b6c7692f42f39cba29e1533f0e97",
            "9a73f8a61cbe43349de00f5ef74c97c3",
            "32b5d143aee7400ebf767b9693f47611",
            "6794b3eaa2944f23ac41afca5aaf796a",
            "baaa8ecb7f32400087b928f28431b4a9",
            "503f8e4c93a841e29a055b5a0e7a7eed",
            "5c52c8c938e2468a8345e7885c3a6c91",
            "a6c28245801a409ab1c064e6d7f4d71a"
          ]
        },
        "outputId": "be95348a-81a7-4d86-841a-86871b2f0a2d"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
            "  from tqdm.autonotebook import tqdm, trange\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9422852399274b0ab3f88afc09191ab0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4759a41091b3482e8f658290d879d2b1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md:   0%|          | 0.00/10.7k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7a4abc76695c411d9f608d1ce57c878f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ff3def560e034184850d354ae0799feb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5ff989c9349b4ad3a2e3db9e6483c8d0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d928c5dbf1c64b838602c5aff0b43f3e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7b5d38b25802462eb227ecfb71c1f4dd"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2c4cee6d1e0648d4923eae9d20eaa6b5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d1a2bc2820cb400eb73a7135d6fecc87"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "55d0c55407914b53a2cb5c09463e45db"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fa107784588f4450bd109042bd8711c2"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Also can use langchain & huggingface integration library :\n",
        "\n",
        "***!pip install langchain-huggingface***"
      ],
      "metadata": {
        "id": "OZS2t7tB0qF8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Indexing: AstraDB**\n",
        "Storing ***Splits*** in form of vectors into a ***VectorDB*** of ***AstraDB***, we ***Embedd*** & store at same time (one after other)"
      ],
      "metadata": {
        "id": "Aa-FrqED1NsM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# AstraDB Configuration\n",
        "ASTRA_DB_API_ENDPOINT = input(\"ASTRA_DB_API_ENDPOINT = \")\n"
      ],
      "metadata": {
        "id": "80frNNPYTZkF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ASTRA_DB_APPLICATION_TOKEN = getpass.getpass(\"ASTRA_DB_APPLICATION_TOKEN = \")"
      ],
      "metadata": {
        "id": "GoRYbxwyTbUc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81d244f5-9cfc-4b43-a2e9-babb6dbbf8e7"
      },
      "execution_count": 10,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ASTRA_DB_APPLICATION_TOKEN = ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ASTRA_DB_ID = getpass.getpass(\"ASTRA_DB_ID = \")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_1iCwp6wYbN2",
        "outputId": "38a20a2b-68c2-4915-a785-7b173e5651b6"
      },
      "execution_count": 11,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ASTRA_DB_ID = ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# connection of the AstraDB\n",
        "\n",
        "from langchain.vectorstores.cassandra import Cassandra\n",
        "import cassio\n",
        "from langchain_astradb import AstraDBVectorStore\n",
        "\n",
        "# Replace with your actual AstraDB credentials\n",
        "# ASTRA_DB_ID = \"8610ebf1-45ab-4bb5-8b91-65788fcbd547\"\n",
        "# ASTRA_DB_REGION = \"us-east-2\""
      ],
      "metadata": {
        "id": "y4zQjI3yUaRj"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cassio.init(token=ASTRA_DB_APPLICATION_TOKEN,database_id=ASTRA_DB_ID)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AC_3hixGWF6j",
        "outputId": "d2376ac3-b55b-47b2-8ef6-1c6d65a5bb99"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:cassandra.cluster:Downgrading core protocol version from 66 to 65 for 8610ebf1-45ab-4bb5-8b91-65788fcbd547-us-east-2.db.astra.datastax.com:29042:00958fd0-af98-391f-9f51-7e3fe763da5c. To avoid this, it is best practice to explicitly set Cluster(protocol_version) to the version supported by your cluster. http://datastax.github.io/python-driver/api/cassandra/cluster.html#cassandra.cluster.Cluster.protocol_version\n",
            "WARNING:cassandra.cluster:Downgrading core protocol version from 65 to 5 for 8610ebf1-45ab-4bb5-8b91-65788fcbd547-us-east-2.db.astra.datastax.com:29042:00958fd0-af98-391f-9f51-7e3fe763da5c. To avoid this, it is best practice to explicitly set Cluster(protocol_version) to the version supported by your cluster. http://datastax.github.io/python-driver/api/cassandra/cluster.html#cassandra.cluster.Cluster.protocol_version\n",
            "WARNING:cassandra.cluster:Downgrading core protocol version from 5 to 4 for 8610ebf1-45ab-4bb5-8b91-65788fcbd547-us-east-2.db.astra.datastax.com:29042:00958fd0-af98-391f-9f51-7e3fe763da5c. To avoid this, it is best practice to explicitly set Cluster(protocol_version) to the version supported by your cluster. http://datastax.github.io/python-driver/api/cassandra/cluster.html#cassandra.cluster.Cluster.protocol_version\n",
            "WARNING:cassandra.cluster:Found multiple hosts with the same endpoint (8610ebf1-45ab-4bb5-8b91-65788fcbd547-us-east-2.db.astra.datastax.com:29042:00958fd0-af98-391f-9f51-7e3fe763da5c). Excluding peer 10.0.0.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Convert Data Into Vectors and store in AstraDB\n",
        "\n",
        "astra_vector_store=Cassandra(\n",
        "    embedding=embeddings,\n",
        "    table_name=\"sklearn_user_guide_markdown_v2\",\n",
        "    session=None,\n",
        "    keyspace=None\n",
        "\n",
        ")"
      ],
      "metadata": {
        "id": "QbCQLUMQWXer"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.indexes.vectorstore import VectorStoreIndexWrapper\n",
        "\n",
        "astra_vector_store.add_documents(splits)\n",
        "print(\"Inserted %i headlines.\" % len(splits))\n",
        "\n",
        "astra_vector_index = VectorStoreIndexWrapper(vectorstore=astra_vector_store)\n",
        "\n",
        "# astra_vector_index, we can store this as a variable named 'retriever'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4x_VBm3kXDWh",
        "outputId": "eaba7f4f-f762-4a41-d638-6cc994d1d203"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inserted 1234 headlines.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the retriever\n",
        "retriever = astra_vector_index.query"
      ],
      "metadata": {
        "id": "k36IHcOkfFfg"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **LLM to use: ChatGroq**\n",
        "\n",
        "* we will be using ***llama3-8b-8192*** model provided by ***GROQ Cloud API***.\n",
        "\n",
        "* by using ***GROQ*** we get ***LPU***(Language Processing Unit), fastest inference(quick response) for LLM"
      ],
      "metadata": {
        "id": "Id5Sayzf5_Lz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_groq import ChatGroq\n",
        "\n",
        "llm = ChatGroq(\n",
        "    model=\"llama3-8b-8192\",\n",
        "    temperature=0.0,\n",
        "    # etc params\n",
        ")"
      ],
      "metadata": {
        "id": "PYLevyyy7FYM"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# query or question\n",
        "\n",
        "query = \"what is task decomposition ?\"\n",
        "response = retriever(query,llm=llm)\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "NMZnlHFuah3u",
        "outputId": "deb30286-d1cf-4762-f623-3cab31c3e28a"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:cassandra.protocol:Server warning: Top-K queries can only be run with consistency level ONE / LOCAL_ONE / NODE_LOCAL. Consistency level LOCAL_QUORUM was requested. Downgrading the consistency level to LOCAL_ONE.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Based on the provided context, it appears that the term \"task decomposition\" is not explicitly mentioned. However, the context discusses various algorithms and techniques for dimensionality reduction, regression, and matrix factorization, which are all related to machine learning and data analysis.\n",
            "\n",
            "Task decomposition, in the context of machine learning and data analysis, refers to the process of breaking down a complex task or problem into smaller, more manageable sub-tasks or components. This can involve decomposing a large dataset into smaller subsets, identifying key features or variables, or applying different algorithms or techniques to specific parts of the data.\n",
            "\n",
            "In the context of the provided text, task decomposition might involve decomposing a complex data analysis problem into smaller sub-tasks, such as:\n",
            "\n",
            "1. Dimensionality reduction using techniques like PCA or PLS\n",
            "2. Feature selection or extraction\n",
            "3. Matrix factorization or decomposition\n",
            "4. Regression or classification modeling\n",
            "\n",
            "By decomposing a complex task into smaller sub-tasks, data analysts and machine learning practitioners can better understand the relationships between different variables, identify patterns and trends, and develop more accurate models or predictions.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"what is essemble? answer in full detail\"\n",
        "response = retriever(query,llm=llm)\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "1YCqG8PUcofR",
        "outputId": "31ab6894-c897-4a1d-85a4-21f5a83e67c5"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:cassandra.protocol:Server warning: Top-K queries can only be run with consistency level ONE / LOCAL_ONE / NODE_LOCAL. Consistency level LOCAL_QUORUM was requested. Downgrading the consistency level to LOCAL_ONE.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ensemble learning is a machine learning technique that combines the predictions or decisions of multiple models or algorithms to improve the overall performance and accuracy of the system. In other words, ensemble methods combine the strengths of multiple models to create a more robust and accurate prediction or decision-making system.\n",
            "\n",
            "Ensemble methods are particularly useful when:\n",
            "\n",
            "1. **Individual models are not accurate enough**: When a single model is not accurate enough to make a reliable prediction or decision, ensemble methods can combine the predictions of multiple models to improve the overall accuracy.\n",
            "2. **Models have different strengths and weaknesses**: Ensemble methods can combine models that have different strengths and weaknesses to create a more robust system.\n",
            "3. **Data is noisy or imbalanced**: Ensemble methods can help to reduce the impact of noisy or imbalanced data by combining the predictions of multiple models.\n",
            "\n",
            "Types of Ensemble Methods:\n",
            "\n",
            "1. **Bagging (Bootstrap Aggregating)**: Bagging involves creating multiple instances of a model and combining their predictions. This method is particularly useful for reducing the impact of noisy data.\n",
            "2. **Boosting**: Boosting involves creating a sequence of models, where each model is trained on the residuals of the previous model. This method is particularly useful for improving the accuracy of a single model.\n",
            "3. **Stacking**: Stacking involves training a meta-model to combine the predictions of multiple base models. This method is particularly useful for combining the strengths of multiple models.\n",
            "4. **Random Forests**: Random forests involve creating multiple decision trees and combining their predictions. This method is particularly useful for handling high-dimensional data.\n",
            "\n",
            "Benefits of Ensemble Methods:\n",
            "\n",
            "1. **Improved accuracy**: Ensemble methods can improve the accuracy of a single model by combining the strengths of multiple models.\n",
            "2. **Reduced overfitting**: Ensemble methods can reduce the impact of overfitting by combining the predictions of multiple models.\n",
            "3. **Improved robustness**: Ensemble methods can improve the robustness of a system by combining the predictions of multiple models.\n",
            "4. **Improved interpretability**: Ensemble methods can provide insights into the strengths and weaknesses of individual models, making it easier to understand the decision-making process.\n",
            "\n",
            "Common Applications of Ensemble Methods:\n",
            "\n",
            "1. **Classification**: Ensemble methods are commonly used in classification problems, such as image classification, text classification, and speech recognition.\n",
            "2. **Regression**: Ensemble methods are also commonly used in regression problems, such as predicting stock prices, weather forecasting, and recommender systems.\n",
            "3. **Time Series Analysis**: Ensemble methods are used in time series analysis to predict future values in a sequence of data.\n",
            "4. **Natural Language Processing**: Ensemble methods are used in natural language processing to improve the accuracy of language models, sentiment analysis, and text summarization.\n",
            "\n",
            "In summary, ensemble methods are a powerful technique for improving the accuracy and robustness of machine learning models. By combining the strengths of multiple models, ensemble methods can improve the overall performance of a system and provide insights into the decision-making process.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"what is football ?\"\n",
        "response = retriever(query,llm=llm)\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QX_o3CvfchM2",
        "outputId": "6ef918b1-e37d-4268-a718-376fd7c94e67",
        "collapsed": true
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:cassandra.protocol:Server warning: Top-K queries can only be run with consistency level ONE / LOCAL_ONE / NODE_LOCAL. Consistency level LOCAL_QUORUM was requested. Downgrading the consistency level to LOCAL_ONE.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I don't know the answer to that question. The context provided is about machine learning and scoring functions, and it doesn't mention football.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***VectorStore (or DB)*** created on ***AstraDB*** & vector's for this embedding store in a ***DB*** (DB_ID = \"...\") with a ***name***(collection_name) to this group"
      ],
      "metadata": {
        "id": "i_FoAMVm3lLH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Retrive Embedded Vectors for use : Retriever**\n",
        "\n",
        "No special import needed , part of ***VectorStore*** i.e part of ***langchain-astradb*** (here)"
      ],
      "metadata": {
        "id": "qtOhcIgg4RM2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Prompts**\n",
        "\n",
        "instruction provided to LLM on what task it will perform on question as well as on custom dataset"
      ],
      "metadata": {
        "id": "uSLctvws8Gpu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "\n",
        "prompt_template = \"\"\"\n",
        "Answer the following question based only on the provided context.\n",
        "Think step by step before providing a detailed answer.\n",
        "I will tip you $1000 if the user finds the answer helpful.\n",
        "<context>\n",
        "{context}\n",
        "</context>\n",
        "\n",
        "Question: {input}\n",
        "\"\"\"\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(prompt_template)"
      ],
      "metadata": {
        "id": "PuBW5qihokih"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import hub\n",
        "\n",
        "# prompt = hub.pull(\"rlm/rag-prompt\")\n",
        "# prompt = hub.pull(\"rlm/rag-prompt\") # eg\n",
        "\n",
        "# this code is giving error , so don't use this for now"
      ],
      "metadata": {
        "id": "15bYJ09D8ZAz"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# **IMPORTANT NOTE**\n",
        "- Define the format_docs function to format retrieved documents\n",
        "- this function will be different for different method of data loading\n",
        "- as i scraped data with online tool, then converted it into ***.md(markdown file)*** , so my logic will be different from the data scraped from website"
      ],
      "metadata": {
        "id": "6t6UnxML_Uil"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the format_docs function to format retrieved documents\n",
        "def format_docs(docs):\n",
        "    if isinstance(docs, list):\n",
        "        return \"\\n\\n\".join(docs)  # Join list of text chunks with double newlines\n",
        "    return str(docs)  # Convert to string if not a list"
      ],
      "metadata": {
        "id": "tvv-k4T5_Y0N"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Chain**:\n",
        "\n",
        "- create a chain(from retrieval to llm response)\n",
        "- so, less code to write and every component at one place , so easy to understand & later to modify"
      ],
      "metadata": {
        "id": "F_VXgAMk7Vyd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "from langchain_core.messages.human import HumanMessage"
      ],
      "metadata": {
        "id": "E54Vb_R8jHRN"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the chain\n",
        "def rag_chain(query):\n",
        "    # Retrieve documents(query) & pass LLM to use\n",
        "    retrieved_docs = retriever(query, llm=llm)\n",
        "\n",
        "    formatted_docs = format_docs(retrieved_docs)  # Format the documents\n",
        "\n",
        "    # Construct the input for the prompt\n",
        "    input_data = {\n",
        "        \"context\": formatted_docs,\n",
        "        \"input\": query  # Ensure to use the correct key 'input'\n",
        "    }\n",
        "\n",
        "    # Format the prompt correctly using the input data\n",
        "    prompt_result = prompt.format(**input_data)\n",
        "\n",
        "    # Ensure the input to the LLM is in the correct format\n",
        "    human_message = HumanMessage(content=prompt_result)\n",
        "\n",
        "    # Pass the message in a list to the LLM\n",
        "    llm_response = llm([human_message])  # Get the LLM response\n",
        "\n",
        "    # Create an instance of StrOutputParser\n",
        "    output_parser = StrOutputParser()\n",
        "\n",
        "    # Parse the LLM response using the instance\n",
        "    final_output = output_parser.parse(llm_response)\n",
        "\n",
        "    return final_output\n"
      ],
      "metadata": {
        "id": "UdGYxDYPzxwq"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Sample Ex. to test our RAG-LLM Chatbot**"
      ],
      "metadata": {
        "id": "-_I0k97gBvMZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"what is Task Decompostion?\"\n",
        "response = rag_chain(query)\n",
        "print(response)"
      ],
      "metadata": {
        "id": "hqIHUjhuB6cG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "166b0962-11a2-41a1-ed91-72d69974296c"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:cassandra.protocol:Server warning: Top-K queries can only be run with consistency level ONE / LOCAL_ONE / NODE_LOCAL. Consistency level LOCAL_QUORUM was requested. Downgrading the consistency level to LOCAL_ONE.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "content='Based on the provided context, Task Decomposition is a technique used to break down a complex task into smaller, more manageable sub-tasks that can be executed in parallel.' response_metadata={'token_usage': {'completion_tokens': 34, 'prompt_tokens': 298, 'total_tokens': 332, 'completion_time': 0.02708508, 'prompt_time': 0.13665724, 'queue_time': None, 'total_time': 0.16374232000000002}, 'model_name': 'llama3-8b-8192', 'system_fingerprint': 'fp_c5f20b5bb1', 'finish_reason': 'stop', 'logprobs': None} id='run-0ce6758b-bcf7-48c3-94b8-119c03dc4202-0'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"what is cross-validation? give a proper example to explain this topic\"\n",
        "response = rag_chain(query)\n",
        "response"
      ],
      "metadata": {
        "id": "YFVwQHxJCEIF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd7d2299-21cc-46d1-a163-d7c7e6d58428"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:cassandra.protocol:Server warning: Top-K queries can only be run with consistency level ONE / LOCAL_ONE / NODE_LOCAL. Consistency level LOCAL_QUORUM was requested. Downgrading the consistency level to LOCAL_ONE.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content=\"Based on the provided context, I will provide a detailed answer.\\n\\nCross-validation is a technique used to evaluate the performance of a machine learning model by splitting the available data into training and testing sets. The goal is to get a more accurate estimate of the model's performance on unseen data.\\n\\nHere's an example to illustrate the concept:\\n\\nSuppose we have a dataset of 100 images of cats and dogs, and we want to train a model to classify them. We split the dataset into 5 folds:\\n\\nFold 1: 20 images (10 cats, 10 dogs)\\nFold 2: 20 images (10 cats, 10 dogs)\\nFold 3: 20 images (10 cats, 10 dogs)\\nFold 4: 20 images (10 cats, 10 dogs)\\nFold 5: 20 images (10 cats, 10 dogs)\\n\\nWe train our model on 4 folds (80 images) and test it on the remaining fold (20 images). We calculate the accuracy of the model on the test set. Let's say the accuracy is 90%.\\n\\nWe repeat this process for each fold, training the model on 4 folds and testing it on the remaining fold. We calculate the accuracy for each fold:\\n\\nFold 1: 85%\\nFold 2: 88%\\nFold 3: 92%\\nFold 4: 89%\\nFold 5: 91%\\n\\nTo get a more accurate estimate of the model's performance, we take the average of the accuracies across all folds:\\n\\n(85% + 88% + 92% + 89% + 91%) / 5 = 89.2%\\n\\nThis gives us a more robust estimate of the model's performance on unseen data.\", response_metadata={'token_usage': {'completion_tokens': 354, 'prompt_tokens': 480, 'total_tokens': 834, 'completion_time': 0.28361716, 'prompt_time': 0.17979247, 'queue_time': None, 'total_time': 0.46340963}, 'model_name': 'llama3-8b-8192', 'system_fingerprint': 'fp_33d61fdfc3', 'finish_reason': 'stop', 'logprobs': None}, id='run-53ceaf72-8da7-4ae3-8723-e5fd8bbcd254-0')"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Making output prettier**"
      ],
      "metadata": {
        "id": "y8L_Ihw81bXp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## fuck this , i am done coding for now"
      ],
      "metadata": {
        "id": "UrKV7gsQ3fTK"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***This Tutorial to develop RAG-Based LLM chatbot Application is created by Abhishek Vidhate***\n",
        "[GITHUB profile](https://github.com/Abhishekvidhate)"
      ],
      "metadata": {
        "id": "aMX84CqACQDC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Next Task**: create UI for this RAG-LLM app using STREAMLIT"
      ],
      "metadata": {
        "id": "wKmYYOd6DAXo"
      }
    }
  ]
}