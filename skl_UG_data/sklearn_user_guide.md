# scikit-learn User Guide

User guide: contents — scikit-learn 1.4.2 documentation
scikit-learn 1.4.2
Other versions
User Guide
## 1. Supervised learning

## 2. Unsupervised learning

## 3. Model selection and evaluation

## 4. Inspection

## 5. Visualizations

## 6. Dataset transformations

## 7. Dataset loading utilities

## 8. Computing with scikit-learn

## 9. Model persistence

## 10. Common pitfalls and recommended practices

## 11. Dispatching

User Guide¶
## 1. Supervised learning

### 1.1. Linear Models

- 1.1.1. Ordinary Least Squares
- 1.1.2. Ridge regression and classification
- 1.1.3. Lasso
- 1.1.4. Multi-task Lasso
- 1.1.5. Elastic-Net
- 1.1.6. Multi-task Elastic-Net
- 1.1.7. Least Angle Regression
- 1.1.8. LARS Lasso
- 1.1.9. Orthogonal Matching Pursuit (OMP)
- 1.1.10. Bayesian Regression
- 1.1.11. Logistic regression
- 1.1.12. Generalized Linear Models
- 1.1.13. Stochastic Gradient Descent - SGD
- 1.1.14. Perceptron
- 1.1.15. Passive Aggressive Algorithms
- 1.1.16. Robustness regression: outliers and modeling errors
- 1.1.17. Quantile Regression
- 1.1.18. Polynomial regression: extending linear models with basis functions
### 1.2. Linear and Quadratic Discriminant Analysis

- 1.2.1. Dimensionality reduction using Linear Discriminant Analysis
- 1.2.2. Mathematical formulation of the LDA and QDA classifiers
- 1.2.3. Mathematical formulation of LDA dimensionality reduction
- 1.2.4. Shrinkage and Covariance Estimator
- 1.2.5. Estimation algorithms
### 1.3. Kernel ridge regression

### 1.4. Support Vector Machines

- 1.4.1. Classification
- 1.4.2. Regression
- 1.4.3. Density estimation, novelty detection
- 1.4.4. Complexity
- 1.4.5. Tips on Practical Use
- 1.4.6. Kernel functions
- 1.4.7. Mathematical formulation
- 1.4.8. Implementation details
### 1.5. Stochastic Gradient Descent

- 1.5.1. Classification
- 1.5.2. Regression
- 1.5.3. Online One-Class SVM
- 1.5.4. Stochastic Gradient Descent for sparse data
- 1.5.5. Complexity
- 1.5.6. Stopping criterion
- 1.5.7. Tips on Practical Use
- 1.5.8. Mathematical formulation
- 1.5.9. Implementation details
### 1.6. Nearest Neighbors

- 1.6.1. Unsupervised Nearest Neighbors
- 1.6.2. Nearest Neighbors Classification
- 1.6.3. Nearest Neighbors Regression
- 1.6.4. Nearest Neighbor Algorithms
- 1.6.5. Nearest Centroid Classifier
- 1.6.6. Nearest Neighbors Transformer
- 1.6.7. Neighborhood Components Analysis
### 1.7. Gaussian Processes

- 1.7.1. Gaussian Process Regression (GPR)
- 1.7.2. Gaussian Process Classification (GPC)
- 1.7.3. GPC examples
- 1.7.4. Kernels for Gaussian Processes
### 1.8. Cross decomposition

- 1.8.1. PLSCanonical
- 1.8.2. PLSSVD
- 1.8.3. PLSRegression
- 1.8.4. Canonical Correlation Analysis
### 1.9. Naive Bayes

- 1.9.1. Gaussian Naive Bayes
- 1.9.2. Multinomial Naive Bayes
- 1.9.3. Complement Naive Bayes
- 1.9.4. Bernoulli Naive Bayes
- 1.9.5. Categorical Naive Bayes
- 1.9.6. Out-of-core naive Bayes model fitting
### 1.10. Decision Trees

- 1.10.1. Classification
- 1.10.2. Regression
- 1.10.3. Multi-output problems
- 1.10.4. Complexity
- 1.10.5. Tips on practical use
- 1.10.6. Tree algorithms: ID3, C4.5, C5.0 and CART
- 1.10.7. Mathematical formulation
- 1.10.8. Missing Values Support
- 1.10.9. Minimal Cost-Complexity Pruning
### 1.11. Ensembles: Gradient boosting, random forests, bagging, voting, stacking

- 1.11.1. Gradient-boosted trees
- 1.11.2. Random forests and other randomized tree ensembles
- 1.11.3. Bagging meta-estimator
- 1.11.4. Voting Classifier
- 1.11.5. Voting Regressor
- 1.11.6. Stacked generalization
- 1.11.7. AdaBoost
### 1.12. Multiclass and multioutput algorithms

- 1.12.1. Multiclass classification
- 1.12.2. Multilabel classification
- 1.12.3. Multiclass-multioutput classification
- 1.12.4. Multioutput regression
### 1.13. Feature selection

- 1.13.1. Removing features with low variance
- 1.13.2. Univariate feature selection
- 1.13.3. Recursive feature elimination
- 1.13.4. Feature selection using SelectFromModel
- 1.13.5. Sequential Feature Selection
- 1.13.6. Feature selection as part of a pipeline
### 1.14. Semi-supervised learning

- 1.14.1. Self Training
- 1.14.2. Label Propagation
### 1.15. Isotonic regression

### 1.16. Probability calibration

- 1.16.1. Calibration curves
- 1.16.2. Calibrating a classifier
- 1.16.3. Usage
### 1.17. Neural network models (supervised)

- 1.17.1. Multi-layer Perceptron
- 1.17.2. Classification
- 1.17.3. Regression
- 1.17.4. Regularization
- 1.17.5. Algorithms
- 1.17.6. Complexity
- 1.17.7. Mathematical formulation
- 1.17.8. Tips on Practical Use
- 1.17.9. More control with warm_start
## 2. Unsupervised learning

### 2.1. Gaussian mixture models

- 2.1.1. Gaussian Mixture
- 2.1.2. Variational Bayesian Gaussian Mixture
### 2.2. Manifold learning

- 2.2.1. Introduction
- 2.2.2. Isomap
- 2.2.3. Locally Linear Embedding
- 2.2.4. Modified Locally Linear Embedding
- 2.2.5. Hessian Eigenmapping
- 2.2.6. Spectral Embedding
- 2.2.7. Local Tangent Space Alignment
- 2.2.8. Multi-dimensional Scaling (MDS)
- 2.2.9. t-distributed Stochastic Neighbor Embedding (t-SNE)
- 2.2.10. Tips on practical use
### 2.3. Clustering

- 2.3.1. Overview of clustering methods
- 2.3.2. K-means
- 2.3.3. Affinity Propagation
- 2.3.4. Mean Shift
- 2.3.5. Spectral clustering
- 2.3.6. Hierarchical clustering
- 2.3.7. DBSCAN
- 2.3.8. HDBSCAN
- 2.3.9. OPTICS
- 2.3.10. BIRCH
- 2.3.11. Clustering performance evaluation
### 2.4. Biclustering

- 2.4.1. Spectral Co-Clustering
- 2.4.2. Spectral Biclustering
- 2.4.3. Biclustering evaluation
### 2.5. Decomposing signals in components (matrix factorization problems)

- 2.5.1. Principal component analysis (PCA)
- 2.5.2. Kernel Principal Component Analysis (kPCA)
- 2.5.3. Truncated singular value decomposition and latent semantic analysis
- 2.5.4. Dictionary Learning
- 2.5.5. Factor Analysis
- 2.5.6. Independent component analysis (ICA)
- 2.5.7. Non-negative matrix factorization (NMF or NNMF)
- 2.5.8. Latent Dirichlet Allocation (LDA)
### 2.6. Covariance estimation

- 2.6.1. Empirical covariance
- 2.6.2. Shrunk Covariance
- 2.6.3. Sparse inverse covariance
- 2.6.4. Robust Covariance Estimation
### 2.7. Novelty and Outlier Detection

- 2.7.1. Overview of outlier detection methods
- 2.7.2. Novelty Detection
- 2.7.3. Outlier Detection
- 2.7.4. Novelty detection with Local Outlier Factor
### 2.8. Density Estimation

- 2.8.1. Density Estimation: Histograms
- 2.8.2. Kernel Density Estimation
### 2.9. Neural network models (unsupervised)

- 2.9.1. Restricted Boltzmann machines
## 3. Model selection and evaluation

### 3.1. Cross-validation: evaluating estimator performance

- 3.1.1. Computing cross-validated metrics
- 3.1.2. Cross validation iterators
- 3.1.3. A note on shuffling
- 3.1.4. Cross validation and model selection
- 3.1.5. Permutation test score
### 3.2. Tuning the hyper-parameters of an estimator

- 3.2.1. Exhaustive Grid Search
- 3.2.2. Randomized Parameter Optimization
- 3.2.3. Searching for optimal parameters with successive halving
- 3.2.4. Tips for parameter search
- 3.2.5. Alternatives to brute force parameter search
### 3.3. Metrics and scoring: quantifying the quality of predictions

- 3.3.1. The scoring parameter: defining model evaluation rules
- 3.3.2. Classification metrics
- 3.3.3. Multilabel ranking metrics
- 3.3.4. Regression metrics
- 3.3.5. Clustering metrics
- 3.3.6. Dummy estimators
### 3.4. Validation curves: plotting scores to evaluate models

- 3.4.1. Validation curve
- 3.4.2. Learning curve
## 4. Inspection

### 4.1. Partial Dependence and Individual Conditional Expectation plots

- 4.1.1. Partial dependence plots
- 4.1.2. Individual conditional expectation (ICE) plot
- 4.1.3. Mathematical Definition
- 4.1.4. Computation methods
### 4.2. Permutation feature importance

- 4.2.1. Outline of the permutation importance algorithm
- 4.2.2. Relation to impurity-based importance in trees
- 4.2.3. Misleading values on strongly correlated features
## 5. Visualizations

### 5.1. Available Plotting Utilities

- 5.1.1. Display Objects
## 6. Dataset transformations

### 6.1. Pipelines and composite estimators

- 6.1.1. Pipeline: chaining estimators
- 6.1.2. Transforming target in regression
- 6.1.3. FeatureUnion: composite feature spaces
- 6.1.4. ColumnTransformer for heterogeneous data
- 6.1.5. Visualizing Composite Estimators
### 6.2. Feature extraction

- 6.2.1. Loading features from dicts
- 6.2.2. Feature hashing
- 6.2.3. Text feature extraction
- 6.2.4. Image feature extraction
### 6.3. Preprocessing data

- 6.3.1. Standardization, or mean removal and variance scaling
- 6.3.2. Non-linear transformation
- 6.3.3. Normalization
- 6.3.4. Encoding categorical features
- 6.3.5. Discretization
- 6.3.6. Imputation of missing values
- 6.3.7. Generating polynomial features
- 6.3.8. Custom transformers
### 6.4. Imputation of missing values

- 6.4.1. Univariate vs. Multivariate Imputation
- 6.4.2. Univariate feature imputation
- 6.4.3. Multivariate feature imputation
- 6.4.4. Nearest neighbors imputation
- 6.4.5. Keeping the number of features constant
- 6.4.6. Marking imputed values
- 6.4.7. Estimators that handle NaN values
### 6.5. Unsupervised dimensionality reduction

- 6.5.1. PCA: principal component analysis
- 6.5.2. Random projections
- 6.5.3. Feature agglomeration
### 6.6. Random Projection

- 6.6.1. The Johnson-Lindenstrauss lemma
- 6.6.2. Gaussian random projection
- 6.6.3. Sparse random projection
- 6.6.4. Inverse Transform
### 6.7. Kernel Approximation

- 6.7.1. Nystroem Method for Kernel Approximation
- 6.7.2. Radial Basis Function Kernel
- 6.7.3. Additive Chi Squared Kernel
- 6.7.4. Skewed Chi Squared Kernel
- 6.7.5. Polynomial Kernel Approximation via Tensor Sketch
- 6.7.6. Mathematical Details
### 6.8. Pairwise metrics, Affinities and Kernels

- 6.8.1. Cosine similarity
- 6.8.2. Linear kernel
- 6.8.3. Polynomial kernel
- 6.8.4. Sigmoid kernel
- 6.8.5. RBF kernel
- 6.8.6. Laplacian kernel
- 6.8.7. Chi-squared kernel
### 6.9. Transforming the prediction target (y)

- 6.9.1. Label binarization
- 6.9.2. Label encoding
## 7. Dataset loading utilities

### 7.1. Toy datasets

- 7.1.1. Iris plants dataset
- 7.1.2. Diabetes dataset
- 7.1.3. Optical recognition of handwritten digits dataset
- 7.1.4. Linnerrud dataset
- 7.1.5. Wine recognition dataset
- 7.1.6. Breast cancer wisconsin (diagnostic) dataset
### 7.2. Real world datasets

- 7.2.1. The Olivetti faces dataset
- 7.2.2. The 20 newsgroups text dataset
- 7.2.3. The Labeled Faces in the Wild face recognition dataset
- 7.2.4. Forest covertypes
- 7.2.5. RCV1 dataset
- 7.2.6. Kddcup 99 dataset
- 7.2.7. California Housing dataset
- 7.2.8. Species distribution dataset
### 7.3. Generated datasets

- 7.3.1. Generators for classification and clustering
- 7.3.2. Generators for regression
- 7.3.3. Generators for manifold learning
- 7.3.4. Generators for decomposition
### 7.4. Loading other datasets

- 7.4.1. Sample images
- 7.4.2. Datasets in svmlight / libsvm format
- 7.4.3. Downloading datasets from the openml.org repository
- 7.4.4. Loading from external datasets
## 8. Computing with scikit-learn

### 8.1. Strategies to scale computationally: bigger data

- 8.1.1. Scaling with instances using out-of-core learning
### 8.2. Computational Performance

- 8.2.1. Prediction Latency
- 8.2.2. Prediction Throughput
- 8.2.3. Tips and Tricks
### 8.3. Parallelism, resource management, and configuration

- 8.3.1. Parallelism
- 8.3.2. Configuration switches
## 9. Model persistence

### 9.1. Python specific serialization

- 9.1.1. Security & maintainability limitations
- 9.1.2. A more secure format: skops
### 9.2. Interoperable formats

## 10. Common pitfalls and recommended practices

### 10.1. Inconsistent preprocessing

### 10.2. Data leakage

- 10.2.1. How to avoid data leakage
- 10.2.2. Data leakage during pre-processing
### 10.3. Controlling randomness

- 10.3.1. Using None or RandomState instances, and repeated calls to fit and split
- 10.3.2. Common pitfalls and subtleties
- 10.3.3. General recommendations
## 11. Dispatching

### 11.1. Array API support (experimental)

- 11.1.1. Example usage
- 11.1.2. Support for Array API-compatible inputs
- 11.1.3. Common estimator checks
### 1.1. Linear Models — scikit-learn 1.4.2 documentation

### 1.1. Linear Models

- 1.1.1. Ordinary Least Squares
1.1.1.1. Non-Negative Least Squares
1.1.1.2. Ordinary Least Squares Complexity
- 1.1.2. Ridge regression and classification
1.1.2.1. Regression
1.1.2.2. Classification
1.1.2.3. Ridge Complexity
1.1.2.4. Setting the regularization parameter: leave-one-out Cross-Validation
- 1.1.3. Lasso
1.1.3.1. Setting regularization parameter
1.1.3.1.1. Using cross-validation
1.1.3.1.2. Information-criteria based model selection
1.1.3.1.3. AIC and BIC criteria
1.1.3.1.4. Comparison with the regularization parameter of SVM
- 1.1.4. Multi-task Lasso
- 1.1.5. Elastic-Net
- 1.1.6. Multi-task Elastic-Net
- 1.1.7. Least Angle Regression
- 1.1.8. LARS Lasso
- 1.1.9. Orthogonal Matching Pursuit (OMP)
- 1.1.10. Bayesian Regression
1.1.10.1. Bayesian Ridge Regression
1.1.10.2. Automatic Relevance Determination - ARD
- 1.1.11. Logistic regression
1.1.11.1. Binary Case
1.1.11.2. Multinomial Case
1.1.11.3. Solvers
1.1.11.3.1. Differences between solvers
- 1.1.12. Generalized Linear Models
1.1.12.1. Usage
- 1.1.13. Stochastic Gradient Descent - SGD
- 1.1.14. Perceptron
- 1.1.15. Passive Aggressive Algorithms
- 1.1.16. Robustness regression: outliers and modeling errors
1.1.16.1. Different scenario and useful concepts
1.1.16.2. RANSAC: RANdom SAmple Consensus
1.1.16.3. Theil-Sen estimator: generalized-median-based estimator
1.1.16.4. Huber Regression
- 1.1.17. Quantile Regression
- 1.1.18. Polynomial regression: extending linear models with basis functions
### 1.1. Linear Models¶

The following are a set of methods intended for regression in which
the target value is expected to be a linear combination of the features.
In mathematical notation, if \(\hat{y}\) is the predicted
value.
\[\hat{y}(w, x) = w_0 + w_1 x_1 + ... + w_p x_p\]
Across the module, we designate the vector \(w = (w_1,
..., w_p)\) as coef_ and \(w_0\) as intercept_.
To perform classification with generalized linear models, see
Logistic regression.
- 1.1.1. Ordinary Least Squares¶
LinearRegression fits a linear model with coefficients
\(w = (w_1, ..., w_p)\) to minimize the residual sum
of squares between the observed targets in the dataset, and the
targets predicted by the linear approximation. Mathematically it
solves a problem of the form:
\[\min_{w} || X w - y||_2^2\]
LinearRegression will take in its fit method arrays X, y
and will store the coefficients \(w\) of the linear model in its
coef_ member:
>>> from sklearn import linear_model
>>> reg = linear_model.LinearRegression()
>>> reg.fit([[0, 0], [1, 1], [2, 2]], [0, 1, 2])
LinearRegression()
>>> reg.coef_
array([0.5, 0.5])
The coefficient estimates for Ordinary Least Squares rely on the
independence of the features. When features are correlated and the
columns of the design matrix \(X\) have an approximately linear
dependence, the design matrix becomes close to singular
and as a result, the least-squares estimate becomes highly sensitive
to random errors in the observed target, producing a large
variance. This situation of multicollinearity can arise, for
example, when data are collected without an experimental design.
Examples:
Linear Regression Example
1.1.1.1. Non-Negative Least Squares¶
It is possible to constrain all the coefficients to be non-negative, which may
be useful when they represent some physical or naturally non-negative
quantities (e.g., frequency counts or prices of goods).
LinearRegression accepts a boolean positive
parameter: when set to True Non-Negative Least Squares are then applied.
Examples:
Non-negative least squares
1.1.1.2. Ordinary Least Squares Complexity¶
The least squares solution is computed using the singular value
decomposition of X. If X is a matrix of shape (n_samples, n_features)
this method has a cost of
\(O(n_{\text{samples}} n_{\text{features}}^2)\), assuming that
\(n_{\text{samples}} \geq n_{\text{features}}\).
- 1.1.2. Ridge regression and classification¶
1.1.2.1. Regression¶
Ridge regression addresses some of the problems of
Ordinary Least Squares by imposing a penalty on the size of the
coefficients. The ridge coefficients minimize a penalized residual sum
of squares:
\[\min_{w} || X w - y||_2^2 + \alpha ||w||_2^2\]
The complexity parameter \(\alpha \geq 0\) controls the amount
of shrinkage: the larger the value of \(\alpha\), the greater the amount
of shrinkage and thus the coefficients become more robust to collinearity.
As with other linear models, Ridge will take in its fit method
arrays X, y and will store the coefficients \(w\) of the linear model in
its coef_ member:
>>> from sklearn import linear_model
>>> reg = linear_model.Ridge(alpha=.5)
>>> reg.fit([[0, 0], [0, 0], [1, 1]], [0, .1, 1])
Ridge(alpha=0.5)
>>> reg.coef_
array([0.34545455, 0.34545455])
>>> reg.intercept_
0.13636...
Note that the class Ridge allows for the user to specify that the
solver be automatically chosen by setting solver="auto". When this option
is specified, Ridge will choose between the "lbfgs", "cholesky",
and "sparse_cg" solvers. Ridge will begin checking the conditions
shown in the following table from top to bottom. If the condition is true,
the corresponding solver is chosen.
Solver
Condition
‘lbfgs’
The positive=True option is specified.
‘cholesky’
The input array X is not sparse.
‘sparse_cg’
None of the above conditions are fulfilled.
1.1.2.2. Classification¶
The Ridge regressor has a classifier variant:
RidgeClassifier. This classifier first converts binary targets to
{-1, 1} and then treats the problem as a regression task, optimizing the
same objective as above. The predicted class corresponds to the sign of the
regressor’s prediction. For multiclass classification, the problem is
treated as multi-output regression, and the predicted class corresponds to
the output with the highest value.
It might seem questionable to use a (penalized) Least Squares loss to fit a
classification model instead of the more traditional logistic or hinge
losses. However, in practice, all those models can lead to similar
cross-validation scores in terms of accuracy or precision/recall, while the
penalized least squares loss used by the RidgeClassifier allows for
a very different choice of the numerical solvers with distinct computational
performance profiles.
The RidgeClassifier can be significantly faster than e.g.
LogisticRegression with a high number of classes because it can
compute the projection matrix \((X^T X)^{-1} X^T\) only once.
This classifier is sometimes referred to as a Least Squares Support Vector
Machines with
a linear kernel.
Examples:
Plot Ridge coefficients as a function of the regularization
Classification of text documents using sparse features
Common pitfalls in the interpretation of coefficients of linear models
1.1.2.3. Ridge Complexity¶
This method has the same order of complexity as
Ordinary Least Squares.
1.1.2.4. Setting the regularization parameter: leave-one-out Cross-Validation¶
RidgeCV implements ridge regression with built-in
cross-validation of the alpha parameter. The object works in the same way
as GridSearchCV except that it defaults to Leave-One-Out Cross-Validation:
>>> import numpy as np
>>> from sklearn import linear_model
>>> reg = linear_model.RidgeCV(alphas=np.logspace(-6, 6, 13))
>>> reg.fit([[0, 0], [0, 0], [1, 1]], [0, .1, 1])
RidgeCV(alphas=array([1.e-06, 1.e-05, 1.e-04, 1.e-03, 1.e-02, 1.e-01, 1.e+00, 1.e+01,
1.e+02, 1.e+03, 1.e+04, 1.e+05, 1.e+06]))
>>> reg.alpha_
0.01
Specifying the value of the cv attribute will trigger the use of
cross-validation with GridSearchCV, for
example cv=10 for 10-fold cross-validation, rather than Leave-One-Out
Cross-Validation.
- 1.1.3. Lasso¶
The Lasso is a linear model that estimates sparse coefficients.
It is useful in some contexts due to its tendency to prefer solutions
with fewer non-zero coefficients, effectively reducing the number of
features upon which the given solution is dependent. For this reason,
Lasso and its variants are fundamental to the field of compressed sensing.
Under certain conditions, it can recover the exact set of non-zero
coefficients (see
Compressive sensing: tomography reconstruction with L1 prior (Lasso)).
Mathematically, it consists of a linear model with an added regularization term.
The objective function to minimize is:
\[\min_{w} { \frac{1}{2n_{\text{samples}}} ||X w - y||_2 ^ 2 + \alpha ||w||_1}\]
The lasso estimate thus solves the minimization of the
least-squares penalty with \(\alpha ||w||_1\) added, where
\(\alpha\) is a constant and \(||w||_1\) is the \(\ell_1\)-norm of
the coefficient vector.
The implementation in the class Lasso uses coordinate descent as
the algorithm to fit the coefficients. See Least Angle Regression
for another implementation:
>>> from sklearn import linear_model
>>> reg = linear_model.Lasso(alpha=0.1)
>>> reg.fit([[0, 0], [1, 1]], [0, 1])
Lasso(alpha=0.1)
>>> reg.predict([[1, 1]])
array([0.8])
The function lasso_path is useful for lower-level tasks, as it
computes the coefficients along the full path of possible values.
Examples:
L1-based models for Sparse Signals
Compressive sensing: tomography reconstruction with L1 prior (Lasso)
Common pitfalls in the interpretation of coefficients of linear models
1.1.3.1. Setting regularization parameter¶
The alpha parameter controls the degree of sparsity of the estimated
coefficients.
1.1.3.1.1. Using cross-validation¶
scikit-learn exposes objects that set the Lasso alpha parameter by
cross-validation: LassoCV and LassoLarsCV.
LassoLarsCV is based on the Least Angle Regression algorithm
explained below.
For high-dimensional datasets with many collinear features,
LassoCV is most often preferable. However, LassoLarsCV has
the advantage of exploring more relevant values of alpha parameter, and
if the number of samples is very small compared to the number of
features, it is often faster than LassoCV.
1.1.3.1.2. Information-criteria based model selection¶
Alternatively, the estimator LassoLarsIC proposes to use the
Akaike information criterion (AIC) and the Bayes Information criterion (BIC).
It is a computationally cheaper alternative to find the optimal value of alpha
as the regularization path is computed only once instead of k+1 times
when using k-fold cross-validation.
Indeed, these criteria are computed on the in-sample training set. In short,
they penalize the over-optimistic scores of the different Lasso models by
their flexibility (cf. to “Mathematical details” section below).
However, such criteria need a proper estimation of the degrees of freedom of
the solution, are derived for large samples (asymptotic results) and assume the
correct model is candidates under investigation. They also tend to break when
the problem is badly conditioned (e.g. more features than samples).
Examples:
Lasso model selection: AIC-BIC / cross-validation
Lasso model selection via information criteria
1.1.3.1.3. AIC and BIC criteria¶
The definition of AIC (and thus BIC) might differ in the literature. In this
section, we give more information regarding the criterion computed in
scikit-learn.
The AIC criterion is defined as:
\[AIC = -2 \log(\hat{L}) + 2 d\]
where \(\hat{L}\) is the maximum likelihood of the model and
\(d\) is the number of parameters (as well referred to as degrees of
freedom in the previous section).
The definition of BIC replace the constant \(2\) by \(\log(N)\):
\[BIC = -2 \log(\hat{L}) + \log(N) d\]
where \(N\) is the number of samples.
For a linear Gaussian model, the maximum log-likelihood is defined as:
\[\log(\hat{L}) = - \frac{n}{2} \log(2 \pi) - \frac{n}{2} \ln(\sigma^2) - \frac{\sum_{i=1}^{n} (y_i - \hat{y}_i)^2}{2\sigma^2}\]
where \(\sigma^2\) is an estimate of the noise variance,
\(y_i\) and \(\hat{y}_i\) are respectively the true and predicted
targets, and \(n\) is the number of samples.
Plugging the maximum log-likelihood in the AIC formula yields:
\[AIC = n \log(2 \pi \sigma^2) + \frac{\sum_{i=1}^{n} (y_i - \hat{y}_i)^2}{\sigma^2} + 2 d\]
The first term of the above expression is sometimes discarded since it is a
constant when \(\sigma^2\) is provided. In addition,
it is sometimes stated that the AIC is equivalent to the \(C_p\) statistic
[12]. In a strict sense, however, it is equivalent only up to some constant
and a multiplicative factor.
At last, we mentioned above that \(\sigma^2\) is an estimate of the
noise variance. In LassoLarsIC when the parameter noise_variance is
not provided (default), the noise variance is estimated via the unbiased
estimator [13] defined as:
\[\sigma^2 = \frac{\sum_{i=1}^{n} (y_i - \hat{y}_i)^2}{n - p}\]
where \(p\) is the number of features and \(\hat{y}_i\) is the
predicted target using an ordinary least squares regression. Note, that this
formula is valid only when n_samples > n_features.
1.1.3.1.4. Comparison with the regularization parameter of SVM¶
The equivalence between alpha and the regularization parameter of SVM,
C is given by alpha = 1 / C or alpha = 1 / (n_samples * C),
depending on the estimator and the exact objective function optimized by the
model.
- 1.1.4. Multi-task Lasso¶
The MultiTaskLasso is a linear model that estimates sparse
coefficients for multiple regression problems jointly: y is a 2D array,
of shape (n_samples, n_tasks). The constraint is that the selected
features are the same for all the regression problems, also called tasks.
The following figure compares the location of the non-zero entries in the
coefficient matrix W obtained with a simple Lasso or a MultiTaskLasso.
The Lasso estimates yield scattered non-zeros while the non-zeros of
the MultiTaskLasso are full columns.
Fitting a time-series model, imposing that any active feature be active at all times.
Examples:
Joint feature selection with multi-task Lasso
Mathematical details
Click for more details
¶
Mathematically, it consists of a linear model trained with a mixed
\(\ell_1\) \(\ell_2\)-norm for regularization.
The objective function to minimize is:
\[\min_{W} { \frac{1}{2n_{\text{samples}}} ||X W - Y||_{\text{Fro}} ^ 2 + \alpha ||W||_{21}}\]
where \(\text{Fro}\) indicates the Frobenius norm
\[||A||_{\text{Fro}} = \sqrt{\sum_{ij} a_{ij}^2}\]
and \(\ell_1\) \(\ell_2\) reads
\[||A||_{2 1} = \sum_i \sqrt{\sum_j a_{ij}^2}.\]
The implementation in the class MultiTaskLasso uses
coordinate descent as the algorithm to fit the coefficients.
- 1.1.5. Elastic-Net¶
ElasticNet is a linear regression model trained with both
\(\ell_1\) and \(\ell_2\)-norm regularization of the coefficients.
This combination  allows for learning a sparse model where few of
the weights are non-zero like Lasso, while still maintaining
the regularization properties of Ridge. We control the convex
combination of \(\ell_1\) and \(\ell_2\) using the l1_ratio
parameter.
Elastic-net is useful when there are multiple features that are
correlated with one another. Lasso is likely to pick one of these
at random, while elastic-net is likely to pick both.
A practical advantage of trading-off between Lasso and Ridge is that it
allows Elastic-Net to inherit some of Ridge’s stability under rotation.
The objective function to minimize is in this case
\[\min_{w} { \frac{1}{2n_{\text{samples}}} ||X w - y||_2 ^ 2 + \alpha \rho ||w||_1 +
\frac{\alpha(1-\rho)}{2} ||w||_2 ^ 2}\]
The class ElasticNetCV can be used to set the parameters
alpha (\(\alpha\)) and l1_ratio (\(\rho\)) by cross-validation.
Examples:
L1-based models for Sparse Signals
Lasso and Elastic Net
- 1.1.6. Multi-task Elastic-Net¶
The MultiTaskElasticNet is an elastic-net model that estimates sparse
coefficients for multiple regression problems jointly: Y is a 2D array
of shape (n_samples, n_tasks). The constraint is that the selected
features are the same for all the regression problems, also called tasks.
Mathematically, it consists of a linear model trained with a mixed
\(\ell_1\) \(\ell_2\)-norm and \(\ell_2\)-norm for regularization.
The objective function to minimize is:
\[\min_{W} { \frac{1}{2n_{\text{samples}}} ||X W - Y||_{\text{Fro}}^2 + \alpha \rho ||W||_{2 1} +
\frac{\alpha(1-\rho)}{2} ||W||_{\text{Fro}}^2}\]
The implementation in the class MultiTaskElasticNet uses coordinate descent as
the algorithm to fit the coefficients.
The class MultiTaskElasticNetCV can be used to set the parameters
alpha (\(\alpha\)) and l1_ratio (\(\rho\)) by cross-validation.
- 1.1.7. Least Angle Regression¶
Least-angle regression (LARS) is a regression algorithm for
high-dimensional data, developed by Bradley Efron, Trevor Hastie, Iain
Johnstone and Robert Tibshirani. LARS is similar to forward stepwise
regression. At each step, it finds the feature most correlated with the
target. When there are multiple features having equal correlation, instead
of continuing along the same feature, it proceeds in a direction equiangular
between the features.
The advantages of LARS are:
It is numerically efficient in contexts where the number of features
is significantly greater than the number of samples.
It is computationally just as fast as forward selection and has
the same order of complexity as ordinary least squares.
It produces a full piecewise linear solution path, which is
useful in cross-validation or similar attempts to tune the model.
If two features are almost equally correlated with the target,
then their coefficients should increase at approximately the same
rate. The algorithm thus behaves as intuition would expect, and
also is more stable.
It is easily modified to produce solutions for other estimators,
like the Lasso.
The disadvantages of the LARS method include:
Because LARS is based upon an iterative refitting of the
residuals, it would appear to be especially sensitive to the
effects of noise. This problem is discussed in detail by Weisberg
in the discussion section of the Efron et al. (2004) Annals of
Statistics article.
The LARS model can be used via the estimator Lars, or its
low-level implementation lars_path or lars_path_gram.
- 1.1.8. LARS Lasso¶
LassoLars is a lasso model implemented using the LARS
algorithm, and unlike the implementation based on coordinate descent,
this yields the exact solution, which is piecewise linear as a
function of the norm of its coefficients.
>>> from sklearn import linear_model
>>> reg = linear_model.LassoLars(alpha=.1)
>>> reg.fit([[0, 0], [1, 1]], [0, 1])
LassoLars(alpha=0.1)
>>> reg.coef_
array([0.6..., 0.        ])
Examples:
Lasso path using LARS
The Lars algorithm provides the full path of the coefficients along
the regularization parameter almost for free, thus a common operation
is to retrieve the path with one of the functions lars_path
or lars_path_gram.
Mathematical formulation
Click for more details
¶
The algorithm is similar to forward stepwise regression, but instead
of including features at each step, the estimated coefficients are
increased in a direction equiangular to each one’s correlations with
the residual.
Instead of giving a vector result, the LARS solution consists of a
curve denoting the solution for each value of the \(\ell_1\) norm of the
parameter vector. The full coefficients path is stored in the array
coef_path_ of shape (n_features, max_features + 1). The first
column is always zero.
- 1.1.9. Orthogonal Matching Pursuit (OMP)¶
OrthogonalMatchingPursuit and orthogonal_mp implement the OMP
algorithm for approximating the fit of a linear model with constraints imposed
on the number of non-zero coefficients (ie. the \(\ell_0\) pseudo-norm).
Being a forward feature selection method like Least Angle Regression,
orthogonal matching pursuit can approximate the optimum solution vector with a
fixed number of non-zero elements:
\[\underset{w}{\operatorname{arg\,min\,}}  ||y - Xw||_2^2 \text{ subject to } ||w||_0 \leq n_{\text{nonzero\_coefs}}\]
Alternatively, orthogonal matching pursuit can target a specific error instead
of a specific number of non-zero coefficients. This can be expressed as:
\[\underset{w}{\operatorname{arg\,min\,}} ||w||_0 \text{ subject to } ||y-Xw||_2^2 \leq \text{tol}\]
OMP is based on a greedy algorithm that includes at each step the atom most
highly correlated with the current residual. It is similar to the simpler
matching pursuit (MP) method, but better in that at each iteration, the
residual is recomputed using an orthogonal projection on the space of the
previously chosen dictionary elements.
Examples:
Orthogonal Matching Pursuit
- 1.1.10. Bayesian Regression¶
Bayesian regression techniques can be used to include regularization
parameters in the estimation procedure: the regularization parameter is
not set in a hard sense but tuned to the data at hand.
This can be done by introducing uninformative priors
over the hyper parameters of the model.
The \(\ell_{2}\) regularization used in Ridge regression and classification is
equivalent to finding a maximum a posteriori estimation under a Gaussian prior
over the coefficients \(w\) with precision \(\lambda^{-1}\).
Instead of setting lambda manually, it is possible to treat it as a random
variable to be estimated from the data.
To obtain a fully probabilistic model, the output \(y\) is assumed
to be Gaussian distributed around \(X w\):
\[p(y|X,w,\alpha) = \mathcal{N}(y|X w,\alpha^{-1})\]
where \(\alpha\) is again treated as a random variable that is to be
estimated from the data.
The advantages of Bayesian Regression are:
It adapts to the data at hand.
It can be used to include regularization parameters in the
estimation procedure.
The disadvantages of Bayesian regression include:
Inference of the model can be time consuming.
1.1.10.1. Bayesian Ridge Regression¶
BayesianRidge estimates a probabilistic model of the
regression problem as described above.
The prior for the coefficient \(w\) is given by a spherical Gaussian:
\[p(w|\lambda) =
\mathcal{N}(w|0,\lambda^{-1}\mathbf{I}_{p})\]
The priors over \(\alpha\) and \(\lambda\) are chosen to be gamma
distributions, the
conjugate prior for the precision of the Gaussian. The resulting model is
called Bayesian Ridge Regression, and is similar to the classical
Ridge.
The parameters \(w\), \(\alpha\) and \(\lambda\) are estimated
jointly during the fit of the model, the regularization parameters
\(\alpha\) and \(\lambda\) being estimated by maximizing the
log marginal likelihood. The scikit-learn implementation
is based on the algorithm described in Appendix A of (Tipping, 2001)
where the update of the parameters \(\alpha\) and \(\lambda\) is done
as suggested in (MacKay, 1992). The initial value of the maximization procedure
can be set with the hyperparameters alpha_init and lambda_init.
There are four more hyperparameters, \(\alpha_1\), \(\alpha_2\),
\(\lambda_1\) and \(\lambda_2\) of the gamma prior distributions over
\(\alpha\) and \(\lambda\). These are usually chosen to be
non-informative. By default \(\alpha_1 = \alpha_2 =  \lambda_1 = \lambda_2 = 10^{-6}\).
Bayesian Ridge Regression is used for regression:
>>> from sklearn import linear_model
>>> X = [[0., 0.], [1., 1.], [2., 2.], [3., 3.]]
>>> Y = [0., 1., 2., 3.]
>>> reg = linear_model.BayesianRidge()
>>> reg.fit(X, Y)
BayesianRidge()
After being fitted, the model can then be used to predict new values:
>>> reg.predict([[1, 0.]])
array([0.50000013])
The coefficients \(w\) of the model can be accessed:
>>> reg.coef_
array([0.49999993, 0.49999993])
Due to the Bayesian framework, the weights found are slightly different to the
ones found by Ordinary Least Squares. However, Bayesian Ridge Regression
is more robust to ill-posed problems.
Examples:
Curve Fitting with Bayesian Ridge Regression
1.1.10.2. Automatic Relevance Determination - ARD¶
The Automatic Relevance Determination (as being implemented in
ARDRegression) is a kind of linear model which is very similar to the
Bayesian Ridge Regression, but that leads to sparser coefficients \(w\)
[1] [2].
ARDRegression poses a different prior over \(w\): it drops
the spherical Gaussian distribution for a centered elliptic Gaussian
distribution. This means each coefficient \(w_{i}\) can itself be drawn from
a Gaussian distribution, centered on zero and with a precision
\(\lambda_{i}\):
\[p(w|\lambda) = \mathcal{N}(w|0,A^{-1})\]
with \(A\) being a positive definite diagonal matrix and
\(\text{diag}(A) = \lambda = \{\lambda_{1},...,\lambda_{p}\}\).
In contrast to the Bayesian Ridge Regression, each coordinate of
\(w_{i}\) has its own standard deviation \(\frac{1}{\lambda_i}\). The
prior over all \(\lambda_i\) is chosen to be the same gamma distribution
given by the hyperparameters \(\lambda_1\) and \(\lambda_2\).
ARD is also known in the literature as Sparse Bayesian Learning and Relevance
Vector Machine [3] [4]. For a worked-out comparison between ARD and Bayesian
Ridge Regression, see the example below.
Examples:
Comparing Linear Bayesian Regressors
- 1.1.11. Logistic regression¶
The logistic regression is implemented in LogisticRegression. Despite
its name, it is implemented as a linear model for classification rather than
regression in terms of the scikit-learn/ML nomenclature. The logistic
regression is also known in the literature as logit regression,
maximum-entropy classification (MaxEnt) or the log-linear classifier. In this
model, the probabilities describing the possible outcomes of a single trial
are modeled using a logistic function.
This implementation can fit binary, One-vs-Rest, or multinomial logistic
regression with optional \(\ell_1\), \(\ell_2\) or Elastic-Net
regularization.
Note
Regularization
Regularization is applied by default, which is common in machine
learning but not in statistics. Another advantage of regularization is
that it improves numerical stability. No regularization amounts to
setting C to a very high value.
Note
Logistic Regression as a special case of the Generalized Linear Models (GLM)
Logistic regression is a special case of
Generalized Linear Models with a Binomial / Bernoulli conditional
distribution and a Logit link. The numerical output of the logistic
regression, which is the predicted probability, can be used as a classifier
by applying a threshold (by default 0.5) to it. This is how it is
implemented in scikit-learn, so it expects a categorical target, making
the Logistic Regression a classifier.
Examples
L1 Penalty and Sparsity in Logistic Regression
Regularization path of L1- Logistic Regression
Plot multinomial and One-vs-Rest Logistic Regression
Multiclass sparse logistic regression on 20newgroups
MNIST classification using multinomial logistic + L1
1.1.11.1. Binary Case¶
For notational ease, we assume that the target \(y_i\) takes values in the
set \(\{0, 1\}\) for data point \(i\).
Once fitted, the predict_proba
method of LogisticRegression predicts
the probability of the positive class \(P(y_i=1|X_i)\) as
\[\hat{p}(X_i) = \operatorname{expit}(X_i w + w_0) = \frac{1}{1 + \exp(-X_i w - w_0)}.\]
As an optimization problem, binary
class logistic regression with regularization term \(r(w)\) minimizes the
following cost function:
(1)¶\[\min_{w} C \sum_{i=1}^n s_i \left(-y_i \log(\hat{p}(X_i)) - (1 - y_i) \log(1 - \hat{p}(X_i))\right) + r(w),\]
where \({s_i}\) corresponds to the weights assigned by the user to a
specific training sample (the vector \(s\) is formed by element-wise
multiplication of the class weights and sample weights).
We currently provide four choices for the regularization term  \(r(w)\)  via
the penalty argument:
penalty
\(r(w)\)
None
\(0\)
\(\ell_1\)
\(\|w\|_1\)
\(\ell_2\)
\(\frac{1}{2}\|w\|_2^2 = \frac{1}{2}w^T w\)
ElasticNet
\(\frac{1 - \rho}{2}w^T w + \rho \|w\|_1\)
For ElasticNet, \(\rho\) (which corresponds to the l1_ratio parameter)
controls the strength of \(\ell_1\) regularization vs. \(\ell_2\)
regularization. Elastic-Net is equivalent to \(\ell_1\) when
\(\rho = 1\) and equivalent to \(\ell_2\) when \(\rho=0\).
Note that the scale of the class weights and the sample weights will influence
the optimization problem. For instance, multiplying the sample weights by a
constant \(b>0\) is equivalent to multiplying the (inverse) regularization
strength C by \(b\).
1.1.11.2. Multinomial Case¶
The binary case can be extended to \(K\) classes leading to the multinomial
logistic regression, see also log-linear model.
Note
It is possible to parameterize a \(K\)-class classification model
using only \(K-1\) weight vectors, leaving one class probability fully
determined by the other class probabilities by leveraging the fact that all
class probabilities must sum to one. We deliberately choose to overparameterize the model
using \(K\) weight vectors for ease of implementation and to preserve the
symmetrical inductive bias regarding ordering of classes, see [16]. This effect becomes
especially important when using regularization. The choice of overparameterization can be
detrimental for unpenalized models since then the solution may not be unique, as shown in [16].
Let \(y_i \in {1, \ldots, K}\) be the label (ordinal) encoded target variable for observation \(i\).
Instead of a single coefficient vector, we now have
a matrix of coefficients \(W\) where each row vector \(W_k\) corresponds to class
\(k\). We aim at predicting the class probabilities \(P(y_i=k|X_i)\) via
predict_proba as:
\[\hat{p}_k(X_i) = \frac{\exp(X_i W_k + W_{0, k})}{\sum_{l=0}^{K-1} \exp(X_i W_l + W_{0, l})}.\]
The objective for the optimization becomes
\[\min_W -C \sum_{i=1}^n \sum_{k=0}^{K-1} [y_i = k] \log(\hat{p}_k(X_i)) + r(W).\]
Where \([P]\) represents the Iverson bracket which evaluates to \(0\)
if \(P\) is false, otherwise it evaluates to \(1\). We currently provide four choices
for the regularization term \(r(W)\) via the penalty argument, where \(m\)
is the number of features:
penalty
\(r(W)\)
None
\(0\)
\(\ell_1\)
\(\|W\|_{1,1} = \sum_{i=1}^m\sum_{j=1}^{K}|W_{i,j}|\)
\(\ell_2\)
\(\frac{1}{2}\|W\|_F^2 = \frac{1}{2}\sum_{i=1}^m\sum_{j=1}^{K} W_{i,j}^2\)
ElasticNet
\(\frac{1 - \rho}{2}\|W\|_F^2 + \rho \|W\|_{1,1}\)
1.1.11.3. Solvers¶
The solvers implemented in the class LogisticRegression
are “lbfgs”, “liblinear”, “newton-cg”, “newton-cholesky”, “sag” and “saga”:
The following table summarizes the penalties supported by each solver:
Solvers
Penalties
‘lbfgs’
‘liblinear’
‘newton-cg’
‘newton-cholesky’
‘sag’
‘saga’
Multinomial + L2 penalty
yes
no
yes
no
yes
yes
OVR + L2 penalty
yes
yes
yes
yes
yes
yes
Multinomial + L1 penalty
no
no
no
no
no
yes
OVR + L1 penalty
no
yes
no
no
no
yes
Elastic-Net
no
no
no
no
no
yes
No penalty (‘none’)
yes
no
yes
yes
yes
yes
Behaviors
Penalize the intercept (bad)
no
yes
no
no
no
no
Faster for large datasets
no
no
no
no
yes
yes
Robust to unscaled datasets
yes
yes
yes
yes
no
no
The “lbfgs” solver is used by default for its robustness. For large datasets
the “saga” solver is usually faster.
For large dataset, you may also consider using SGDClassifier
with loss="log_loss", which might be even faster but requires more tuning.
1.1.11.3.1. Differences between solvers¶
There might be a difference in the scores obtained between
LogisticRegression with solver=liblinear or
LinearSVC and the external liblinear library directly,
when fit_intercept=False and the fit coef_ (or) the data to be predicted
are zeroes. This is because for the sample(s) with decision_function zero,
LogisticRegression and LinearSVC predict the
negative class, while liblinear predicts the positive class. Note that a model
with fit_intercept=False and having many samples with decision_function
zero, is likely to be a underfit, bad model and you are advised to set
fit_intercept=True and increase the intercept_scaling.
The solver “liblinear” uses a coordinate descent (CD) algorithm, and relies
on the excellent C++ LIBLINEAR library, which is shipped with
scikit-learn. However, the CD algorithm implemented in liblinear cannot learn
a true multinomial (multiclass) model; instead, the optimization problem is
decomposed in a “one-vs-rest” fashion so separate binary classifiers are
trained for all classes. This happens under the hood, so
LogisticRegression instances using this solver behave as multiclass
classifiers. For \(\ell_1\) regularization sklearn.svm.l1_min_c allows to
calculate the lower bound for C in order to get a non “null” (all feature
weights to zero) model.
The “lbfgs”, “newton-cg” and “sag” solvers only support \(\ell_2\)
regularization or no regularization, and are found to converge faster for some
high-dimensional data. Setting multi_class to “multinomial” with these solvers
learns a true multinomial logistic regression model [5], which means that its
probability estimates should be better calibrated than the default “one-vs-rest”
setting.
The “sag” solver uses Stochastic Average Gradient descent [6]. It is faster
than other solvers for large datasets, when both the number of samples and the
number of features are large.
The “saga” solver [7] is a variant of “sag” that also supports the
non-smooth penalty="l1". This is therefore the solver of choice for sparse
multinomial logistic regression. It is also the only solver that supports
penalty="elasticnet".
The “lbfgs” is an optimization algorithm that approximates the
Broyden–Fletcher–Goldfarb–Shanno algorithm [8], which belongs to
quasi-Newton methods. As such, it can deal with a wide range of different training
data and is therefore the default solver. Its performance, however, suffers on poorly
scaled datasets and on datasets with one-hot encoded categorical features with rare
categories.
The “newton-cholesky” solver is an exact Newton solver that calculates the hessian
matrix and solves the resulting linear system. It is a very good choice for
n_samples >> n_features, but has a few shortcomings: Only \(\ell_2\)
regularization is supported. Furthermore, because the hessian matrix is explicitly
computed, the memory usage has a quadratic dependency on n_features as well as on
n_classes. As a consequence, only the one-vs-rest scheme is implemented for the
multiclass case.
For a comparison of some of these solvers, see [9].
References:
[5]
Christopher M. Bishop: Pattern Recognition and Machine Learning, Chapter 4.3.4
[6]
Mark Schmidt, Nicolas Le Roux, and Francis Bach: Minimizing Finite Sums with the Stochastic Average Gradient.
[7]
Aaron Defazio, Francis Bach, Simon Lacoste-Julien:
SAGA: A Fast Incremental Gradient Method With Support for
Non-Strongly Convex Composite Objectives.
[8]
https://en.wikipedia.org/wiki/Broyden%E2%80%93Fletcher%E2%80%93Goldfarb%E2%80%93Shanno_algorithm
[9]
Thomas P. Minka “A comparison of numerical optimizers for logistic regression”
[16]
(1,2)
Simon, Noah, J. Friedman and T. Hastie.
“A Blockwise Descent Algorithm for Group-penalized Multiresponse and
Multinomial Regression.”
Note
Feature selection with sparse logistic regression
A logistic regression with \(\ell_1\) penalty yields sparse models, and can
thus be used to perform feature selection, as detailed in
L1-based feature selection.
Note
P-value estimation
It is possible to obtain the p-values and confidence intervals for
coefficients in cases of regression without penalization. The statsmodels
package natively supports this.
Within sklearn, one could use bootstrapping instead as well.
LogisticRegressionCV implements Logistic Regression with built-in
cross-validation support, to find the optimal C and l1_ratio parameters
according to the scoring attribute. The “newton-cg”, “sag”, “saga” and
“lbfgs” solvers are found to be faster for high-dimensional dense data, due
to warm-starting (see Glossary).
- 1.1.12. Generalized Linear Models¶
Generalized Linear Models (GLM) extend linear models in two ways
[10]. First, the predicted values \(\hat{y}\) are linked to a linear
combination of the input variables \(X\) via an inverse link function
\(h\) as
\[\hat{y}(w, X) = h(Xw).\]
Secondly, the squared loss function is replaced by the unit deviance
\(d\) of a distribution in the exponential family (or more precisely, a
reproductive exponential dispersion model (EDM) [11]).
The minimization problem becomes:
\[\min_{w} \frac{1}{2 n_{\text{samples}}} \sum_i d(y_i, \hat{y}_i) + \frac{\alpha}{2} ||w||_2^2,\]
where \(\alpha\) is the L2 regularization penalty. When sample weights are
provided, the average becomes a weighted average.
The following table lists some specific EDMs and their unit deviance :
Distribution
Target Domain
Unit Deviance \(d(y, \hat{y})\)
Normal
\(y \in (-\infty, \infty)\)
\((y-\hat{y})^2\)
Bernoulli
\(y \in \{0, 1\}\)
\(2({y}\log\frac{y}{\hat{y}}+({1}-{y})\log\frac{{1}-{y}}{{1}-\hat{y}})\)
Categorical
\(y \in \{0, 1, ..., k\}\)
\(2\sum_{i \in \{0, 1, ..., k\}} I(y = i) y_\text{i}\log\frac{I(y = i)}{\hat{I(y = i)}}\)
Poisson
\(y \in [0, \infty)\)
\(2(y\log\frac{y}{\hat{y}}-y+\hat{y})\)
Gamma
\(y \in (0, \infty)\)
\(2(\log\frac{\hat{y}}{y}+\frac{y}{\hat{y}}-1)\)
Inverse Gaussian
\(y \in (0, \infty)\)
\(\frac{(y-\hat{y})^2}{y\hat{y}^2}\)
The Probability Density Functions (PDF) of these distributions are illustrated
in the following figure,
PDF of a random variable Y following Poisson, Tweedie (power=1.5) and Gamma
distributions with different mean values (\(\mu\)). Observe the point
mass at \(Y=0\) for the Poisson distribution and the Tweedie (power=1.5)
distribution, but not for the Gamma distribution which has a strictly
positive target domain.¶
The Bernoulli distribution is a discrete probability distribution modelling a
Bernoulli trial - an event that has only two mutually exclusive outcomes.
The Categorical distribution is a generalization of the Bernoulli distribution
for a categorical random variable. While a random variable in a Bernoulli
distribution has two possible outcomes, a Categorical random variable can take
on one of K possible categories, with the probability of each category
specified separately.
The choice of the distribution depends on the problem at hand:
If the target values \(y\) are counts (non-negative integer valued) or
relative frequencies (non-negative), you might use a Poisson distribution
with a log-link.
If the target values are positive valued and skewed, you might try a Gamma
distribution with a log-link.
If the target values seem to be heavier tailed than a Gamma distribution, you
might try an Inverse Gaussian distribution (or even higher variance powers of
the Tweedie family).
If the target values \(y\) are probabilities, you can use the Bernoulli
distribution. The Bernoulli distribution with a logit link can be used for
binary classification. The Categorical distribution with a softmax link can be
used for multiclass classification.
Agriculture / weather modeling:  number of rain events per year (Poisson),
amount of rainfall per event (Gamma), total rainfall per year (Tweedie /
Compound Poisson Gamma).
Risk modeling / insurance policy pricing:  number of claim events /
policyholder per year (Poisson), cost per event (Gamma), total cost per
policyholder per year (Tweedie / Compound Poisson Gamma).
Credit Default: probability that a loan can’t be paid back (Bernoulli).
Fraud Detection: probability that a financial transaction like a cash transfer
is a fraudulent transaction (Bernoulli).
Predictive maintenance: number of production interruption events per year
(Poisson), duration of interruption (Gamma), total interruption time per year
(Tweedie / Compound Poisson Gamma).
Medical Drug Testing: probability of curing a patient in a set of trials or
probability that a patient will experience side effects (Bernoulli).
News Classification: classification of news articles into three categories
namely Business News, Politics and Entertainment news (Categorical).
1.1.12.1. Usage¶
TweedieRegressor implements a generalized linear model for the
Tweedie distribution, that allows to model any of the above mentioned
distributions using the appropriate power parameter. In particular:
power = 0: Normal distribution. Specific estimators such as
Ridge, ElasticNet are generally more appropriate in
this case.
power = 1: Poisson distribution. PoissonRegressor is exposed
for convenience. However, it is strictly equivalent to
TweedieRegressor(power=1, link='log').
power = 2: Gamma distribution. GammaRegressor is exposed for
convenience. However, it is strictly equivalent to
TweedieRegressor(power=2, link='log').
power = 3: Inverse Gaussian distribution.
The link function is determined by the link parameter.
Usage example:
>>> from sklearn.linear_model import TweedieRegressor
>>> reg = TweedieRegressor(power=1, alpha=0.5, link='log')
>>> reg.fit([[0, 0], [0, 1], [2, 2]], [0, 1, 2])
TweedieRegressor(alpha=0.5, link='log', power=1)
>>> reg.coef_
array([0.2463..., 0.4337...])
>>> reg.intercept_
-0.7638...
Examples
Poisson regression and non-normal loss
Tweedie regression on insurance claims
The feature matrix X should be standardized before fitting. This ensures
that the penalty treats features equally.
Since the linear predictor \(Xw\) can be negative and Poisson,
Gamma and Inverse Gaussian distributions don’t support negative values, it
is necessary to apply an inverse link function that guarantees the
non-negativeness. For example with link='log', the inverse link function
becomes \(h(Xw)=\exp(Xw)\).
If you want to model a relative frequency, i.e. counts per exposure (time,
volume, …) you can do so by using a Poisson distribution and passing
\(y=\frac{\mathrm{counts}}{\mathrm{exposure}}\) as target values
together with \(\mathrm{exposure}\) as sample weights. For a concrete
example see e.g.
Tweedie regression on insurance claims.
When performing cross-validation for the power parameter of
TweedieRegressor, it is advisable to specify an explicit scoring function,
because the default scorer TweedieRegressor.score is a function of
power itself.
- 1.1.13. Stochastic Gradient Descent - SGD¶
Stochastic gradient descent is a simple yet very efficient approach
to fit linear models. It is particularly useful when the number of samples
(and the number of features) is very large.
The partial_fit method allows online/out-of-core learning.
The classes SGDClassifier and SGDRegressor provide
functionality to fit linear models for classification and regression
using different (convex) loss functions and different penalties.
E.g., with loss="log", SGDClassifier
fits a logistic regression model,
while with loss="hinge" it fits a linear support vector machine (SVM).
You can refer to the dedicated Stochastic Gradient Descent documentation section for more details.
- 1.1.14. Perceptron¶
The Perceptron is another simple classification algorithm suitable for
large scale learning. By default:
It does not require a learning rate.
It is not regularized (penalized).
It updates its model only on mistakes.
The last characteristic implies that the Perceptron is slightly faster to
train than SGD with the hinge loss and that the resulting models are
sparser.
In fact, the Perceptron is a wrapper around the SGDClassifier
class using a perceptron loss and a constant learning rate. Refer to
mathematical section of the SGD procedure
for more details.
- 1.1.15. Passive Aggressive Algorithms¶
The passive-aggressive algorithms are a family of algorithms for large-scale
learning. They are similar to the Perceptron in that they do not require a
learning rate. However, contrary to the Perceptron, they include a
regularization parameter C.
For classification, PassiveAggressiveClassifier can be used with
loss='hinge' (PA-I) or loss='squared_hinge' (PA-II).  For regression,
PassiveAggressiveRegressor can be used with
loss='epsilon_insensitive' (PA-I) or
loss='squared_epsilon_insensitive' (PA-II).
“Online Passive-Aggressive Algorithms”
K. Crammer, O. Dekel, J. Keshat, S. Shalev-Shwartz, Y. Singer - JMLR 7 (2006)
- 1.1.16. Robustness regression: outliers and modeling errors¶
Robust regression aims to fit a regression model in the
presence of corrupt data: either outliers, or error in the model.
1.1.16.1. Different scenario and useful concepts¶
There are different things to keep in mind when dealing with data
corrupted by outliers:
Outliers in X or in y?
Outliers in the y direction
Outliers in the X direction
Fraction of outliers versus amplitude of error
The number of outlying points matters, but also how much they are
outliers.
Small outliers
Large outliers
An important notion of robust fitting is that of breakdown point: the
fraction of data that can be outlying for the fit to start missing the
inlying data.
Note that in general, robust fitting in high-dimensional setting (large
n_features) is very hard. The robust models here will probably not work
in these settings.
Trade-offs: which estimator ?
Scikit-learn provides 3 robust regression estimators:
RANSAC,
Theil Sen and
HuberRegressor.
HuberRegressor should be faster than
RANSAC and Theil Sen
unless the number of samples are very large, i.e. n_samples >> n_features.
This is because RANSAC and Theil Sen
fit on smaller subsets of the data. However, both Theil Sen
and RANSAC are unlikely to be as robust as
HuberRegressor for the default parameters.
RANSAC is faster than Theil Sen
and scales much better with the number of samples.
RANSAC will deal better with large
outliers in the y direction (most common situation).
Theil Sen will cope better with
medium-size outliers in the X direction, but this property will
disappear in high-dimensional settings.
When in doubt, use RANSAC.
1.1.16.2. RANSAC: RANdom SAmple Consensus¶
RANSAC (RANdom SAmple Consensus) fits a model from random subsets of
inliers from the complete data set.
RANSAC is a non-deterministic algorithm producing only a reasonable result with
a certain probability, which is dependent on the number of iterations (see
max_trials parameter). It is typically used for linear and non-linear
regression problems and is especially popular in the field of photogrammetric
computer vision.
The algorithm splits the complete input sample data into a set of inliers,
which may be subject to noise, and outliers, which are e.g. caused by erroneous
measurements or invalid hypotheses about the data. The resulting model is then
estimated only from the determined inliers.
Examples
Robust linear model estimation using RANSAC
Robust linear estimator fitting
Each iteration performs the following steps:
Select min_samples random samples from the original data and check
whether the set of data is valid (see is_data_valid).
Fit a model to the random subset (base_estimator.fit) and check
whether the estimated model is valid (see is_model_valid).
Classify all data as inliers or outliers by calculating the residuals
to the estimated model (base_estimator.predict(X) - y) - all data
samples with absolute residuals smaller than or equal to the
residual_threshold are considered as inliers.
Save fitted model as best model if number of inlier samples is
maximal. In case the current estimated model has the same number of
inliers, it is only considered as the best model if it has better score.
These steps are performed either a maximum number of times (max_trials) or
until one of the special stop criteria are met (see stop_n_inliers and
stop_score). The final model is estimated using all inlier samples (consensus
set) of the previously determined best model.
The is_data_valid and is_model_valid functions allow to identify and reject
degenerate combinations of random sub-samples. If the estimated model is not
needed for identifying degenerate cases, is_data_valid should be used as it
is called prior to fitting the model and thus leading to better computational
performance.
1.1.16.3. Theil-Sen estimator: generalized-median-based estimator¶
The TheilSenRegressor estimator uses a generalization of the median in
multiple dimensions. It is thus robust to multivariate outliers. Note however
that the robustness of the estimator decreases quickly with the dimensionality
of the problem. It loses its robustness properties and becomes no
better than an ordinary least squares in high dimension.
Examples:
Theil-Sen Regression
Robust linear estimator fitting
Theoretical considerations
Click for more details
¶
TheilSenRegressor is comparable to the Ordinary Least Squares
(OLS) in terms of asymptotic efficiency and as an
unbiased estimator. In contrast to OLS, Theil-Sen is a non-parametric
method which means it makes no assumption about the underlying
distribution of the data. Since Theil-Sen is a median-based estimator, it
is more robust against corrupted data aka outliers. In univariate
setting, Theil-Sen has a breakdown point of about 29.3% in case of a
simple linear regression which means that it can tolerate arbitrary
corrupted data of up to 29.3%.
The implementation of TheilSenRegressor in scikit-learn follows a
generalization to a multivariate linear regression model [14] using the
spatial median which is a generalization of the median to multiple
dimensions [15].
In terms of time and space complexity, Theil-Sen scales according to
\[\binom{n_{\text{samples}}}{n_{\text{subsamples}}}\]
which makes it infeasible to be applied exhaustively to problems with a
large number of samples and features. Therefore, the magnitude of a
subpopulation can be chosen to limit the time and space complexity by
considering only a random subset of all possible combinations.
References:
[14]
Xin Dang, Hanxiang Peng, Xueqin Wang and Heping Zhang: Theil-Sen Estimators in a Multiple Linear Regression Model.
[15]
Kärkkäinen and S. Äyrämö: On Computation of Spatial Median for Robust Data Mining.
Also see the Wikipedia page
1.1.16.4. Huber Regression¶
The HuberRegressor is different to Ridge because it applies a
linear loss to samples that are classified as outliers.
A sample is classified as an inlier if the absolute error of that sample is
lesser than a certain threshold. It differs from TheilSenRegressor
and RANSACRegressor because it does not ignore the effect of the outliers
but gives a lesser weight to them.
Examples:
HuberRegressor vs Ridge on dataset with strong outliers
Mathematical details
Click for more details
¶
The loss function that HuberRegressor minimizes is given by
\[\min_{w, \sigma} {\sum_{i=1}^n\left(\sigma + H_{\epsilon}\left(\frac{X_{i}w - y_{i}}{\sigma}\right)\sigma\right) + \alpha {||w||_2}^2}\]
where
\[\begin{split}H_{\epsilon}(z) = \begin{cases}
z^2, & \text {if } |z| < \epsilon, \\
2\epsilon|z| - \epsilon^2, & \text{otherwise}
\end{cases}\end{split}\]
It is advised to set the parameter epsilon to 1.35 to achieve 95%
statistical efficiency.
References:
Peter J. Huber, Elvezio M. Ronchetti: Robust Statistics, Concomitant scale
estimates, pg 172
The HuberRegressor differs from using SGDRegressor with loss set to huber
in the following ways.
HuberRegressor is scaling invariant. Once epsilon is set, scaling X and y
down or up by different values would produce the same robustness to outliers as before.
as compared to SGDRegressor where epsilon has to be set again when X and y are
scaled.
HuberRegressor should be more efficient to use on data with small number of
samples while SGDRegressor needs a number of passes on the training data to
produce the same robustness.
Note that this estimator is different from the R implementation of Robust Regression
(https://stats.oarc.ucla.edu/r/dae/robust-regression/) because the R implementation does a weighted least
squares implementation with weights given to each sample on the basis of how much the residual is
greater than a certain threshold.
- 1.1.17. Quantile Regression¶
Quantile regression estimates the median or other quantiles of \(y\)
conditional on \(X\), while ordinary least squares (OLS) estimates the
conditional mean.
Quantile regression may be useful if one is interested in predicting an
interval instead of point prediction. Sometimes, prediction intervals are
calculated based on the assumption that prediction error is distributed
normally with zero mean and constant variance. Quantile regression provides
sensible prediction intervals even for errors with non-constant (but
predictable) variance or non-normal distribution.
Based on minimizing the pinball loss, conditional quantiles can also be
estimated by models other than linear models. For example,
GradientBoostingRegressor can predict conditional
quantiles if its parameter loss is set to "quantile" and parameter
alpha is set to the quantile that should be predicted. See the example in
Prediction Intervals for Gradient Boosting Regression.
Most implementations of quantile regression are based on linear programming
problem. The current implementation is based on
scipy.optimize.linprog.
Examples:
Quantile regression
Mathematical details
Click for more details
¶
As a linear model, the QuantileRegressor gives linear predictions
\(\hat{y}(w, X) = Xw\) for the \(q\)-th quantile, \(q \in (0, 1)\).
The weights or coefficients \(w\) are then found by the following
minimization problem:
\[\min_{w} {\frac{1}{n_{\text{samples}}}
\sum_i PB_q(y_i - X_i w) + \alpha ||w||_1}.\]
This consists of the pinball loss (also known as linear loss),
see also mean_pinball_loss,
\[\begin{split}PB_q(t) = q \max(t, 0) + (1 - q) \max(-t, 0) =
\begin{cases}
q t, & t > 0, \\
0,    & t = 0, \\
(q-1) t, & t < 0
\end{cases}\end{split}\]
and the L1 penalty controlled by parameter alpha, similar to
Lasso.
As the pinball loss is only linear in the residuals, quantile regression is
much more robust to outliers than squared error based estimation of the mean.
Somewhat in between is the HuberRegressor.
References
Click for more details
¶
Koenker, R., & Bassett Jr, G. (1978). Regression quantiles.
Econometrica: journal of the Econometric Society, 33-50.
Portnoy, S., & Koenker, R. (1997). The Gaussian hare and the Laplacian
tortoise: computability of squared-error versus absolute-error estimators.
Statistical Science, 12, 279-300.
Koenker, R. (2005). Quantile Regression.
Cambridge University Press.
- 1.1.18. Polynomial regression: extending linear models with basis functions¶
One common pattern within machine learning is to use linear models trained
on nonlinear functions of the data.  This approach maintains the generally
fast performance of linear methods, while allowing them to fit a much wider
range of data.
Mathematical details
Click for more details
¶
For example, a simple linear regression can be extended by constructing
polynomial features from the coefficients.  In the standard linear
regression case, you might have a model that looks like this for
two-dimensional data:
\[\hat{y}(w, x) = w_0 + w_1 x_1 + w_2 x_2\]
If we want to fit a paraboloid to the data instead of a plane, we can combine
the features in second-order polynomials, so that the model looks like this:
\[\hat{y}(w, x) = w_0 + w_1 x_1 + w_2 x_2 + w_3 x_1 x_2 + w_4 x_1^2 + w_5 x_2^2\]
The (sometimes surprising) observation is that this is still a linear model:
to see this, imagine creating a new set of features
\[z = [x_1, x_2, x_1 x_2, x_1^2, x_2^2]\]
With this re-labeling of the data, our problem can be written
\[\hat{y}(w, z) = w_0 + w_1 z_1 + w_2 z_2 + w_3 z_3 + w_4 z_4 + w_5 z_5\]
We see that the resulting polynomial regression is in the same class of
linear models we considered above (i.e. the model is linear in \(w\))
and can be solved by the same techniques.  By considering linear fits within
a higher-dimensional space built with these basis functions, the model has the
flexibility to fit a much broader range of data.
Here is an example of applying this idea to one-dimensional data, using
polynomial features of varying degrees:
This figure is created using the PolynomialFeatures transformer, which
transforms an input data matrix into a new data matrix of a given degree.
It can be used as follows:
>>> from sklearn.preprocessing import PolynomialFeatures
>>> import numpy as np
>>> X = np.arange(6).reshape(3, 2)
>>> X
array([[0, 1],
[2, 3],
[4, 5]])
>>> poly = PolynomialFeatures(degree=2)
>>> poly.fit_transform(X)
array([[ 1.,  0.,  1.,  0.,  0.,  1.],
[ 1.,  2.,  3.,  4.,  6.,  9.],
[ 1.,  4.,  5., 16., 20., 25.]])
The features of X have been transformed from \([x_1, x_2]\) to
\([1, x_1, x_2, x_1^2, x_1 x_2, x_2^2]\), and can now be used within
any linear model.
This sort of preprocessing can be streamlined with the
Pipeline tools. A single object representing a simple
polynomial regression can be created and used as follows:
>>> from sklearn.preprocessing import PolynomialFeatures
>>> from sklearn.linear_model import LinearRegression
>>> from sklearn.pipeline import Pipeline
>>> import numpy as np
>>> model = Pipeline([('poly', PolynomialFeatures(degree=3)),
...                   ('linear', LinearRegression(fit_intercept=False))])
>>> # fit to an order-3 polynomial data
>>> x = np.arange(5)
>>> y = 3 - 2 * x + x ** 2 - x ** 3
>>> model = model.fit(x[:, np.newaxis], y)
>>> model.named_steps['linear'].coef_
array([ 3., -2.,  1., -1.])
The linear model trained on polynomial features is able to exactly recover
the input polynomial coefficients.
In some cases it’s not necessary to include higher powers of any single feature,
but only the so-called interaction features
that multiply together at most \(d\) distinct features.
These can be gotten from PolynomialFeatures with the setting
interaction_only=True.
For example, when dealing with boolean features,
\(x_i^n = x_i\) for all \(n\) and is therefore useless;
but \(x_i x_j\) represents the conjunction of two booleans.
This way, we can solve the XOR problem with a linear classifier:
>>> from sklearn.linear_model import Perceptron
>>> from sklearn.preprocessing import PolynomialFeatures
>>> import numpy as np
>>> X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])
>>> y = X[:, 0] ^ X[:, 1]
>>> y
array([0, 1, 1, 0])
>>> X = PolynomialFeatures(interaction_only=True).fit_transform(X).astype(int)
>>> X
array([[1, 0, 0, 0],
[1, 0, 1, 0],
[1, 1, 0, 0],
[1, 1, 1, 1]])
>>> clf = Perceptron(fit_intercept=False, max_iter=10, tol=None,
...                  shuffle=False).fit(X, y)
And the classifier “predictions” are perfect:
>>> clf.predict(X)
array([0, 1, 1, 0])
>>> clf.score(X, y)
1.0
### 1.2. Linear and Quadratic Discriminant Analysis — scikit-learn 1.4.2 documentation

Install
User Guide
API
Examples
Community
Getting Started
Tutorial
What's new
Glossary
Development
FAQ
Support
Related packages
Roadmap
Governance
About us
GitHub
Other Versions and Download
More
Getting Started
Tutorial
What's new
Glossary
Development
FAQ
Support
Related packages
Roadmap
Governance
About us
GitHub
Other Versions and Download
Toggle Menu
PrevUp
Next
scikit-learn 1.4.2
Other versions
Please cite us if you use the software.
### 1.2. Linear and Quadratic Discriminant Analysis

- 1.2.1. Dimensionality reduction using Linear Discriminant Analysis
- 1.2.2. Mathematical formulation of the LDA and QDA classifiers
1.2.2.1. QDA
1.2.2.2. LDA
- 1.2.3. Mathematical formulation of LDA dimensionality reduction
- 1.2.4. Shrinkage and Covariance Estimator
- 1.2.5. Estimation algorithms
### 1.2. Linear and Quadratic Discriminant Analysis¶

Linear Discriminant Analysis
(LinearDiscriminantAnalysis) and Quadratic
Discriminant Analysis
(QuadraticDiscriminantAnalysis) are two classic
classifiers, with, as their names suggest, a linear and a quadratic decision
surface, respectively.
These classifiers are attractive because they have closed-form solutions that
can be easily computed, are inherently multiclass, have proven to work well in
practice, and have no hyperparameters to tune.
The plot shows decision boundaries for Linear Discriminant Analysis and
Quadratic Discriminant Analysis. The bottom row demonstrates that Linear
Discriminant Analysis can only learn linear boundaries, while Quadratic
Discriminant Analysis can learn quadratic boundaries and is therefore more
flexible.
Examples:
Linear and Quadratic Discriminant Analysis with covariance ellipsoid: Comparison of LDA and QDA
on synthetic data.
- 1.2.1. Dimensionality reduction using Linear Discriminant Analysis¶
LinearDiscriminantAnalysis can be used to
perform supervised dimensionality reduction, by projecting the input data to a
linear subspace consisting of the directions which maximize the separation
between classes (in a precise sense discussed in the mathematics section
below). The dimension of the output is necessarily less than the number of
classes, so this is in general a rather strong dimensionality reduction, and
only makes sense in a multiclass setting.
This is implemented in the transform method. The desired dimensionality can
be set using the n_components parameter. This parameter has no influence
on the fit and predict methods.
Examples:
Comparison of LDA and PCA 2D projection of Iris dataset: Comparison of LDA and PCA
for dimensionality reduction of the Iris dataset
- 1.2.2. Mathematical formulation of the LDA and QDA classifiers¶
Both LDA and QDA can be derived from simple probabilistic models which model
the class conditional distribution of the data \(P(X|y=k)\) for each class
\(k\). Predictions can then be obtained by using Bayes’ rule, for each
training sample \(x \in \mathcal{R}^d\):
\[P(y=k | x) = \frac{P(x | y=k) P(y=k)}{P(x)} = \frac{P(x | y=k) P(y = k)}{ \sum_{l} P(x | y=l) \cdot P(y=l)}\]
and we select the class \(k\) which maximizes this posterior probability.
More specifically, for linear and quadratic discriminant analysis,
\(P(x|y)\) is modeled as a multivariate Gaussian distribution with
density:
\[P(x | y=k) = \frac{1}{(2\pi)^{d/2} |\Sigma_k|^{1/2}}\exp\left(-\frac{1}{2} (x-\mu_k)^t \Sigma_k^{-1} (x-\mu_k)\right)\]
where \(d\) is the number of features.
1.2.2.1. QDA¶
According to the model above, the log of the posterior is:
\[\begin{split}\log P(y=k | x) &= \log P(x | y=k) + \log P(y = k) + Cst \\
&= -\frac{1}{2} \log |\Sigma_k| -\frac{1}{2} (x-\mu_k)^t \Sigma_k^{-1} (x-\mu_k) + \log P(y = k) + Cst,\end{split}\]
where the constant term \(Cst\) corresponds to the denominator
\(P(x)\), in addition to other constant terms from the Gaussian. The
predicted class is the one that maximises this log-posterior.
Note
Relation with Gaussian Naive Bayes
If in the QDA model one assumes that the covariance matrices are diagonal,
then the inputs are assumed to be conditionally independent in each class,
and the resulting classifier is equivalent to the Gaussian Naive Bayes
classifier naive_bayes.GaussianNB.
1.2.2.2. LDA¶
LDA is a special case of QDA, where the Gaussians for each class are assumed
to share the same covariance matrix: \(\Sigma_k = \Sigma\) for all
\(k\). This reduces the log posterior to:
\[\log P(y=k | x) = -\frac{1}{2} (x-\mu_k)^t \Sigma^{-1} (x-\mu_k) + \log P(y = k) + Cst.\]
The term \((x-\mu_k)^t \Sigma^{-1} (x-\mu_k)\) corresponds to the
Mahalanobis Distance
between the sample \(x\) and the mean \(\mu_k\). The Mahalanobis
distance tells how close \(x\) is from \(\mu_k\), while also
accounting for the variance of each feature. We can thus interpret LDA as
assigning \(x\) to the class whose mean is the closest in terms of
Mahalanobis distance, while also accounting for the class prior
probabilities.
The log-posterior of LDA can also be written [3] as:
\[\log P(y=k | x) = \omega_k^t x + \omega_{k0} + Cst.\]
where \(\omega_k = \Sigma^{-1} \mu_k\) and \(\omega_{k0} =
-\frac{1}{2} \mu_k^t\Sigma^{-1}\mu_k + \log P (y = k)\). These quantities
correspond to the coef_ and intercept_ attributes, respectively.
From the above formula, it is clear that LDA has a linear decision surface.
In the case of QDA, there are no assumptions on the covariance matrices
\(\Sigma_k\) of the Gaussians, leading to quadratic decision surfaces.
See [1] for more details.
- 1.2.3. Mathematical formulation of LDA dimensionality reduction¶
First note that the K means \(\mu_k\) are vectors in
\(\mathcal{R}^d\), and they lie in an affine subspace \(H\) of
dimension at most \(K - 1\) (2 points lie on a line, 3 points lie on a
plane, etc.).
As mentioned above, we can interpret LDA as assigning \(x\) to the class
whose mean \(\mu_k\) is the closest in terms of Mahalanobis distance,
while also accounting for the class prior probabilities. Alternatively, LDA
is equivalent to first sphering the data so that the covariance matrix is
the identity, and then assigning \(x\) to the closest mean in terms of
Euclidean distance (still accounting for the class priors).
Computing Euclidean distances in this d-dimensional space is equivalent to
first projecting the data points into \(H\), and computing the distances
there (since the other dimensions will contribute equally to each class in
terms of distance). In other words, if \(x\) is closest to \(\mu_k\)
in the original space, it will also be the case in \(H\).
This shows that, implicit in the LDA
classifier, there is a dimensionality reduction by linear projection onto a
\(K-1\) dimensional space.
We can reduce the dimension even more, to a chosen \(L\), by projecting
onto the linear subspace \(H_L\) which maximizes the variance of the
\(\mu^*_k\) after projection (in effect, we are doing a form of PCA for the
transformed class means \(\mu^*_k\)). This \(L\) corresponds to the
n_components parameter used in the
transform method. See
[1] for more details.
- 1.2.4. Shrinkage and Covariance Estimator¶
Shrinkage is a form of regularization used to improve the estimation of
covariance matrices in situations where the number of training samples is
small compared to the number of features.
In this scenario, the empirical sample covariance is a poor
estimator, and shrinkage helps improving the generalization performance of
the classifier.
Shrinkage LDA can be used by setting the shrinkage parameter of
the LinearDiscriminantAnalysis class to ‘auto’.
This automatically determines the optimal shrinkage parameter in an analytic
way following the lemma introduced by Ledoit and Wolf [2]. Note that
currently shrinkage only works when setting the solver parameter to ‘lsqr’
or ‘eigen’.
The shrinkage parameter can also be manually set between 0 and 1. In
particular, a value of 0 corresponds to no shrinkage (which means the empirical
covariance matrix will be used) and a value of 1 corresponds to complete
shrinkage (which means that the diagonal matrix of variances will be used as
an estimate for the covariance matrix). Setting this parameter to a value
between these two extrema will estimate a shrunk version of the covariance
matrix.
The shrunk Ledoit and Wolf estimator of covariance may not always be the
best choice. For example if the distribution of the data
is normally distributed, the
Oracle Approximating Shrinkage estimator sklearn.covariance.OAS
yields a smaller Mean Squared Error than the one given by Ledoit and Wolf’s
formula used with shrinkage=”auto”. In LDA, the data are assumed to be gaussian
conditionally to the class. If these assumptions hold, using LDA with
the OAS estimator of covariance will yield a better classification
accuracy than if Ledoit and Wolf or the empirical covariance estimator is used.
The covariance estimator can be chosen using with the covariance_estimator
parameter of the discriminant_analysis.LinearDiscriminantAnalysis
class. A covariance estimator should have a fit method and a
covariance_ attribute like all covariance estimators in the
sklearn.covariance module.
Examples:
Normal, Ledoit-Wolf and OAS Linear Discriminant Analysis for classification: Comparison of LDA classifiers
with Empirical, Ledoit Wolf and OAS covariance estimator.
- 1.2.5. Estimation algorithms¶
Using LDA and QDA requires computing the log-posterior which depends on the
class priors \(P(y=k)\), the class means \(\mu_k\), and the
covariance matrices.
The ‘svd’ solver is the default solver used for
LinearDiscriminantAnalysis, and it is
the only available solver for
QuadraticDiscriminantAnalysis.
It can perform both classification and transform (for LDA).
As it does not rely on the calculation of the covariance matrix, the ‘svd’
solver may be preferable in situations where the number of features is large.
The ‘svd’ solver cannot be used with shrinkage.
For QDA, the use of the SVD solver relies on the fact that the covariance
matrix \(\Sigma_k\) is, by definition, equal to \(\frac{1}{n - 1}
X_k^tX_k = \frac{1}{n - 1} V S^2 V^t\) where \(V\) comes from the SVD of the (centered)
matrix: \(X_k = U S V^t\). It turns out that we can compute the
log-posterior above without having to explicitly compute \(\Sigma\):
computing \(S\) and \(V\) via the SVD of \(X\) is enough. For
LDA, two SVDs are computed: the SVD of the centered input matrix \(X\)
and the SVD of the class-wise mean vectors.
The ‘lsqr’ solver is an efficient algorithm that only works for
classification. It needs to explicitly compute the covariance matrix
\(\Sigma\), and supports shrinkage and custom covariance estimators.
This solver computes the coefficients
\(\omega_k = \Sigma^{-1}\mu_k\) by solving for \(\Sigma \omega =
\mu_k\), thus avoiding the explicit computation of the inverse
\(\Sigma^{-1}\).
The ‘eigen’ solver is based on the optimization of the between class scatter to
within class scatter ratio. It can be used for both classification and
transform, and it supports shrinkage. However, the ‘eigen’ solver needs to
compute the covariance matrix, so it might not be suitable for situations with
a high number of features.
References:
[1]
(1,2)
“The Elements of Statistical Learning”, Hastie T., Tibshirani R.,
Friedman J., Section 4.3, p.106-119, 2008.
[2]
Ledoit O, Wolf M. Honey, I Shrunk the Sample Covariance Matrix.
The Journal of Portfolio Management 30(4), 110-119, 2004.
[3]
R. O. Duda, P. E. Hart, D. G. Stork. Pattern Classification
(Second Edition), section 2.6.2.
© 2007 - 2024, scikit-learn developers (BSD License).
Show this page source
Click to add a cell.
### 1.3. Kernel ridge regression — scikit-learn 1.4.2 documentation

Install
User Guide
API
Examples
Community
Getting Started
Tutorial
What's new
Glossary
Development
FAQ
Support
Related packages
Roadmap
Governance
About us
GitHub
Other Versions and Download
More
Getting Started
Tutorial
What's new
Glossary
Development
FAQ
Support
Related packages
Roadmap
Governance
About us
GitHub
Other Versions and Download
Toggle Menu
PrevUp
Next
scikit-learn 1.4.2
Other versions
Please cite us if you use the software.
### 1.3. Kernel ridge regression

### 1.3. Kernel ridge regression¶

Kernel ridge regression (KRR) [M2012] combines Ridge regression and classification
(linear least squares with l2-norm regularization) with the kernel trick. It thus learns a linear
function in the space induced by the respective kernel and the data. For
non-linear kernels, this corresponds to a non-linear function in the original
space.
The form of the model learned by KernelRidge is identical to support
vector regression (SVR). However, different loss
functions are used: KRR uses squared error loss while support vector
regression uses \(\epsilon\)-insensitive loss, both combined with l2
regularization. In contrast to SVR, fitting
KernelRidge can be done in closed-form and is typically faster for
medium-sized datasets. On the other hand, the learned model is non-sparse and
thus slower than SVR, which learns a sparse model for
\(\epsilon > 0\), at prediction-time.
The following figure compares KernelRidge and
SVR on an artificial dataset, which consists of a
sinusoidal target function and strong noise added to every fifth datapoint.
The learned model of KernelRidge and SVR is
plotted, where both complexity/regularization and bandwidth of the RBF kernel
have been optimized using grid-search. The learned functions are very
similar; however, fitting KernelRidge is approximately seven times
faster than fitting SVR (both with grid-search).
However, prediction of 100000 target values is more than three times faster
with SVR since it has learned a sparse model using only
approximately 1/3 of the 100 training datapoints as support vectors.
The next figure compares the time for fitting and prediction of
KernelRidge and SVR for different sizes of the
training set. Fitting KernelRidge is faster than
SVR for medium-sized training sets (less than 1000
samples); however, for larger training sets SVR scales
better. With regard to prediction time, SVR is faster
than KernelRidge for all sizes of the training set because of the
learned sparse solution. Note that the degree of sparsity and thus the
prediction time depends on the parameters \(\epsilon\) and \(C\) of
the SVR; \(\epsilon = 0\) would correspond to a
dense model.
References:
[M2012]
“Machine Learning: A Probabilistic Perspective”
Murphy, K. P. - chapter 14.4.3, pp. 492-493, The MIT Press, 2012
© 2007 - 2024, scikit-learn developers (BSD License).
Show this page source
### 1.4. Support Vector Machines — scikit-learn 1.4.2 documentation

Install
User Guide
API
Examples
Community
Getting Started
Tutorial
What's new
Glossary
Development
FAQ
Support
Related packages
Roadmap
Governance
About us
GitHub
Other Versions and Download
More
Getting Started
Tutorial
What's new
Glossary
Development
FAQ
Support
Related packages
Roadmap
Governance
About us
GitHub
Other Versions and Download
Toggle Menu
PrevUp
Next
scikit-learn 1.4.2
Other versions
Please cite us if you use the software.
### 1.4. Support Vector Machines

- 1.4.1. Classification
1.4.1.1. Multi-class classification
1.4.1.2. Scores and probabilities
1.4.1.3. Unbalanced problems
- 1.4.2. Regression
- 1.4.3. Density estimation, novelty detection
- 1.4.4. Complexity
- 1.4.5. Tips on Practical Use
- 1.4.6. Kernel functions
1.4.6.1. Parameters of the RBF Kernel
1.4.6.2. Custom Kernels
- 1.4.7. Mathematical formulation
1.4.7.1. SVC
1.4.7.2. SVR
- 1.4.8. Implementation details
### 1.4. Support Vector Machines¶

Support vector machines (SVMs) are a set of supervised learning
methods used for classification,
regression and outliers detection.
The advantages of support vector machines are:
Effective in high dimensional spaces.
Still effective in cases where number of dimensions is greater
than the number of samples.
Uses a subset of training points in the decision function (called
support vectors), so it is also memory efficient.
Versatile: different Kernel functions can be
specified for the decision function. Common kernels are
provided, but it is also possible to specify custom kernels.
The disadvantages of support vector machines include:
If the number of features is much greater than the number of
samples, avoid over-fitting in choosing Kernel functions and regularization
term is crucial.
SVMs do not directly provide probability estimates, these are
calculated using an expensive five-fold cross-validation
(see Scores and probabilities, below).
The support vector machines in scikit-learn support both dense
(numpy.ndarray and convertible to that by numpy.asarray) and
sparse (any scipy.sparse) sample vectors as input. However, to use
an SVM to make predictions for sparse data, it must have been fit on such
data. For optimal performance, use C-ordered numpy.ndarray (dense) or
scipy.sparse.csr_matrix (sparse) with dtype=float64.
- 1.4.1. Classification¶
SVC, NuSVC and LinearSVC are classes
capable of performing binary and multi-class classification on a dataset.
SVC and NuSVC are similar methods, but accept slightly
different sets of parameters and have different mathematical formulations (see
section Mathematical formulation). On the other hand,
LinearSVC is another (faster) implementation of Support Vector
Classification for the case of a linear kernel. It also
lacks some of the attributes of SVC and NuSVC, like
support_. LinearSVC uses squared_hinge loss and due to its
implementation in liblinear it also regularizes the intercept, if considered.
This effect can however be reduced by carefully fine tuning its
intercept_scaling parameter, which allows the intercept term to have a
different regularization behavior compared to the other features. The
classification results and score can therefore differ from the other two
classifiers.
As other classifiers, SVC, NuSVC and
LinearSVC take as input two arrays: an array X of shape
(n_samples, n_features) holding the training samples, and an array y of
class labels (strings or integers), of shape (n_samples):
>>> from sklearn import svm
>>> X = [[0, 0], [1, 1]]
>>> y = [0, 1]
>>> clf = svm.SVC()
>>> clf.fit(X, y)
SVC()
After being fitted, the model can then be used to predict new values:
>>> clf.predict([[2., 2.]])
array([1])
SVMs decision function (detailed in the Mathematical formulation)
depends on some subset of the training data, called the support vectors. Some
properties of these support vectors can be found in attributes
support_vectors_, support_ and n_support_:
>>> # get support vectors
>>> clf.support_vectors_
array([[0., 0.],
[1., 1.]])
>>> # get indices of support vectors
>>> clf.support_
array([0, 1]...)
>>> # get number of support vectors for each class
>>> clf.n_support_
array([1, 1]...)
Examples:
SVM: Maximum margin separating hyperplane,
Non-linear SVM
SVM-Anova: SVM with univariate feature selection,
1.4.1.1. Multi-class classification¶
SVC and NuSVC implement the “one-versus-one”
approach for multi-class classification. In total,
n_classes * (n_classes - 1) / 2
classifiers are constructed and each one trains data from two classes.
To provide a consistent interface with other classifiers, the
decision_function_shape option allows to monotonically transform the
results of the “one-versus-one” classifiers to a “one-vs-rest” decision
function of shape (n_samples, n_classes).
>>> X = [[0], [1], [2], [3]]
>>> Y = [0, 1, 2, 3]
>>> clf = svm.SVC(decision_function_shape='ovo')
>>> clf.fit(X, Y)
SVC(decision_function_shape='ovo')
>>> dec = clf.decision_function([[1]])
>>> dec.shape[1] # 4 classes: 4*3/2 = 6
6
>>> clf.decision_function_shape = "ovr"
>>> dec = clf.decision_function([[1]])
>>> dec.shape[1] # 4 classes
4
On the other hand, LinearSVC implements “one-vs-the-rest”
multi-class strategy, thus training n_classes models.
>>> lin_clf = svm.LinearSVC(dual="auto")
>>> lin_clf.fit(X, Y)
LinearSVC(dual='auto')
>>> dec = lin_clf.decision_function([[1]])
>>> dec.shape[1]
4
See Mathematical formulation for a complete description of
the decision function.
Details on multi-class strategies
Click for more details
¶
Note that the LinearSVC also implements an alternative multi-class
strategy, the so-called multi-class SVM formulated by Crammer and Singer
[16], by using the option multi_class='crammer_singer'. In practice,
one-vs-rest classification is usually preferred, since the results are mostly
similar, but the runtime is significantly less.
For “one-vs-rest” LinearSVC the attributes coef_ and intercept_
have the shape (n_classes, n_features) and (n_classes,) respectively.
Each row of the coefficients corresponds to one of the n_classes
“one-vs-rest” classifiers and similar for the intercepts, in the
order of the “one” class.
In the case of “one-vs-one” SVC and NuSVC, the layout of
the attributes is a little more involved. In the case of a linear
kernel, the attributes coef_ and intercept_ have the shape
(n_classes * (n_classes - 1) / 2, n_features) and (n_classes *
(n_classes - 1) / 2) respectively. This is similar to the layout for
LinearSVC described above, with each row now corresponding
to a binary classifier. The order for classes
0 to n is “0 vs 1”, “0 vs 2” , … “0 vs n”, “1 vs 2”, “1 vs 3”, “1 vs n”, . .
. “n-1 vs n”.
The shape of dual_coef_ is (n_classes-1, n_SV) with
a somewhat hard to grasp layout.
The columns correspond to the support vectors involved in any
of the n_classes * (n_classes - 1) / 2 “one-vs-one” classifiers.
Each support vector v has a dual coefficient in each of the
n_classes - 1 classifiers comparing the class of v against another class.
Note that some, but not all, of these dual coefficients, may be zero.
The n_classes - 1 entries in each column are these dual coefficients,
ordered by the opposing class.
This might be clearer with an example: consider a three class problem with
class 0 having three support vectors
\(v^{0}_0, v^{1}_0, v^{2}_0\) and class 1 and 2 having two support vectors
\(v^{0}_1, v^{1}_1\) and \(v^{0}_2, v^{1}_2\) respectively.  For each
support vector \(v^{j}_i\), there are two dual coefficients.  Let’s call
the coefficient of support vector \(v^{j}_i\) in the classifier between
classes \(i\) and \(k\) \(\alpha^{j}_{i,k}\).
Then dual_coef_ looks like this:
\(\alpha^{0}_{0,1}\)
\(\alpha^{1}_{0,1}\)
\(\alpha^{2}_{0,1}\)
\(\alpha^{0}_{1,0}\)
\(\alpha^{1}_{1,0}\)
\(\alpha^{0}_{2,0}\)
\(\alpha^{1}_{2,0}\)
\(\alpha^{0}_{0,2}\)
\(\alpha^{1}_{0,2}\)
\(\alpha^{2}_{0,2}\)
\(\alpha^{0}_{1,2}\)
\(\alpha^{1}_{1,2}\)
\(\alpha^{0}_{2,1}\)
\(\alpha^{1}_{2,1}\)
Coefficients
for SVs of class 0
Coefficients
for SVs of class 1
Coefficients
for SVs of class 2
Examples:
Plot different SVM classifiers in the iris dataset,
1.4.1.2. Scores and probabilities¶
The decision_function method of SVC and NuSVC gives
per-class scores for each sample (or a single score per sample in the binary
case). When the constructor option probability is set to True,
class membership probability estimates (from the methods predict_proba and
predict_log_proba) are enabled. In the binary case, the probabilities are
calibrated using Platt scaling [9]: logistic regression on the SVM’s scores,
fit by an additional cross-validation on the training data.
In the multiclass case, this is extended as per [10].
Note
The same probability calibration procedure is available for all estimators
via the CalibratedClassifierCV (see
Probability calibration). In the case of SVC and NuSVC, this
procedure is builtin in libsvm which is used under the hood, so it does
not rely on scikit-learn’s
CalibratedClassifierCV.
The cross-validation involved in Platt scaling
is an expensive operation for large datasets.
In addition, the probability estimates may be inconsistent with the scores:
the “argmax” of the scores may not be the argmax of the probabilities
in binary classification, a sample may be labeled by predict as
belonging to the positive class even if the output of predict_proba is
less than 0.5; and similarly, it could be labeled as negative even if the
output of predict_proba is more than 0.5.
Platt’s method is also known to have theoretical issues.
If confidence scores are required, but these do not have to be probabilities,
then it is advisable to set probability=False
and use decision_function instead of predict_proba.
Please note that when decision_function_shape='ovr' and n_classes > 2,
unlike decision_function, the predict method does not try to break ties
by default. You can set break_ties=True for the output of predict to be
the same as np.argmax(clf.decision_function(...), axis=1), otherwise the
first class among the tied classes will always be returned; but have in mind
that it comes with a computational cost. See
SVM Tie Breaking Example for an example on
tie breaking.
1.4.1.3. Unbalanced problems¶
In problems where it is desired to give more importance to certain
classes or certain individual samples, the parameters class_weight and
sample_weight can be used.
SVC (but not NuSVC) implements the parameter
class_weight in the fit method. It’s a dictionary of the form
{class_label : value}, where value is a floating point number > 0
that sets the parameter C of class class_label to C * value.
The figure below illustrates the decision boundary of an unbalanced problem,
with and without weight correction.
SVC, NuSVC, SVR, NuSVR, LinearSVC,
LinearSVR and OneClassSVM implement also weights for
individual samples in the fit method through the sample_weight parameter.
Similar to class_weight, this sets the parameter C for the i-th
example to C * sample_weight[i], which will encourage the classifier to
get these samples right. The figure below illustrates the effect of sample
weighting on the decision boundary. The size of the circles is proportional
to the sample weights:
Examples:
SVM: Separating hyperplane for unbalanced classes
SVM: Weighted samples,
- 1.4.2. Regression¶
The method of Support Vector Classification can be extended to solve
regression problems. This method is called Support Vector Regression.
The model produced by support vector classification (as described
above) depends only on a subset of the training data, because the cost
function for building the model does not care about training points
that lie beyond the margin. Analogously, the model produced by Support
Vector Regression depends only on a subset of the training data,
because the cost function ignores samples whose prediction is close to their
target.
There are three different implementations of Support Vector Regression:
SVR, NuSVR and LinearSVR. LinearSVR
provides a faster implementation than SVR but only considers the
linear kernel, while NuSVR implements a slightly different formulation
than SVR and LinearSVR. Due to its implementation in
liblinear LinearSVR also regularizes the intercept, if considered.
This effect can however be reduced by carefully fine tuning its
intercept_scaling parameter, which allows the intercept term to have a
different regularization behavior compared to the other features. The
classification results and score can therefore differ from the other two
classifiers. See Implementation details for further details.
As with classification classes, the fit method will take as
argument vectors X, y, only that in this case y is expected to have
floating point values instead of integer values:
>>> from sklearn import svm
>>> X = [[0, 0], [2, 2]]
>>> y = [0.5, 2.5]
>>> regr = svm.SVR()
>>> regr.fit(X, y)
SVR()
>>> regr.predict([[1, 1]])
array([1.5])
Examples:
Support Vector Regression (SVR) using linear and non-linear kernels
- 1.4.3. Density estimation, novelty detection¶
The class OneClassSVM implements a One-Class SVM which is used in
outlier detection.
See Novelty and Outlier Detection for the description and usage of OneClassSVM.
- 1.4.4. Complexity¶
Support Vector Machines are powerful tools, but their compute and
storage requirements increase rapidly with the number of training
vectors. The core of an SVM is a quadratic programming problem (QP),
separating support vectors from the rest of the training data. The QP
solver used by the libsvm-based implementation scales between
\(O(n_{features} \times n_{samples}^2)\) and
\(O(n_{features} \times n_{samples}^3)\) depending on how efficiently
the libsvm cache is used in practice (dataset dependent). If the data
is very sparse \(n_{features}\) should be replaced by the average number
of non-zero features in a sample vector.
For the linear case, the algorithm used in
LinearSVC by the liblinear implementation is much more
efficient than its libsvm-based SVC counterpart and can
scale almost linearly to millions of samples and/or features.
- 1.4.5. Tips on Practical Use¶
Avoiding data copy: For SVC, SVR, NuSVC and
NuSVR, if the data passed to certain methods is not C-ordered
contiguous and double precision, it will be copied before calling the
underlying C implementation. You can check whether a given numpy array is
C-contiguous by inspecting its flags attribute.
For LinearSVC (and LogisticRegression) any input passed as a numpy
array will be copied and converted to the liblinear internal sparse data
representation (double precision floats and int32 indices of non-zero
components). If you want to fit a large-scale linear classifier without
copying a dense numpy C-contiguous double precision array as input, we
suggest to use the SGDClassifier class instead.  The objective
function can be configured to be almost the same as the LinearSVC
model.
Kernel cache size: For SVC, SVR, NuSVC and
NuSVR, the size of the kernel cache has a strong impact on run
times for larger problems.  If you have enough RAM available, it is
recommended to set cache_size to a higher value than the default of
200(MB), such as 500(MB) or 1000(MB).
Setting C: C is 1 by default and it’s a reasonable default
choice.  If you have a lot of noisy observations you should decrease it:
decreasing C corresponds to more regularization.
LinearSVC and LinearSVR are less sensitive to C when
it becomes large, and prediction results stop improving after a certain
threshold. Meanwhile, larger C values will take more time to train,
sometimes up to 10 times longer, as shown in [11].
Support Vector Machine algorithms are not scale invariant, so it
is highly recommended to scale your data. For example, scale each
attribute on the input vector X to [0,1] or [-1,+1], or standardize it
to have mean 0 and variance 1. Note that the same scaling must be
applied to the test vector to obtain meaningful results. This can be done
easily by using a Pipeline:
>>> from sklearn.pipeline import make_pipeline
>>> from sklearn.preprocessing import StandardScaler
>>> from sklearn.svm import SVC
>>> clf = make_pipeline(StandardScaler(), SVC())
See section Preprocessing data for more details on scaling and
normalization.
Regarding the shrinking parameter, quoting [12]: We found that if the
number of iterations is large, then shrinking can shorten the training
time. However, if we loosely solve the optimization problem (e.g., by
using a large stopping tolerance), the code without using shrinking may
be much faster
Parameter nu in NuSVC/OneClassSVM/NuSVR
approximates the fraction of training errors and support vectors.
In SVC, if the data is unbalanced (e.g. many
positive and few negative), set class_weight='balanced' and/or try
different penalty parameters C.
Randomness of the underlying implementations: The underlying
implementations of SVC and NuSVC use a random number
generator only to shuffle the data for probability estimation (when
probability is set to True). This randomness can be controlled
with the random_state parameter. If probability is set to False
these estimators are not random and random_state has no effect on the
results. The underlying OneClassSVM implementation is similar to
the ones of SVC and NuSVC. As no probability estimation
is provided for OneClassSVM, it is not random.
The underlying LinearSVC implementation uses a random number
generator to select features when fitting the model with a dual coordinate
descent (i.e. when dual is set to True). It is thus not uncommon
to have slightly different results for the same input data. If that
happens, try with a smaller tol parameter. This randomness can also be
controlled with the random_state parameter. When dual is
set to False the underlying implementation of LinearSVC is
not random and random_state has no effect on the results.
Using L1 penalization as provided by LinearSVC(penalty='l1',
dual=False) yields a sparse solution, i.e. only a subset of feature
weights is different from zero and contribute to the decision function.
Increasing C yields a more complex model (more features are selected).
The C value that yields a “null” model (all weights equal to zero) can
be calculated using l1_min_c.
- 1.4.6. Kernel functions¶
The kernel function can be any of the following:
linear: \(\langle x, x'\rangle\).
polynomial: \((\gamma \langle x, x'\rangle + r)^d\), where
\(d\) is specified by parameter degree, \(r\) by coef0.
rbf: \(\exp(-\gamma \|x-x'\|^2)\), where \(\gamma\) is
specified by parameter gamma, must be greater than 0.
sigmoid \(\tanh(\gamma \langle x,x'\rangle + r)\),
where \(r\) is specified by coef0.
Different kernels are specified by the kernel parameter:
>>> linear_svc = svm.SVC(kernel='linear')
>>> linear_svc.kernel
'linear'
>>> rbf_svc = svm.SVC(kernel='rbf')
>>> rbf_svc.kernel
'rbf'
See also Kernel Approximation for a solution to use RBF kernels that is much faster and more scalable.
1.4.6.1. Parameters of the RBF Kernel¶
When training an SVM with the Radial Basis Function (RBF) kernel, two
parameters must be considered: C and gamma.  The parameter C,
common to all SVM kernels, trades off misclassification of training examples
against simplicity of the decision surface. A low C makes the decision
surface smooth, while a high C aims at classifying all training examples
correctly.  gamma defines how much influence a single training example has.
The larger gamma is, the closer other examples must be to be affected.
Proper choice of C and gamma is critical to the SVM’s performance.  One
is advised to use GridSearchCV with
C and gamma spaced exponentially far apart to choose good values.
Examples:
RBF SVM parameters
Non-linear SVM
1.4.6.2. Custom Kernels¶
You can define your own kernels by either giving the kernel as a
python function or by precomputing the Gram matrix.
Classifiers with custom kernels behave the same way as any other
classifiers, except that:
Field support_vectors_ is now empty, only indices of support
vectors are stored in support_
A reference (and not a copy) of the first argument in the fit()
method is stored for future reference. If that array changes between the
use of fit() and predict() you will have unexpected results.
Using Python functions as kernels
Click for more details
¶
You can use your own defined kernels by passing a function to the
kernel parameter.
Your kernel must take as arguments two matrices of shape
(n_samples_1, n_features), (n_samples_2, n_features)
and return a kernel matrix of shape (n_samples_1, n_samples_2).
The following code defines a linear kernel and creates a classifier
instance that will use that kernel:
>>> import numpy as np
>>> from sklearn import svm
>>> def my_kernel(X, Y):
...     return np.dot(X, Y.T)
...
>>> clf = svm.SVC(kernel=my_kernel)
Using the Gram matrix
Click for more details
¶
You can pass pre-computed kernels by using the kernel='precomputed'
option. You should then pass Gram matrix instead of X to the fit and
predict methods. The kernel values between all training vectors and the
test vectors must be provided:
>>> import numpy as np
>>> from sklearn.datasets import make_classification
>>> from sklearn.model_selection import train_test_split
>>> from sklearn import svm
>>> X, y = make_classification(n_samples=10, random_state=0)
>>> X_train , X_test , y_train, y_test = train_test_split(X, y, random_state=0)
>>> clf = svm.SVC(kernel='precomputed')
>>> # linear kernel computation
>>> gram_train = np.dot(X_train, X_train.T)
>>> clf.fit(gram_train, y_train)
SVC(kernel='precomputed')
>>> # predict on training examples
>>> gram_test = np.dot(X_test, X_train.T)
>>> clf.predict(gram_test)
array([0, 1, 0])
Examples:
SVM with custom kernel.
- 1.4.7. Mathematical formulation¶
A support vector machine constructs a hyper-plane or set of hyper-planes in a
high or infinite dimensional space, which can be used for
classification, regression or other tasks. Intuitively, a good
separation is achieved by the hyper-plane that has the largest distance
to the nearest training data points of any class (so-called functional
margin), since in general the larger the margin the lower the
generalization error of the classifier. The figure below shows the decision
function for a linearly separable problem, with three samples on the
margin boundaries, called “support vectors”:
In general, when the problem isn’t linearly separable, the support vectors
are the samples within the margin boundaries.
We recommend [13] and [14] as good references for the theory and
practicalities of SVMs.
1.4.7.1. SVC¶
Given training vectors \(x_i \in \mathbb{R}^p\), i=1,…, n, in two classes, and a
vector \(y \in \{1, -1\}^n\), our goal is to find \(w \in
\mathbb{R}^p\) and \(b \in \mathbb{R}\) such that the prediction given by
\(\text{sign} (w^T\phi(x) + b)\) is correct for most samples.
SVC solves the following primal problem:
\[ \begin{align}\begin{aligned}\min_ {w, b, \zeta} \frac{1}{2} w^T w + C \sum_{i=1}^{n} \zeta_i\\\begin{split}\textrm {subject to } & y_i (w^T \phi (x_i) + b) \geq 1 - \zeta_i,\\
& \zeta_i \geq 0, i=1, ..., n\end{split}\end{aligned}\end{align} \]
Intuitively, we’re trying to maximize the margin (by minimizing
\(||w||^2 = w^Tw\)), while incurring a penalty when a sample is
misclassified or within the margin boundary. Ideally, the value \(y_i
(w^T \phi (x_i) + b)\) would be \(\geq 1\) for all samples, which
indicates a perfect prediction. But problems are usually not always perfectly
separable with a hyperplane, so we allow some samples to be at a distance \(\zeta_i\) from
their correct margin boundary. The penalty term C controls the strength of
this penalty, and as a result, acts as an inverse regularization parameter
(see note below).
The dual problem to the primal is
\[ \begin{align}\begin{aligned}\min_{\alpha} \frac{1}{2} \alpha^T Q \alpha - e^T \alpha\\\begin{split}
\textrm {subject to } & y^T \alpha = 0\\
& 0 \leq \alpha_i \leq C, i=1, ..., n\end{split}\end{aligned}\end{align} \]
where \(e\) is the vector of all ones,
and \(Q\) is an \(n\) by \(n\) positive semidefinite matrix,
\(Q_{ij} \equiv y_i y_j K(x_i, x_j)\), where \(K(x_i, x_j) = \phi (x_i)^T \phi (x_j)\)
is the kernel. The terms \(\alpha_i\) are called the dual coefficients,
and they are upper-bounded by \(C\).
This dual representation highlights the fact that training vectors are
implicitly mapped into a higher (maybe infinite)
dimensional space by the function \(\phi\): see kernel trick.
Once the optimization problem is solved, the output of
decision_function for a given sample \(x\) becomes:
\[\sum_{i\in SV} y_i \alpha_i K(x_i, x) + b,\]
and the predicted class correspond to its sign. We only need to sum over the
support vectors (i.e. the samples that lie within the margin) because the
dual coefficients \(\alpha_i\) are zero for the other samples.
These parameters can be accessed through the attributes dual_coef_
which holds the product \(y_i \alpha_i\), support_vectors_ which
holds the support vectors, and intercept_ which holds the independent
term \(b\)
Note
While SVM models derived from libsvm and liblinear use C as
regularization parameter, most other estimators use alpha. The exact
equivalence between the amount of regularization of two models depends on
the exact objective function optimized by the model. For example, when the
estimator used is Ridge regression,
the relation between them is given as \(C = \frac{1}{alpha}\).
LinearSVC
Click for more details
¶
The primal problem can be equivalently formulated as
\[\min_ {w, b} \frac{1}{2} w^T w + C \sum_{i=1}^{n}\max(0, 1 - y_i (w^T \phi(x_i) + b)),\]
where we make use of the hinge loss. This is the form that is
directly optimized by LinearSVC, but unlike the dual form, this one
does not involve inner products between samples, so the famous kernel trick
cannot be applied. This is why only the linear kernel is supported by
LinearSVC (\(\phi\) is the identity function).
NuSVC
Click for more details
¶
The \(\nu\)-SVC formulation [15] is a reparameterization of the
\(C\)-SVC and therefore mathematically equivalent.
We introduce a new parameter \(\nu\) (instead of \(C\)) which
controls the number of support vectors and margin errors:
\(\nu \in (0, 1]\) is an upper bound on the fraction of margin errors and
a lower bound of the fraction of support vectors. A margin error corresponds
to a sample that lies on the wrong side of its margin boundary: it is either
misclassified, or it is correctly classified but does not lie beyond the
margin.
1.4.7.2. SVR¶
Given training vectors \(x_i \in \mathbb{R}^p\), i=1,…, n, and a
vector \(y \in \mathbb{R}^n\) \(\varepsilon\)-SVR solves the following primal problem:
\[ \begin{align}\begin{aligned}\min_ {w, b, \zeta, \zeta^*} \frac{1}{2} w^T w + C \sum_{i=1}^{n} (\zeta_i + \zeta_i^*)\\\begin{split}\textrm {subject to } & y_i - w^T \phi (x_i) - b \leq \varepsilon + \zeta_i,\\
& w^T \phi (x_i) + b - y_i \leq \varepsilon + \zeta_i^*,\\
& \zeta_i, \zeta_i^* \geq 0, i=1, ..., n\end{split}\end{aligned}\end{align} \]
Here, we are penalizing samples whose prediction is at least \(\varepsilon\)
away from their true target. These samples penalize the objective by
\(\zeta_i\) or \(\zeta_i^*\), depending on whether their predictions
lie above or below the \(\varepsilon\) tube.
The dual problem is
\[ \begin{align}\begin{aligned}\min_{\alpha, \alpha^*} \frac{1}{2} (\alpha - \alpha^*)^T Q (\alpha - \alpha^*) + \varepsilon e^T (\alpha + \alpha^*) - y^T (\alpha - \alpha^*)\\\begin{split}
\textrm {subject to } & e^T (\alpha - \alpha^*) = 0\\
& 0 \leq \alpha_i, \alpha_i^* \leq C, i=1, ..., n\end{split}\end{aligned}\end{align} \]
where \(e\) is the vector of all ones,
\(Q\) is an \(n\) by \(n\) positive semidefinite matrix,
\(Q_{ij} \equiv K(x_i, x_j) = \phi (x_i)^T \phi (x_j)\)
is the kernel. Here training vectors are implicitly mapped into a higher
(maybe infinite) dimensional space by the function \(\phi\).
The prediction is:
\[\sum_{i \in SV}(\alpha_i - \alpha_i^*) K(x_i, x) + b\]
These parameters can be accessed through the attributes dual_coef_
which holds the difference \(\alpha_i - \alpha_i^*\), support_vectors_ which
holds the support vectors, and intercept_ which holds the independent
term \(b\)
LinearSVR
Click for more details
¶
The primal problem can be equivalently formulated as
\[\min_ {w, b} \frac{1}{2} w^T w + C \sum_{i=1}^{n}\max(0, |y_i - (w^T \phi(x_i) + b)| - \varepsilon),\]
where we make use of the epsilon-insensitive loss, i.e. errors of less than
\(\varepsilon\) are ignored. This is the form that is directly optimized
by LinearSVR.
- 1.4.8. Implementation details¶
Internally, we use libsvm [12] and liblinear [11] to handle all
computations. These libraries are wrapped using C and Cython.
For a description of the implementation and details of the algorithms
used, please refer to their respective papers.
References:
[9]
Platt “Probabilistic outputs for SVMs and comparisons to
regularized likelihood methods”.
[10]
Wu, Lin and Weng, “Probability estimates for multi-class
classification by pairwise coupling”, JMLR
5:975-1005, 2004.
[11]
(1,2)
Fan, Rong-En, et al.,
“LIBLINEAR: A library for large linear classification.”,
Journal of machine learning research 9.Aug (2008): 1871-1874.
[12]
(1,2)
Chang and Lin, LIBSVM: A Library for Support Vector Machines.
[13]
Bishop, Pattern recognition and machine learning,
chapter 7 Sparse Kernel Machines
[14]
“A Tutorial on Support Vector Regression”
Alex J. Smola, Bernhard Schölkopf - Statistics and Computing archive
Volume 14 Issue 3, August 2004, p. 199-222.
[15]
Schölkopf et. al New Support Vector Algorithms
[16]
Crammer and Singer On the Algorithmic Implementation ofMulticlass
Kernel-based Vector Machines,
JMLR 2001.
© 2007 - 2024, scikit-learn developers (BSD License).
Show this page source
Click to add a cell.
### 1.5. Stochastic Gradient Descent — scikit-learn 1.4.2 documentation

Install
User Guide
API
Examples
Community
Getting Started
Tutorial
What's new
Glossary
Development
FAQ
Support
Related packages
Roadmap
Governance
About us
GitHub
Other Versions and Download
More
Getting Started
Tutorial
What's new
Glossary
Development
FAQ
Support
Related packages
Roadmap
Governance
About us
GitHub
Other Versions and Download
Toggle Menu
PrevUp
Next
scikit-learn 1.4.2
Other versions
Please cite us if you use the software.
### 1.5. Stochastic Gradient Descent

- 1.5.1. Classification
- 1.5.2. Regression
- 1.5.3. Online One-Class SVM
- 1.5.4. Stochastic Gradient Descent for sparse data
- 1.5.5. Complexity
- 1.5.6. Stopping criterion
- 1.5.7. Tips on Practical Use
- 1.5.8. Mathematical formulation
1.5.8.1. SGD
- 1.5.9. Implementation details
### 1.5. Stochastic Gradient Descent¶

Stochastic Gradient Descent (SGD) is a simple yet very efficient
approach to fitting linear classifiers and regressors under
convex loss functions such as (linear) Support Vector Machines and Logistic
Regression.
Even though SGD has been around in the machine learning community for
a long time, it has received a considerable amount of attention just
recently in the context of large-scale learning.
SGD has been successfully applied to large-scale and sparse machine
learning problems often encountered in text classification and natural
language processing.  Given that the data is sparse, the classifiers
in this module easily scale to problems with more than 10^5 training
examples and more than 10^5 features.
Strictly speaking, SGD is merely an optimization technique and does not
correspond to a specific family of machine learning models. It is only a
way to train a model. Often, an instance of SGDClassifier or
SGDRegressor will have an equivalent estimator in
the scikit-learn API, potentially using a different optimization technique.
For example, using SGDClassifier(loss='log_loss') results in logistic regression,
i.e. a model equivalent to LogisticRegression
which is fitted via SGD instead of being fitted by one of the other solvers
in LogisticRegression. Similarly,
SGDRegressor(loss='squared_error', penalty='l2') and
Ridge solve the same optimization problem, via
different means.
The advantages of Stochastic Gradient Descent are:
Efficiency.
Ease of implementation (lots of opportunities for code tuning).
The disadvantages of Stochastic Gradient Descent include:
SGD requires a number of hyperparameters such as the regularization
parameter and the number of iterations.
SGD is sensitive to feature scaling.
Warning
Make sure you permute (shuffle) your training data before fitting the model
or use shuffle=True to shuffle after each iteration (used by default).
Also, ideally, features should be standardized using e.g.
make_pipeline(StandardScaler(), SGDClassifier()) (see Pipelines).
- 1.5.1. Classification¶
The class SGDClassifier implements a plain stochastic gradient
descent learning routine which supports different loss functions and
penalties for classification. Below is the decision boundary of a
SGDClassifier trained with the hinge loss, equivalent to a linear SVM.
As other classifiers, SGD has to be fitted with two arrays: an array X
of shape (n_samples, n_features) holding the training samples, and an
array y of shape (n_samples,) holding the target values (class labels)
for the training samples:
>>> from sklearn.linear_model import SGDClassifier
>>> X = [[0., 0.], [1., 1.]]
>>> y = [0, 1]
>>> clf = SGDClassifier(loss="hinge", penalty="l2", max_iter=5)
>>> clf.fit(X, y)
SGDClassifier(max_iter=5)
After being fitted, the model can then be used to predict new values:
>>> clf.predict([[2., 2.]])
array([1])
SGD fits a linear model to the training data. The coef_ attribute holds
the model parameters:
>>> clf.coef_
array([[9.9..., 9.9...]])
The intercept_ attribute holds the intercept (aka offset or bias):
>>> clf.intercept_
array([-9.9...])
Whether or not the model should use an intercept, i.e. a biased
hyperplane, is controlled by the parameter fit_intercept.
The signed distance to the hyperplane (computed as the dot product between
the coefficients and the input sample, plus the intercept) is given by
SGDClassifier.decision_function:
>>> clf.decision_function([[2., 2.]])
array([29.6...])
The concrete loss function can be set via the loss
parameter. SGDClassifier supports the following loss functions:
loss="hinge": (soft-margin) linear Support Vector Machine,
loss="modified_huber": smoothed hinge loss,
loss="log_loss": logistic regression,
and all regression losses below. In this case the target is encoded as -1
or 1, and the problem is treated as a regression problem. The predicted
class then correspond to the sign of the predicted target.
Please refer to the mathematical section below for formulas.
The first two loss functions are lazy, they only update the model
parameters if an example violates the margin constraint, which makes
training very efficient and may result in sparser models (i.e. with more zero
coefficients), even when L2 penalty is used.
Using loss="log_loss" or loss="modified_huber" enables the
predict_proba method, which gives a vector of probability estimates
\(P(y|x)\) per sample \(x\):
>>> clf = SGDClassifier(loss="log_loss", max_iter=5).fit(X, y)
>>> clf.predict_proba([[1., 1.]])
array([[0.00..., 0.99...]])
The concrete penalty can be set via the penalty parameter.
SGD supports the following penalties:
penalty="l2": L2 norm penalty on coef_.
penalty="l1": L1 norm penalty on coef_.
penalty="elasticnet": Convex combination of L2 and L1;
(1 - l1_ratio) * L2 + l1_ratio * L1.
The default setting is penalty="l2". The L1 penalty leads to sparse
solutions, driving most coefficients to zero. The Elastic Net [11] solves
some deficiencies of the L1 penalty in the presence of highly correlated
attributes. The parameter l1_ratio controls the convex combination
of L1 and L2 penalty.
SGDClassifier supports multi-class classification by combining
multiple binary classifiers in a “one versus all” (OVA) scheme. For each
of the \(K\) classes, a binary classifier is learned that discriminates
between that and all other \(K-1\) classes. At testing time, we compute the
confidence score (i.e. the signed distances to the hyperplane) for each
classifier and choose the class with the highest confidence. The Figure
below illustrates the OVA approach on the iris dataset.  The dashed
lines represent the three OVA classifiers; the background colors show
the decision surface induced by the three classifiers.
In the case of multi-class classification coef_ is a two-dimensional
array of shape (n_classes, n_features) and intercept_ is a
one-dimensional array of shape (n_classes,). The i-th row of coef_ holds
the weight vector of the OVA classifier for the i-th class; classes are
indexed in ascending order (see attribute classes_).
Note that, in principle, since they allow to create a probability model,
loss="log_loss" and loss="modified_huber" are more suitable for
one-vs-all classification.
SGDClassifier supports both weighted classes and weighted
instances via the fit parameters class_weight and sample_weight. See
the examples below and the docstring of SGDClassifier.fit for
further information.
SGDClassifier supports averaged SGD (ASGD) [10]. Averaging can be
enabled by setting average=True. ASGD performs the same updates as the
regular SGD (see Mathematical formulation), but instead of using
the last value of the coefficients as the coef_ attribute (i.e. the values
of the last update), coef_ is set instead to the average value of the
coefficients across all updates. The same is done for the intercept_
attribute. When using ASGD the learning rate can be larger and even constant,
leading on some datasets to a speed up in training time.
For classification with a logistic loss, another variant of SGD with an
averaging strategy is available with Stochastic Average Gradient (SAG)
algorithm, available as a solver in LogisticRegression.
Examples:
SGD: Maximum margin separating hyperplane,
Plot multi-class SGD on the iris dataset
SGD: Weighted samples
Comparing various online solvers
SVM: Separating hyperplane for unbalanced classes
(See the Note in the example)
- 1.5.2. Regression¶
The class SGDRegressor implements a plain stochastic gradient
descent learning routine which supports different loss functions and
penalties to fit linear regression models. SGDRegressor is
well suited for regression problems with a large number of training
samples (> 10.000), for other problems we recommend Ridge,
Lasso, or ElasticNet.
The concrete loss function can be set via the loss
parameter. SGDRegressor supports the following loss functions:
loss="squared_error": Ordinary least squares,
loss="huber": Huber loss for robust regression,
loss="epsilon_insensitive": linear Support Vector Regression.
Please refer to the mathematical section below for formulas.
The Huber and epsilon-insensitive loss functions can be used for
robust regression. The width of the insensitive region has to be
specified via the parameter epsilon. This parameter depends on the
scale of the target variables.
The penalty parameter determines the regularization to be used (see
description above in the classification section).
SGDRegressor also supports averaged SGD [10] (here again, see
description above in the classification section).
For regression with a squared loss and a l2 penalty, another variant of
SGD with an averaging strategy is available with Stochastic Average
Gradient (SAG) algorithm, available as a solver in Ridge.
- 1.5.3. Online One-Class SVM¶
The class sklearn.linear_model.SGDOneClassSVM implements an online
linear version of the One-Class SVM using a stochastic gradient descent.
Combined with kernel approximation techniques,
sklearn.linear_model.SGDOneClassSVM can be used to approximate the
solution of a kernelized One-Class SVM, implemented in
sklearn.svm.OneClassSVM, with a linear complexity in the number of
samples. Note that the complexity of a kernelized One-Class SVM is at best
quadratic in the number of samples.
sklearn.linear_model.SGDOneClassSVM is thus well suited for datasets
with a large number of training samples (> 10,000) for which the SGD
variant can be several orders of magnitude faster.
Mathematical details
Click for more details
¶
Its implementation is based on the implementation of the stochastic
gradient descent. Indeed, the original optimization problem of the One-Class
SVM is given by
\[\begin{split}\begin{aligned}
\min_{w, \rho, \xi} & \quad \frac{1}{2}\Vert w \Vert^2 - \rho + \frac{1}{\nu n} \sum_{i=1}^n \xi_i \\
\text{s.t.} & \quad \langle w, x_i \rangle \geq \rho - \xi_i \quad 1 \leq i \leq n \\
& \quad \xi_i \geq 0 \quad 1 \leq i \leq n
\end{aligned}\end{split}\]
where \(\nu \in (0, 1]\) is the user-specified parameter controlling the
proportion of outliers and the proportion of support vectors. Getting rid of
the slack variables \(\xi_i\) this problem is equivalent to
\[\min_{w, \rho} \frac{1}{2}\Vert w \Vert^2 - \rho + \frac{1}{\nu n} \sum_{i=1}^n \max(0, \rho - \langle w, x_i \rangle) \, .\]
Multiplying by the constant \(\nu\) and introducing the intercept
\(b = 1 - \rho\) we obtain the following equivalent optimization problem
\[\min_{w, b} \frac{\nu}{2}\Vert w \Vert^2 + b\nu + \frac{1}{n} \sum_{i=1}^n \max(0, 1 - (\langle w, x_i \rangle + b)) \, .\]
This is similar to the optimization problems studied in section
Mathematical formulation with \(y_i = 1, 1 \leq i \leq n\) and
\(\alpha = \nu/2\), \(L\) being the hinge loss function and \(R\)
being the L2 norm. We just need to add the term \(b\nu\) in the
optimization loop.
As SGDClassifier and SGDRegressor, SGDOneClassSVM
supports averaged SGD. Averaging can be enabled by setting average=True.
- 1.5.4. Stochastic Gradient Descent for sparse data¶
Note
The sparse implementation produces slightly different results
from the dense implementation, due to a shrunk learning rate for the
intercept. See Implementation details.
There is built-in support for sparse data given in any matrix in a format
supported by scipy.sparse. For maximum
efficiency, however, use the CSR
matrix format as defined in scipy.sparse.csr_matrix.
Examples:
Classification of text documents using sparse features
- 1.5.5. Complexity¶
The major advantage of SGD is its efficiency, which is basically
linear in the number of training examples. If X is a matrix of size (n, p)
training has a cost of \(O(k n \bar p)\), where k is the number
of iterations (epochs) and \(\bar p\) is the average number of
non-zero attributes per sample.
Recent theoretical results, however, show that the runtime to get some
desired optimization accuracy does not increase as the training set size increases.
- 1.5.6. Stopping criterion¶
The classes SGDClassifier and SGDRegressor provide two
criteria to stop the algorithm when a given level of convergence is reached:
With early_stopping=True, the input data is split into a training set
and a validation set. The model is then fitted on the training set, and the
stopping criterion is based on the prediction score (using the score
method) computed on the validation set. The size of the validation set
can be changed with the parameter validation_fraction.
With early_stopping=False, the model is fitted on the entire input data
and the stopping criterion is based on the objective function computed on
the training data.
In both cases, the criterion is evaluated once by epoch, and the algorithm stops
when the criterion does not improve n_iter_no_change times in a row. The
improvement is evaluated with absolute tolerance tol, and the algorithm
stops in any case after a maximum number of iteration max_iter.
- 1.5.7. Tips on Practical Use¶
Stochastic Gradient Descent is sensitive to feature scaling, so it
is highly recommended to scale your data. For example, scale each
attribute on the input vector X to [0,1] or [-1,+1], or standardize
it to have mean 0 and variance 1. Note that the same scaling must be
applied to the test vector to obtain meaningful results. This can be easily
done using StandardScaler:
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
scaler.fit(X_train)  # Don't cheat - fit only on training data
X_train = scaler.transform(X_train)
X_test = scaler.transform(X_test)  # apply same transformation to test data
# Or better yet: use a pipeline!
from sklearn.pipeline import make_pipeline
est = make_pipeline(StandardScaler(), SGDClassifier())
est.fit(X_train)
est.predict(X_test)
If your attributes have an intrinsic scale (e.g. word frequencies or
indicator features) scaling is not needed.
Finding a reasonable regularization term \(\alpha\) is
best done using automatic hyper-parameter search, e.g.
GridSearchCV or
RandomizedSearchCV, usually in the
range 10.0**-np.arange(1,7).
Empirically, we found that SGD converges after observing
approximately 10^6 training samples. Thus, a reasonable first guess
for the number of iterations is max_iter = np.ceil(10**6 / n),
where n is the size of the training set.
If you apply SGD to features extracted using PCA we found that
it is often wise to scale the feature values by some constant c
such that the average L2 norm of the training data equals one.
We found that Averaged SGD works best with a larger number of features
and a higher eta0.
References:
“Efficient BackProp”
Y. LeCun, L. Bottou, G. Orr, K. Müller - In Neural Networks: Tricks
of the Trade 1998.
- 1.5.8. Mathematical formulation¶
We describe here the mathematical details of the SGD procedure. A good
overview with convergence rates can be found in [12].
Given a set of training examples \((x_1, y_1), \ldots, (x_n, y_n)\) where
\(x_i \in \mathbf{R}^m\) and \(y_i \in \mathcal{R}\) (\(y_i \in
{-1, 1}\) for classification), our goal is to learn a linear scoring function
\(f(x) = w^T x + b\) with model parameters \(w \in \mathbf{R}^m\) and
intercept \(b \in \mathbf{R}\). In order to make predictions for binary
classification, we simply look at the sign of \(f(x)\). To find the model
parameters, we minimize the regularized training error given by
\[E(w,b) = \frac{1}{n}\sum_{i=1}^{n} L(y_i, f(x_i)) + \alpha R(w)\]
where \(L\) is a loss function that measures model (mis)fit and
\(R\) is a regularization term (aka penalty) that penalizes model
complexity; \(\alpha > 0\) is a non-negative hyperparameter that controls
the regularization strength.
Loss functions details
Click for more details
¶
Different choices for \(L\) entail different classifiers or regressors:
Hinge (soft-margin): equivalent to Support Vector Classification.
\(L(y_i, f(x_i)) = \max(0, 1 - y_i f(x_i))\).
Perceptron:
\(L(y_i, f(x_i)) = \max(0, - y_i f(x_i))\).
Modified Huber:
\(L(y_i, f(x_i)) = \max(0, 1 - y_i f(x_i))^2\) if \(y_i f(x_i) >
-1\), and \(L(y_i, f(x_i)) = -4 y_i f(x_i)\) otherwise.
Log Loss: equivalent to Logistic Regression.
\(L(y_i, f(x_i)) = \log(1 + \exp (-y_i f(x_i)))\).
Squared Error: Linear regression (Ridge or Lasso depending on
\(R\)).
\(L(y_i, f(x_i)) = \frac{1}{2}(y_i - f(x_i))^2\).
Huber: less sensitive to outliers than least-squares. It is equivalent to
least squares when \(|y_i - f(x_i)| \leq \varepsilon\), and
\(L(y_i, f(x_i)) = \varepsilon |y_i - f(x_i)| - \frac{1}{2}
\varepsilon^2\) otherwise.
Epsilon-Insensitive: (soft-margin) equivalent to Support Vector Regression.
\(L(y_i, f(x_i)) = \max(0, |y_i - f(x_i)| - \varepsilon)\).
All of the above loss functions can be regarded as an upper bound on the
misclassification error (Zero-one loss) as shown in the Figure below.
Popular choices for the regularization term \(R\) (the penalty
parameter) include:
L2 norm: \(R(w) := \frac{1}{2} \sum_{j=1}^{m} w_j^2 = ||w||_2^2\),
L1 norm: \(R(w) := \sum_{j=1}^{m} |w_j|\), which leads to sparse
solutions.
Elastic Net: \(R(w) := \frac{\rho}{2} \sum_{j=1}^{n} w_j^2 +
(1-\rho) \sum_{j=1}^{m} |w_j|\), a convex combination of L2 and L1, where
\(\rho\) is given by 1 - l1_ratio.
The Figure below shows the contours of the different regularization terms
in a 2-dimensional parameter space (\(m=2\)) when \(R(w) = 1\).
1.5.8.1. SGD¶
Stochastic gradient descent is an optimization method for unconstrained
optimization problems. In contrast to (batch) gradient descent, SGD
approximates the true gradient of \(E(w,b)\) by considering a
single training example at a time.
The class SGDClassifier implements a first-order SGD learning
routine.  The algorithm iterates over the training examples and for each
example updates the model parameters according to the update rule given by
\[w \leftarrow w - \eta \left[\alpha \frac{\partial R(w)}{\partial w}
+ \frac{\partial L(w^T x_i + b, y_i)}{\partial w}\right]\]
where \(\eta\) is the learning rate which controls the step-size in
the parameter space.  The intercept \(b\) is updated similarly but
without regularization (and with additional decay for sparse matrices, as
detailed in Implementation details).
The learning rate \(\eta\) can be either constant or gradually decaying. For
classification, the default learning rate schedule (learning_rate='optimal')
is given by
\[\eta^{(t)} = \frac {1}{\alpha  (t_0 + t)}\]
where \(t\) is the time step (there are a total of n_samples * n_iter
time steps), \(t_0\) is determined based on a heuristic proposed by Léon Bottou
such that the expected initial updates are comparable with the expected
size of the weights (this assuming that the norm of the training samples is
approx. 1). The exact definition can be found in _init_t in BaseSGD.
For regression the default learning rate schedule is inverse scaling
(learning_rate='invscaling'), given by
\[\eta^{(t)} = \frac{eta_0}{t^{power\_t}}\]
where \(eta_0\) and \(power\_t\) are hyperparameters chosen by the
user via eta0 and power_t, resp.
For a constant learning rate use learning_rate='constant' and use eta0
to specify the learning rate.
For an adaptively decreasing learning rate, use learning_rate='adaptive'
and use eta0 to specify the starting learning rate. When the stopping
criterion is reached, the learning rate is divided by 5, and the algorithm
does not stop. The algorithm stops when the learning rate goes below 1e-6.
The model parameters can be accessed through the coef_ and
intercept_ attributes: coef_ holds the weights \(w\) and
intercept_ holds \(b\).
When using Averaged SGD (with the average parameter), coef_ is set to the
average weight across all updates:
coef_ \(= \frac{1}{T} \sum_{t=0}^{T-1} w^{(t)}\),
where \(T\) is the total number of updates, found in the t_ attribute.
- 1.5.9. Implementation details¶
The implementation of SGD is influenced by the Stochastic Gradient SVM of
[7].
Similar to SvmSGD,
the weight vector is represented as the product of a scalar and a vector
which allows an efficient weight update in the case of L2 regularization.
In the case of sparse input X, the intercept is updated with a
smaller learning rate (multiplied by 0.01) to account for the fact that
it is updated more frequently. Training examples are picked up sequentially
and the learning rate is lowered after each observed example. We adopted the
learning rate schedule from [8].
For multi-class classification, a “one versus all” approach is used.
We use the truncated gradient algorithm proposed in [9]
for L1 regularization (and the Elastic Net).
The code is written in Cython.
References:
[7]
“Stochastic Gradient Descent” L. Bottou - Website, 2010.
[8]
“Pegasos: Primal estimated sub-gradient solver for svm”
S. Shalev-Shwartz, Y. Singer, N. Srebro - In Proceedings of ICML ‘07.
[9]
“Stochastic gradient descent training for l1-regularized
log-linear models with cumulative penalty”
Y. Tsuruoka, J. Tsujii, S. Ananiadou - In Proceedings of the AFNLP/ACL
‘09.
[10]
(1,2)
“Towards Optimal One Pass Large Scale Learning with
Averaged Stochastic Gradient Descent”
Xu, Wei (2011)
[11]
“Regularization and variable selection via the elastic net”
H. Zou, T. Hastie - Journal of the Royal Statistical Society Series B,
67 (2), 301-320.
[12]
“Solving large scale linear prediction problems using stochastic
gradient descent algorithms”
T. Zhang - In Proceedings of ICML ‘04.
© 2007 - 2024, scikit-learn developers (BSD License).
Show this page source
### 1.6. Nearest Neighbors — scikit-learn 1.4.2 documentation

Install
User Guide
API
Examples
Community
Getting Started
Tutorial
What's new
Glossary
Development
FAQ
Support
Related packages
Roadmap
Governance
About us
GitHub
Other Versions and Download
More
Getting Started
Tutorial
What's new
Glossary
Development
FAQ
Support
Related packages
Roadmap
Governance
About us
GitHub
Other Versions and Download
Toggle Menu
PrevUp
Next
scikit-learn 1.4.2
Other versions
Please cite us if you use the software.
### 1.6. Nearest Neighbors

- 1.6.1. Unsupervised Nearest Neighbors
1.6.1.1. Finding the Nearest Neighbors
1.6.1.2. KDTree and BallTree Classes
- 1.6.2. Nearest Neighbors Classification
- 1.6.3. Nearest Neighbors Regression
- 1.6.4. Nearest Neighbor Algorithms
1.6.4.1. Brute Force
1.6.4.2. K-D Tree
1.6.4.3. Ball Tree
1.6.4.4. Choice of Nearest Neighbors Algorithm
1.6.4.5. Effect of leaf_size
1.6.4.6. Valid Metrics for Nearest Neighbor Algorithms
- 1.6.5. Nearest Centroid Classifier
1.6.5.1. Nearest Shrunken Centroid
- 1.6.6. Nearest Neighbors Transformer
- 1.6.7. Neighborhood Components Analysis
1.6.7.1. Classification
1.6.7.2. Dimensionality reduction
1.6.7.3. Mathematical formulation
1.6.7.3.1. Mahalanobis distance
1.6.7.4. Implementation
1.6.7.5. Complexity
1.6.7.5.1. Training
1.6.7.5.2. Transform
### 1.6. Nearest Neighbors¶

sklearn.neighbors provides functionality for unsupervised and
supervised neighbors-based learning methods.  Unsupervised nearest neighbors
is the foundation of many other learning methods,
notably manifold learning and spectral clustering.  Supervised neighbors-based
learning comes in two flavors: classification for data with
discrete labels, and regression for data with continuous labels.
The principle behind nearest neighbor methods is to find a predefined number
of training samples closest in distance to the new point, and
predict the label from these.  The number of samples can be a user-defined
constant (k-nearest neighbor learning), or vary based
on the local density of points (radius-based neighbor learning).
The distance can, in general, be any metric measure: standard Euclidean
distance is the most common choice.
Neighbors-based methods are known as non-generalizing machine
learning methods, since they simply “remember” all of its training data
(possibly transformed into a fast indexing structure such as a
Ball Tree or KD Tree).
Despite its simplicity, nearest neighbors has been successful in a
large number of classification and regression problems, including
handwritten digits and satellite image scenes. Being a non-parametric method,
it is often successful in classification situations where the decision
boundary is very irregular.
The classes in sklearn.neighbors can handle either NumPy arrays or
scipy.sparse matrices as input.  For dense matrices, a large number of
possible distance metrics are supported.  For sparse matrices, arbitrary
Minkowski metrics are supported for searches.
There are many learning routines which rely on nearest neighbors at their
core.  One example is kernel density estimation,
discussed in the density estimation section.
- 1.6.1. Unsupervised Nearest Neighbors¶
NearestNeighbors implements unsupervised nearest neighbors learning.
It acts as a uniform interface to three different nearest neighbors
algorithms: BallTree, KDTree, and a
brute-force algorithm based on routines in sklearn.metrics.pairwise.
The choice of neighbors search algorithm is controlled through the keyword
'algorithm', which must be one of
['auto', 'ball_tree', 'kd_tree', 'brute'].  When the default value
'auto' is passed, the algorithm attempts to determine the best approach
from the training data.  For a discussion of the strengths and weaknesses
of each option, see Nearest Neighbor Algorithms.
Warning
Regarding the Nearest Neighbors algorithms, if two
neighbors \(k+1\) and \(k\) have identical distances
but different labels, the result will depend on the ordering of the
training data.
1.6.1.1. Finding the Nearest Neighbors¶
For the simple task of finding the nearest neighbors between two sets of
data, the unsupervised algorithms within sklearn.neighbors can be
used:
>>> from sklearn.neighbors import NearestNeighbors
>>> import numpy as np
>>> X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])
>>> nbrs = NearestNeighbors(n_neighbors=2, algorithm='ball_tree').fit(X)
>>> distances, indices = nbrs.kneighbors(X)
>>> indices
array([[0, 1],
[1, 0],
[2, 1],
[3, 4],
[4, 3],
[5, 4]]...)
>>> distances
array([[0.        , 1.        ],
[0.        , 1.        ],
[0.        , 1.41421356],
[0.        , 1.        ],
[0.        , 1.        ],
[0.        , 1.41421356]])
Because the query set matches the training set, the nearest neighbor of each
point is the point itself, at a distance of zero.
It is also possible to efficiently produce a sparse graph showing the
connections between neighboring points:
>>> nbrs.kneighbors_graph(X).toarray()
array([[1., 1., 0., 0., 0., 0.],
[1., 1., 0., 0., 0., 0.],
[0., 1., 1., 0., 0., 0.],
[0., 0., 0., 1., 1., 0.],
[0., 0., 0., 1., 1., 0.],
[0., 0., 0., 0., 1., 1.]])
The dataset is structured such that points nearby in index order are nearby
in parameter space, leading to an approximately block-diagonal matrix of
K-nearest neighbors.  Such a sparse graph is useful in a variety of
circumstances which make use of spatial relationships between points for
unsupervised learning: in particular, see Isomap,
LocallyLinearEmbedding, and
SpectralClustering.
1.6.1.2. KDTree and BallTree Classes¶
Alternatively, one can use the KDTree or BallTree classes
directly to find nearest neighbors.  This is the functionality wrapped by
the NearestNeighbors class used above.  The Ball Tree and KD Tree
have the same interface; we’ll show an example of using the KD Tree here:
>>> from sklearn.neighbors import KDTree
>>> import numpy as np
>>> X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])
>>> kdt = KDTree(X, leaf_size=30, metric='euclidean')
>>> kdt.query(X, k=2, return_distance=False)
array([[0, 1],
[1, 0],
[2, 1],
[3, 4],
[4, 3],
[5, 4]]...)
Refer to the KDTree and BallTree class documentation
for more information on the options available for nearest neighbors searches,
including specification of query strategies, distance metrics, etc. For a list
of valid metrics use KDTree.valid_metrics and BallTree.valid_metrics:
>>> from sklearn.neighbors import KDTree, BallTree
>>> KDTree.valid_metrics
['euclidean', 'l2', 'minkowski', 'p', 'manhattan', 'cityblock', 'l1', 'chebyshev', 'infinity']
>>> BallTree.valid_metrics
['euclidean', 'l2', 'minkowski', 'p', 'manhattan', 'cityblock', 'l1', 'chebyshev', 'infinity', 'seuclidean', 'mahalanobis', 'hamming', 'canberra', 'braycurtis', 'jaccard', 'dice', 'rogerstanimoto', 'russellrao', 'sokalmichener', 'sokalsneath', 'haversine', 'pyfunc']
- 1.6.2. Nearest Neighbors Classification¶
Neighbors-based classification is a type of instance-based learning or
non-generalizing learning: it does not attempt to construct a general
internal model, but simply stores instances of the training data.
Classification is computed from a simple majority vote of the nearest
neighbors of each point: a query point is assigned the data class which
has the most representatives within the nearest neighbors of the point.
scikit-learn implements two different nearest neighbors classifiers:
KNeighborsClassifier implements learning based on the \(k\)
nearest neighbors of each query point, where \(k\) is an integer value
specified by the user.  RadiusNeighborsClassifier implements learning
based on the number of neighbors within a fixed radius \(r\) of each
training point, where \(r\) is a floating-point value specified by
the user.
The \(k\)-neighbors classification in KNeighborsClassifier
is the most commonly used technique. The optimal choice of the value \(k\)
is highly data-dependent: in general a larger \(k\) suppresses the effects
of noise, but makes the classification boundaries less distinct.
In cases where the data is not uniformly sampled, radius-based neighbors
classification in RadiusNeighborsClassifier can be a better choice.
The user specifies a fixed radius \(r\), such that points in sparser
neighborhoods use fewer nearest neighbors for the classification.  For
high-dimensional parameter spaces, this method becomes less effective due
to the so-called “curse of dimensionality”.
The basic nearest neighbors classification uses uniform weights: that is, the
value assigned to a query point is computed from a simple majority vote of
the nearest neighbors.  Under some circumstances, it is better to weight the
neighbors such that nearer neighbors contribute more to the fit.  This can
be accomplished through the weights keyword.  The default value,
weights = 'uniform', assigns uniform weights to each neighbor.
weights = 'distance' assigns weights proportional to the inverse of the
distance from the query point.  Alternatively, a user-defined function of the
distance can be supplied to compute the weights.
Examples:
Nearest Neighbors Classification: an example of
classification using nearest neighbors.
- 1.6.3. Nearest Neighbors Regression¶
Neighbors-based regression can be used in cases where the data labels are
continuous rather than discrete variables.  The label assigned to a query
point is computed based on the mean of the labels of its nearest neighbors.
scikit-learn implements two different neighbors regressors:
KNeighborsRegressor implements learning based on the \(k\)
nearest neighbors of each query point, where \(k\) is an integer
value specified by the user.  RadiusNeighborsRegressor implements
learning based on the neighbors within a fixed radius \(r\) of the
query point, where \(r\) is a floating-point value specified by the
user.
The basic nearest neighbors regression uses uniform weights: that is,
each point in the local neighborhood contributes uniformly to the
classification of a query point.  Under some circumstances, it can be
advantageous to weight points such that nearby points contribute more
to the regression than faraway points.  This can be accomplished through
the weights keyword.  The default value, weights = 'uniform',
assigns equal weights to all points.  weights = 'distance' assigns
weights proportional to the inverse of the distance from the query point.
Alternatively, a user-defined function of the distance can be supplied,
which will be used to compute the weights.
The use of multi-output nearest neighbors for regression is demonstrated in
Face completion with a multi-output estimators. In this example, the inputs
X are the pixels of the upper half of faces and the outputs Y are the pixels of
the lower half of those faces.
Examples:
Nearest Neighbors regression: an example of regression
using nearest neighbors.
Face completion with a multi-output estimators: an example of
multi-output regression using nearest neighbors.
- 1.6.4. Nearest Neighbor Algorithms¶
1.6.4.1. Brute Force¶
Fast computation of nearest neighbors is an active area of research in
machine learning. The most naive neighbor search implementation involves
the brute-force computation of distances between all pairs of points in the
dataset: for \(N\) samples in \(D\) dimensions, this approach scales
as \(O[D N^2]\).  Efficient brute-force neighbors searches can be very
competitive for small data samples.
However, as the number of samples \(N\) grows, the brute-force
approach quickly becomes infeasible.  In the classes within
sklearn.neighbors, brute-force neighbors searches are specified
using the keyword algorithm = 'brute', and are computed using the
routines available in sklearn.metrics.pairwise.
1.6.4.2. K-D Tree¶
To address the computational inefficiencies of the brute-force approach, a
variety of tree-based data structures have been invented.  In general, these
structures attempt to reduce the required number of distance calculations
by efficiently encoding aggregate distance information for the sample.
The basic idea is that if point \(A\) is very distant from point
\(B\), and point \(B\) is very close to point \(C\),
then we know that points \(A\) and \(C\)
are very distant, without having to explicitly calculate their distance.
In this way, the computational cost of a nearest neighbors search can be
reduced to \(O[D N \log(N)]\) or better. This is a significant
improvement over brute-force for large \(N\).
An early approach to taking advantage of this aggregate information was
the KD tree data structure (short for K-dimensional tree), which
generalizes two-dimensional Quad-trees and 3-dimensional Oct-trees
to an arbitrary number of dimensions.  The KD tree is a binary tree
structure which recursively partitions the parameter space along the data
axes, dividing it into nested orthotropic regions into which data points
are filed.  The construction of a KD tree is very fast: because partitioning
is performed only along the data axes, no \(D\)-dimensional distances
need to be computed. Once constructed, the nearest neighbor of a query
point can be determined with only \(O[\log(N)]\) distance computations.
Though the KD tree approach is very fast for low-dimensional (\(D < 20\))
neighbors searches, it becomes inefficient as \(D\) grows very large:
this is one manifestation of the so-called “curse of dimensionality”.
In scikit-learn, KD tree neighbors searches are specified using the
keyword algorithm = 'kd_tree', and are computed using the class
KDTree.
References:
“Multidimensional binary search trees used for associative searching”,
Bentley, J.L., Communications of the ACM (1975)
1.6.4.3. Ball Tree¶
To address the inefficiencies of KD Trees in higher dimensions, the ball tree
data structure was developed.  Where KD trees partition data along
Cartesian axes, ball trees partition data in a series of nesting
hyper-spheres.  This makes tree construction more costly than that of the
KD tree, but results in a data structure which can be very efficient on
highly structured data, even in very high dimensions.
A ball tree recursively divides the data into
nodes defined by a centroid \(C\) and radius \(r\), such that each
point in the node lies within the hyper-sphere defined by \(r\) and
\(C\). The number of candidate points for a neighbor search
is reduced through use of the triangle inequality:
\[|x+y| \leq |x| + |y|\]
With this setup, a single distance calculation between a test point and
the centroid is sufficient to determine a lower and upper bound on the
distance to all points within the node.
Because of the spherical geometry of the ball tree nodes, it can out-perform
a KD-tree in high dimensions, though the actual performance is highly
dependent on the structure of the training data.
In scikit-learn, ball-tree-based
neighbors searches are specified using the keyword algorithm = 'ball_tree',
and are computed using the class BallTree.
Alternatively, the user can work with the BallTree class directly.
References:
“Five Balltree Construction Algorithms”,
Omohundro, S.M., International Computer Science Institute
Technical Report (1989)
1.6.4.4. Choice of Nearest Neighbors Algorithm¶
The optimal algorithm for a given dataset is a complicated choice, and
depends on a number of factors:
number of samples \(N\) (i.e. n_samples) and dimensionality
\(D\) (i.e. n_features).
Brute force query time grows as \(O[D N]\)
Ball tree query time grows as approximately \(O[D \log(N)]\)
KD tree query time changes with \(D\) in a way that is difficult
to precisely characterise.  For small \(D\) (less than 20 or so)
the cost is approximately \(O[D\log(N)]\), and the KD tree
query can be very efficient.
For larger \(D\), the cost increases to nearly \(O[DN]\), and
the overhead due to the tree
structure can lead to queries which are slower than brute force.
For small data sets (\(N\) less than 30 or so), \(\log(N)\) is
comparable to \(N\), and brute force algorithms can be more efficient
than a tree-based approach.  Both KDTree and BallTree
address this through providing a leaf size parameter: this controls the
number of samples at which a query switches to brute-force.  This allows both
algorithms to approach the efficiency of a brute-force computation for small
\(N\).
data structure: intrinsic dimensionality of the data and/or sparsity
of the data. Intrinsic dimensionality refers to the dimension
\(d \le D\) of a manifold on which the data lies, which can be linearly
or non-linearly embedded in the parameter space. Sparsity refers to the
degree to which the data fills the parameter space (this is to be
distinguished from the concept as used in “sparse” matrices.  The data
matrix may have no zero entries, but the structure can still be
“sparse” in this sense).
Brute force query time is unchanged by data structure.
Ball tree and KD tree query times can be greatly influenced
by data structure.  In general, sparser data with a smaller intrinsic
dimensionality leads to faster query times.  Because the KD tree
internal representation is aligned with the parameter axes, it will not
generally show as much improvement as ball tree for arbitrarily
structured data.
Datasets used in machine learning tend to be very structured, and are
very well-suited for tree-based queries.
number of neighbors \(k\) requested for a query point.
Brute force query time is largely unaffected by the value of \(k\)
Ball tree and KD tree query time will become slower as \(k\)
increases.  This is due to two effects: first, a larger \(k\) leads
to the necessity to search a larger portion of the parameter space.
Second, using \(k > 1\) requires internal queueing of results
as the tree is traversed.
As \(k\) becomes large compared to \(N\), the ability to prune
branches in a tree-based query is reduced.  In this situation, Brute force
queries can be more efficient.
number of query points.  Both the ball tree and the KD Tree
require a construction phase.  The cost of this construction becomes
negligible when amortized over many queries.  If only a small number of
queries will be performed, however, the construction can make up
a significant fraction of the total cost.  If very few query points
will be required, brute force is better than a tree-based method.
Currently, algorithm = 'auto' selects 'brute' if any of the following
conditions are verified:
input data is sparse
metric = 'precomputed'
\(D > 15\)
\(k >= N/2\)
effective_metric_ isn’t in the VALID_METRICS list for either
'kd_tree' or 'ball_tree'
Otherwise, it selects the first out of 'kd_tree' and 'ball_tree' that
has effective_metric_ in its VALID_METRICS list. This heuristic is
based on the following assumptions:
the number of query points is at least the same order as the number of
training points
leaf_size is close to its default value of 30
when \(D > 15\), the intrinsic dimensionality of the data is generally
too high for tree-based methods
1.6.4.5. Effect of leaf_size¶
As noted above, for small sample sizes a brute force search can be more
efficient than a tree-based query.  This fact is accounted for in the ball
tree and KD tree by internally switching to brute force searches within
leaf nodes.  The level of this switch can be specified with the parameter
leaf_size.  This parameter choice has many effects:
construction timeA larger leaf_size leads to a faster tree construction time, because
fewer nodes need to be created
query timeBoth a large or small leaf_size can lead to suboptimal query cost.
For leaf_size approaching 1, the overhead involved in traversing
nodes can significantly slow query times.  For leaf_size approaching
the size of the training set, queries become essentially brute force.
A good compromise between these is leaf_size = 30, the default value
of the parameter.
memoryAs leaf_size increases, the memory required to store a tree structure
decreases.  This is especially important in the case of ball tree, which
stores a \(D\)-dimensional centroid for each node.  The required
storage space for BallTree is approximately 1 / leaf_size times
the size of the training set.
leaf_size is not referenced for brute force queries.
1.6.4.6. Valid Metrics for Nearest Neighbor Algorithms¶
For a list of available metrics, see the documentation of the
DistanceMetric class and the metrics listed in
sklearn.metrics.pairwise.PAIRWISE_DISTANCE_FUNCTIONS. Note that the “cosine”
metric uses cosine_distances.
A list of valid metrics for any of the above algorithms can be obtained by using their
valid_metric attribute. For example, valid metrics for KDTree can be generated by:
>>> from sklearn.neighbors import KDTree
>>> print(sorted(KDTree.valid_metrics))
['chebyshev', 'cityblock', 'euclidean', 'infinity', 'l1', 'l2', 'manhattan', 'minkowski', 'p']
- 1.6.5. Nearest Centroid Classifier¶
The NearestCentroid classifier is a simple algorithm that represents
each class by the centroid of its members. In effect, this makes it
similar to the label updating phase of the KMeans algorithm.
It also has no parameters to choose, making it a good baseline classifier. It
does, however, suffer on non-convex classes, as well as when classes have
drastically different variances, as equal variance in all dimensions is
assumed. See Linear Discriminant Analysis (LinearDiscriminantAnalysis)
and Quadratic Discriminant Analysis (QuadraticDiscriminantAnalysis)
for more complex methods that do not make this assumption. Usage of the default
NearestCentroid is simple:
>>> from sklearn.neighbors import NearestCentroid
>>> import numpy as np
>>> X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])
>>> y = np.array([1, 1, 1, 2, 2, 2])
>>> clf = NearestCentroid()
>>> clf.fit(X, y)
NearestCentroid()
>>> print(clf.predict([[-0.8, -1]]))
[1]
1.6.5.1. Nearest Shrunken Centroid¶
The NearestCentroid classifier has a shrink_threshold parameter,
which implements the nearest shrunken centroid classifier. In effect, the value
of each feature for each centroid is divided by the within-class variance of
that feature. The feature values are then reduced by shrink_threshold. Most
notably, if a particular feature value crosses zero, it is set
to zero. In effect, this removes the feature from affecting the classification.
This is useful, for example, for removing noisy features.
In the example below, using a small shrink threshold increases the accuracy of
the model from 0.81 to 0.82.
Examples:
Nearest Centroid Classification: an example of
classification using nearest centroid with different shrink thresholds.
- 1.6.6. Nearest Neighbors Transformer¶
Many scikit-learn estimators rely on nearest neighbors: Several classifiers and
regressors such as KNeighborsClassifier and
KNeighborsRegressor, but also some clustering methods such as
DBSCAN and
SpectralClustering, and some manifold embeddings such
as TSNE and Isomap.
All these estimators can compute internally the nearest neighbors, but most of
them also accept precomputed nearest neighbors sparse graph,
as given by kneighbors_graph and
radius_neighbors_graph. With mode
mode='connectivity', these functions return a binary adjacency sparse graph
as required, for instance, in SpectralClustering.
Whereas with mode='distance', they return a distance sparse graph as required,
for instance, in DBSCAN. To include these functions in
a scikit-learn pipeline, one can also use the corresponding classes
KNeighborsTransformer and RadiusNeighborsTransformer.
The benefits of this sparse graph API are multiple.
First, the precomputed graph can be re-used multiple times, for instance while
varying a parameter of the estimator. This can be done manually by the user, or
using the caching properties of the scikit-learn pipeline:
>>> import tempfile
>>> from sklearn.manifold import Isomap
>>> from sklearn.neighbors import KNeighborsTransformer
>>> from sklearn.pipeline import make_pipeline
>>> from sklearn.datasets import make_regression
>>> cache_path = tempfile.gettempdir()  # we use a temporary folder here
>>> X, _ = make_regression(n_samples=50, n_features=25, random_state=0)
>>> estimator = make_pipeline(
...     KNeighborsTransformer(mode='distance'),
...     Isomap(n_components=3, metric='precomputed'),
...     memory=cache_path)
>>> X_embedded = estimator.fit_transform(X)
>>> X_embedded.shape
(50, 3)
Second, precomputing the graph can give finer control on the nearest neighbors
estimation, for instance enabling multiprocessing though the parameter
n_jobs, which might not be available in all estimators.
Finally, the precomputation can be performed by custom estimators to use
different implementations, such as approximate nearest neighbors methods, or
implementation with special data types. The precomputed neighbors
sparse graph needs to be formatted as in
radius_neighbors_graph output:
a CSR matrix (although COO, CSC or LIL will be accepted).
only explicitly store nearest neighborhoods of each sample with respect to the
training data. This should include those at 0 distance from a query point,
including the matrix diagonal when computing the nearest neighborhoods
between the training data and itself.
each row’s data should store the distance in increasing order (optional.
Unsorted data will be stable-sorted, adding a computational overhead).
all values in data should be non-negative.
there should be no duplicate indices in any row
(see https://github.com/scipy/scipy/issues/5807).
if the algorithm being passed the precomputed matrix uses k nearest neighbors
(as opposed to radius neighborhood), at least k neighbors must be stored in
each row (or k+1, as explained in the following note).
Note
When a specific number of neighbors is queried (using
KNeighborsTransformer), the definition of n_neighbors is ambiguous
since it can either include each training point as its own neighbor, or
exclude them. Neither choice is perfect, since including them leads to a
different number of non-self neighbors during training and testing, while
excluding them leads to a difference between fit(X).transform(X) and
fit_transform(X), which is against scikit-learn API.
In KNeighborsTransformer we use the definition which includes each
training point as its own neighbor in the count of n_neighbors. However,
for compatibility reasons with other estimators which use the other
definition, one extra neighbor will be computed when mode == 'distance'.
To maximise compatibility with all estimators, a safe choice is to always
include one extra neighbor in a custom nearest neighbors estimator, since
unnecessary neighbors will be filtered by following estimators.
Examples:
Approximate nearest neighbors in TSNE:
an example of pipelining KNeighborsTransformer and
TSNE. Also proposes two custom nearest neighbors
estimators based on external packages.
Caching nearest neighbors:
an example of pipelining KNeighborsTransformer and
KNeighborsClassifier to enable caching of the neighbors graph
during a hyper-parameter grid-search.
- 1.6.7. Neighborhood Components Analysis¶
Neighborhood Components Analysis (NCA, NeighborhoodComponentsAnalysis)
is a distance metric learning algorithm which aims to improve the accuracy of
nearest neighbors classification compared to the standard Euclidean distance.
The algorithm directly maximizes a stochastic variant of the leave-one-out
k-nearest neighbors (KNN) score on the training set. It can also learn a
low-dimensional linear projection of data that can be used for data
visualization and fast classification.
In the above illustrating figure, we consider some points from a randomly
generated dataset. We focus on the stochastic KNN classification of point no.
## 3. The thickness of a link between sample 3 and another point is proportional

to their distance, and can be seen as the relative weight (or probability) that
a stochastic nearest neighbor prediction rule would assign to this point. In
the original space, sample 3 has many stochastic neighbors from various
classes, so the right class is not very likely. However, in the projected space
learned by NCA, the only stochastic neighbors with non-negligible weight are
from the same class as sample 3, guaranteeing that the latter will be well
classified. See the mathematical formulation
for more details.
1.6.7.1. Classification¶
Combined with a nearest neighbors classifier (KNeighborsClassifier),
NCA is attractive for classification because it can naturally handle
multi-class problems without any increase in the model size, and does not
introduce additional parameters that require fine-tuning by the user.
NCA classification has been shown to work well in practice for data sets of
varying size and difficulty. In contrast to related methods such as Linear
Discriminant Analysis, NCA does not make any assumptions about the class
distributions. The nearest neighbor classification can naturally produce highly
irregular decision boundaries.
To use this model for classification, one needs to combine a
NeighborhoodComponentsAnalysis instance that learns the optimal
transformation with a KNeighborsClassifier instance that performs the
classification in the projected space. Here is an example using the two
classes:
>>> from sklearn.neighbors import (NeighborhoodComponentsAnalysis,
... KNeighborsClassifier)
>>> from sklearn.datasets import load_iris
>>> from sklearn.model_selection import train_test_split
>>> from sklearn.pipeline import Pipeline
>>> X, y = load_iris(return_X_y=True)
>>> X_train, X_test, y_train, y_test = train_test_split(X, y,
... stratify=y, test_size=0.7, random_state=42)
>>> nca = NeighborhoodComponentsAnalysis(random_state=42)
>>> knn = KNeighborsClassifier(n_neighbors=3)
>>> nca_pipe = Pipeline([('nca', nca), ('knn', knn)])
>>> nca_pipe.fit(X_train, y_train)
Pipeline(...)
>>> print(nca_pipe.score(X_test, y_test))
0.96190476...
The plot shows decision boundaries for Nearest Neighbor Classification and
Neighborhood Components Analysis classification on the iris dataset, when
training and scoring on only two features, for visualisation purposes.
1.6.7.2. Dimensionality reduction¶
NCA can be used to perform supervised dimensionality reduction. The input data
are projected onto a linear subspace consisting of the directions which
minimize the NCA objective. The desired dimensionality can be set using the
parameter n_components. For instance, the following figure shows a
comparison of dimensionality reduction with Principal Component Analysis
(PCA), Linear Discriminant Analysis
(LinearDiscriminantAnalysis) and
Neighborhood Component Analysis (NeighborhoodComponentsAnalysis) on
the Digits dataset, a dataset with size \(n_{samples} = 1797\) and
\(n_{features} = 64\). The data set is split into a training and a test set
of equal size, then standardized. For evaluation the 3-nearest neighbor
classification accuracy is computed on the 2-dimensional projected points found
by each method. Each data sample belongs to one of 10 classes.
Examples:
Comparing Nearest Neighbors with and without Neighborhood Components Analysis
Dimensionality Reduction with Neighborhood Components Analysis
Manifold learning on handwritten digits: Locally Linear Embedding, Isomap…
1.6.7.3. Mathematical formulation¶
The goal of NCA is to learn an optimal linear transformation matrix of size
(n_components, n_features), which maximises the sum over all samples
\(i\) of the probability \(p_i\) that \(i\) is correctly
classified, i.e.:
\[\underset{L}{\arg\max} \sum\limits_{i=0}^{N - 1} p_{i}\]
with \(N\) = n_samples and \(p_i\) the probability of sample
\(i\) being correctly classified according to a stochastic nearest
neighbors rule in the learned embedded space:
\[p_{i}=\sum\limits_{j \in C_i}{p_{i j}}\]
where \(C_i\) is the set of points in the same class as sample \(i\),
and \(p_{i j}\) is the softmax over Euclidean distances in the embedded
space:
\[p_{i j} = \frac{\exp(-||L x_i - L x_j||^2)}{\sum\limits_{k \ne
i} {\exp{-(||L x_i - L x_k||^2)}}} , \quad p_{i i} = 0\]
1.6.7.3.1. Mahalanobis distance¶
NCA can be seen as learning a (squared) Mahalanobis distance metric:
\[|| L(x_i - x_j)||^2 = (x_i - x_j)^TM(x_i - x_j),\]
where \(M = L^T L\) is a symmetric positive semi-definite matrix of size
(n_features, n_features).
1.6.7.4. Implementation¶
This implementation follows what is explained in the original paper [1]. For
the optimisation method, it currently uses scipy’s L-BFGS-B with a full
gradient computation at each iteration, to avoid to tune the learning rate and
provide stable learning.
See the examples below and the docstring of
NeighborhoodComponentsAnalysis.fit for further information.
1.6.7.5. Complexity¶
1.6.7.5.1. Training¶
NCA stores a matrix of pairwise distances, taking n_samples ** 2 memory.
Time complexity depends on the number of iterations done by the optimisation
algorithm. However, one can set the maximum number of iterations with the
argument max_iter. For each iteration, time complexity is
O(n_components x n_samples x min(n_samples, n_features)).
1.6.7.5.2. Transform¶
Here the transform operation returns \(LX^T\), therefore its time
complexity equals n_components * n_features * n_samples_test. There is no
added space complexity in the operation.
References:
[1]
“Neighbourhood Components Analysis”,
J. Goldberger, S. Roweis, G. Hinton, R. Salakhutdinov, Advances in
Neural Information Processing Systems, Vol. 17, May 2005, pp. 513-520.
Wikipedia entry on Neighborhood Components Analysis
© 2007 - 2024, scikit-learn developers (BSD License).
Show this page source
### 1.7. Gaussian Processes — scikit-learn 1.4.2 documentation

Install
User Guide
API
Examples
Community
Getting Started
Tutorial
What's new
Glossary
Development
FAQ
Support
Related packages
Roadmap
Governance
About us
GitHub
Other Versions and Download
More
Getting Started
Tutorial
What's new
Glossary
Development
FAQ
Support
Related packages
Roadmap
Governance
About us
GitHub
Other Versions and Download
Toggle Menu
PrevUp
Next
scikit-learn 1.4.2
Other versions
Please cite us if you use the software.
### 1.7. Gaussian Processes

- 1.7.1. Gaussian Process Regression (GPR)
- 1.7.2. Gaussian Process Classification (GPC)
- 1.7.3. GPC examples
1.7.3.1. Probabilistic predictions with GPC
1.7.3.2. Illustration of GPC on the XOR dataset
1.7.3.3. Gaussian process classification (GPC) on iris dataset
- 1.7.4. Kernels for Gaussian Processes
1.7.4.1. Basic kernels
1.7.4.2. Kernel operators
1.7.4.3. Radial basis function (RBF) kernel
1.7.4.4. Matérn kernel
1.7.4.5. Rational quadratic kernel
1.7.4.6. Exp-Sine-Squared kernel
1.7.4.7. Dot-Product kernel
1.7.4.8. References
### 1.7. Gaussian Processes¶

Gaussian Processes (GP) are a nonparametric supervised learning method used
to solve regression and probabilistic classification problems.
The advantages of Gaussian processes are:
The prediction interpolates the observations (at least for regular
kernels).
The prediction is probabilistic (Gaussian) so that one can compute
empirical confidence intervals and decide based on those if one should
refit (online fitting, adaptive fitting) the prediction in some
region of interest.
Versatile: different kernels can be specified. Common kernels are provided, but
it is also possible to specify custom kernels.
The disadvantages of Gaussian processes include:
Our implementation is not sparse, i.e., they use the whole samples/features
information to perform the prediction.
They lose efficiency in high dimensional spaces – namely when the number
of features exceeds a few dozens.
- 1.7.1. Gaussian Process Regression (GPR)¶
The GaussianProcessRegressor implements Gaussian processes (GP) for
regression purposes. For this, the prior of the GP needs to be specified. GP
will combine this prior and the likelihood function based on training samples.
It allows to give a probabilistic approach to prediction by giving the mean and
standard deviation as output when predicting.
The prior mean is assumed to be constant and zero (for normalize_y=False) or
the training data’s mean (for normalize_y=True). The prior’s covariance is
specified by passing a kernel object. The hyperparameters
of the kernel are optimized when fitting the GaussianProcessRegressor
by maximizing the log-marginal-likelihood (LML) based on the passed
optimizer. As the LML may have multiple local optima, the optimizer can be
started repeatedly by specifying n_restarts_optimizer. The first run is
always conducted starting from the initial hyperparameter values of the kernel;
subsequent runs are conducted from hyperparameter values that have been chosen
randomly from the range of allowed values. If the initial hyperparameters
should be kept fixed, None can be passed as optimizer.
The noise level in the targets can be specified by passing it via the parameter
alpha, either globally as a scalar or per datapoint. Note that a moderate
noise level can also be helpful for dealing with numeric instabilities during
fitting as it is effectively implemented as Tikhonov regularization, i.e., by
adding it to the diagonal of the kernel matrix. An alternative to specifying
the noise level explicitly is to include a
WhiteKernel component into the
kernel, which can estimate the global noise level from the data (see example
below). The figure below shows the effect of noisy target handled by setting
the parameter alpha.
The implementation is based on Algorithm 2.1 of [RW2006]. In addition to
the API of standard scikit-learn estimators, GaussianProcessRegressor:
allows prediction without prior fitting (based on the GP prior)
provides an additional method sample_y(X), which evaluates samples
drawn from the GPR (prior or posterior) at given inputs
exposes a method log_marginal_likelihood(theta), which can be used
externally for other ways of selecting hyperparameters, e.g., via
Markov chain Monte Carlo.
Examples
Gaussian Processes regression: basic introductory example
Ability of Gaussian process regression (GPR) to estimate data noise-level
Comparison of kernel ridge and Gaussian process regression
Forecasting of CO2 level on Mona Loa dataset using Gaussian process regression (GPR)
- 1.7.2. Gaussian Process Classification (GPC)¶
The GaussianProcessClassifier implements Gaussian processes (GP) for
classification purposes, more specifically for probabilistic classification,
where test predictions take the form of class probabilities.
GaussianProcessClassifier places a GP prior on a latent function \(f\),
which is then squashed through a link function to obtain the probabilistic
classification. The latent function \(f\) is a so-called nuisance function,
whose values are not observed and are not relevant by themselves.
Its purpose is to allow a convenient formulation of the model, and \(f\)
is removed (integrated out) during prediction. GaussianProcessClassifier
implements the logistic link function, for which the integral cannot be
computed analytically but is easily approximated in the binary case.
In contrast to the regression setting, the posterior of the latent function
\(f\) is not Gaussian even for a GP prior since a Gaussian likelihood is
inappropriate for discrete class labels. Rather, a non-Gaussian likelihood
corresponding to the logistic link function (logit) is used.
GaussianProcessClassifier approximates the non-Gaussian posterior with a
Gaussian based on the Laplace approximation. More details can be found in
Chapter 3 of [RW2006].
The GP prior mean is assumed to be zero. The prior’s
covariance is specified by passing a kernel object. The
hyperparameters of the kernel are optimized during fitting of
GaussianProcessRegressor by maximizing the log-marginal-likelihood (LML) based
on the passed optimizer. As the LML may have multiple local optima, the
optimizer can be started repeatedly by specifying n_restarts_optimizer. The
first run is always conducted starting from the initial hyperparameter values
of the kernel; subsequent runs are conducted from hyperparameter values
that have been chosen randomly from the range of allowed values.
If the initial hyperparameters should be kept fixed, None can be passed as
optimizer.
GaussianProcessClassifier supports multi-class classification
by performing either one-versus-rest or one-versus-one based training and
prediction.  In one-versus-rest, one binary Gaussian process classifier is
fitted for each class, which is trained to separate this class from the rest.
In “one_vs_one”, one binary Gaussian process classifier is fitted for each pair
of classes, which is trained to separate these two classes. The predictions of
these binary predictors are combined into multi-class predictions. See the
section on multi-class classification for more details.
In the case of Gaussian process classification, “one_vs_one” might be
computationally  cheaper since it has to solve many problems involving only a
subset of the whole training set rather than fewer problems on the whole
dataset. Since Gaussian process classification scales cubically with the size
of the dataset, this might be considerably faster. However, note that
“one_vs_one” does not support predicting probability estimates but only plain
predictions. Moreover, note that GaussianProcessClassifier does not
(yet) implement a true multi-class Laplace approximation internally, but
as discussed above is based on solving several binary classification tasks
internally, which are combined using one-versus-rest or one-versus-one.
- 1.7.3. GPC examples¶
1.7.3.1. Probabilistic predictions with GPC¶
This example illustrates the predicted probability of GPC for an RBF kernel
with different choices of the hyperparameters. The first figure shows the
predicted probability of GPC with arbitrarily chosen hyperparameters and with
the hyperparameters corresponding to the maximum log-marginal-likelihood (LML).
While the hyperparameters chosen by optimizing LML have a considerably larger
LML, they perform slightly worse according to the log-loss on test data. The
figure shows that this is because they exhibit a steep change of the class
probabilities at the class boundaries (which is good) but have predicted
probabilities close to 0.5 far away from the class boundaries (which is bad)
This undesirable effect is caused by the Laplace approximation used
internally by GPC.
The second figure shows the log-marginal-likelihood for different choices of
the kernel’s hyperparameters, highlighting the two choices of the
hyperparameters used in the first figure by black dots.
1.7.3.2. Illustration of GPC on the XOR dataset¶
This example illustrates GPC on XOR data. Compared are a stationary, isotropic
kernel (RBF) and a non-stationary kernel (DotProduct). On
this particular dataset, the DotProduct kernel obtains considerably
better results because the class-boundaries are linear and coincide with the
coordinate axes. In practice, however, stationary kernels such as RBF
often obtain better results.
1.7.3.3. Gaussian process classification (GPC) on iris dataset¶
This example illustrates the predicted probability of GPC for an isotropic
and anisotropic RBF kernel on a two-dimensional version for the iris-dataset.
This illustrates the applicability of GPC to non-binary classification.
The anisotropic RBF kernel obtains slightly higher log-marginal-likelihood by
assigning different length-scales to the two feature dimensions.
- 1.7.4. Kernels for Gaussian Processes¶
Kernels (also called “covariance functions” in the context of GPs) are a crucial
ingredient of GPs which determine the shape of prior and posterior of the GP.
They encode the assumptions on the function being learned by defining the “similarity”
of two datapoints combined with the assumption that similar datapoints should
have similar target values. Two categories of kernels can be distinguished:
stationary kernels depend only on the distance of two datapoints and not on their
absolute values \(k(x_i, x_j)= k(d(x_i, x_j))\) and are thus invariant to
translations in the input space, while non-stationary kernels
depend also on the specific values of the datapoints. Stationary kernels can further
be subdivided into isotropic and anisotropic kernels, where isotropic kernels are
also invariant to rotations in the input space. For more details, we refer to
Chapter 4 of [RW2006]. For guidance on how to best combine different kernels,
we refer to [Duv2014].
Gaussian Process Kernel API
Click for more details
¶
The main usage of a Kernel is to compute the GP’s covariance between
datapoints. For this, the method __call__ of the kernel can be called. This
method can either be used to compute the “auto-covariance” of all pairs of
datapoints in a 2d array X, or the “cross-covariance” of all combinations
of datapoints of a 2d array X with datapoints in a 2d array Y. The following
identity holds true for all kernels k (except for the WhiteKernel):
k(X) == K(X, Y=X)
If only the diagonal of the auto-covariance is being used, the method diag()
of a kernel can be called, which is more computationally efficient than the
equivalent call to __call__: np.diag(k(X, X)) == k.diag(X)
Kernels are parameterized by a vector \(\theta\) of hyperparameters. These
hyperparameters can for instance control length-scales or periodicity of a
kernel (see below). All kernels support computing analytic gradients
of the kernel’s auto-covariance with respect to \(log(\theta)\) via setting
eval_gradient=True in the __call__ method.
That is, a (len(X), len(X), len(theta)) array is returned where the entry
[i, j, l] contains \(\frac{\partial k_\theta(x_i, x_j)}{\partial log(\theta_l)}\).
This gradient is used by the Gaussian process (both regressor and classifier)
in computing the gradient of the log-marginal-likelihood, which in turn is used
to determine the value of \(\theta\), which maximizes the log-marginal-likelihood,
via gradient ascent. For each hyperparameter, the initial value and the
bounds need to be specified when creating an instance of the kernel. The
current value of \(\theta\) can be get and set via the property
theta of the kernel object. Moreover, the bounds of the hyperparameters can be
accessed by the property bounds of the kernel. Note that both properties
(theta and bounds) return log-transformed values of the internally used values
since those are typically more amenable to gradient-based optimization.
The specification of each hyperparameter is stored in the form of an instance of
Hyperparameter in the respective kernel. Note that a kernel using a
hyperparameter with name “x” must have the attributes self.x and self.x_bounds.
The abstract base class for all kernels is Kernel. Kernel implements a
similar interface as BaseEstimator, providing the
methods get_params(), set_params(), and clone(). This allows
setting kernel values also via meta-estimators such as
Pipeline or
GridSearchCV. Note that due to the nested
structure of kernels (by applying kernel operators, see below), the names of
kernel parameters might become relatively complicated. In general, for a binary
kernel operator, parameters of the left operand are prefixed with k1__ and
parameters of the right operand with k2__. An additional convenience method
is clone_with_theta(theta), which returns a cloned version of the kernel
but with the hyperparameters set to theta. An illustrative example:
>>> from sklearn.gaussian_process.kernels import ConstantKernel, RBF
>>> kernel = ConstantKernel(constant_value=1.0, constant_value_bounds=(0.0, 10.0)) * RBF(length_scale=0.5, length_scale_bounds=(0.0, 10.0)) + RBF(length_scale=2.0, length_scale_bounds=(0.0, 10.0))
>>> for hyperparameter in kernel.hyperparameters: print(hyperparameter)
Hyperparameter(name='k1__k1__constant_value', value_type='numeric', bounds=array([[ 0., 10.]]), n_elements=1, fixed=False)
Hyperparameter(name='k1__k2__length_scale', value_type='numeric', bounds=array([[ 0., 10.]]), n_elements=1, fixed=False)
Hyperparameter(name='k2__length_scale', value_type='numeric', bounds=array([[ 0., 10.]]), n_elements=1, fixed=False)
>>> params = kernel.get_params()
>>> for key in sorted(params): print("%s : %s" % (key, params[key]))
k1 : 1**2 * RBF(length_scale=0.5)
k1__k1 : 1**2
k1__k1__constant_value : 1.0
k1__k1__constant_value_bounds : (0.0, 10.0)
k1__k2 : RBF(length_scale=0.5)
k1__k2__length_scale : 0.5
k1__k2__length_scale_bounds : (0.0, 10.0)
k2 : RBF(length_scale=2)
k2__length_scale : 2.0
k2__length_scale_bounds : (0.0, 10.0)
>>> print(kernel.theta)  # Note: log-transformed
[ 0.         -0.69314718  0.69314718]
>>> print(kernel.bounds)  # Note: log-transformed
[[      -inf 2.30258509]
[      -inf 2.30258509]
[      -inf 2.30258509]]
All Gaussian process kernels are interoperable with sklearn.metrics.pairwise
and vice versa: instances of subclasses of Kernel can be passed as
metric to pairwise_kernels from sklearn.metrics.pairwise. Moreover,
kernel functions from pairwise can be used as GP kernels by using the wrapper
class PairwiseKernel. The only caveat is that the gradient of
the hyperparameters is not analytic but numeric and all those kernels support
only isotropic distances. The parameter gamma is considered to be a
hyperparameter and may be optimized. The other kernel parameters are set
directly at initialization and are kept fixed.
1.7.4.1. Basic kernels¶
The ConstantKernel kernel can be used as part of a Product
kernel where it scales the magnitude of the other factor (kernel) or as part
of a Sum kernel, where it modifies the mean of the Gaussian process.
It depends on a parameter \(constant\_value\). It is defined as:
\[k(x_i, x_j) = constant\_value \;\forall\; x_1, x_2\]
The main use-case of the WhiteKernel kernel is as part of a
sum-kernel where it explains the noise-component of the signal. Tuning its
parameter \(noise\_level\) corresponds to estimating the noise-level.
It is defined as:
\[k(x_i, x_j) = noise\_level \text{ if } x_i == x_j \text{ else } 0\]
1.7.4.2. Kernel operators¶
Kernel operators take one or two base kernels and combine them into a new
kernel. The Sum kernel takes two kernels \(k_1\) and \(k_2\)
and combines them via \(k_{sum}(X, Y) = k_1(X, Y) + k_2(X, Y)\).
The  Product kernel takes two kernels \(k_1\) and \(k_2\)
and combines them via \(k_{product}(X, Y) = k_1(X, Y) * k_2(X, Y)\).
The Exponentiation kernel takes one base kernel and a scalar parameter
\(p\) and combines them via
\(k_{exp}(X, Y) = k(X, Y)^p\).
Note that magic methods __add__, __mul___ and __pow__ are
overridden on the Kernel objects, so one can use e.g. RBF() + RBF() as
a shortcut for Sum(RBF(), RBF()).
1.7.4.3. Radial basis function (RBF) kernel¶
The RBF kernel is a stationary kernel. It is also known as the “squared
exponential” kernel. It is parameterized by a length-scale parameter \(l>0\), which
can either be a scalar (isotropic variant of the kernel) or a vector with the same
number of dimensions as the inputs \(x\) (anisotropic variant of the kernel).
The kernel is given by:
\[k(x_i, x_j) = \text{exp}\left(- \frac{d(x_i, x_j)^2}{2l^2} \right)\]
where \(d(\cdot, \cdot)\) is the Euclidean distance.
This kernel is infinitely differentiable, which implies that GPs with this
kernel as covariance function have mean square derivatives of all orders, and are thus
very smooth. The prior and posterior of a GP resulting from an RBF kernel are shown in
the following figure:
1.7.4.4. Matérn kernel¶
The Matern kernel is a stationary kernel and a generalization of the
RBF kernel. It has an additional parameter \(\nu\) which controls
the smoothness of the resulting function. It is parameterized by a length-scale parameter \(l>0\), which can either be a scalar (isotropic variant of the kernel) or a vector with the same number of dimensions as the inputs \(x\) (anisotropic variant of the kernel).
Mathematical implementation of Matérn kernel
Click for more details
¶
The kernel is given by:
\[k(x_i, x_j) = \frac{1}{\Gamma(\nu)2^{\nu-1}}\Bigg(\frac{\sqrt{2\nu}}{l} d(x_i , x_j )\Bigg)^\nu K_\nu\Bigg(\frac{\sqrt{2\nu}}{l} d(x_i , x_j )\Bigg),\]
where \(d(\cdot,\cdot)\) is the Euclidean distance, \(K_\nu(\cdot)\) is a modified Bessel function and \(\Gamma(\cdot)\) is the gamma function.
As \(\nu\rightarrow\infty\), the Matérn kernel converges to the RBF kernel.
When \(\nu = 1/2\), the Matérn kernel becomes identical to the absolute
exponential kernel, i.e.,
\[k(x_i, x_j) = \exp \Bigg(- \frac{1}{l} d(x_i , x_j ) \Bigg) \quad \quad \nu= \tfrac{1}{2}\]
In particular, \(\nu = 3/2\):
\[k(x_i, x_j) =  \Bigg(1 + \frac{\sqrt{3}}{l} d(x_i , x_j )\Bigg) \exp \Bigg(-\frac{\sqrt{3}}{l} d(x_i , x_j ) \Bigg) \quad \quad \nu= \tfrac{3}{2}\]
and \(\nu = 5/2\):
\[k(x_i, x_j) = \Bigg(1 + \frac{\sqrt{5}}{l} d(x_i , x_j ) +\frac{5}{3l} d(x_i , x_j )^2 \Bigg) \exp \Bigg(-\frac{\sqrt{5}}{l} d(x_i , x_j ) \Bigg) \quad \quad \nu= \tfrac{5}{2}\]
are popular choices for learning functions that are not infinitely
differentiable (as assumed by the RBF kernel) but at least once (\(\nu =
3/2\)) or twice differentiable (\(\nu = 5/2\)).
The flexibility of controlling the smoothness of the learned function via \(\nu\)
allows adapting to the properties of the true underlying functional relation.
The prior and posterior of a GP resulting from a Matérn kernel are shown in
the following figure:
See [RW2006], pp84 for further details regarding the
different variants of the Matérn kernel.
1.7.4.5. Rational quadratic kernel¶
The RationalQuadratic kernel can be seen as a scale mixture (an infinite sum)
of RBF kernels with different characteristic length-scales. It is parameterized
by a length-scale parameter \(l>0\) and a scale mixture parameter  \(\alpha>0\)
Only the isotropic variant where \(l\) is a scalar is supported at the moment.
The kernel is given by:
\[k(x_i, x_j) = \left(1 + \frac{d(x_i, x_j)^2}{2\alpha l^2}\right)^{-\alpha}\]
The prior and posterior of a GP resulting from a RationalQuadratic kernel are shown in
the following figure:
1.7.4.6. Exp-Sine-Squared kernel¶
The ExpSineSquared kernel allows modeling periodic functions.
It is parameterized by a length-scale parameter \(l>0\) and a periodicity parameter
\(p>0\). Only the isotropic variant where \(l\) is a scalar is supported at the moment.
The kernel is given by:
\[k(x_i, x_j) = \text{exp}\left(- \frac{ 2\sin^2(\pi d(x_i, x_j) / p) }{ l^ 2} \right)\]
The prior and posterior of a GP resulting from an ExpSineSquared kernel are shown in
the following figure:
1.7.4.7. Dot-Product kernel¶
The DotProduct kernel is non-stationary and can be obtained from linear regression
by putting \(N(0, 1)\) priors on the coefficients of \(x_d (d = 1, . . . , D)\) and
a prior of \(N(0, \sigma_0^2)\) on the bias. The DotProduct kernel is invariant to a rotation
of the coordinates about the origin, but not translations.
It is parameterized by a parameter \(\sigma_0^2\). For \(\sigma_0^2 = 0\), the kernel
is called the homogeneous linear kernel, otherwise it is inhomogeneous. The kernel is given by
\[k(x_i, x_j) = \sigma_0 ^ 2 + x_i \cdot x_j\]
The DotProduct kernel is commonly combined with exponentiation. An example with exponent 2 is
shown in the following figure:
1.7.4.8. References¶
[RW2006]
(1,2,3,4)
Carl E. Rasmussen and Christopher K.I. Williams,
“Gaussian Processes for Machine Learning”,
MIT Press 2006
[Duv2014]
David Duvenaud, “The Kernel Cookbook: Advice on Covariance functions”, 2014
© 2007 - 2024, scikit-learn developers (BSD License).
Show this page source
### 1.8. Cross decomposition — scikit-learn 1.4.2 documentation

Install
User Guide
API
Examples
Community
Getting Started
Tutorial
What's new
Glossary
Development
FAQ
Support
Related packages
Roadmap
Governance
About us
GitHub
Other Versions and Download
More
Getting Started
Tutorial
What's new
Glossary
Development
FAQ
Support
Related packages
Roadmap
Governance
About us
GitHub
Other Versions and Download
Toggle Menu
PrevUp
Next
scikit-learn 1.4.2
Other versions
Please cite us if you use the software.
### 1.8. Cross decomposition

- 1.8.1. PLSCanonical
- 1.8.2. PLSSVD
- 1.8.3. PLSRegression
- 1.8.4. Canonical Correlation Analysis
### 1.8. Cross decomposition¶

The cross decomposition module contains supervised estimators for
dimensionality reduction and regression, belonging to the “Partial Least
Squares” family.
Cross decomposition algorithms find the fundamental relations between two
matrices (X and Y). They are latent variable approaches to modeling the
covariance structures in these two spaces. They will try to find the
multidimensional direction in the X space that explains the maximum
multidimensional variance direction in the Y space. In other words, PLS
projects both X and Y into a lower-dimensional subspace such that the
covariance between transformed(X) and transformed(Y) is maximal.
PLS draws similarities with Principal Component Regression (PCR), where
the samples are first projected into a lower-dimensional subspace, and the
targets y are predicted using transformed(X). One issue with PCR is that
the dimensionality reduction is unsupervised, and may lose some important
variables: PCR would keep the features with the most variance, but it’s
possible that features with a small variances are relevant from predicting
the target. In a way, PLS allows for the same kind of dimensionality
reduction, but by taking into account the targets y. An illustration of
this fact is given in the following example:
* Principal Component Regression vs Partial Least Squares Regression.
Apart from CCA, the PLS estimators are particularly suited when the matrix of
predictors has more variables than observations, and when there is
multicollinearity among the features. By contrast, standard linear regression
would fail in these cases unless it is regularized.
Classes included in this module are PLSRegression,
PLSCanonical, CCA and PLSSVD
- 1.8.1. PLSCanonical¶
We here describe the algorithm used in PLSCanonical. The other
estimators use variants of this algorithm, and are detailed below.
We recommend section [1] for more details and comparisons between these
algorithms. In [1], PLSCanonical corresponds to “PLSW2A”.
Given two centered matrices \(X \in \mathbb{R}^{n \times d}\) and
\(Y \in \mathbb{R}^{n \times t}\), and a number of components \(K\),
PLSCanonical proceeds as follows:
Set \(X_1\) to \(X\) and \(Y_1\) to \(Y\). Then, for each
\(k \in [1, K]\):
a) compute \(u_k \in \mathbb{R}^d\) and \(v_k \in \mathbb{R}^t\),
the first left and right singular vectors of the cross-covariance matrix
\(C = X_k^T Y_k\).
\(u_k\) and \(v_k\) are called the weights.
By definition, \(u_k\) and \(v_k\) are
chosen so that they maximize the covariance between the projected
\(X_k\) and the projected target, that is \(\text{Cov}(X_k u_k,
Y_k v_k)\).
b) Project \(X_k\) and \(Y_k\) on the singular vectors to obtain
scores: \(\xi_k = X_k u_k\) and \(\omega_k = Y_k v_k\)
c) Regress \(X_k\) on \(\xi_k\), i.e. find a vector \(\gamma_k
\in \mathbb{R}^d\) such that the rank-1 matrix \(\xi_k \gamma_k^T\)
is as close as possible to \(X_k\). Do the same on \(Y_k\) with
\(\omega_k\) to obtain \(\delta_k\). The vectors
\(\gamma_k\) and \(\delta_k\) are called the loadings.
d) deflate \(X_k\) and \(Y_k\), i.e. subtract the rank-1
approximations: \(X_{k+1} = X_k - \xi_k \gamma_k^T\), and
\(Y_{k + 1} = Y_k - \omega_k \delta_k^T\).
At the end, we have approximated \(X\) as a sum of rank-1 matrices:
\(X = \Xi \Gamma^T\) where \(\Xi \in \mathbb{R}^{n \times K}\)
contains the scores in its columns, and \(\Gamma^T \in \mathbb{R}^{K
\times d}\) contains the loadings in its rows. Similarly for \(Y\), we
have \(Y = \Omega \Delta^T\).
Note that the scores matrices \(\Xi\) and \(\Omega\) correspond to
the projections of the training data \(X\) and \(Y\), respectively.
Step a) may be performed in two ways: either by computing the whole SVD of
\(C\) and only retain the singular vectors with the biggest singular
values, or by directly computing the singular vectors using the power method (cf section 11.3 in [1]),
which corresponds to the 'nipals' option of the algorithm parameter.
Transforming data
Click for more details
¶
To transform \(X\) into \(\bar{X}\), we need to find a projection
matrix \(P\) such that \(\bar{X} = XP\). We know that for the
training data, \(\Xi = XP\), and \(X = \Xi \Gamma^T\). Setting
\(P = U(\Gamma^T U)^{-1}\) where \(U\) is the matrix with the
\(u_k\) in the columns, we have \(XP = X U(\Gamma^T U)^{-1} = \Xi
(\Gamma^T U) (\Gamma^T U)^{-1} = \Xi\) as desired. The rotation matrix
\(P\) can be accessed from the x_rotations_ attribute.
Similarly, \(Y\) can be transformed using the rotation matrix
\(V(\Delta^T V)^{-1}\), accessed via the y_rotations_ attribute.
Predicting the targets Y
Click for more details
¶
To predict the targets of some data \(X\), we are looking for a
coefficient matrix \(\beta \in R^{d \times t}\) such that \(Y =
X\beta\).
The idea is to try to predict the transformed targets \(\Omega\) as a
function of the transformed samples \(\Xi\), by computing \(\alpha
\in \mathbb{R}\) such that \(\Omega = \alpha \Xi\).
Then, we have \(Y = \Omega \Delta^T = \alpha \Xi \Delta^T\), and since
\(\Xi\) is the transformed training data we have that \(Y = X \alpha
P \Delta^T\), and as a result the coefficient matrix \(\beta = \alpha P
\Delta^T\).
\(\beta\) can be accessed through the coef_ attribute.
- 1.8.2. PLSSVD¶
PLSSVD is a simplified version of PLSCanonical
described earlier: instead of iteratively deflating the matrices \(X_k\)
and \(Y_k\), PLSSVD computes the SVD of \(C = X^TY\)
only once, and stores the n_components singular vectors corresponding to
the biggest singular values in the matrices U and V, corresponding to the
x_weights_ and y_weights_ attributes. Here, the transformed data is
simply transformed(X) = XU and transformed(Y) = YV.
If n_components == 1, PLSSVD and PLSCanonical are
strictly equivalent.
- 1.8.3. PLSRegression¶
The PLSRegression estimator is similar to
PLSCanonical with algorithm='nipals', with 2 significant
differences:
at step a) in the power method to compute \(u_k\) and \(v_k\),
\(v_k\) is never normalized.
at step c), the targets \(Y_k\) are approximated using the projection
of \(X_k\) (i.e. \(\xi_k\)) instead of the projection of
\(Y_k\) (i.e. \(\omega_k\)). In other words, the loadings
computation is different. As a result, the deflation in step d) will also
be affected.
These two modifications affect the output of predict and transform,
which are not the same as for PLSCanonical. Also, while the number
of components is limited by min(n_samples, n_features, n_targets) in
PLSCanonical, here the limit is the rank of \(X^TX\), i.e.
min(n_samples, n_features).
PLSRegression is also known as PLS1 (single targets) and PLS2
(multiple targets). Much like Lasso,
PLSRegression is a form of regularized linear regression where the
number of components controls the strength of the regularization.
- 1.8.4. Canonical Correlation Analysis¶
Canonical Correlation Analysis was developed prior and independently to PLS.
But it turns out that CCA is a special case of PLS, and corresponds
to PLS in “Mode B” in the literature.
CCA differs from PLSCanonical in the way the weights
\(u_k\) and \(v_k\) are computed in the power method of step a).
Details can be found in section 10 of [1].
Since CCA involves the inversion of \(X_k^TX_k\) and
\(Y_k^TY_k\), this estimator can be unstable if the number of features or
targets is greater than the number of samples.
Reference
Click for more details
¶
[1]
(1,2,3,4)
A survey of Partial Least Squares (PLS) methods, with emphasis on
the two-block case
JA Wegelin
Examples:
Compare cross decomposition methods
Principal Component Regression vs Partial Least Squares Regression
© 2007 - 2024, scikit-learn developers (BSD License).
Show this page source
### 1.9. Naive Bayes — scikit-learn 1.4.2 documentation

Install
User Guide
API
Examples
Community
Getting Started
Tutorial
What's new
Glossary
Development
FAQ
Support
Related packages
Roadmap
Governance
About us
GitHub
Other Versions and Download
More
Getting Started
Tutorial
What's new
Glossary
Development
FAQ
Support
Related packages
Roadmap
Governance
About us
GitHub
Other Versions and Download
Toggle Menu
PrevUp
Next
scikit-learn 1.4.2
Other versions
Please cite us if you use the software.
### 1.9. Naive Bayes

- 1.9.1. Gaussian Naive Bayes
- 1.9.2. Multinomial Naive Bayes
- 1.9.3. Complement Naive Bayes
- 1.9.4. Bernoulli Naive Bayes
- 1.9.5. Categorical Naive Bayes
- 1.9.6. Out-of-core naive Bayes model fitting
### 1.9. Naive Bayes¶

Naive Bayes methods are a set of supervised learning algorithms
based on applying Bayes’ theorem with the “naive” assumption of
conditional independence between every pair of features given the
value of the class variable. Bayes’ theorem states the following
relationship, given class variable \(y\) and dependent feature
vector \(x_1\) through \(x_n\), :
\[P(y \mid x_1, \dots, x_n) = \frac{P(y) P(x_1, \dots, x_n \mid y)}
{P(x_1, \dots, x_n)}\]
Using the naive conditional independence assumption that
\[P(x_i | y, x_1, \dots, x_{i-1}, x_{i+1}, \dots, x_n) = P(x_i | y),\]
for all \(i\), this relationship is simplified to
\[P(y \mid x_1, \dots, x_n) = \frac{P(y) \prod_{i=1}^{n} P(x_i \mid y)}
{P(x_1, \dots, x_n)}\]
Since \(P(x_1, \dots, x_n)\) is constant given the input,
we can use the following classification rule:
\[ \begin{align}\begin{aligned}P(y \mid x_1, \dots, x_n) \propto P(y) \prod_{i=1}^{n} P(x_i \mid y)\\\Downarrow\\\hat{y} = \arg\max_y P(y) \prod_{i=1}^{n} P(x_i \mid y),\end{aligned}\end{align} \]
and we can use Maximum A Posteriori (MAP) estimation to estimate
\(P(y)\) and \(P(x_i \mid y)\);
the former is then the relative frequency of class \(y\)
in the training set.
The different naive Bayes classifiers differ mainly by the assumptions they
make regarding the distribution of \(P(x_i \mid y)\).
In spite of their apparently over-simplified assumptions, naive Bayes
classifiers have worked quite well in many real-world situations, famously
document classification and spam filtering. They require a small amount
of training data to estimate the necessary parameters. (For theoretical
reasons why naive Bayes works well, and on which types of data it does, see
the references below.)
Naive Bayes learners and classifiers can be extremely fast compared to more
sophisticated methods.
The decoupling of the class conditional feature distributions means that each
distribution can be independently estimated as a one dimensional distribution.
This in turn helps to alleviate problems stemming from the curse of
dimensionality.
On the flip side, although naive Bayes is known as a decent classifier,
it is known to be a bad estimator, so the probability outputs from
predict_proba are not to be taken too seriously.
References
Click for more details
¶
H. Zhang (2004). The optimality of Naive Bayes.
Proc. FLAIRS.
- 1.9.1. Gaussian Naive Bayes¶
GaussianNB implements the Gaussian Naive Bayes algorithm for
classification. The likelihood of the features is assumed to be Gaussian:
\[P(x_i \mid y) = \frac{1}{\sqrt{2\pi\sigma^2_y}} \exp\left(-\frac{(x_i - \mu_y)^2}{2\sigma^2_y}\right)\]
The parameters \(\sigma_y\) and \(\mu_y\)
are estimated using maximum likelihood.
>>> from sklearn.datasets import load_iris
>>> from sklearn.model_selection import train_test_split
>>> from sklearn.naive_bayes import GaussianNB
>>> X, y = load_iris(return_X_y=True)
>>> X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=0)
>>> gnb = GaussianNB()
>>> y_pred = gnb.fit(X_train, y_train).predict(X_test)
>>> print("Number of mislabeled points out of a total %d points : %d"
...       % (X_test.shape[0], (y_test != y_pred).sum()))
Number of mislabeled points out of a total 75 points : 4
- 1.9.2. Multinomial Naive Bayes¶
MultinomialNB implements the naive Bayes algorithm for multinomially
distributed data, and is one of the two classic naive Bayes variants used in
text classification (where the data are typically represented as word vector
counts, although tf-idf vectors are also known to work well in practice).
The distribution is parametrized by vectors
\(\theta_y = (\theta_{y1},\ldots,\theta_{yn})\)
for each class \(y\), where \(n\) is the number of features
(in text classification, the size of the vocabulary)
and \(\theta_{yi}\) is the probability \(P(x_i \mid y)\)
of feature \(i\) appearing in a sample belonging to class \(y\).
The parameters \(\theta_y\) is estimated by a smoothed
version of maximum likelihood, i.e. relative frequency counting:
\[\hat{\theta}_{yi} = \frac{ N_{yi} + \alpha}{N_y + \alpha n}\]
where \(N_{yi} = \sum_{x \in T} x_i\) is
the number of times feature \(i\) appears in a sample of class \(y\)
in the training set \(T\),
and \(N_{y} = \sum_{i=1}^{n} N_{yi}\) is the total count of
all features for class \(y\).
The smoothing priors \(\alpha \ge 0\) accounts for
features not present in the learning samples and prevents zero probabilities
in further computations.
Setting \(\alpha = 1\) is called Laplace smoothing,
while \(\alpha < 1\) is called Lidstone smoothing.
- 1.9.3. Complement Naive Bayes¶
ComplementNB implements the complement naive Bayes (CNB) algorithm.
CNB is an adaptation of the standard multinomial naive Bayes (MNB) algorithm
that is particularly suited for imbalanced data sets. Specifically, CNB uses
statistics from the complement of each class to compute the model’s weights.
The inventors of CNB show empirically that the parameter estimates for CNB are
more stable than those for MNB. Further, CNB regularly outperforms MNB (often
by a considerable margin) on text classification tasks.
Weights calculation
Click for more details
¶
The procedure for calculating the weights is as follows:
\[ \begin{align}\begin{aligned}\hat{\theta}_{ci} = \frac{\alpha_i + \sum_{j:y_j \neq c} d_{ij}}
{\alpha + \sum_{j:y_j \neq c} \sum_{k} d_{kj}}\\w_{ci} = \log \hat{\theta}_{ci}\\w_{ci} = \frac{w_{ci}}{\sum_{j} |w_{cj}|}\end{aligned}\end{align} \]
where the summations are over all documents \(j\) not in class \(c\),
\(d_{ij}\) is either the count or tf-idf value of term \(i\) in document
\(j\), \(\alpha_i\) is a smoothing hyperparameter like that found in
MNB, and \(\alpha = \sum_{i} \alpha_i\). The second normalization addresses
the tendency for longer documents to dominate parameter estimates in MNB. The
classification rule is:
\[\hat{c} = \arg\min_c \sum_{i} t_i w_{ci}\]
i.e., a document is assigned to the class that is the poorest complement
match.
References
Click for more details
¶
Rennie, J. D., Shih, L., Teevan, J., & Karger, D. R. (2003).
Tackling the poor assumptions of naive bayes text classifiers.
In ICML (Vol. 3, pp. 616-623).
- 1.9.4. Bernoulli Naive Bayes¶
BernoulliNB implements the naive Bayes training and classification
algorithms for data that is distributed according to multivariate Bernoulli
distributions; i.e., there may be multiple features but each one is assumed
to be a binary-valued (Bernoulli, boolean) variable.
Therefore, this class requires samples to be represented as binary-valued
feature vectors; if handed any other kind of data, a BernoulliNB instance
may binarize its input (depending on the binarize parameter).
The decision rule for Bernoulli naive Bayes is based on
\[P(x_i \mid y) = P(x_i = 1 \mid y) x_i + (1 - P(x_i = 1 \mid y)) (1 - x_i)\]
which differs from multinomial NB’s rule
in that it explicitly penalizes the non-occurrence of a feature \(i\)
that is an indicator for class \(y\),
where the multinomial variant would simply ignore a non-occurring feature.
In the case of text classification, word occurrence vectors (rather than word
count vectors) may be used to train and use this classifier. BernoulliNB
might perform better on some datasets, especially those with shorter documents.
It is advisable to evaluate both models, if time permits.
References
Click for more details
¶
C.D. Manning, P. Raghavan and H. Schütze (2008). Introduction to
Information Retrieval. Cambridge University Press, pp. 234-265.
A. McCallum and K. Nigam (1998).
A comparison of event models for Naive Bayes text classification.
Proc. AAAI/ICML-98 Workshop on Learning for Text Categorization, pp. 41-48.
V. Metsis, I. Androutsopoulos and G. Paliouras (2006).
Spam filtering with Naive Bayes – Which Naive Bayes?
3rd Conf. on Email and Anti-Spam (CEAS).
- 1.9.5. Categorical Naive Bayes¶
CategoricalNB implements the categorical naive Bayes
algorithm for categorically distributed data. It assumes that each feature,
which is described by the index \(i\), has its own categorical
distribution.
For each feature \(i\) in the training set \(X\),
CategoricalNB estimates a categorical distribution for each feature i
of X conditioned on the class y. The index set of the samples is defined as
\(J = \{ 1, \dots, m \}\), with \(m\) as the number of samples.
Probability calculation
Click for more details
¶
The probability of category \(t\) in feature \(i\) given class
\(c\) is estimated as:
\[P(x_i = t \mid y = c \: ;\, \alpha) = \frac{ N_{tic} + \alpha}{N_{c} +
\alpha n_i},\]
where \(N_{tic} = |\{j \in J \mid x_{ij} = t, y_j = c\}|\) is the number
of times category \(t\) appears in the samples \(x_{i}\), which belong
to class \(c\), \(N_{c} = |\{ j \in J\mid y_j = c\}|\) is the number
of samples with class c, \(\alpha\) is a smoothing parameter and
\(n_i\) is the number of available categories of feature \(i\).
CategoricalNB assumes that the sample matrix \(X\) is encoded (for
instance with the help of OrdinalEncoder) such
that all categories for each feature \(i\) are represented with numbers
\(0, ..., n_i - 1\) where \(n_i\) is the number of available categories
of feature \(i\).
- 1.9.6. Out-of-core naive Bayes model fitting¶
Naive Bayes models can be used to tackle large scale classification problems
for which the full training set might not fit in memory. To handle this case,
MultinomialNB, BernoulliNB, and GaussianNB
expose a partial_fit method that can be used
incrementally as done with other classifiers as demonstrated in
Out-of-core classification of text documents. All naive Bayes
classifiers support sample weighting.
Contrary to the fit method, the first call to partial_fit needs to be
passed the list of all the expected class labels.
For an overview of available strategies in scikit-learn, see also the
out-of-core learning documentation.
Note
The partial_fit method call of naive Bayes models introduces some
computational overhead. It is recommended to use data chunk sizes that are as
large as possible, that is as the available RAM allows.
© 2007 - 2024, scikit-learn developers (BSD License).
Show this page source
### 1.10. Decision Trees — scikit-learn 1.4.2 documentation

Install
User Guide
API
Examples
Community
Getting Started
Tutorial
What's new
Glossary
Development
FAQ
Support
Related packages
Roadmap
Governance
About us
GitHub
Other Versions and Download
More
Getting Started
Tutorial
What's new
Glossary
Development
FAQ
Support
Related packages
Roadmap
Governance
About us
GitHub
Other Versions and Download
Toggle Menu
PrevUp
Next
scikit-learn 1.4.2
Other versions
Please cite us if you use the software.
### 1.10. Decision Trees

- 1.10.1. Classification
- 1.10.2. Regression
- 1.10.3. Multi-output problems
- 1.10.4. Complexity
- 1.10.5. Tips on practical use
- 1.10.6. Tree algorithms: ID3, C4.5, C5.0 and CART
- 1.10.7. Mathematical formulation
1.10.7.1. Classification criteria
1.10.7.2. Regression criteria
- 1.10.8. Missing Values Support
- 1.10.9. Minimal Cost-Complexity Pruning
### 1.10. Decision Trees¶

Decision Trees (DTs) are a non-parametric supervised learning method used
for classification and regression. The goal is to create a model that predicts the value of a
target variable by learning simple decision rules inferred from the data
features. A tree can be seen as a piecewise constant approximation.
For instance, in the example below, decision trees learn from data to
approximate a sine curve with a set of if-then-else decision rules. The deeper
the tree, the more complex the decision rules and the fitter the model.
Some advantages of decision trees are:
Simple to understand and to interpret. Trees can be visualized.
Requires little data preparation. Other techniques often require data
normalization, dummy variables need to be created and blank values to
be removed. Some tree and algorithm combinations support
missing values.
The cost of using the tree (i.e., predicting data) is logarithmic in the
number of data points used to train the tree.
Able to handle both numerical and categorical data. However, the scikit-learn
implementation does not support categorical variables for now. Other
techniques are usually specialized in analyzing datasets that have only one type
of variable. See algorithms for more
information.
Able to handle multi-output problems.
Uses a white box model. If a given situation is observable in a model,
the explanation for the condition is easily explained by boolean logic.
By contrast, in a black box model (e.g., in an artificial neural
network), results may be more difficult to interpret.
Possible to validate a model using statistical tests. That makes it
possible to account for the reliability of the model.
Performs well even if its assumptions are somewhat violated by
the true model from which the data were generated.
The disadvantages of decision trees include:
Decision-tree learners can create over-complex trees that do not
generalize the data well. This is called overfitting. Mechanisms
such as pruning, setting the minimum number of samples required
at a leaf node or setting the maximum depth of the tree are
necessary to avoid this problem.
Decision trees can be unstable because small variations in the
data might result in a completely different tree being generated.
This problem is mitigated by using decision trees within an
ensemble.
Predictions of decision trees are neither smooth nor continuous, but
piecewise constant approximations as seen in the above figure. Therefore,
they are not good at extrapolation.
The problem of learning an optimal decision tree is known to be
NP-complete under several aspects of optimality and even for simple
concepts. Consequently, practical decision-tree learning algorithms
are based on heuristic algorithms such as the greedy algorithm where
locally optimal decisions are made at each node. Such algorithms
cannot guarantee to return the globally optimal decision tree.  This
can be mitigated by training multiple trees in an ensemble learner,
where the features and samples are randomly sampled with replacement.
There are concepts that are hard to learn because decision trees
do not express them easily, such as XOR, parity or multiplexer problems.
Decision tree learners create biased trees if some classes dominate.
It is therefore recommended to balance the dataset prior to fitting
with the decision tree.
- 1.10.1. Classification¶
DecisionTreeClassifier is a class capable of performing multi-class
classification on a dataset.
As with other classifiers, DecisionTreeClassifier takes as input two arrays:
an array X, sparse or dense, of shape (n_samples, n_features) holding the
training samples, and an array Y of integer values, shape (n_samples,),
holding the class labels for the training samples:
>>> from sklearn import tree
>>> X = [[0, 0], [1, 1]]
>>> Y = [0, 1]
>>> clf = tree.DecisionTreeClassifier()
>>> clf = clf.fit(X, Y)
After being fitted, the model can then be used to predict the class of samples:
>>> clf.predict([[2., 2.]])
array([1])
In case that there are multiple classes with the same and highest
probability, the classifier will predict the class with the lowest index
amongst those classes.
As an alternative to outputting a specific class, the probability of each class
can be predicted, which is the fraction of training samples of the class in a
leaf:
>>> clf.predict_proba([[2., 2.]])
array([[0., 1.]])
DecisionTreeClassifier is capable of both binary (where the
labels are [-1, 1]) classification and multiclass (where the labels are
[0, …, K-1]) classification.
Using the Iris dataset, we can construct a tree as follows:
>>> from sklearn.datasets import load_iris
>>> from sklearn import tree
>>> iris = load_iris()
>>> X, y = iris.data, iris.target
>>> clf = tree.DecisionTreeClassifier()
>>> clf = clf.fit(X, y)
Once trained, you can plot the tree with the plot_tree function:
>>> tree.plot_tree(clf)
[...]
Alternative ways to export trees
Click for more details
¶
We can also export the tree in Graphviz format using the export_graphviz
exporter. If you use the conda package manager, the graphviz binaries
and the python package can be installed with conda install python-graphviz.
Alternatively binaries for graphviz can be downloaded from the graphviz project homepage,
and the Python wrapper installed from pypi with pip install graphviz.
Below is an example graphviz export of the above tree trained on the entire
iris dataset; the results are saved in an output file iris.pdf:
>>> import graphviz
>>> dot_data = tree.export_graphviz(clf, out_file=None)
>>> graph = graphviz.Source(dot_data)
>>> graph.render("iris")
The export_graphviz exporter also supports a variety of aesthetic
options, including coloring nodes by their class (or value for regression) and
using explicit variable and class names if desired. Jupyter notebooks also
render these plots inline automatically:
>>> dot_data = tree.export_graphviz(clf, out_file=None,
...                      feature_names=iris.feature_names,
...                      class_names=iris.target_names,
...                      filled=True, rounded=True,
...                      special_characters=True)
>>> graph = graphviz.Source(dot_data)
>>> graph
Alternatively, the tree can also be exported in textual format with the
function export_text. This method doesn’t require the installation
of external libraries and is more compact:
>>> from sklearn.datasets import load_iris
>>> from sklearn.tree import DecisionTreeClassifier
>>> from sklearn.tree import export_text
>>> iris = load_iris()
>>> decision_tree = DecisionTreeClassifier(random_state=0, max_depth=2)
>>> decision_tree = decision_tree.fit(iris.data, iris.target)
>>> r = export_text(decision_tree, feature_names=iris['feature_names'])
>>> print(r)
|--- petal width (cm) <= 0.80
|   |--- class: 0
|--- petal width (cm) >  0.80
|   |--- petal width (cm) <= 1.75
|   |   |--- class: 1
|   |--- petal width (cm) >  1.75
|   |   |--- class: 2
Examples:
Plot the decision surface of decision trees trained on the iris dataset
Understanding the decision tree structure
- 1.10.2. Regression¶
Decision trees can also be applied to regression problems, using the
DecisionTreeRegressor class.
As in the classification setting, the fit method will take as argument arrays X
and y, only that in this case y is expected to have floating point values
instead of integer values:
>>> from sklearn import tree
>>> X = [[0, 0], [2, 2]]
>>> y = [0.5, 2.5]
>>> clf = tree.DecisionTreeRegressor()
>>> clf = clf.fit(X, y)
>>> clf.predict([[1, 1]])
array([0.5])
Examples:
Decision Tree Regression
- 1.10.3. Multi-output problems¶
A multi-output problem is a supervised learning problem with several outputs
to predict, that is when Y is a 2d array of shape (n_samples, n_outputs).
When there is no correlation between the outputs, a very simple way to solve
this kind of problem is to build n independent models, i.e. one for each
output, and then to use those models to independently predict each one of the n
outputs. However, because it is likely that the output values related to the
same input are themselves correlated, an often better way is to build a single
model capable of predicting simultaneously all n outputs. First, it requires
lower training time since only a single estimator is built. Second, the
generalization accuracy of the resulting estimator may often be increased.
With regard to decision trees, this strategy can readily be used to support
multi-output problems. This requires the following changes:
Store n output values in leaves, instead of 1;
Use splitting criteria that compute the average reduction across all
n outputs.
This module offers support for multi-output problems by implementing this
strategy in both DecisionTreeClassifier and
DecisionTreeRegressor. If a decision tree is fit on an output array Y
of shape (n_samples, n_outputs) then the resulting estimator will:
Output n_output values upon predict;
Output a list of n_output arrays of class probabilities upon
predict_proba.
The use of multi-output trees for regression is demonstrated in
Multi-output Decision Tree Regression. In this example, the input
X is a single real value and the outputs Y are the sine and cosine of X.
The use of multi-output trees for classification is demonstrated in
Face completion with a multi-output estimators. In this example, the inputs
X are the pixels of the upper half of faces and the outputs Y are the pixels of
the lower half of those faces.
Examples:
Multi-output Decision Tree Regression
Face completion with a multi-output estimators
References
Click for more details
¶
M. Dumont et al,  Fast multi-class image annotation with random subwindows
and multiple output randomized trees, International Conference on
Computer Vision Theory and Applications 2009
- 1.10.4. Complexity¶
In general, the run time cost to construct a balanced binary tree is
\(O(n_{samples}n_{features}\log(n_{samples}))\) and query time
\(O(\log(n_{samples}))\).  Although the tree construction algorithm attempts
to generate balanced trees, they will not always be balanced.  Assuming that the
subtrees remain approximately balanced, the cost at each node consists of
searching through \(O(n_{features})\) to find the feature that offers the
largest reduction in the impurity criterion, e.g. log loss (which is equivalent to an
information gain). This has a cost of
\(O(n_{features}n_{samples}\log(n_{samples}))\) at each node, leading to a
total cost over the entire trees (by summing the cost at each node) of
\(O(n_{features}n_{samples}^{2}\log(n_{samples}))\).
- 1.10.5. Tips on practical use¶
Decision trees tend to overfit on data with a large number of features.
Getting the right ratio of samples to number of features is important, since
a tree with few samples in high dimensional space is very likely to overfit.
Consider performing  dimensionality reduction (PCA,
ICA, or Feature selection) beforehand to
give your tree a better chance of finding features that are discriminative.
Understanding the decision tree structure will help
in gaining more insights about how the decision tree makes predictions, which is
important for understanding the important features in the data.
Visualize your tree as you are training by using the export
function.  Use max_depth=3 as an initial tree depth to get a feel for
how the tree is fitting to your data, and then increase the depth.
Remember that the number of samples required to populate the tree doubles
for each additional level the tree grows to.  Use max_depth to control
the size of the tree to prevent overfitting.
Use min_samples_split or min_samples_leaf to ensure that multiple
samples inform every decision in the tree, by controlling which splits will
be considered. A very small number will usually mean the tree will overfit,
whereas a large number will prevent the tree from learning the data. Try
min_samples_leaf=5 as an initial value. If the sample size varies
greatly, a float number can be used as percentage in these two parameters.
While min_samples_split can create arbitrarily small leaves,
min_samples_leaf guarantees that each leaf has a minimum size, avoiding
low-variance, over-fit leaf nodes in regression problems.  For
classification with few classes, min_samples_leaf=1 is often the best
choice.
Note that min_samples_split considers samples directly and independent of
sample_weight, if provided (e.g. a node with m weighted samples is still
treated as having exactly m samples). Consider min_weight_fraction_leaf or
min_impurity_decrease if accounting for sample weights is required at splits.
Balance your dataset before training to prevent the tree from being biased
toward the classes that are dominant. Class balancing can be done by
sampling an equal number of samples from each class, or preferably by
normalizing the sum of the sample weights (sample_weight) for each
class to the same value. Also note that weight-based pre-pruning criteria,
such as min_weight_fraction_leaf, will then be less biased toward
dominant classes than criteria that are not aware of the sample weights,
like min_samples_leaf.
If the samples are weighted, it will be easier to optimize the tree
structure using weight-based pre-pruning criterion such as
min_weight_fraction_leaf, which ensure that leaf nodes contain at least
a fraction of the overall sum of the sample weights.
All decision trees use np.float32 arrays internally.
If training data is not in this format, a copy of the dataset will be made.
If the input matrix X is very sparse, it is recommended to convert to sparse
csc_matrix before calling fit and sparse csr_matrix before calling
predict. Training time can be orders of magnitude faster for a sparse
matrix input compared to a dense matrix when features have zero values in
most of the samples.
- 1.10.6. Tree algorithms: ID3, C4.5, C5.0 and CART¶
What are all the various decision tree algorithms and how do they differ
from each other? Which one is implemented in scikit-learn?
Various decision tree algorithms
Click for more details
¶
ID3 (Iterative Dichotomiser 3) was developed in 1986 by Ross Quinlan.
The algorithm creates a multiway tree, finding for each node (i.e. in
a greedy manner) the categorical feature that will yield the largest
information gain for categorical targets. Trees are grown to their
maximum size and then a pruning step is usually applied to improve the
ability of the tree to generalize to unseen data.
C4.5 is the successor to ID3 and removed the restriction that features
must be categorical by dynamically defining a discrete attribute (based
on numerical variables) that partitions the continuous attribute value
into a discrete set of intervals. C4.5 converts the trained trees
(i.e. the output of the ID3 algorithm) into sets of if-then rules.
The accuracy of each rule is then evaluated to determine the order
in which they should be applied. Pruning is done by removing a rule’s
precondition if the accuracy of the rule improves without it.
C5.0 is Quinlan’s latest version release under a proprietary license.
It uses less memory and builds smaller rulesets than C4.5 while being
more accurate.
CART (Classification and Regression Trees) is very similar to C4.5, but
it differs in that it supports numerical target variables (regression) and
does not compute rule sets. CART constructs binary trees using the feature
and threshold that yield the largest information gain at each node.
scikit-learn uses an optimized version of the CART algorithm; however, the
scikit-learn implementation does not support categorical variables for now.
- 1.10.7. Mathematical formulation¶
Given training vectors \(x_i \in R^n\), i=1,…, l and a label vector
\(y \in R^l\), a decision tree recursively partitions the feature space
such that the samples with the same labels or similar target values are grouped
together.
Let the data at node \(m\) be represented by \(Q_m\) with \(n_m\)
samples. For each candidate split \(\theta = (j, t_m)\) consisting of a
feature \(j\) and threshold \(t_m\), partition the data into
\(Q_m^{left}(\theta)\) and \(Q_m^{right}(\theta)\) subsets
\[ \begin{align}\begin{aligned}Q_m^{left}(\theta) = \{(x, y) | x_j \leq t_m\}\\Q_m^{right}(\theta) = Q_m \setminus Q_m^{left}(\theta)\end{aligned}\end{align} \]
The quality of a candidate split of node \(m\) is then computed using an
impurity function or loss function \(H()\), the choice of which depends on
the task being solved (classification or regression)
\[G(Q_m, \theta) = \frac{n_m^{left}}{n_m} H(Q_m^{left}(\theta))
+ \frac{n_m^{right}}{n_m} H(Q_m^{right}(\theta))\]
Select the parameters that minimises the impurity
\[\theta^* = \operatorname{argmin}_\theta  G(Q_m, \theta)\]
Recurse for subsets \(Q_m^{left}(\theta^*)\) and
\(Q_m^{right}(\theta^*)\) until the maximum allowable depth is reached,
\(n_m < \min_{samples}\) or \(n_m = 1\).
1.10.7.1. Classification criteria¶
If a target is a classification outcome taking on values 0,1,…,K-1,
for node \(m\), let
\[p_{mk} = \frac{1}{n_m} \sum_{y \in Q_m} I(y = k)\]
be the proportion of class k observations in node \(m\). If \(m\) is a
terminal node, predict_proba for this region is set to \(p_{mk}\).
Common measures of impurity are the following.
Gini:
\[H(Q_m) = \sum_k p_{mk} (1 - p_{mk})\]
Log Loss or Entropy:
\[H(Q_m) = - \sum_k p_{mk} \log(p_{mk})\]
Shannon entropy
Click for more details
¶
The entropy criterion computes the Shannon entropy of the possible classes. It
takes the class frequencies of the training data points that reached a given
leaf \(m\) as their probability. Using the Shannon entropy as tree node
splitting criterion is equivalent to minimizing the log loss (also known as
cross-entropy and multinomial deviance) between the true labels \(y_i\)
and the probabilistic predictions \(T_k(x_i)\) of the tree model \(T\) for class \(k\).
To see this, first recall that the log loss of a tree model \(T\)
computed on a dataset \(D\) is defined as follows:
\[\mathrm{LL}(D, T) = -\frac{1}{n} \sum_{(x_i, y_i) \in D} \sum_k I(y_i = k) \log(T_k(x_i))\]
where \(D\) is a training dataset of \(n\) pairs \((x_i, y_i)\).
In a classification tree, the predicted class probabilities within leaf nodes
are constant, that is: for all \((x_i, y_i) \in Q_m\), one has:
\(T_k(x_i) = p_{mk}\) for each class \(k\).
This property makes it possible to rewrite \(\mathrm{LL}(D, T)\) as the
sum of the Shannon entropies computed for each leaf of \(T\) weighted by
the number of training data points that reached each leaf:
\[\mathrm{LL}(D, T) = \sum_{m \in T} \frac{n_m}{n} H(Q_m)\]
1.10.7.2. Regression criteria¶
If the target is a continuous value, then for node \(m\), common
criteria to minimize as for determining locations for future splits are Mean
Squared Error (MSE or L2 error), Poisson deviance as well as Mean Absolute
Error (MAE or L1 error). MSE and Poisson deviance both set the predicted value
of terminal nodes to the learned mean value \(\bar{y}_m\) of the node
whereas the MAE sets the predicted value of terminal nodes to the median
\(median(y)_m\).
Mean Squared Error:
\[ \begin{align}\begin{aligned}\bar{y}_m = \frac{1}{n_m} \sum_{y \in Q_m} y\\H(Q_m) = \frac{1}{n_m} \sum_{y \in Q_m} (y - \bar{y}_m)^2\end{aligned}\end{align} \]
Half Poisson deviance:
\[H(Q_m) = \frac{1}{n_m} \sum_{y \in Q_m} (y \log\frac{y}{\bar{y}_m}
- y + \bar{y}_m)\]
Setting criterion="poisson" might be a good choice if your target is a count
or a frequency (count per some unit). In any case, \(y >= 0\) is a
necessary condition to use this criterion. Note that it fits much slower than
the MSE criterion.
Mean Absolute Error:
\[ \begin{align}\begin{aligned}median(y)_m = \underset{y \in Q_m}{\mathrm{median}}(y)\\H(Q_m) = \frac{1}{n_m} \sum_{y \in Q_m} |y - median(y)_m|\end{aligned}\end{align} \]
Note that it fits much slower than the MSE criterion.
- 1.10.8. Missing Values Support¶
DecisionTreeClassifier and DecisionTreeRegressor
have built-in support for missing values when splitter='best' and criterion is
'gini', 'entropy’, or 'log_loss', for classification or
'squared_error', 'friedman_mse', or 'poisson' for regression.
For each potential threshold on the non-missing data, the splitter will evaluate
the split with all the missing values going to the left node or the right node.
Decisions are made as follows:
By default when predicting, the samples with missing values are classified
with the class used in the split found during training:
>>> from sklearn.tree import DecisionTreeClassifier
>>> import numpy as np
>>> X = np.array([0, 1, 6, np.nan]).reshape(-1, 1)
>>> y = [0, 0, 1, 1]
>>> tree = DecisionTreeClassifier(random_state=0).fit(X, y)
>>> tree.predict(X)
array([0, 0, 1, 1])
If the criterion evaluation is the same for both nodes,
then the tie for missing value at predict time is broken by going to the
right node. The splitter also checks the split where all the missing
values go to one child and non-missing values go to the other:
>>> from sklearn.tree import DecisionTreeClassifier
>>> import numpy as np
>>> X = np.array([np.nan, -1, np.nan, 1]).reshape(-1, 1)
>>> y = [0, 0, 1, 1]
>>> tree = DecisionTreeClassifier(random_state=0).fit(X, y)
>>> X_test = np.array([np.nan]).reshape(-1, 1)
>>> tree.predict(X_test)
array([1])
If no missing values are seen during training for a given feature, then during
prediction missing values are mapped to the child with the most samples:
>>> from sklearn.tree import DecisionTreeClassifier
>>> import numpy as np
>>> X = np.array([0, 1, 2, 3]).reshape(-1, 1)
>>> y = [0, 1, 1, 1]
>>> tree = DecisionTreeClassifier(random_state=0).fit(X, y)
>>> X_test = np.array([np.nan]).reshape(-1, 1)
>>> tree.predict(X_test)
array([1])
- 1.10.9. Minimal Cost-Complexity Pruning¶
Minimal cost-complexity pruning is an algorithm used to prune a tree to avoid
over-fitting, described in Chapter 3 of [BRE]. This algorithm is parameterized
by \(\alpha\ge0\) known as the complexity parameter. The complexity
parameter is used to define the cost-complexity measure, \(R_\alpha(T)\) of
a given tree \(T\):
\[R_\alpha(T) = R(T) + \alpha|\widetilde{T}|\]
where \(|\widetilde{T}|\) is the number of terminal nodes in \(T\) and \(R(T)\)
is traditionally defined as the total misclassification rate of the terminal
nodes. Alternatively, scikit-learn uses the total sample weighted impurity of
the terminal nodes for \(R(T)\). As shown above, the impurity of a node
depends on the criterion. Minimal cost-complexity pruning finds the subtree of
\(T\) that minimizes \(R_\alpha(T)\).
The cost complexity measure of a single node is
\(R_\alpha(t)=R(t)+\alpha\). The branch, \(T_t\), is defined to be a
tree where node \(t\) is its root. In general, the impurity of a node
is greater than the sum of impurities of its terminal nodes,
\(R(T_t)<R(t)\). However, the cost complexity measure of a node,
\(t\), and its branch, \(T_t\), can be equal depending on
\(\alpha\). We define the effective \(\alpha\) of a node to be the
value where they are equal, \(R_\alpha(T_t)=R_\alpha(t)\) or
\(\alpha_{eff}(t)=\frac{R(t)-R(T_t)}{|T|-1}\). A non-terminal node
with the smallest value of \(\alpha_{eff}\) is the weakest link and will
be pruned. This process stops when the pruned tree’s minimal
\(\alpha_{eff}\) is greater than the ccp_alpha parameter.
Examples:
Post pruning decision trees with cost complexity pruning
References
Click for more details
¶
[BRE]
L. Breiman, J. Friedman, R. Olshen, and C. Stone. Classification
and Regression Trees. Wadsworth, Belmont, CA, 1984.
https://en.wikipedia.org/wiki/Decision_tree_learning
https://en.wikipedia.org/wiki/Predictive_analytics
J.R. Quinlan. C4. 5: programs for machine learning. Morgan
Kaufmann, 1993.
T. Hastie, R. Tibshirani and J. Friedman. Elements of Statistical
Learning, Springer, 2009.
© 2007 - 2024, scikit-learn developers (BSD License).
Show this page source
### 1.11. Ensembles: Gradient boosting, random forests, bagging, voting, stacking — scikit-learn 1.4.2 documentation

Install
User Guide
API
Examples
Community
Getting Started
Tutorial
What's new
Glossary
Development
FAQ
Support
Related packages
Roadmap
Governance
About us
GitHub
Other Versions and Download
More
Getting Started
Tutorial
What's new
Glossary
Development
FAQ
Support
Related packages
Roadmap
Governance
About us
GitHub
Other Versions and Download
Toggle Menu
PrevUp
Next
scikit-learn 1.4.2
Other versions
Please cite us if you use the software.
### 1.11. Ensembles: Gradient boosting, random forests, bagging, voting, stacking

- 1.11.1. Gradient-boosted trees
1.11.1.1. Histogram-Based Gradient Boosting
1.11.1.1.1. Usage
1.11.1.1.2. Missing values support
1.11.1.1.3. Sample weight support
1.11.1.1.4. Categorical Features Support
1.11.1.1.5. Monotonic Constraints
1.11.1.1.6. Interaction constraints
1.11.1.1.7. Low-level parallelism
1.11.1.1.8. Why it’s faster
1.11.1.2. GradientBoostingClassifier and GradientBoostingRegressor
1.11.1.2.1. Classification
1.11.1.2.2. Regression
1.11.1.2.3. Fitting additional weak-learners
1.11.1.2.4. Controlling the tree size
1.11.1.2.5. Mathematical formulation
1.11.1.2.5.1. Regression
1.11.1.2.5.2. Classification
1.11.1.2.6. Loss Functions
1.11.1.2.7. Shrinkage via learning rate
1.11.1.2.8. Subsampling
1.11.1.2.9. Interpretation with feature importance
- 1.11.2. Random forests and other randomized tree ensembles
1.11.2.1. Random Forests
1.11.2.2. Extremely Randomized Trees
1.11.2.3. Parameters
1.11.2.4. Parallelization
1.11.2.5. Feature importance evaluation
1.11.2.6. Totally Random Trees Embedding
- 1.11.3. Bagging meta-estimator
- 1.11.4. Voting Classifier
1.11.4.1. Majority Class Labels (Majority/Hard Voting)
1.11.4.2. Usage
1.11.4.3. Weighted Average Probabilities (Soft Voting)
1.11.4.4. Using the VotingClassifier with GridSearchCV
1.11.4.5. Usage
- 1.11.5. Voting Regressor
1.11.5.1. Usage
- 1.11.6. Stacked generalization
- 1.11.7. AdaBoost
1.11.7.1. Usage
### 1.11. Ensembles: Gradient boosting, random forests, bagging, voting, stacking¶

Ensemble methods combine the predictions of several
base estimators built with a given learning algorithm in order to improve
generalizability / robustness over a single estimator.
Two very famous examples of ensemble methods are gradient-boosted trees and random forests.
More generally, ensemble models can be applied to any base learner beyond
trees, in averaging methods such as Bagging methods,
model stacking, or Voting, or in
boosting, as AdaBoost.
Gradient-boosted trees
Random forests and other randomized tree ensembles
Bagging meta-estimator
Voting Classifier
Voting Regressor
Stacked generalization
AdaBoost
- 1.11.1. Gradient-boosted trees¶
Gradient Tree Boosting
or Gradient Boosted Decision Trees (GBDT) is a generalization
of boosting to arbitrary differentiable loss functions, see the seminal work of
[Friedman2001]. GBDT is an excellent model for both regression and
classification, in particular for tabular data.
GradientBoostingClassifier vs HistGradientBoostingClassifier
Scikit-learn provides two implementations of gradient-boosted trees:
HistGradientBoostingClassifier vs
GradientBoostingClassifier for classification, and the
corresponding classes for regression. The former can be orders of
magnitude faster than the latter when the number of samples is
larger than tens of thousands of samples.
Missing values and categorical data are natively supported by the
Hist… version, removing the need for additional preprocessing such as
imputation.
GradientBoostingClassifier and
GradientBoostingRegressor, might be preferred for small sample
sizes since binning may lead to split points that are too approximate
in this setting.
1.11.1.1. Histogram-Based Gradient Boosting¶
Scikit-learn 0.21 introduced two new implementations of
gradient boosted trees, namely HistGradientBoostingClassifier
and HistGradientBoostingRegressor, inspired by
LightGBM (See [LightGBM]).
These histogram-based estimators can be orders of magnitude faster
than GradientBoostingClassifier and
GradientBoostingRegressor when the number of samples is larger
than tens of thousands of samples.
They also have built-in support for missing values, which avoids the need
for an imputer.
These fast estimators first bin the input samples X into
integer-valued bins (typically 256 bins) which tremendously reduces the
number of splitting points to consider, and allows the algorithm to
leverage integer-based data structures (histograms) instead of relying on
sorted continuous values when building the trees. The API of these
estimators is slightly different, and some of the features from
GradientBoostingClassifier and GradientBoostingRegressor
are not yet supported, for instance some loss functions.
Examples:
Partial Dependence and Individual Conditional Expectation Plots
1.11.1.1.1. Usage¶
Most of the parameters are unchanged from
GradientBoostingClassifier and GradientBoostingRegressor.
One exception is the max_iter parameter that replaces n_estimators, and
controls the number of iterations of the boosting process:
>>> from sklearn.ensemble import HistGradientBoostingClassifier
>>> from sklearn.datasets import make_hastie_10_2
>>> X, y = make_hastie_10_2(random_state=0)
>>> X_train, X_test = X[:2000], X[2000:]
>>> y_train, y_test = y[:2000], y[2000:]
>>> clf = HistGradientBoostingClassifier(max_iter=100).fit(X_train, y_train)
>>> clf.score(X_test, y_test)
0.8965
Available losses for regression are ‘squared_error’,
‘absolute_error’, which is less sensitive to outliers, and
‘poisson’, which is well suited to model counts and frequencies. For
classification, ‘log_loss’ is the only option. For binary classification it uses the
binary log loss, also known as binomial deviance or binary cross-entropy. For
n_classes >= 3, it uses the multi-class log loss function, with multinomial deviance
and categorical cross-entropy as alternative names. The appropriate loss version is
selected based on y passed to fit.
The size of the trees can be controlled through the max_leaf_nodes,
max_depth, and min_samples_leaf parameters.
The number of bins used to bin the data is controlled with the max_bins
parameter. Using less bins acts as a form of regularization. It is
generally recommended to use as many bins as possible (256), which is the default.
The l2_regularization parameter is a regularizer on the loss function and
corresponds to \(\lambda\) in equation (2) of [XGBoost].
Note that early-stopping is enabled by default if the number of samples is
larger than 10,000. The early-stopping behaviour is controlled via the
early_stopping, scoring, validation_fraction,
n_iter_no_change, and tol parameters. It is possible to early-stop
using an arbitrary scorer, or just the training or validation loss.
Note that for technical reasons, using a callable as a scorer is significantly slower
than using the loss. By default, early-stopping is performed if there are at least
10,000 samples in the training set, using the validation loss.
1.11.1.1.2. Missing values support¶
HistGradientBoostingClassifier and
HistGradientBoostingRegressor have built-in support for missing
values (NaNs).
During training, the tree grower learns at each split point whether samples
with missing values should go to the left or right child, based on the
potential gain. When predicting, samples with missing values are assigned to
the left or right child consequently:
>>> from sklearn.ensemble import HistGradientBoostingClassifier
>>> import numpy as np
>>> X = np.array([0, 1, 2, np.nan]).reshape(-1, 1)
>>> y = [0, 0, 1, 1]
>>> gbdt = HistGradientBoostingClassifier(min_samples_leaf=1).fit(X, y)
>>> gbdt.predict(X)
array([0, 0, 1, 1])
When the missingness pattern is predictive, the splits can be performed on
whether the feature value is missing or not:
>>> X = np.array([0, np.nan, 1, 2, np.nan]).reshape(-1, 1)
>>> y = [0, 1, 0, 0, 1]
>>> gbdt = HistGradientBoostingClassifier(min_samples_leaf=1,
...                                       max_depth=2,
...                                       learning_rate=1,
...                                       max_iter=1).fit(X, y)
>>> gbdt.predict(X)
array([0, 1, 0, 0, 1])
If no missing values were encountered for a given feature during training,
then samples with missing values are mapped to whichever child has the most
samples.
1.11.1.1.3. Sample weight support¶
HistGradientBoostingClassifier and
HistGradientBoostingRegressor support sample weights during
fit.
The following toy example demonstrates that samples with a sample weight of zero are ignored:
>>> X = [[1, 0],
...      [1, 0],
...      [1, 0],
...      [0, 1]]
>>> y = [0, 0, 1, 0]
>>> # ignore the first 2 training samples by setting their weight to 0
>>> sample_weight = [0, 0, 1, 1]
>>> gb = HistGradientBoostingClassifier(min_samples_leaf=1)
>>> gb.fit(X, y, sample_weight=sample_weight)
HistGradientBoostingClassifier(...)
>>> gb.predict([[1, 0]])
array([1])
>>> gb.predict_proba([[1, 0]])[0, 1]
0.99...
As you can see, the [1, 0] is comfortably classified as 1 since the first
two samples are ignored due to their sample weights.
Implementation detail: taking sample weights into account amounts to
multiplying the gradients (and the hessians) by the sample weights. Note that
the binning stage (specifically the quantiles computation) does not take the
weights into account.
1.11.1.1.4. Categorical Features Support¶
HistGradientBoostingClassifier and
HistGradientBoostingRegressor have native support for categorical
features: they can consider splits on non-ordered, categorical data.
For datasets with categorical features, using the native categorical support
is often better than relying on one-hot encoding
(OneHotEncoder), because one-hot encoding
requires more tree depth to achieve equivalent splits. It is also usually
better to rely on the native categorical support rather than to treat
categorical features as continuous (ordinal), which happens for ordinal-encoded
categorical data, since categories are nominal quantities where order does not
matter.
To enable categorical support, a boolean mask can be passed to the
categorical_features parameter, indicating which feature is categorical. In
the following, the first feature will be treated as categorical and the
second feature as numerical:
>>> gbdt = HistGradientBoostingClassifier(categorical_features=[True, False])
Equivalently, one can pass a list of integers indicating the indices of the
categorical features:
>>> gbdt = HistGradientBoostingClassifier(categorical_features=[0])
When the input is a DataFrame, it is also possible to pass a list of column
names:
>>> gbdt = HistGradientBoostingClassifier(categorical_features=["site", "manufacturer"])
Finally, when the input is a DataFrame we can use
categorical_features="from_dtype" in which case all columns with a categorical
dtype will be treated as categorical features.
The cardinality of each categorical feature must be less than the max_bins
parameter. For an example using histogram-based gradient boosting on categorical
features, see
Categorical Feature Support in Gradient Boosting.
If there are missing values during training, the missing values will be
treated as a proper category. If there are no missing values during training,
then at prediction time, missing values are mapped to the child node that has
the most samples (just like for continuous features). When predicting,
categories that were not seen during fit time will be treated as missing
values.
Split finding with categorical features: The canonical way of considering
categorical splits in a tree is to consider
all of the \(2^{K - 1} - 1\) partitions, where \(K\) is the number of
categories. This can quickly become prohibitive when \(K\) is large.
Fortunately, since gradient boosting trees are always regression trees (even
for classification problems), there exist a faster strategy that can yield
equivalent splits. First, the categories of a feature are sorted according to
the variance of the target, for each category k. Once the categories are
sorted, one can consider continuous partitions, i.e. treat the categories
as if they were ordered continuous values (see Fisher [Fisher1958] for a
formal proof). As a result, only \(K - 1\) splits need to be considered
instead of \(2^{K - 1} - 1\). The initial sorting is a
\(\mathcal{O}(K \log(K))\) operation, leading to a total complexity of
\(\mathcal{O}(K \log(K) + K)\), instead of \(\mathcal{O}(2^K)\).
Examples:
Categorical Feature Support in Gradient Boosting
1.11.1.1.5. Monotonic Constraints¶
Depending on the problem at hand, you may have prior knowledge indicating
that a given feature should in general have a positive (or negative) effect
on the target value. For example, all else being equal, a higher credit
score should increase the probability of getting approved for a loan.
Monotonic constraints allow you to incorporate such prior knowledge into the
model.
For a predictor \(F\) with two features:
a monotonic increase constraint is a constraint of the form:
\[x_1 \leq x_1' \implies F(x_1, x_2) \leq F(x_1', x_2)\]
a monotonic decrease constraint is a constraint of the form:
\[x_1 \leq x_1' \implies F(x_1, x_2) \geq F(x_1', x_2)\]
You can specify a monotonic constraint on each feature using the
monotonic_cst parameter. For each feature, a value of 0 indicates no
constraint, while 1 and -1 indicate a monotonic increase and
monotonic decrease constraint, respectively:
>>> from sklearn.ensemble import HistGradientBoostingRegressor
... # monotonic increase, monotonic decrease, and no constraint on the 3 features
>>> gbdt = HistGradientBoostingRegressor(monotonic_cst=[1, -1, 0])
In a binary classification context, imposing a monotonic increase (decrease) constraint means that higher values of the feature are supposed
to have a positive (negative) effect on the probability of samples
to belong to the positive class.
Nevertheless, monotonic constraints only marginally constrain feature effects on the output.
For instance, monotonic increase and decrease constraints cannot be used to enforce the
following modelling constraint:
\[x_1 \leq x_1' \implies F(x_1, x_2) \leq F(x_1', x_2')\]
Also, monotonic constraints are not supported for multiclass classification.
Note
Since categories are unordered quantities, it is not possible to enforce
monotonic constraints on categorical features.
Examples:
Monotonic Constraints
1.11.1.1.6. Interaction constraints¶
A priori, the histogram gradient boosted trees are allowed to use any feature
to split a node into child nodes. This creates so called interactions between
features, i.e. usage of different features as split along a branch. Sometimes,
one wants to restrict the possible interactions, see [Mayer2022]. This can be
done by the parameter interaction_cst, where one can specify the indices
of features that are allowed to interact.
For instance, with 3 features in total, interaction_cst=[{0}, {1}, {2}]
forbids all interactions.
The constraints [{0, 1}, {1, 2}] specifies two groups of possibly
interacting features. Features 0 and 1 may interact with each other, as well
as features 1 and 2. But note that features 0 and 2 are forbidden to interact.
The following depicts a tree and the possible splits of the tree:
1      <- Both constraint groups could be applied from now on
/ \
1   2    <- Left split still fulfills both constraint groups.
/ \ / \      Right split at feature 2 has only group {1, 2} from now on.
LightGBM uses the same logic for overlapping groups.
Note that features not listed in interaction_cst are automatically
assigned an interaction group for themselves. With again 3 features, this
means that [{0}] is equivalent to [{0}, {1, 2}].
Examples:
Partial Dependence and Individual Conditional Expectation Plots
References
[Mayer2022]
M. Mayer, S.C. Bourassa, M. Hoesli, and D.F. Scognamiglio.
## 2022. Machine Learning Applications to Land and Structure Valuation.

Journal of Risk and Financial Management 15, no. 5: 193
1.11.1.1.7. Low-level parallelism¶
HistGradientBoostingClassifier and
HistGradientBoostingRegressor use OpenMP
for parallelization through Cython. For more details on how to control the
number of threads, please refer to our Parallelism notes.
The following parts are parallelized:
mapping samples from real values to integer-valued bins (finding the bin
thresholds is however sequential)
building histograms is parallelized over features
finding the best split point at a node is parallelized over features
during fit, mapping samples into the left and right children is
parallelized over samples
gradient and hessians computations are parallelized over samples
predicting is parallelized over samples
1.11.1.1.8. Why it’s faster¶
The bottleneck of a gradient boosting procedure is building the decision
trees. Building a traditional decision tree (as in the other GBDTs
GradientBoostingClassifier and GradientBoostingRegressor)
requires sorting the samples at each node (for
each feature). Sorting is needed so that the potential gain of a split point
can be computed efficiently. Splitting a single node has thus a complexity
of \(\mathcal{O}(n_\text{features} \times n \log(n))\) where \(n\)
is the number of samples at the node.
HistGradientBoostingClassifier and
HistGradientBoostingRegressor, in contrast, do not require sorting the
feature values and instead use a data-structure called a histogram, where the
samples are implicitly ordered. Building a histogram has a
\(\mathcal{O}(n)\) complexity, so the node splitting procedure has a
\(\mathcal{O}(n_\text{features} \times n)\) complexity, much smaller
than the previous one. In addition, instead of considering \(n\) split
points, we consider only max_bins split points, which might be much
smaller.
In order to build histograms, the input data X needs to be binned into
integer-valued bins. This binning procedure does require sorting the feature
values, but it only happens once at the very beginning of the boosting process
(not at each node, like in GradientBoostingClassifier and
GradientBoostingRegressor).
Finally, many parts of the implementation of
HistGradientBoostingClassifier and
HistGradientBoostingRegressor are parallelized.
References
[XGBoost]
Tianqi Chen, Carlos Guestrin, “XGBoost: A Scalable Tree
Boosting System”
[LightGBM]
Ke et. al. “LightGBM: A Highly Efficient Gradient
BoostingDecision Tree”
[Fisher1958]
Fisher, W.D. (1958). “On Grouping for Maximum Homogeneity”
Journal of the American Statistical Association, 53, 789-798.
1.11.1.2. GradientBoostingClassifier and GradientBoostingRegressor¶
The usage and the parameters of GradientBoostingClassifier and
GradientBoostingRegressor are described below. The 2 most important
parameters of these estimators are n_estimators and learning_rate.
1.11.1.2.1. Classification¶
GradientBoostingClassifier supports both binary and multi-class
classification.
The following example shows how to fit a gradient boosting classifier
with 100 decision stumps as weak learners:
>>> from sklearn.datasets import make_hastie_10_2
>>> from sklearn.ensemble import GradientBoostingClassifier
>>> X, y = make_hastie_10_2(random_state=0)
>>> X_train, X_test = X[:2000], X[2000:]
>>> y_train, y_test = y[:2000], y[2000:]
>>> clf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,
...     max_depth=1, random_state=0).fit(X_train, y_train)
>>> clf.score(X_test, y_test)
0.913...
The number of weak learners (i.e. regression trees) is controlled by the
parameter n_estimators; The size of each tree can be controlled either by setting the tree
depth via max_depth or by setting the number of leaf nodes via
max_leaf_nodes. The learning_rate is a hyper-parameter in the range
(0.0, 1.0] that controls overfitting via shrinkage .
Note
Classification with more than 2 classes requires the induction
of n_classes regression trees at each iteration,
thus, the total number of induced trees equals
n_classes * n_estimators. For datasets with a large number
of classes we strongly recommend to use
HistGradientBoostingClassifier as an alternative to
GradientBoostingClassifier .
1.11.1.2.2. Regression¶
GradientBoostingRegressor supports a number of
different loss functions
for regression which can be specified via the argument
loss; the default loss function for regression is squared error
('squared_error').
>>> import numpy as np
>>> from sklearn.metrics import mean_squared_error
>>> from sklearn.datasets import make_friedman1
>>> from sklearn.ensemble import GradientBoostingRegressor
>>> X, y = make_friedman1(n_samples=1200, random_state=0, noise=1.0)
>>> X_train, X_test = X[:200], X[200:]
>>> y_train, y_test = y[:200], y[200:]
>>> est = GradientBoostingRegressor(
...     n_estimators=100, learning_rate=0.1, max_depth=1, random_state=0,
...     loss='squared_error'
... ).fit(X_train, y_train)
>>> mean_squared_error(y_test, est.predict(X_test))
5.00...
The figure below shows the results of applying GradientBoostingRegressor
with least squares loss and 500 base learners to the diabetes dataset
(sklearn.datasets.load_diabetes).
The plot shows the train and test error at each iteration.
The train error at each iteration is stored in the
train_score_ attribute of the gradient boosting model.
The test error at each iterations can be obtained
via the staged_predict method which returns a
generator that yields the predictions at each stage. Plots like these can be used
to determine the optimal number of trees (i.e. n_estimators) by early stopping.
Examples:
Gradient Boosting regression
Gradient Boosting Out-of-Bag estimates
1.11.1.2.3. Fitting additional weak-learners¶
Both GradientBoostingRegressor and GradientBoostingClassifier
support warm_start=True which allows you to add more estimators to an already
fitted model.
>>> _ = est.set_params(n_estimators=200, warm_start=True)  # set warm_start and new nr of trees
>>> _ = est.fit(X_train, y_train) # fit additional 100 trees to est
>>> mean_squared_error(y_test, est.predict(X_test))
3.84...
1.11.1.2.4. Controlling the tree size¶
The size of the regression tree base learners defines the level of variable
interactions that can be captured by the gradient boosting model. In general,
a tree of depth h can capture interactions of order h .
There are two ways in which the size of the individual regression trees can
be controlled.
If you specify max_depth=h then complete binary trees
of depth h will be grown. Such trees will have (at most) 2**h leaf nodes
and 2**h - 1 split nodes.
Alternatively, you can control the tree size by specifying the number of
leaf nodes via the parameter max_leaf_nodes. In this case,
trees will be grown using best-first search where nodes with the highest improvement
in impurity will be expanded first.
A tree with max_leaf_nodes=k has k - 1 split nodes and thus can
model interactions of up to order max_leaf_nodes - 1 .
We found that max_leaf_nodes=k gives comparable results to max_depth=k-1
but is significantly faster to train at the expense of a slightly higher
training error.
The parameter max_leaf_nodes corresponds to the variable J in the
chapter on gradient boosting in [Friedman2001] and is related to the parameter
interaction.depth in R’s gbm package where max_leaf_nodes == interaction.depth + 1 .
1.11.1.2.5. Mathematical formulation¶
We first present GBRT for regression, and then detail the classification
case.
1.11.1.2.5.1. Regression¶
GBRT regressors are additive models whose prediction \(\hat{y}_i\) for a
given input \(x_i\) is of the following form:
\[\hat{y}_i = F_M(x_i) = \sum_{m=1}^{M} h_m(x_i)\]
where the \(h_m\) are estimators called weak learners in the context
of boosting. Gradient Tree Boosting uses decision tree regressors of fixed size as weak learners. The constant M corresponds to the
n_estimators parameter.
Similar to other boosting algorithms, a GBRT is built in a greedy fashion:
\[F_m(x) = F_{m-1}(x) + h_m(x),\]
where the newly added tree \(h_m\) is fitted in order to minimize a sum
of losses \(L_m\), given the previous ensemble \(F_{m-1}\):
\[h_m =  \arg\min_{h} L_m = \arg\min_{h} \sum_{i=1}^{n}
l(y_i, F_{m-1}(x_i) + h(x_i)),\]
where \(l(y_i, F(x_i))\) is defined by the loss parameter, detailed
in the next section.
By default, the initial model \(F_{0}\) is chosen as the constant that
minimizes the loss: for a least-squares loss, this is the empirical mean of
the target values. The initial model can also be specified via the init
argument.
Using a first-order Taylor approximation, the value of \(l\) can be
approximated as follows:
\[l(y_i, F_{m-1}(x_i) + h_m(x_i)) \approx
l(y_i, F_{m-1}(x_i))
+ h_m(x_i)
\left[ \frac{\partial l(y_i, F(x_i))}{\partial F(x_i)} \right]_{F=F_{m - 1}}.\]
Note
Briefly, a first-order Taylor approximation says that
\(l(z) \approx l(a) + (z - a) \frac{\partial l}{\partial z}(a)\).
Here, \(z\) corresponds to \(F_{m - 1}(x_i) + h_m(x_i)\), and
\(a\) corresponds to \(F_{m-1}(x_i)\)
The quantity \(\left[ \frac{\partial l(y_i, F(x_i))}{\partial F(x_i)}
\right]_{F=F_{m - 1}}\) is the derivative of the loss with respect to its
second parameter, evaluated at \(F_{m-1}(x)\). It is easy to compute for
any given \(F_{m - 1}(x_i)\) in a closed form since the loss is
differentiable. We will denote it by \(g_i\).
Removing the constant terms, we have:
\[h_m \approx \arg\min_{h} \sum_{i=1}^{n} h(x_i) g_i\]
This is minimized if \(h(x_i)\) is fitted to predict a value that is
proportional to the negative gradient \(-g_i\). Therefore, at each
iteration, the estimator \(h_m\) is fitted to predict the negative
gradients of the samples. The gradients are updated at each iteration.
This can be considered as some kind of gradient descent in a functional
space.
Note
For some losses, e.g. 'absolute_error' where the gradients
are \(\pm 1\), the values predicted by a fitted \(h_m\) are not
accurate enough: the tree can only output integer values. As a result, the
leaves values of the tree \(h_m\) are modified once the tree is
fitted, such that the leaves values minimize the loss \(L_m\). The
update is loss-dependent: for the absolute error loss, the value of
a leaf is updated to the median of the samples in that leaf.
1.11.1.2.5.2. Classification¶
Gradient boosting for classification is very similar to the regression case.
However, the sum of the trees \(F_M(x_i) = \sum_m h_m(x_i)\) is not
homogeneous to a prediction: it cannot be a class, since the trees predict
continuous values.
The mapping from the value \(F_M(x_i)\) to a class or a probability is
loss-dependent. For the log-loss, the probability that
\(x_i\) belongs to the positive class is modeled as \(p(y_i = 1 |
x_i) = \sigma(F_M(x_i))\) where \(\sigma\) is the sigmoid or expit function.
For multiclass classification, K trees (for K classes) are built at each of
the \(M\) iterations. The probability that \(x_i\) belongs to class
k is modeled as a softmax of the \(F_{M,k}(x_i)\) values.
Note that even for a classification task, the \(h_m\) sub-estimator is
still a regressor, not a classifier. This is because the sub-estimators are
trained to predict (negative) gradients, which are always continuous
quantities.
1.11.1.2.6. Loss Functions¶
The following loss functions are supported and can be specified using
the parameter loss:
Regression
Squared error ('squared_error'): The natural choice for regression
due to its superior computational properties. The initial model is
given by the mean of the target values.
Absolute error ('absolute_error'): A robust loss function for
regression. The initial model is given by the median of the
target values.
Huber ('huber'): Another robust loss function that combines
least squares and least absolute deviation; use alpha to
control the sensitivity with regards to outliers (see [Friedman2001] for
more details).
Quantile ('quantile'): A loss function for quantile regression.
Use 0 < alpha < 1 to specify the quantile. This loss function
can be used to create prediction intervals
(see Prediction Intervals for Gradient Boosting Regression).
Classification
Binary log-loss ('log-loss'): The binomial
negative log-likelihood loss function for binary classification. It provides
probability estimates.  The initial model is given by the
log odds-ratio.
Multi-class log-loss ('log-loss'): The multinomial
negative log-likelihood loss function for multi-class classification with
n_classes mutually exclusive classes. It provides
probability estimates.  The initial model is given by the
prior probability of each class. At each iteration n_classes
regression trees have to be constructed which makes GBRT rather
inefficient for data sets with a large number of classes.
Exponential loss ('exponential'): The same loss function
as AdaBoostClassifier. Less robust to mislabeled
examples than 'log-loss'; can only be used for binary
classification.
1.11.1.2.7. Shrinkage via learning rate¶
[Friedman2001] proposed a simple regularization strategy that scales
the contribution of each weak learner by a constant factor \(\nu\):
\[F_m(x) = F_{m-1}(x) + \nu h_m(x)\]
The parameter \(\nu\) is also called the learning rate because
it scales the step length the gradient descent procedure; it can
be set via the learning_rate parameter.
The parameter learning_rate strongly interacts with the parameter
n_estimators, the number of weak learners to fit. Smaller values
of learning_rate require larger numbers of weak learners to maintain
a constant training error. Empirical evidence suggests that small
values of learning_rate favor better test error. [HTF]
recommend to set the learning rate to a small constant
(e.g. learning_rate <= 0.1) and choose n_estimators large enough
that early stopping applies,
see Early stopping in Gradient Boosting
for a more detailed discussion of the interaction between
learning_rate and n_estimators see [R2007].
1.11.1.2.8. Subsampling¶
[Friedman2002] proposed stochastic gradient boosting, which combines gradient
boosting with bootstrap averaging (bagging). At each iteration
the base classifier is trained on a fraction subsample of
the available training data. The subsample is drawn without replacement.
A typical value of subsample is 0.5.
The figure below illustrates the effect of shrinkage and subsampling
on the goodness-of-fit of the model. We can clearly see that shrinkage
outperforms no-shrinkage. Subsampling with shrinkage can further increase
the accuracy of the model. Subsampling without shrinkage, on the other hand,
does poorly.
Another strategy to reduce the variance is by subsampling the features
analogous to the random splits in RandomForestClassifier.
The number of subsampled features can be controlled via the max_features
parameter.
Note
Using a small max_features value can significantly decrease the runtime.
Stochastic gradient boosting allows to compute out-of-bag estimates of the
test deviance by computing the improvement in deviance on the examples that are
not included in the bootstrap sample (i.e. the out-of-bag examples).
The improvements are stored in the attribute oob_improvement_.
oob_improvement_[i] holds the improvement in terms of the loss on the OOB samples
if you add the i-th stage to the current predictions.
Out-of-bag estimates can be used for model selection, for example to determine
the optimal number of iterations. OOB estimates are usually very pessimistic thus
we recommend to use cross-validation instead and only use OOB if cross-validation
is too time consuming.
Examples:
Gradient Boosting regularization
Gradient Boosting Out-of-Bag estimates
OOB Errors for Random Forests
1.11.1.2.9. Interpretation with feature importance¶
Individual decision trees can be interpreted easily by simply
visualizing the tree structure. Gradient boosting models, however,
comprise hundreds of regression trees thus they cannot be easily
interpreted by visual inspection of the individual trees. Fortunately,
a number of techniques have been proposed to summarize and interpret
gradient boosting models.
Often features do not contribute equally to predict the target
response; in many situations the majority of the features are in fact
irrelevant.
When interpreting a model, the first question usually is: what are
those important features and how do they contributing in predicting
the target response?
Individual decision trees intrinsically perform feature selection by selecting
appropriate split points. This information can be used to measure the
importance of each feature; the basic idea is: the more often a
feature is used in the split points of a tree the more important that
feature is. This notion of importance can be extended to decision tree
ensembles by simply averaging the impurity-based feature importance of each tree (see
Feature importance evaluation for more details).
The feature importance scores of a fit gradient boosting model can be
accessed via the feature_importances_ property:
>>> from sklearn.datasets import make_hastie_10_2
>>> from sklearn.ensemble import GradientBoostingClassifier
>>> X, y = make_hastie_10_2(random_state=0)
>>> clf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,
...     max_depth=1, random_state=0).fit(X, y)
>>> clf.feature_importances_
array([0.10..., 0.10..., 0.11..., ...
Note that this computation of feature importance is based on entropy, and it
is distinct from sklearn.inspection.permutation_importance which is
based on permutation of the features.
Examples:
Gradient Boosting regression
References
[Friedman2001]
(1,2,3,4)
Friedman, J.H. (2001). Greedy function approximation: A gradient
boosting machine.
Annals of Statistics, 29, 1189-1232.
[Friedman2002]
Friedman, J.H. (2002). Stochastic gradient boosting..
Computational Statistics & Data Analysis, 38, 367-378.
[R2007]
G. Ridgeway (2006). Generalized Boosted Models: A guide to the gbm
package
- 1.11.2. Random forests and other randomized tree ensembles¶
The sklearn.ensemble module includes two averaging algorithms based
on randomized decision trees: the RandomForest algorithm
and the Extra-Trees method. Both algorithms are perturb-and-combine
techniques [B1998] specifically designed for trees. This means a diverse
set of classifiers is created by introducing randomness in the classifier
construction.  The prediction of the ensemble is given as the averaged
prediction of the individual classifiers.
As other classifiers, forest classifiers have to be fitted with two
arrays: a sparse or dense array X of shape (n_samples, n_features)
holding the training samples, and an array Y of shape (n_samples,)
holding the target values (class labels) for the training samples:
>>> from sklearn.ensemble import RandomForestClassifier
>>> X = [[0, 0], [1, 1]]
>>> Y = [0, 1]
>>> clf = RandomForestClassifier(n_estimators=10)
>>> clf = clf.fit(X, Y)
Like decision trees, forests of trees also extend to
multi-output problems  (if Y is an array
of shape (n_samples, n_outputs)).
1.11.2.1. Random Forests¶
In random forests (see RandomForestClassifier and
RandomForestRegressor classes), each tree in the ensemble is built
from a sample drawn with replacement (i.e., a bootstrap sample) from the
training set.
Furthermore, when splitting each node during the construction of a tree, the
best split is found through an exhaustive search of the features values of
either all input features or a random subset of size max_features.
(See the parameter tuning guidelines for more details.)
The purpose of these two sources of randomness is to decrease the variance of
the forest estimator. Indeed, individual decision trees typically exhibit high
variance and tend to overfit. The injected randomness in forests yield decision
trees with somewhat decoupled prediction errors. By taking an average of those
predictions, some errors can cancel out. Random forests achieve a reduced
variance by combining diverse trees, sometimes at the cost of a slight increase
in bias. In practice the variance reduction is often significant hence yielding
an overall better model.
In contrast to the original publication [B2001], the scikit-learn
implementation combines classifiers by averaging their probabilistic
prediction, instead of letting each classifier vote for a single class.
A competitive alternative to random forests are
Histogram-Based Gradient Boosting (HGBT) models:
Building trees: Random forests typically rely on deep trees (that overfit
individually) which uses much computational resources, as they require
several splittings and evaluations of candidate splits. Boosting models
build shallow trees (that underfit individually) which are faster to fit
and predict.
Sequential boosting: In HGBT, the decision trees are built sequentially,
where each tree is trained to correct the errors made by the previous ones.
This allows them to iteratively improve the model’s performance using
relatively few trees. In contrast, random forests use a majority vote to
predict the outcome, which can require a larger number of trees to achieve
the same level of accuracy.
Efficient binning: HGBT uses an efficient binning algorithm that can handle
large datasets with a high number of features. The binning algorithm can
pre-process the data to speed up the subsequent tree construction (see
Why it’s faster). In contrast, the scikit-learn
implementation of random forests does not use binning and relies on exact
splitting, which can be computationally expensive.
Overall, the computational cost of HGBT versus RF depends on the specific
characteristics of the dataset and the modeling task. It’s a good idea
to try both models and compare their performance and computational efficiency
on your specific problem to determine which model is the best fit.
Examples:
Comparing Random Forests and Histogram Gradient Boosting models
1.11.2.2. Extremely Randomized Trees¶
In extremely randomized trees (see ExtraTreesClassifier
and ExtraTreesRegressor classes), randomness goes one step
further in the way splits are computed. As in random forests, a random
subset of candidate features is used, but instead of looking for the
most discriminative thresholds, thresholds are drawn at random for each
candidate feature and the best of these randomly-generated thresholds is
picked as the splitting rule. This usually allows to reduce the variance
of the model a bit more, at the expense of a slightly greater increase
in bias:
>>> from sklearn.model_selection import cross_val_score
>>> from sklearn.datasets import make_blobs
>>> from sklearn.ensemble import RandomForestClassifier
>>> from sklearn.ensemble import ExtraTreesClassifier
>>> from sklearn.tree import DecisionTreeClassifier
>>> X, y = make_blobs(n_samples=10000, n_features=10, centers=100,
...     random_state=0)
>>> clf = DecisionTreeClassifier(max_depth=None, min_samples_split=2,
...     random_state=0)
>>> scores = cross_val_score(clf, X, y, cv=5)
>>> scores.mean()
0.98...
>>> clf = RandomForestClassifier(n_estimators=10, max_depth=None,
...     min_samples_split=2, random_state=0)
>>> scores = cross_val_score(clf, X, y, cv=5)
>>> scores.mean()
0.999...
>>> clf = ExtraTreesClassifier(n_estimators=10, max_depth=None,
...     min_samples_split=2, random_state=0)
>>> scores = cross_val_score(clf, X, y, cv=5)
>>> scores.mean() > 0.999
True
1.11.2.3. Parameters¶
The main parameters to adjust when using these methods is n_estimators and
max_features. The former is the number of trees in the forest. The larger
the better, but also the longer it will take to compute. In addition, note that
results will stop getting significantly better beyond a critical number of
trees. The latter is the size of the random subsets of features to consider
when splitting a node. The lower the greater the reduction of variance, but
also the greater the increase in bias. Empirical good default values are
max_features=1.0 or equivalently max_features=None (always considering
all features instead of a random subset) for regression problems, and
max_features="sqrt" (using a random subset of size sqrt(n_features))
for classification tasks (where n_features is the number of features in
the data). The default value of max_features=1.0 is equivalent to bagged
trees and more randomness can be achieved by setting smaller values (e.g. 0.3
is a typical default in the literature). Good results are often achieved when
setting max_depth=None in combination with min_samples_split=2 (i.e.,
when fully developing the trees). Bear in mind though that these values are
usually not optimal, and might result in models that consume a lot of RAM.
The best parameter values should always be cross-validated. In addition, note
that in random forests, bootstrap samples are used by default
(bootstrap=True) while the default strategy for extra-trees is to use the
whole dataset (bootstrap=False). When using bootstrap sampling the
generalization error can be estimated on the left out or out-of-bag samples.
This can be enabled by setting oob_score=True.
Note
The size of the model with the default parameters is \(O( M * N * log (N) )\),
where \(M\) is the number of trees and \(N\) is the number of samples.
In order to reduce the size of the model, you can change these parameters:
min_samples_split, max_leaf_nodes, max_depth and min_samples_leaf.
1.11.2.4. Parallelization¶
Finally, this module also features the parallel construction of the trees
and the parallel computation of the predictions through the n_jobs
parameter. If n_jobs=k then computations are partitioned into
k jobs, and run on k cores of the machine. If n_jobs=-1
then all cores available on the machine are used. Note that because of
inter-process communication overhead, the speedup might not be linear
(i.e., using k jobs will unfortunately not be k times as
fast). Significant speedup can still be achieved though when building
a large number of trees, or when building a single tree requires a fair
amount of time (e.g., on large datasets).
Examples:
Plot the decision surfaces of ensembles of trees on the iris dataset
Pixel importances with a parallel forest of trees
Face completion with a multi-output estimators
References
[B2001]
Breiman, “Random Forests”, Machine Learning, 45(1), 5-32, 2001.
[B1998]
Breiman, “Arcing Classifiers”, Annals of Statistics 1998.
P. Geurts, D. Ernst., and L. Wehenkel, “Extremely randomized
trees”, Machine Learning, 63(1), 3-42, 2006.
1.11.2.5. Feature importance evaluation¶
The relative rank (i.e. depth) of a feature used as a decision node in a
tree can be used to assess the relative importance of that feature with
respect to the predictability of the target variable. Features used at
the top of the tree contribute to the final prediction decision of a
larger fraction of the input samples. The expected fraction of the
samples they contribute to can thus be used as an estimate of the
relative importance of the features. In scikit-learn, the fraction of
samples a feature contributes to is combined with the decrease in impurity
from splitting them to create a normalized estimate of the predictive power
of that feature.
By averaging the estimates of predictive ability over several randomized
trees one can reduce the variance of such an estimate and use it
for feature selection. This is known as the mean decrease in impurity, or MDI.
Refer to [L2014] for more information on MDI and feature importance
evaluation with Random Forests.
Warning
The impurity-based feature importances computed on tree-based models suffer
from two flaws that can lead to misleading conclusions. First they are
computed on statistics derived from the training dataset and therefore do
not necessarily inform us on which features are most important to make good
predictions on held-out dataset. Secondly, they favor high cardinality
features, that is features with many unique values.
Permutation feature importance is an alternative to impurity-based feature
importance that does not suffer from these flaws. These two methods of
obtaining feature importance are explored in:
Permutation Importance vs Random Forest Feature Importance (MDI).
The following example shows a color-coded representation of the relative
importances of each individual pixel for a face recognition task using
a ExtraTreesClassifier model.
In practice those estimates are stored as an attribute named
feature_importances_ on the fitted model. This is an array with shape
(n_features,) whose values are positive and sum to 1.0. The higher
the value, the more important is the contribution of the matching feature
to the prediction function.
Examples:
Pixel importances with a parallel forest of trees
Feature importances with a forest of trees
References
[L2014]
G. Louppe, “Understanding Random Forests: From Theory to
Practice”,
PhD Thesis, U. of Liege, 2014.
1.11.2.6. Totally Random Trees Embedding¶
RandomTreesEmbedding implements an unsupervised transformation of the
data.  Using a forest of completely random trees, RandomTreesEmbedding
encodes the data by the indices of the leaves a data point ends up in.  This
index is then encoded in a one-of-K manner, leading to a high dimensional,
sparse binary coding.
This coding can be computed very efficiently and can then be used as a basis
for other learning tasks.
The size and sparsity of the code can be influenced by choosing the number of
trees and the maximum depth per tree. For each tree in the ensemble, the coding
contains one entry of one. The size of the coding is at most n_estimators * 2
** max_depth, the maximum number of leaves in the forest.
As neighboring data points are more likely to lie within the same leaf of a
tree, the transformation performs an implicit, non-parametric density
estimation.
Examples:
Hashing feature transformation using Totally Random Trees
Manifold learning on handwritten digits: Locally Linear Embedding, Isomap… compares non-linear
dimensionality reduction techniques on handwritten digits.
Feature transformations with ensembles of trees compares
supervised and unsupervised tree based feature transformations.
See also
Manifold learning techniques can also be useful to derive non-linear
representations of feature space, also these approaches focus also on
dimensionality reduction.
- 1.11.3. Bagging meta-estimator¶
In ensemble algorithms, bagging methods form a class of algorithms which build
several instances of a black-box estimator on random subsets of the original
training set and then aggregate their individual predictions to form a final
prediction. These methods are used as a way to reduce the variance of a base
estimator (e.g., a decision tree), by introducing randomization into its
construction procedure and then making an ensemble out of it. In many cases,
bagging methods constitute a very simple way to improve with respect to a
single model, without making it necessary to adapt the underlying base
algorithm. As they provide a way to reduce overfitting, bagging methods work
best with strong and complex models (e.g., fully developed decision trees), in
contrast with boosting methods which usually work best with weak models (e.g.,
shallow decision trees).
Bagging methods come in many flavours but mostly differ from each other by the
way they draw random subsets of the training set:
When random subsets of the dataset are drawn as random subsets of the
samples, then this algorithm is known as Pasting [B1999].
When samples are drawn with replacement, then the method is known as
Bagging [B1996].
When random subsets of the dataset are drawn as random subsets of
the features, then the method is known as Random Subspaces [H1998].
Finally, when base estimators are built on subsets of both samples and
features, then the method is known as Random Patches [LG2012].
In scikit-learn, bagging methods are offered as a unified
BaggingClassifier meta-estimator  (resp. BaggingRegressor),
taking as input a user-specified estimator along with parameters
specifying the strategy to draw random subsets. In particular, max_samples
and max_features control the size of the subsets (in terms of samples and
features), while bootstrap and bootstrap_features control whether
samples and features are drawn with or without replacement. When using a subset
of the available samples the generalization accuracy can be estimated with the
out-of-bag samples by setting oob_score=True. As an example, the
snippet below illustrates how to instantiate a bagging ensemble of
KNeighborsClassifier estimators, each built on random
subsets of 50% of the samples and 50% of the features.
>>> from sklearn.ensemble import BaggingClassifier
>>> from sklearn.neighbors import KNeighborsClassifier
>>> bagging = BaggingClassifier(KNeighborsClassifier(),
...                             max_samples=0.5, max_features=0.5)
Examples:
Single estimator versus bagging: bias-variance decomposition
References
[B1999]
L. Breiman, “Pasting small votes for classification in large
databases and on-line”, Machine Learning, 36(1), 85-103, 1999.
[B1996]
L. Breiman, “Bagging predictors”, Machine Learning, 24(2),
123-140, 1996.
[H1998]
T. Ho, “The random subspace method for constructing decision
forests”, Pattern Analysis and Machine Intelligence, 20(8), 832-844,
1998.
[LG2012]
G. Louppe and P. Geurts, “Ensembles on Random Patches”,
Machine Learning and Knowledge Discovery in Databases, 346-361, 2012.
- 1.11.4. Voting Classifier¶
The idea behind the VotingClassifier is to combine
conceptually different machine learning classifiers and use a majority vote
or the average predicted probabilities (soft vote) to predict the class labels.
Such a classifier can be useful for a set of equally well performing models
in order to balance out their individual weaknesses.
1.11.4.1. Majority Class Labels (Majority/Hard Voting)¶
In majority voting, the predicted class label for a particular sample is
the class label that represents the majority (mode) of the class labels
predicted by each individual classifier.
E.g., if the prediction for a given sample is
classifier 1 -> class 1
classifier 2 -> class 1
classifier 3 -> class 2
the VotingClassifier (with voting='hard') would classify the sample
as “class 1” based on the majority class label.
In the cases of a tie, the VotingClassifier will select the class
based on the ascending sort order. E.g., in the following scenario
classifier 1 -> class 2
classifier 2 -> class 1
the class label 1 will be assigned to the sample.
1.11.4.2. Usage¶
The following example shows how to fit the majority rule classifier:
>>> from sklearn import datasets
>>> from sklearn.model_selection import cross_val_score
>>> from sklearn.linear_model import LogisticRegression
>>> from sklearn.naive_bayes import GaussianNB
>>> from sklearn.ensemble import RandomForestClassifier
>>> from sklearn.ensemble import VotingClassifier
>>> iris = datasets.load_iris()
>>> X, y = iris.data[:, 1:3], iris.target
>>> clf1 = LogisticRegression(random_state=1)
>>> clf2 = RandomForestClassifier(n_estimators=50, random_state=1)
>>> clf3 = GaussianNB()
>>> eclf = VotingClassifier(
...     estimators=[('lr', clf1), ('rf', clf2), ('gnb', clf3)],
...     voting='hard')
>>> for clf, label in zip([clf1, clf2, clf3, eclf], ['Logistic Regression', 'Random Forest', 'naive Bayes', 'Ensemble']):
...     scores = cross_val_score(clf, X, y, scoring='accuracy', cv=5)
...     print("Accuracy: %0.2f (+/- %0.2f) [%s]" % (scores.mean(), scores.std(), label))
Accuracy: 0.95 (+/- 0.04) [Logistic Regression]
Accuracy: 0.94 (+/- 0.04) [Random Forest]
Accuracy: 0.91 (+/- 0.04) [naive Bayes]
Accuracy: 0.95 (+/- 0.04) [Ensemble]
1.11.4.3. Weighted Average Probabilities (Soft Voting)¶
In contrast to majority voting (hard voting), soft voting
returns the class label as argmax of the sum of predicted probabilities.
Specific weights can be assigned to each classifier via the weights
parameter. When weights are provided, the predicted class probabilities
for each classifier are collected, multiplied by the classifier weight,
and averaged. The final class label is then derived from the class label
with the highest average probability.
To illustrate this with a simple example, let’s assume we have 3
classifiers and a 3-class classification problems where we assign
equal weights to all classifiers: w1=1, w2=1, w3=1.
The weighted average probabilities for a sample would then be
calculated as follows:
classifier
class 1
class 2
class 3
classifier 1
w1 * 0.2
w1 * 0.5
w1 * 0.3
classifier 2
w2 * 0.6
w2 * 0.3
w2 * 0.1
classifier 3
w3 * 0.3
w3 * 0.4
w3 * 0.3
weighted average
0.37
0.4
0.23
Here, the predicted class label is 2, since it has the
highest average probability.
The following example illustrates how the decision regions may change
when a soft VotingClassifier is used based on a linear Support
Vector Machine, a Decision Tree, and a K-nearest neighbor classifier:
>>> from sklearn import datasets
>>> from sklearn.tree import DecisionTreeClassifier
>>> from sklearn.neighbors import KNeighborsClassifier
>>> from sklearn.svm import SVC
>>> from itertools import product
>>> from sklearn.ensemble import VotingClassifier
>>> # Loading some example data
>>> iris = datasets.load_iris()
>>> X = iris.data[:, [0, 2]]
>>> y = iris.target
>>> # Training classifiers
>>> clf1 = DecisionTreeClassifier(max_depth=4)
>>> clf2 = KNeighborsClassifier(n_neighbors=7)
>>> clf3 = SVC(kernel='rbf', probability=True)
>>> eclf = VotingClassifier(estimators=[('dt', clf1), ('knn', clf2), ('svc', clf3)],
...                         voting='soft', weights=[2, 1, 2])
>>> clf1 = clf1.fit(X, y)
>>> clf2 = clf2.fit(X, y)
>>> clf3 = clf3.fit(X, y)
>>> eclf = eclf.fit(X, y)
1.11.4.4. Using the VotingClassifier with GridSearchCV¶
The VotingClassifier can also be used together with
GridSearchCV in order to tune the
hyperparameters of the individual estimators:
>>> from sklearn.model_selection import GridSearchCV
>>> clf1 = LogisticRegression(random_state=1)
>>> clf2 = RandomForestClassifier(random_state=1)
>>> clf3 = GaussianNB()
>>> eclf = VotingClassifier(
...     estimators=[('lr', clf1), ('rf', clf2), ('gnb', clf3)],
...     voting='soft'
... )
>>> params = {'lr__C': [1.0, 100.0], 'rf__n_estimators': [20, 200]}
>>> grid = GridSearchCV(estimator=eclf, param_grid=params, cv=5)
>>> grid = grid.fit(iris.data, iris.target)
1.11.4.5. Usage¶
In order to predict the class labels based on the predicted
class-probabilities (scikit-learn estimators in the VotingClassifier
must support predict_proba method):
>>> eclf = VotingClassifier(
...     estimators=[('lr', clf1), ('rf', clf2), ('gnb', clf3)],
...     voting='soft'
... )
Optionally, weights can be provided for the individual classifiers:
>>> eclf = VotingClassifier(
...     estimators=[('lr', clf1), ('rf', clf2), ('gnb', clf3)],
...     voting='soft', weights=[2,5,1]
... )
- 1.11.5. Voting Regressor¶
The idea behind the VotingRegressor is to combine conceptually
different machine learning regressors and return the average predicted values.
Such a regressor can be useful for a set of equally well performing models
in order to balance out their individual weaknesses.
1.11.5.1. Usage¶
The following example shows how to fit the VotingRegressor:
>>> from sklearn.datasets import load_diabetes
>>> from sklearn.ensemble import GradientBoostingRegressor
>>> from sklearn.ensemble import RandomForestRegressor
>>> from sklearn.linear_model import LinearRegression
>>> from sklearn.ensemble import VotingRegressor
>>> # Loading some example data
>>> X, y = load_diabetes(return_X_y=True)
>>> # Training classifiers
>>> reg1 = GradientBoostingRegressor(random_state=1)
>>> reg2 = RandomForestRegressor(random_state=1)
>>> reg3 = LinearRegression()
>>> ereg = VotingRegressor(estimators=[('gb', reg1), ('rf', reg2), ('lr', reg3)])
>>> ereg = ereg.fit(X, y)
Examples:
Plot individual and voting regression predictions
- 1.11.6. Stacked generalization¶
Stacked generalization is a method for combining estimators to reduce their
biases [W1992] [HTF]. More precisely, the predictions of each individual
estimator are stacked together and used as input to a final estimator to
compute the prediction. This final estimator is trained through
cross-validation.
The StackingClassifier and StackingRegressor provide such
strategies which can be applied to classification and regression problems.
The estimators parameter corresponds to the list of the estimators which
are stacked together in parallel on the input data. It should be given as a
list of names and estimators:
>>> from sklearn.linear_model import RidgeCV, LassoCV
>>> from sklearn.neighbors import KNeighborsRegressor
>>> estimators = [('ridge', RidgeCV()),
...               ('lasso', LassoCV(random_state=42)),
...               ('knr', KNeighborsRegressor(n_neighbors=20,
...                                           metric='euclidean'))]
The final_estimator will use the predictions of the estimators as input. It
needs to be a classifier or a regressor when using StackingClassifier
or StackingRegressor, respectively:
>>> from sklearn.ensemble import GradientBoostingRegressor
>>> from sklearn.ensemble import StackingRegressor
>>> final_estimator = GradientBoostingRegressor(
...     n_estimators=25, subsample=0.5, min_samples_leaf=25, max_features=1,
...     random_state=42)
>>> reg = StackingRegressor(
...     estimators=estimators,
...     final_estimator=final_estimator)
To train the estimators and final_estimator, the fit method needs
to be called on the training data:
>>> from sklearn.datasets import load_diabetes
>>> X, y = load_diabetes(return_X_y=True)
>>> from sklearn.model_selection import train_test_split
>>> X_train, X_test, y_train, y_test = train_test_split(X, y,
...                                                     random_state=42)
>>> reg.fit(X_train, y_train)
StackingRegressor(...)
During training, the estimators are fitted on the whole training data
X_train. They will be used when calling predict or predict_proba. To
generalize and avoid over-fitting, the final_estimator is trained on
out-samples using sklearn.model_selection.cross_val_predict internally.
For StackingClassifier, note that the output of the estimators is
controlled by the parameter stack_method and it is called by each estimator.
This parameter is either a string, being estimator method names, or 'auto'
which will automatically identify an available method depending on the
availability, tested in the order of preference: predict_proba,
decision_function and predict.
A StackingRegressor and StackingClassifier can be used as
any other regressor or classifier, exposing a predict, predict_proba, and
decision_function methods, e.g.:
>>> y_pred = reg.predict(X_test)
>>> from sklearn.metrics import r2_score
>>> print('R2 score: {:.2f}'.format(r2_score(y_test, y_pred)))
R2 score: 0.53
Note that it is also possible to get the output of the stacked
estimators using the transform method:
>>> reg.transform(X_test[:5])
array([[142..., 138..., 146...],
[179..., 182..., 151...],
[139..., 132..., 158...],
[286..., 292..., 225...],
[126..., 124..., 164...]])
In practice, a stacking predictor predicts as good as the best predictor of the
base layer and even sometimes outperforms it by combining the different
strengths of the these predictors. However, training a stacking predictor is
computationally expensive.
Note
For StackingClassifier, when using stack_method_='predict_proba',
the first column is dropped when the problem is a binary classification
problem. Indeed, both probability columns predicted by each estimator are
perfectly collinear.
Note
Multiple stacking layers can be achieved by assigning final_estimator to
a StackingClassifier or StackingRegressor:
>>> final_layer_rfr = RandomForestRegressor(
...     n_estimators=10, max_features=1, max_leaf_nodes=5,random_state=42)
>>> final_layer_gbr = GradientBoostingRegressor(
...     n_estimators=10, max_features=1, max_leaf_nodes=5,random_state=42)
>>> final_layer = StackingRegressor(
...     estimators=[('rf', final_layer_rfr),
...                 ('gbrt', final_layer_gbr)],
...     final_estimator=RidgeCV()
...     )
>>> multi_layer_regressor = StackingRegressor(
...     estimators=[('ridge', RidgeCV()),
...                 ('lasso', LassoCV(random_state=42)),
...                 ('knr', KNeighborsRegressor(n_neighbors=20,
...                                             metric='euclidean'))],
...     final_estimator=final_layer
... )
>>> multi_layer_regressor.fit(X_train, y_train)
StackingRegressor(...)
>>> print('R2 score: {:.2f}'
...       .format(multi_layer_regressor.score(X_test, y_test)))
R2 score: 0.53
References
[W1992]
Wolpert, David H. “Stacked generalization.” Neural networks 5.2
(1992): 241-259.
- 1.11.7. AdaBoost¶
The module sklearn.ensemble includes the popular boosting algorithm
AdaBoost, introduced in 1995 by Freund and Schapire [FS1995].
The core principle of AdaBoost is to fit a sequence of weak learners (i.e.,
models that are only slightly better than random guessing, such as small
decision trees) on repeatedly modified versions of the data. The predictions
from all of them are then combined through a weighted majority vote (or sum) to
produce the final prediction. The data modifications at each so-called boosting
iteration consists of applying weights \(w_1\), \(w_2\), …, \(w_N\)
to each of the training samples. Initially, those weights are all set to
\(w_i = 1/N\), so that the first step simply trains a weak learner on the
original data. For each successive iteration, the sample weights are
individually modified and the learning algorithm is reapplied to the reweighted
data. At a given step, those training examples that were incorrectly predicted
by the boosted model induced at the previous step have their weights increased,
whereas the weights are decreased for those that were predicted correctly. As
iterations proceed, examples that are difficult to predict receive
ever-increasing influence. Each subsequent weak learner is thereby forced to
concentrate on the examples that are missed by the previous ones in the sequence
[HTF].
AdaBoost can be used both for classification and regression problems:
For multi-class classification, AdaBoostClassifier implements
AdaBoost.SAMME [ZZRH2009].
For regression, AdaBoostRegressor implements AdaBoost.R2 [D1997].
1.11.7.1. Usage¶
The following example shows how to fit an AdaBoost classifier with 100 weak
learners:
>>> from sklearn.model_selection import cross_val_score
>>> from sklearn.datasets import load_iris
>>> from sklearn.ensemble import AdaBoostClassifier
>>> X, y = load_iris(return_X_y=True)
>>> clf = AdaBoostClassifier(n_estimators=100, algorithm="SAMME",)
>>> scores = cross_val_score(clf, X, y, cv=5)
>>> scores.mean()
0.9...
The number of weak learners is controlled by the parameter n_estimators. The
learning_rate parameter controls the contribution of the weak learners in
the final combination. By default, weak learners are decision stumps. Different
weak learners can be specified through the estimator parameter.
The main parameters to tune to obtain good results are n_estimators and
the complexity of the base estimators (e.g., its depth max_depth or
minimum required number of samples to consider a split min_samples_split).
Examples:
Multi-class AdaBoosted Decision Trees shows the performance
of AdaBoost on a multi-class problem.
Two-class AdaBoost shows the decision boundary
and decision function values for a non-linearly separable two-class problem
using AdaBoost-SAMME.
Decision Tree Regression with AdaBoost demonstrates regression
with the AdaBoost.R2 algorithm.
References
[FS1995]
Y. Freund, and R. Schapire, “A Decision-Theoretic Generalization of
On-Line Learning and an Application to Boosting”, 1997.
[ZZRH2009]
J. Zhu, H. Zou, S. Rosset, T. Hastie. “Multi-class AdaBoost”,
2009.
[D1997]
Drucker. “Improving Regressors using Boosting Techniques”, 1997.
[HTF]
(1,2,3)
T. Hastie, R. Tibshirani and J. Friedman, “Elements of
Statistical Learning Ed. 2”, Springer, 2009.
© 2007 - 2024, scikit-learn developers (BSD License).
Show this page source
### 1.12. Multiclass and multioutput algorithms — scikit-learn 1.4.2 documentation

Install
User Guide
API
Examples
Community
Getting Started
Tutorial
What's new
Glossary
Development
FAQ
Support
Related packages
Roadmap
Governance
About us
GitHub
Other Versions and Download
More
Getting Started
Tutorial
What's new
Glossary
Development
FAQ
Support
Related packages
Roadmap
Governance
About us
GitHub
Other Versions and Download
Toggle Menu
PrevUp
Next
scikit-learn 1.4.2
Other versions
Please cite us if you use the software.
### 1.12. Multiclass and multioutput algorithms

- 1.12.1. Multiclass classification
1.12.1.1. Target format
1.12.1.2. OneVsRestClassifier
1.12.1.3. OneVsOneClassifier
1.12.1.4. OutputCodeClassifier
- 1.12.2. Multilabel classification
1.12.2.1. Target format
1.12.2.2. MultiOutputClassifier
1.12.2.3. ClassifierChain
- 1.12.3. Multiclass-multioutput classification
1.12.3.1. Target format
- 1.12.4. Multioutput regression
1.12.4.1. Target format
1.12.4.2. MultiOutputRegressor
1.12.4.3. RegressorChain
### 1.12. Multiclass and multioutput algorithms¶

This section of the user guide covers functionality related to multi-learning
problems, including multiclass, multilabel, and
multioutput classification and regression.
The modules in this section implement meta-estimators, which require a
base estimator to be provided in their constructor. Meta-estimators extend the
functionality of the base estimator to support multi-learning problems, which
is accomplished by transforming the multi-learning problem into a set of
simpler problems, then fitting one estimator per problem.
This section covers two modules: sklearn.multiclass and
sklearn.multioutput. The chart below demonstrates the problem types
that each module is responsible for, and the corresponding meta-estimators
that each module provides.
The table below provides a quick reference on the differences between problem
types. More detailed explanations can be found in subsequent sections of this
guide.
Number of targets
Target cardinality
Valid
type_of_target
Multiclass
classification
1
>2
‘multiclass’
Multilabel
classification
>1
2 (0 or 1)
‘multilabel-indicator’
Multiclass-multioutput
classification
>1
>2
‘multiclass-multioutput’
Multioutput
regression
>1
Continuous
‘continuous-multioutput’
Below is a summary of scikit-learn estimators that have multi-learning support
built-in, grouped by strategy. You don’t need the meta-estimators provided by
this section if you’re using one of these estimators. However, meta-estimators
can provide additional strategies beyond what is built-in:
Inherently multiclass:
naive_bayes.BernoulliNB
tree.DecisionTreeClassifier
tree.ExtraTreeClassifier
ensemble.ExtraTreesClassifier
naive_bayes.GaussianNB
neighbors.KNeighborsClassifier
semi_supervised.LabelPropagation
semi_supervised.LabelSpreading
discriminant_analysis.LinearDiscriminantAnalysis
svm.LinearSVC (setting multi_class=”crammer_singer”)
linear_model.LogisticRegression (setting multi_class=”multinomial”)
linear_model.LogisticRegressionCV (setting multi_class=”multinomial”)
neural_network.MLPClassifier
neighbors.NearestCentroid
discriminant_analysis.QuadraticDiscriminantAnalysis
neighbors.RadiusNeighborsClassifier
ensemble.RandomForestClassifier
linear_model.RidgeClassifier
linear_model.RidgeClassifierCV
Multiclass as One-Vs-One:
svm.NuSVC
svm.SVC.
gaussian_process.GaussianProcessClassifier (setting multi_class = “one_vs_one”)
Multiclass as One-Vs-The-Rest:
ensemble.GradientBoostingClassifier
gaussian_process.GaussianProcessClassifier (setting multi_class = “one_vs_rest”)
svm.LinearSVC (setting multi_class=”ovr”)
linear_model.LogisticRegression (setting multi_class=”ovr”)
linear_model.LogisticRegressionCV (setting multi_class=”ovr”)
linear_model.SGDClassifier
linear_model.Perceptron
linear_model.PassiveAggressiveClassifier
Support multilabel:
tree.DecisionTreeClassifier
tree.ExtraTreeClassifier
ensemble.ExtraTreesClassifier
neighbors.KNeighborsClassifier
neural_network.MLPClassifier
neighbors.RadiusNeighborsClassifier
ensemble.RandomForestClassifier
linear_model.RidgeClassifier
linear_model.RidgeClassifierCV
Support multiclass-multioutput:
tree.DecisionTreeClassifier
tree.ExtraTreeClassifier
ensemble.ExtraTreesClassifier
neighbors.KNeighborsClassifier
neighbors.RadiusNeighborsClassifier
ensemble.RandomForestClassifier
- 1.12.1. Multiclass classification¶
Warning
All classifiers in scikit-learn do multiclass classification
out-of-the-box. You don’t need to use the sklearn.multiclass module
unless you want to experiment with different multiclass strategies.
Multiclass classification is a classification task with more than two
classes. Each sample can only be labeled as one class.
For example, classification using features extracted from a set of images of
fruit, where each image may either be of an orange, an apple, or a pear.
Each image is one sample and is labeled as one of the 3 possible classes.
Multiclass classification makes the assumption that each sample is assigned
to one and only one label - one sample cannot, for example, be both a pear
and an apple.
While all scikit-learn classifiers are capable of multiclass classification,
the meta-estimators offered by sklearn.multiclass
permit changing the way they handle more than two classes
because this may have an effect on classifier performance
(either in terms of generalization error or required computational resources).
1.12.1.1. Target format¶
Valid multiclass representations for
type_of_target (y) are:
1d or column vector containing more than two discrete values. An
example of a vector y for 4 samples:
>>> import numpy as np
>>> y = np.array(['apple', 'pear', 'apple', 'orange'])
>>> print(y)
['apple' 'pear' 'apple' 'orange']
Dense or sparse binary matrix of shape (n_samples, n_classes)
with a single sample per row, where each column represents one class. An
example of both a dense and sparse binary matrix y for 4
samples, where the columns, in order, are apple, orange, and pear:
>>> import numpy as np
>>> from sklearn.preprocessing import LabelBinarizer
>>> y = np.array(['apple', 'pear', 'apple', 'orange'])
>>> y_dense = LabelBinarizer().fit_transform(y)
>>> print(y_dense)
[[1 0 0]
[0 0 1]
[1 0 0]
[0 1 0]]
>>> from scipy import sparse
>>> y_sparse = sparse.csr_matrix(y_dense)
>>> print(y_sparse)
(0, 0)    1
(1, 2)    1
(2, 0)    1
(3, 1)    1
For more information about LabelBinarizer,
refer to Transforming the prediction target (y).
1.12.1.2. OneVsRestClassifier¶
The one-vs-rest strategy, also known as one-vs-all, is implemented in
OneVsRestClassifier.  The strategy consists in
fitting one classifier per class. For each classifier, the class is fitted
against all the other classes. In addition to its computational efficiency
(only n_classes classifiers are needed), one advantage of this approach is
its interpretability. Since each class is represented by one and only one
classifier, it is possible to gain knowledge about the class by inspecting its
corresponding classifier. This is the most commonly used strategy and is a fair
default choice.
Below is an example of multiclass learning using OvR:
>>> from sklearn import datasets
>>> from sklearn.multiclass import OneVsRestClassifier
>>> from sklearn.svm import LinearSVC
>>> X, y = datasets.load_iris(return_X_y=True)
>>> OneVsRestClassifier(LinearSVC(dual="auto", random_state=0)).fit(X, y).predict(X)
array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1,
1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 1, 2, 2, 2, 2,
2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])
OneVsRestClassifier also supports multilabel
classification. To use this feature, feed the classifier an indicator matrix,
in which cell [i, j] indicates the presence of label j in sample i.
Examples:
Multilabel classification
1.12.1.3. OneVsOneClassifier¶
OneVsOneClassifier constructs one classifier per
pair of classes. At prediction time, the class which received the most votes
is selected. In the event of a tie (among two classes with an equal number of
votes), it selects the class with the highest aggregate classification
confidence by summing over the pair-wise classification confidence levels
computed by the underlying binary classifiers.
Since it requires to fit n_classes * (n_classes - 1) / 2 classifiers,
this method is usually slower than one-vs-the-rest, due to its
O(n_classes^2) complexity. However, this method may be advantageous for
algorithms such as kernel algorithms which don’t scale well with
n_samples. This is because each individual learning problem only involves
a small subset of the data whereas, with one-vs-the-rest, the complete
dataset is used n_classes times. The decision function is the result
of a monotonic transformation of the one-versus-one classification.
Below is an example of multiclass learning using OvO:
>>> from sklearn import datasets
>>> from sklearn.multiclass import OneVsOneClassifier
>>> from sklearn.svm import LinearSVC
>>> X, y = datasets.load_iris(return_X_y=True)
>>> OneVsOneClassifier(LinearSVC(dual="auto", random_state=0)).fit(X, y).predict(X)
array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
1, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1,
1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])
References:
“Pattern Recognition and Machine Learning. Springer”,
Christopher M. Bishop, page 183, (First Edition)
1.12.1.4. OutputCodeClassifier¶
Error-Correcting Output Code-based strategies are fairly different from
one-vs-the-rest and one-vs-one. With these strategies, each class is
represented in a Euclidean space, where each dimension can only be 0 or 1.
Another way to put it is that each class is represented by a binary code (an
array of 0 and 1). The matrix which keeps track of the location/code of each
class is called the code book. The code size is the dimensionality of the
aforementioned space. Intuitively, each class should be represented by a code
as unique as possible and a good code book should be designed to optimize
classification accuracy. In this implementation, we simply use a
randomly-generated code book as advocated in [3] although more elaborate
methods may be added in the future.
At fitting time, one binary classifier per bit in the code book is fitted.
At prediction time, the classifiers are used to project new points in the
class space and the class closest to the points is chosen.
In OutputCodeClassifier, the code_size
attribute allows the user to control the number of classifiers which will be
used. It is a percentage of the total number of classes.
A number between 0 and 1 will require fewer classifiers than
one-vs-the-rest. In theory, log2(n_classes) / n_classes is sufficient to
represent each class unambiguously. However, in practice, it may not lead to
good accuracy since log2(n_classes) is much smaller than n_classes.
A number greater than 1 will require more classifiers than
one-vs-the-rest. In this case, some classifiers will in theory correct for
the mistakes made by other classifiers, hence the name “error-correcting”.
In practice, however, this may not happen as classifier mistakes will
typically be correlated. The error-correcting output codes have a similar
effect to bagging.
Below is an example of multiclass learning using Output-Codes:
>>> from sklearn import datasets
>>> from sklearn.multiclass import OutputCodeClassifier
>>> from sklearn.svm import LinearSVC
>>> X, y = datasets.load_iris(return_X_y=True)
>>> clf = OutputCodeClassifier(LinearSVC(dual="auto", random_state=0),
...                            code_size=2, random_state=0)
>>> clf.fit(X, y).predict(X)
array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1,
1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 2, 2, 2, 1, 1, 1, 1, 1, 1,
1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 1, 1, 2, 2, 2,
2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])
References:
“Solving multiclass learning problems via error-correcting output codes”,
Dietterich T., Bakiri G.,
Journal of Artificial Intelligence Research 2,
1995.
[3]
“The error coding method and PICTs”,
James G., Hastie T.,
Journal of Computational and Graphical statistics 7,
1998.
“The Elements of Statistical Learning”,
Hastie T., Tibshirani R., Friedman J., page 606 (second-edition)
2008.
- 1.12.2. Multilabel classification¶
Multilabel classification (closely related to multioutput
classification) is a classification task labeling each sample with m
labels from n_classes possible classes, where m can be 0 to
n_classes inclusive. This can be thought of as predicting properties of a
sample that are not mutually exclusive. Formally, a binary output is assigned
to each class, for every sample. Positive classes are indicated with 1 and
negative classes with 0 or -1. It is thus comparable to running n_classes
binary classification tasks, for example with
MultiOutputClassifier. This approach treats
each label independently whereas multilabel classifiers may treat the
multiple classes simultaneously, accounting for correlated behavior among
them.
For example, prediction of the topics relevant to a text document or video.
The document or video may be about one of ‘religion’, ‘politics’, ‘finance’
or ‘education’, several of the topic classes or all of the topic classes.
1.12.2.1. Target format¶
A valid representation of multilabel y is an either dense or sparse
binary matrix of shape (n_samples, n_classes). Each column
represents a class. The 1’s in each row denote the positive classes a
sample has been labeled with. An example of a dense matrix y for 3
samples:
>>> y = np.array([[1, 0, 0, 1], [0, 0, 1, 1], [0, 0, 0, 0]])
>>> print(y)
[[1 0 0 1]
[0 0 1 1]
[0 0 0 0]]
Dense binary matrices can also be created using
MultiLabelBinarizer. For more information,
refer to Transforming the prediction target (y).
An example of the same y in sparse matrix form:
>>> y_sparse = sparse.csr_matrix(y)
>>> print(y_sparse)
(0, 0)      1
(0, 3)      1
(1, 2)      1
(1, 3)      1
1.12.2.2. MultiOutputClassifier¶
Multilabel classification support can be added to any classifier with
MultiOutputClassifier. This strategy consists of
fitting one classifier per target.  This allows multiple target variable
classifications. The purpose of this class is to extend estimators
to be able to estimate a series of target functions (f1,f2,f3…,fn)
that are trained on a single X predictor matrix to predict a series
of responses (y1,y2,y3…,yn).
You can find a usage example for
MultiOutputClassifier
as part of the section on Multiclass-multioutput classification
since it is a generalization of multilabel classification to
multiclass outputs instead of binary outputs.
1.12.2.3. ClassifierChain¶
Classifier chains (see ClassifierChain) are a way
of combining a number of binary classifiers into a single multi-label model
that is capable of exploiting correlations among targets.
For a multi-label classification problem with N classes, N binary
classifiers are assigned an integer between 0 and N-1. These integers
define the order of models in the chain. Each classifier is then fit on the
available training data plus the true labels of the classes whose
models were assigned a lower number.
When predicting, the true labels will not be available. Instead the
predictions of each model are passed on to the subsequent models in the
chain to be used as features.
Clearly the order of the chain is important. The first model in the chain
has no information about the other labels while the last model in the chain
has features indicating the presence of all of the other labels. In general
one does not know the optimal ordering of the models in the chain so
typically many randomly ordered chains are fit and their predictions are
averaged together.
References:
Jesse Read, Bernhard Pfahringer, Geoff Holmes, Eibe Frank,“Classifier Chains for Multi-label Classification”, 2009.
- 1.12.3. Multiclass-multioutput classification¶
Multiclass-multioutput classification
(also known as multitask classification) is a
classification task which labels each sample with a set of non-binary
properties. Both the number of properties and the number of
classes per property is greater than 2. A single estimator thus
handles several joint classification tasks. This is both a generalization of
the multilabel classification task, which only considers binary
attributes, as well as a generalization of the multiclass classification
task, where only one property is considered.
For example, classification of the properties “type of fruit” and “colour”
for a set of images of fruit. The property “type of fruit” has the possible
classes: “apple”, “pear” and “orange”. The property “colour” has the
possible classes: “green”, “red”, “yellow” and “orange”. Each sample is an
image of a fruit, a label is output for both properties and each label is
one of the possible classes of the corresponding property.
Note that all classifiers handling multiclass-multioutput (also known as
multitask classification) tasks, support the multilabel classification task
as a special case. Multitask classification is similar to the multioutput
classification task with different model formulations. For more information,
see the relevant estimator documentation.
Below is an example of multiclass-multioutput classification:
>>> from sklearn.datasets import make_classification
>>> from sklearn.multioutput import MultiOutputClassifier
>>> from sklearn.ensemble import RandomForestClassifier
>>> from sklearn.utils import shuffle
>>> import numpy as np
>>> X, y1 = make_classification(n_samples=10, n_features=100,
...                             n_informative=30, n_classes=3,
...                             random_state=1)
>>> y2 = shuffle(y1, random_state=1)
>>> y3 = shuffle(y1, random_state=2)
>>> Y = np.vstack((y1, y2, y3)).T
>>> n_samples, n_features = X.shape # 10,100
>>> n_outputs = Y.shape[1] # 3
>>> n_classes = 3
>>> forest = RandomForestClassifier(random_state=1)
>>> multi_target_forest = MultiOutputClassifier(forest, n_jobs=2)
>>> multi_target_forest.fit(X, Y).predict(X)
array([[2, 2, 0],
[1, 2, 1],
[2, 1, 0],
[0, 0, 2],
[0, 2, 1],
[0, 0, 2],
[1, 1, 0],
[1, 1, 1],
[0, 0, 2],
[2, 0, 0]])
Warning
At present, no metric in sklearn.metrics
supports the multiclass-multioutput classification task.
1.12.3.1. Target format¶
A valid representation of multioutput y is a dense matrix of shape
(n_samples, n_classes) of class labels. A column wise concatenation of 1d
multiclass variables. An example of y for 3 samples:
>>> y = np.array([['apple', 'green'], ['orange', 'orange'], ['pear', 'green']])
>>> print(y)
[['apple' 'green']
['orange' 'orange']
['pear' 'green']]
- 1.12.4. Multioutput regression¶
Multioutput regression predicts multiple numerical properties for each
sample. Each property is a numerical variable and the number of properties
to be predicted for each sample is greater than or equal to 2. Some estimators
that support multioutput regression are faster than just running n_output
estimators.
For example, prediction of both wind speed and wind direction, in degrees,
using data obtained at a certain location. Each sample would be data
obtained at one location and both wind speed and direction would be
output for each sample.
1.12.4.1. Target format¶
A valid representation of multioutput y is a dense matrix of shape
(n_samples, n_output) of floats. A column wise concatenation of
continuous variables. An example of y for 3 samples:
>>> y = np.array([[31.4, 94], [40.5, 109], [25.0, 30]])
>>> print(y)
[[ 31.4  94. ]
[ 40.5 109. ]
[ 25.   30. ]]
1.12.4.2. MultiOutputRegressor¶
Multioutput regression support can be added to any regressor with
MultiOutputRegressor.  This strategy consists of
fitting one regressor per target. Since each target is represented by exactly
one regressor it is possible to gain knowledge about the target by
inspecting its corresponding regressor. As
MultiOutputRegressor fits one regressor per
target it can not take advantage of correlations between targets.
Below is an example of multioutput regression:
>>> from sklearn.datasets import make_regression
>>> from sklearn.multioutput import MultiOutputRegressor
>>> from sklearn.ensemble import GradientBoostingRegressor
>>> X, y = make_regression(n_samples=10, n_targets=3, random_state=1)
>>> MultiOutputRegressor(GradientBoostingRegressor(random_state=0)).fit(X, y).predict(X)
array([[-154.75474165, -147.03498585,  -50.03812219],
[   7.12165031,    5.12914884,  -81.46081961],
[-187.8948621 , -100.44373091,   13.88978285],
[-141.62745778,   95.02891072, -191.48204257],
[  97.03260883,  165.34867495,  139.52003279],
[ 123.92529176,   21.25719016,   -7.84253   ],
[-122.25193977,  -85.16443186, -107.12274212],
[ -30.170388  ,  -94.80956739,   12.16979946],
[ 140.72667194,  176.50941682,  -17.50447799],
[ 149.37967282,  -81.15699552,   -5.72850319]])
1.12.4.3. RegressorChain¶
Regressor chains (see RegressorChain) is
analogous to ClassifierChain as a way of
combining a number of regressions into a single multi-target model that is
capable of exploiting correlations among targets.
© 2007 - 2024, scikit-learn developers (BSD License).
Show this page source
### 1.13. Feature selection — scikit-learn 1.4.2 documentation

Install
User Guide
API
Examples
Community
Getting Started
Tutorial
What's new
Glossary
Development
FAQ
Support
Related packages
Roadmap
Governance
About us
GitHub
Other Versions and Download
More
Getting Started
Tutorial
What's new
Glossary
Development
FAQ
Support
Related packages
Roadmap
Governance
About us
GitHub
Other Versions and Download
Toggle Menu
PrevUp
Next
scikit-learn 1.4.2
Other versions
Please cite us if you use the software.
### 1.13. Feature selection

- 1.13.1. Removing features with low variance
- 1.13.2. Univariate feature selection
- 1.13.3. Recursive feature elimination
- 1.13.4. Feature selection using SelectFromModel
1.13.4.1. L1-based feature selection
1.13.4.2. Tree-based feature selection
- 1.13.5. Sequential Feature Selection
- 1.13.6. Feature selection as part of a pipeline
### 1.13. Feature selection¶

The classes in the sklearn.feature_selection module can be used
for feature selection/dimensionality reduction on sample sets, either to
improve estimators’ accuracy scores or to boost their performance on very
high-dimensional datasets.
- 1.13.1. Removing features with low variance¶
VarianceThreshold is a simple baseline approach to feature selection.
It removes all features whose variance doesn’t meet some threshold.
By default, it removes all zero-variance features,
i.e. features that have the same value in all samples.
As an example, suppose that we have a dataset with boolean features,
and we want to remove all features that are either one or zero (on or off)
in more than 80% of the samples.
Boolean features are Bernoulli random variables,
and the variance of such variables is given by
\[\mathrm{Var}[X] = p(1 - p)\]
so we can select using the threshold .8 * (1 - .8):
>>> from sklearn.feature_selection import VarianceThreshold
>>> X = [[0, 0, 1], [0, 1, 0], [1, 0, 0], [0, 1, 1], [0, 1, 0], [0, 1, 1]]
>>> sel = VarianceThreshold(threshold=(.8 * (1 - .8)))
>>> sel.fit_transform(X)
array([[0, 1],
[1, 0],
[0, 0],
[1, 1],
[1, 0],
[1, 1]])
As expected, VarianceThreshold has removed the first column,
which has a probability \(p = 5/6 > .8\) of containing a zero.
- 1.13.2. Univariate feature selection¶
Univariate feature selection works by selecting the best features based on
univariate statistical tests. It can be seen as a preprocessing step
to an estimator. Scikit-learn exposes feature selection routines
as objects that implement the transform method:
SelectKBest removes all but the \(k\) highest scoring features
SelectPercentile removes all but a user-specified highest scoring
percentage of features
using common univariate statistical tests for each feature:
false positive rate SelectFpr, false discovery rate
SelectFdr, or family wise error SelectFwe.
GenericUnivariateSelect allows to perform univariate feature
selection with a configurable strategy. This allows to select the best
univariate selection strategy with hyper-parameter search estimator.
For instance, we can use a F-test to retrieve the two
best features for a dataset as follows:
>>> from sklearn.datasets import load_iris
>>> from sklearn.feature_selection import SelectKBest
>>> from sklearn.feature_selection import f_classif
>>> X, y = load_iris(return_X_y=True)
>>> X.shape
(150, 4)
>>> X_new = SelectKBest(f_classif, k=2).fit_transform(X, y)
>>> X_new.shape
(150, 2)
These objects take as input a scoring function that returns univariate scores
and p-values (or only scores for SelectKBest and
SelectPercentile):
For regression: r_regression, f_regression, mutual_info_regression
For classification: chi2, f_classif, mutual_info_classif
The methods based on F-test estimate the degree of linear dependency between
two random variables. On the other hand, mutual information methods can capture
any kind of statistical dependency, but being nonparametric, they require more
samples for accurate estimation. Note that the \(\chi^2\)-test should only be
applied to non-negative features, such as frequencies.
Feature selection with sparse data
If you use sparse data (i.e. data represented as sparse matrices),
chi2, mutual_info_regression, mutual_info_classif
will deal with the data without making it dense.
Warning
Beware not to use a regression scoring function with a classification
problem, you will get useless results.
Note
The SelectPercentile and SelectKBest support unsupervised
feature selection as well. One needs to provide a score_func where y=None.
The score_func should use internally X to compute the scores.
Examples:
Univariate Feature Selection
Comparison of F-test and mutual information
- 1.13.3. Recursive feature elimination¶
Given an external estimator that assigns weights to features (e.g., the
coefficients of a linear model), the goal of recursive feature elimination (RFE)
is to select features by recursively considering smaller and smaller sets of
features. First, the estimator is trained on the initial set of features and
the importance of each feature is obtained either through any specific attribute
(such as coef_, feature_importances_) or callable. Then, the least important
features are pruned from current set of features. That procedure is recursively
repeated on the pruned set until the desired number of features to select is
eventually reached.
RFECV performs RFE in a cross-validation loop to find the optimal
number of features. In more details, the number of features selected is tuned
automatically by fitting an RFE selector on the different
cross-validation splits (provided by the cv parameter). The performance
of the RFE selector are evaluated using scorer for different number
of selected features and aggregated together. Finally, the scores are averaged
across folds and the number of features selected is set to the number of
features that maximize the cross-validation score.
Examples:
Recursive feature elimination: A recursive feature elimination example
showing the relevance of pixels in a digit classification task.
Recursive feature elimination with cross-validation: A recursive feature
elimination example with automatic tuning of the number of features
selected with cross-validation.
- 1.13.4. Feature selection using SelectFromModel¶
SelectFromModel is a meta-transformer that can be used alongside any
estimator that assigns importance to each feature through a specific attribute (such as
coef_, feature_importances_) or via an importance_getter callable after fitting.
The features are considered unimportant and removed if the corresponding
importance of the feature values are below the provided
threshold parameter. Apart from specifying the threshold numerically,
there are built-in heuristics for finding a threshold using a string argument.
Available heuristics are “mean”, “median” and float multiples of these like
“0.1*mean”. In combination with the threshold criteria, one can use the
max_features parameter to set a limit on the number of features to select.
For examples on how it is to be used refer to the sections below.
Examples
Model-based and sequential feature selection
1.13.4.1. L1-based feature selection¶
Linear models penalized with the L1 norm have
sparse solutions: many of their estimated coefficients are zero. When the goal
is to reduce the dimensionality of the data to use with another classifier,
they can be used along with SelectFromModel
to select the non-zero coefficients. In particular, sparse estimators useful
for this purpose are the Lasso for regression, and
of LogisticRegression and LinearSVC
for classification:
>>> from sklearn.svm import LinearSVC
>>> from sklearn.datasets import load_iris
>>> from sklearn.feature_selection import SelectFromModel
>>> X, y = load_iris(return_X_y=True)
>>> X.shape
(150, 4)
>>> lsvc = LinearSVC(C=0.01, penalty="l1", dual=False).fit(X, y)
>>> model = SelectFromModel(lsvc, prefit=True)
>>> X_new = model.transform(X)
>>> X_new.shape
(150, 3)
With SVMs and logistic-regression, the parameter C controls the sparsity:
the smaller C the fewer features selected. With Lasso, the higher the
alpha parameter, the fewer features selected.
Examples:
Lasso on dense and sparse data.
L1-recovery and compressive sensing
Click for more details
¶
For a good choice of alpha, the Lasso can fully recover the
exact set of non-zero variables using only few observations, provided
certain specific conditions are met. In particular, the number of
samples should be “sufficiently large”, or L1 models will perform at
random, where “sufficiently large” depends on the number of non-zero
coefficients, the logarithm of the number of features, the amount of
noise, the smallest absolute value of non-zero coefficients, and the
structure of the design matrix X. In addition, the design matrix must
display certain specific properties, such as not being too correlated.
There is no general rule to select an alpha parameter for recovery of
non-zero coefficients. It can by set by cross-validation
(LassoCV or
LassoLarsCV), though this may lead to
under-penalized models: including a small number of non-relevant variables
is not detrimental to prediction score. BIC
(LassoLarsIC) tends, on the opposite, to set
high values of alpha.
Reference
Richard G. Baraniuk “Compressive Sensing”, IEEE Signal
Processing Magazine [120] July 2007
http://users.isr.ist.utl.pt/~aguiar/CS_notes.pdf
1.13.4.2. Tree-based feature selection¶
Tree-based estimators (see the sklearn.tree module and forest
of trees in the sklearn.ensemble module) can be used to compute
impurity-based feature importances, which in turn can be used to discard irrelevant
features (when coupled with the SelectFromModel
meta-transformer):
>>> from sklearn.ensemble import ExtraTreesClassifier
>>> from sklearn.datasets import load_iris
>>> from sklearn.feature_selection import SelectFromModel
>>> X, y = load_iris(return_X_y=True)
>>> X.shape
(150, 4)
>>> clf = ExtraTreesClassifier(n_estimators=50)
>>> clf = clf.fit(X, y)
>>> clf.feature_importances_
array([ 0.04...,  0.05...,  0.4...,  0.4...])
>>> model = SelectFromModel(clf, prefit=True)
>>> X_new = model.transform(X)
>>> X_new.shape
(150, 2)
Examples:
Feature importances with a forest of trees: example on
synthetic data showing the recovery of the actually meaningful
features.
Pixel importances with a parallel forest of trees: example
on face recognition data.
- 1.13.5. Sequential Feature Selection¶
Sequential Feature Selection [sfs] (SFS) is available in the
SequentialFeatureSelector transformer.
SFS can be either forward or backward:
Forward-SFS is a greedy procedure that iteratively finds the best new feature
to add to the set of selected features. Concretely, we initially start with
zero features and find the one feature that maximizes a cross-validated score
when an estimator is trained on this single feature. Once that first feature
is selected, we repeat the procedure by adding a new feature to the set of
selected features. The procedure stops when the desired number of selected
features is reached, as determined by the n_features_to_select parameter.
Backward-SFS follows the same idea but works in the opposite direction:
instead of starting with no features and greedily adding features, we start
with all the features and greedily remove features from the set. The
direction parameter controls whether forward or backward SFS is used.
Detail on Sequential Feature Selection
Click for more details
¶
In general, forward and backward selection do not yield equivalent results.
Also, one may be much faster than the other depending on the requested number
of selected features: if we have 10 features and ask for 7 selected features,
forward selection would need to perform 7 iterations while backward selection
would only need to perform 3.
SFS differs from RFE and
SelectFromModel in that it does not
require the underlying model to expose a coef_ or feature_importances_
attribute. It may however be slower considering that more models need to be
evaluated, compared to the other approaches. For example in backward
selection, the iteration going from m features to m - 1 features using k-fold
cross-validation requires fitting m * k models, while
RFE would require only a single fit, and
SelectFromModel always just does a single
fit and requires no iterations.
Reference
[sfs]
Ferri et al, Comparative study of techniques for
large-scale feature selection.
Examples
Model-based and sequential feature selection
- 1.13.6. Feature selection as part of a pipeline¶
Feature selection is usually used as a pre-processing step before doing
the actual learning. The recommended way to do this in scikit-learn is
to use a Pipeline:
clf = Pipeline([
('feature_selection', SelectFromModel(LinearSVC(dual="auto", penalty="l1"))),
('classification', RandomForestClassifier())
])
clf.fit(X, y)
In this snippet we make use of a LinearSVC
coupled with SelectFromModel
to evaluate feature importances and select the most relevant features.
Then, a RandomForestClassifier is trained on the
transformed output, i.e. using only relevant features. You can perform
similar operations with the other feature selection methods and also
classifiers that provide a way to evaluate feature importances of course.
See the Pipeline examples for more details.
© 2007 - 2024, scikit-learn developers (BSD License).
Show this page source
### 1.14. Semi-supervised learning — scikit-learn 1.4.2 documentation

Install
User Guide
API
Examples
Community
Getting Started
Tutorial
What's new
Glossary
Development
FAQ
Support
Related packages
Roadmap
Governance
About us
GitHub
Other Versions and Download
More
Getting Started
Tutorial
What's new
Glossary
Development
FAQ
Support
Related packages
Roadmap
Governance
About us
GitHub
Other Versions and Download
Toggle Menu
PrevUp
Next
scikit-learn 1.4.2
Other versions
Please cite us if you use the software.
### 1.14. Semi-supervised learning

- 1.14.1. Self Training
- 1.14.2. Label Propagation
### 1.14. Semi-supervised learning¶

Semi-supervised learning is a situation
in which in your training data some of the samples are not labeled. The
semi-supervised estimators in sklearn.semi_supervised are able to
make use of this additional unlabeled data to better capture the shape of
the underlying data distribution and generalize better to new samples.
These algorithms can perform well when we have a very small amount of
labeled points and a large amount of unlabeled points.
Unlabeled entries in y
It is important to assign an identifier to unlabeled points along with the
labeled data when training the model with the fit method. The
identifier that this implementation uses is the integer value \(-1\).
Note that for string labels, the dtype of y should be object so that it
can contain both strings and integers.
Note
Semi-supervised algorithms need to make assumptions about the distribution
of the dataset in order to achieve performance gains. See here
for more details.
- 1.14.1. Self Training¶
This self-training implementation is based on Yarowsky’s [1] algorithm. Using
this algorithm, a given supervised classifier can function as a semi-supervised
classifier, allowing it to learn from unlabeled data.
SelfTrainingClassifier can be called with any classifier that
implements predict_proba, passed as the parameter base_classifier. In
each iteration, the base_classifier predicts labels for the unlabeled
samples and adds a subset of these labels to the labeled dataset.
The choice of this subset is determined by the selection criterion. This
selection can be done using a threshold on the prediction probabilities, or
by choosing the k_best samples according to the prediction probabilities.
The labels used for the final fit as well as the iteration in which each sample
was labeled are available as attributes. The optional max_iter parameter
specifies how many times the loop is executed at most.
The max_iter parameter may be set to None, causing the algorithm to iterate
until all samples have labels or no new samples are selected in that iteration.
Note
When using the self-training classifier, the
calibration of the classifier is important.
Examples
Effect of varying threshold for self-training
Decision boundary of semi-supervised classifiers versus SVM on the Iris dataset
References
[1]
“Unsupervised word sense disambiguation rivaling supervised methods”
David Yarowsky, Proceedings of the 33rd annual meeting on Association for
Computational Linguistics (ACL ‘95). Association for Computational Linguistics,
Stroudsburg, PA, USA, 189-196.
- 1.14.2. Label Propagation¶
Label propagation denotes a few variations of semi-supervised graph
inference algorithms.
A few features available in this model:
Used for classification tasks
Kernel methods to project data into alternate dimensional spaces
scikit-learn provides two label propagation models:
LabelPropagation and LabelSpreading. Both work by
constructing a similarity graph over all items in the input dataset.
An illustration of label-propagation: the structure of unlabeled
observations is consistent with the class structure, and thus the
class label can be propagated to the unlabeled observations of the
training set.¶
LabelPropagation and LabelSpreading
differ in modifications to the similarity matrix that graph and the
clamping effect on the label distributions.
Clamping allows the algorithm to change the weight of the true ground labeled
data to some degree. The LabelPropagation algorithm performs hard
clamping of input labels, which means \(\alpha=0\). This clamping factor
can be relaxed, to say \(\alpha=0.2\), which means that we will always
retain 80 percent of our original label distribution, but the algorithm gets to
change its confidence of the distribution within 20 percent.
LabelPropagation uses the raw similarity matrix constructed from
the data with no modifications. In contrast, LabelSpreading
minimizes a loss function that has regularization properties, as such it
is often more robust to noise. The algorithm iterates on a modified
version of the original graph and normalizes the edge weights by
computing the normalized graph Laplacian matrix. This procedure is also
used in Spectral clustering.
Label propagation models have two built-in kernel methods. Choice of kernel
effects both scalability and performance of the algorithms. The following are
available:
rbf (\(\exp(-\gamma |x-y|^2), \gamma > 0\)). \(\gamma\) is
specified by keyword gamma.
knn (\(1[x' \in kNN(x)]\)). \(k\) is specified by keyword
n_neighbors.
The RBF kernel will produce a fully connected graph which is represented in memory
by a dense matrix. This matrix may be very large and combined with the cost of
performing a full matrix multiplication calculation for each iteration of the
algorithm can lead to prohibitively long running times. On the other hand,
the KNN kernel will produce a much more memory-friendly sparse matrix
which can drastically reduce running times.
Examples
Decision boundary of semi-supervised classifiers versus SVM on the Iris dataset
Label Propagation learning a complex structure
Label Propagation digits: Demonstrating performance
Label Propagation digits active learning
References
[2] Yoshua Bengio, Olivier Delalleau, Nicolas Le Roux. In Semi-Supervised
Learning (2006), pp. 193-216
[3] Olivier Delalleau, Yoshua Bengio, Nicolas Le Roux. Efficient
Non-Parametric Function Induction in Semi-Supervised Learning. AISTAT 2005
https://www.gatsby.ucl.ac.uk/aistats/fullpapers/204.pdf
© 2007 - 2024, scikit-learn developers (BSD License).
Show this page source
### 1.15. Isotonic regression — scikit-learn 1.4.2 documentation

Install
User Guide
API
Examples
Community
Getting Started
Tutorial
What's new
Glossary
Development
FAQ
Support
Related packages
Roadmap
Governance
About us
GitHub
Other Versions and Download
More
Getting Started
Tutorial
What's new
Glossary
Development
FAQ
Support
Related packages
Roadmap
Governance
About us
GitHub
Other Versions and Download
Toggle Menu
PrevUp
Next
scikit-learn 1.4.2
Other versions
Please cite us if you use the software.
### 1.15. Isotonic regression

### 1.15. Isotonic regression¶

The class IsotonicRegression fits a non-decreasing real function to
1-dimensional data. It solves the following problem:
\[\min \sum_i w_i (y_i - \hat{y}_i)^2\]
subject to \(\hat{y}_i \le \hat{y}_j\) whenever \(X_i \le X_j\),
where the weights \(w_i\) are strictly positive, and both X and y are
arbitrary real quantities.
The increasing parameter changes the constraint to
\(\hat{y}_i \ge \hat{y}_j\) whenever \(X_i \le X_j\). Setting it to
‘auto’ will automatically choose the constraint based on Spearman’s rank
correlation coefficient.
IsotonicRegression produces a series of predictions
\(\hat{y}_i\) for the training data which are the closest to the targets
\(y\) in terms of mean squared error. These predictions are interpolated
for predicting to unseen data. The predictions of IsotonicRegression
thus form a function that is piecewise linear:
© 2007 - 2024, scikit-learn developers (BSD License).
Show this page source
### 1.16. Probability calibration — scikit-learn 1.4.2 documentation

Install
User Guide
API
Examples
Community
Getting Started
Tutorial
What's new
Glossary
Development
FAQ
Support
Related packages
Roadmap
Governance
About us
GitHub
Other Versions and Download
More
Getting Started
Tutorial
What's new
Glossary
Development
FAQ
Support
Related packages
Roadmap
Governance
About us
GitHub
Other Versions and Download
Toggle Menu
PrevUp
Next
scikit-learn 1.4.2
Other versions
Please cite us if you use the software.
### 1.16. Probability calibration

- 1.16.1. Calibration curves
- 1.16.2. Calibrating a classifier
- 1.16.3. Usage
1.16.3.1. Sigmoid
1.16.3.2. Isotonic
1.16.3.3. Multiclass support
### 1.16. Probability calibration¶

When performing classification you often want not only to predict the class
label, but also obtain a probability of the respective label. This probability
gives you some kind of confidence on the prediction. Some models can give you
poor estimates of the class probabilities and some even do not support
probability prediction (e.g., some instances of
SGDClassifier).
The calibration module allows you to better calibrate
the probabilities of a given model, or to add support for probability
prediction.
Well calibrated classifiers are probabilistic classifiers for which the output
of the predict_proba method can be directly interpreted as a confidence
level.
For instance, a well calibrated (binary) classifier should classify the samples such
that among the samples to which it gave a predict_proba value close to, say,
0.8, approximately 80% actually belong to the positive class.
Before we show how to re-calibrate a classifier, we first need a way to detect how
good a classifier is calibrated.
Note
Strictly proper scoring rules for probabilistic predictions like
sklearn.metrics.brier_score_loss and
sklearn.metrics.log_loss assess calibration (reliability) and
discriminative power (resolution) of a model, as well as the randomness of the data
(uncertainty) at the same time. This follows from the well-known Brier score
decomposition of Murphy [1]. As it is not clear which term dominates, the score is
of limited use for assessing calibration alone (unless one computes each term of
the decomposition). A lower Brier loss, for instance, does not necessarily
mean a better calibrated model, it could also mean a worse calibrated model with much
more discriminatory power, e.g. using many more features.
- 1.16.1. Calibration curves¶
Calibration curves, also referred to as reliability diagrams (Wilks 1995 [2]),
compare how well the probabilistic predictions of a binary classifier are calibrated.
It plots the frequency of the positive label (to be more precise, an estimation of the
conditional event probability \(P(Y=1|\text{predict_proba})\)) on the y-axis
against the predicted probability predict_proba of a model on the x-axis.
The tricky part is to get values for the y-axis.
In scikit-learn, this is accomplished by binning the predictions such that the x-axis
represents the average predicted probability in each bin.
The y-axis is then the fraction of positives given the predictions of that bin, i.e.
the proportion of samples whose class is the positive class (in each bin).
The top calibration curve plot is created with
CalibrationDisplay.from_estimator, which uses calibration_curve to
calculate the per bin average predicted probabilities and fraction of positives.
CalibrationDisplay.from_estimator
takes as input a fitted classifier, which is used to calculate the predicted
probabilities. The classifier thus must have predict_proba method. For
the few classifiers that do not have a predict_proba method, it is
possible to use CalibratedClassifierCV to calibrate the classifier
outputs to probabilities.
The bottom histogram gives some insight into the behavior of each classifier
by showing the number of samples in each predicted probability bin.
LogisticRegression returns well calibrated predictions by default as it has a
canonical link function for its loss, i.e. the logit-link for the Log loss.
This leads to the so-called balance property, see [8] and
Logistic regression.
In contrast to that, the other shown models return biased probabilities; with
different biases per model.
GaussianNB (Naive Bayes) tends to push probabilities to 0 or 1 (note the counts
in the histograms). This is mainly because it makes the assumption that
features are conditionally independent given the class, which is not the
case in this dataset which contains 2 redundant features.
RandomForestClassifier shows the opposite behavior: the histograms
show peaks at probabilities approximately 0.2 and 0.9, while probabilities
close to 0 or 1 are very rare. An explanation for this is given by
Niculescu-Mizil and Caruana [3]: “Methods such as bagging and random
forests that average predictions from a base set of models can have
difficulty making predictions near 0 and 1 because variance in the
underlying base models will bias predictions that should be near zero or one
away from these values. Because predictions are restricted to the interval
[0,1], errors caused by variance tend to be one-sided near zero and one. For
example, if a model should predict p = 0 for a case, the only way bagging
can achieve this is if all bagged trees predict zero. If we add noise to the
trees that bagging is averaging over, this noise will cause some trees to
predict values larger than 0 for this case, thus moving the average
prediction of the bagged ensemble away from 0. We observe this effect most
strongly with random forests because the base-level trees trained with
random forests have relatively high variance due to feature subsetting.” As
a result, the calibration curve shows a characteristic sigmoid shape, indicating that
the classifier could trust its “intuition” more and return probabilities closer
to 0 or 1 typically.
LinearSVC (SVC) shows an even more sigmoid curve than the random forest, which
is typical for maximum-margin methods (compare Niculescu-Mizil and Caruana [3]), which
focus on difficult to classify samples that are close to the decision boundary (the
support vectors).
- 1.16.2. Calibrating a classifier¶
Calibrating a classifier consists of fitting a regressor (called a
calibrator) that maps the output of the classifier (as given by
decision_function or predict_proba) to a calibrated probability
in [0, 1]. Denoting the output of the classifier for a given sample by \(f_i\),
the calibrator tries to predict the conditional event probability
\(P(y_i = 1 | f_i)\).
Ideally, the calibrator is fit on a dataset independent of the training data used to
fit the classifier in the first place.
This is because performance of the classifier on its training data would be
better than for novel data. Using the classifier output of training data
to fit the calibrator would thus result in a biased calibrator that maps to
probabilities closer to 0 and 1 than it should.
- 1.16.3. Usage¶
The CalibratedClassifierCV class is used to calibrate a classifier.
CalibratedClassifierCV uses a cross-validation approach to ensure
unbiased data is always used to fit the calibrator. The data is split into k
(train_set, test_set) couples (as determined by cv). When ensemble=True
(default), the following procedure is repeated independently for each
cross-validation split: a clone of base_estimator is first trained on the
train subset. Then its predictions on the test subset are used to fit a
calibrator (either a sigmoid or isotonic regressor). This results in an
ensemble of k (classifier, calibrator) couples where each calibrator maps
the output of its corresponding classifier into [0, 1]. Each couple is exposed
in the calibrated_classifiers_ attribute, where each entry is a calibrated
classifier with a predict_proba method that outputs calibrated
probabilities. The output of predict_proba for the main
CalibratedClassifierCV instance corresponds to the average of the
predicted probabilities of the k estimators in the calibrated_classifiers_
list. The output of predict is the class that has the highest
probability.
When ensemble=False, cross-validation is used to obtain ‘unbiased’
predictions for all the data, via
cross_val_predict.
These unbiased predictions are then used to train the calibrator. The attribute
calibrated_classifiers_ consists of only one (classifier, calibrator)
couple where the classifier is the base_estimator trained on all the data.
In this case the output of predict_proba for
CalibratedClassifierCV is the predicted probabilities obtained
from the single (classifier, calibrator) couple.
The main advantage of ensemble=True is to benefit from the traditional
ensembling effect (similar to Bagging meta-estimator). The resulting ensemble should
both be well calibrated and slightly more accurate than with ensemble=False.
The main advantage of using ensemble=False is computational: it reduces the
overall fit time by training only a single base classifier and calibrator
pair, decreases the final model size and increases prediction speed.
Alternatively an already fitted classifier can be calibrated by setting
cv="prefit". In this case, the data is not split and all of it is used to
fit the regressor. It is up to the user to
make sure that the data used for fitting the classifier is disjoint from the
data used for fitting the regressor.
CalibratedClassifierCV supports the use of two regression techniques
for calibration via the method parameter: "sigmoid" and "isotonic".
1.16.3.1. Sigmoid¶
The sigmoid regressor, method="sigmoid" is based on Platt’s logistic model [4]:
\[p(y_i = 1 | f_i) = \frac{1}{1 + \exp(A f_i + B)} \,,\]
where \(y_i\) is the true label of sample \(i\) and \(f_i\)
is the output of the un-calibrated classifier for sample \(i\). \(A\)
and \(B\) are real numbers to be determined when fitting the regressor via
maximum likelihood.
The sigmoid method assumes the calibration curve
can be corrected by applying a sigmoid function to the raw predictions. This
assumption has been empirically justified in the case of Support Vector Machines with
common kernel functions on various benchmark datasets in section 2.1 of Platt
1999 [4] but does not necessarily hold in general. Additionally, the
logistic model works best if the calibration error is symmetrical, meaning
the classifier output for each binary class is normally distributed with
the same variance [7]. This can be a problem for highly imbalanced
classification problems, where outputs do not have equal variance.
In general this method is most effective for small sample sizes or when the
un-calibrated model is under-confident and has similar calibration errors for both
high and low outputs.
1.16.3.2. Isotonic¶
The method="isotonic" fits a non-parametric isotonic regressor, which outputs
a step-wise non-decreasing function, see sklearn.isotonic. It minimizes:
\[\sum_{i=1}^{n} (y_i - \hat{f}_i)^2\]
subject to \(\hat{f}_i \geq \hat{f}_j\) whenever
\(f_i \geq f_j\). \(y_i\) is the true
label of sample \(i\) and \(\hat{f}_i\) is the output of the
calibrated classifier for sample \(i\) (i.e., the calibrated probability).
This method is more general when compared to ‘sigmoid’ as the only restriction
is that the mapping function is monotonically increasing. It is thus more
powerful as it can correct any monotonic distortion of the un-calibrated model.
However, it is more prone to overfitting, especially on small datasets [6].
Overall, ‘isotonic’ will perform as well as or better than ‘sigmoid’ when
there is enough data (greater than ~ 1000 samples) to avoid overfitting [3].
Note
Impact on ranking metrics like AUC
It is generally expected that calibration does not affect ranking metrics such as
ROC-AUC. However, these metrics might differ after calibration when using
method="isotonic" since isotonic regression introduces ties in the predicted
probabilities. This can be seen as within the uncertainty of the model predictions.
In case, you strictly want to keep the ranking and thus AUC scores, use
method="sigmoid" which is a strictly monotonic transformation and thus keeps
the ranking.
1.16.3.3. Multiclass support¶
Both isotonic and sigmoid regressors only
support 1-dimensional data (e.g., binary classification output) but are
extended for multiclass classification if the base_estimator supports
multiclass predictions. For multiclass predictions,
CalibratedClassifierCV calibrates for
each class separately in a OneVsRestClassifier fashion [5]. When
predicting
probabilities, the calibrated probabilities for each class
are predicted separately. As those probabilities do not necessarily sum to
one, a postprocessing is performed to normalize them.
Examples:
Probability Calibration curves
Probability Calibration for 3-class classification
Probability calibration of classifiers
Comparison of Calibration of Classifiers
References:
[1]
Allan H. Murphy (1973).
“A New Vector Partition of the Probability Score”
Journal of Applied Meteorology and Climatology
[2]
On the combination of forecast probabilities for
consecutive precipitation periods.
Wea. Forecasting, 5, 640–650., Wilks, D. S., 1990a
[3]
(1,2,3)
Predicting Good Probabilities with Supervised Learning,
A. Niculescu-Mizil & R. Caruana, ICML 2005
[4]
(1,2)
Probabilistic Outputs for Support Vector Machines and Comparisons
to Regularized Likelihood Methods.
J. Platt, (1999)
[5]
Transforming Classifier Scores into Accurate Multiclass
Probability Estimates.
B. Zadrozny & C. Elkan, (KDD 2002)
[6]
Predicting accurate probabilities with a ranking loss.
Menon AK, Jiang XJ, Vembu S, Elkan C, Ohno-Machado L.
Proc Int Conf Mach Learn. 2012;2012:703-710
[7]
Beyond sigmoids: How to obtain well-calibrated probabilities from
binary classifiers with beta calibration
Kull, M., Silva Filho, T. M., & Flach, P. (2017).
[8]
Mario V. Wüthrich, Michael Merz (2023).
“Statistical Foundations of Actuarial Learning and its Applications”
Springer Actuarial
© 2007 - 2024, scikit-learn developers (BSD License).
Show this page source
### 1.17. Neural network models (supervised) — scikit-learn 1.4.2 documentation

Install
User Guide
API
Examples
Community
Getting Started
Tutorial
What's new
Glossary
Development
FAQ
Support
Related packages
Roadmap
Governance
About us
GitHub
Other Versions and Download
More
Getting Started
Tutorial
What's new
Glossary
Development
FAQ
Support
Related packages
Roadmap
Governance
About us
GitHub
Other Versions and Download
Toggle Menu
PrevUp
Next
scikit-learn 1.4.2
Other versions
Please cite us if you use the software.
### 1.17. Neural network models (supervised)

- 1.17.1. Multi-layer Perceptron
- 1.17.2. Classification
- 1.17.3. Regression
- 1.17.4. Regularization
- 1.17.5. Algorithms
- 1.17.6. Complexity
- 1.17.7. Mathematical formulation
- 1.17.8. Tips on Practical Use
- 1.17.9. More control with warm_start
### 1.17. Neural network models (supervised)¶

Warning
This implementation is not intended for large-scale applications. In particular,
scikit-learn offers no GPU support. For much faster, GPU-based implementations,
as well as frameworks offering much more flexibility to build deep learning
architectures, see  Related Projects.
- 1.17.1. Multi-layer Perceptron¶
Multi-layer Perceptron (MLP) is a supervised learning algorithm that learns
a function \(f(\cdot): R^m \rightarrow R^o\) by training on a dataset,
where \(m\) is the number of dimensions for input and \(o\) is the
number of dimensions for output. Given a set of features \(X = {x_1, x_2, ..., x_m}\)
and a target \(y\), it can learn a non-linear function approximator for either
classification or regression. It is different from logistic regression, in that
between the input and the output layer, there can be one or more non-linear
layers, called hidden layers. Figure 1 shows a one hidden layer MLP with scalar
output.
Figure 1 : One hidden layer MLP.¶
The leftmost layer, known as the input layer, consists of a set of neurons
\(\{x_i | x_1, x_2, ..., x_m\}\) representing the input features. Each
neuron in the hidden layer transforms the values from the previous layer with
a weighted linear summation \(w_1x_1 + w_2x_2 + ... + w_mx_m\), followed
by a non-linear activation function \(g(\cdot):R \rightarrow R\) - like
the hyperbolic tan function. The output layer receives the values from the
last hidden layer and transforms them into output values.
The module contains the public attributes coefs_ and intercepts_.
coefs_ is a list of weight matrices, where weight matrix at index
\(i\) represents the weights between layer \(i\) and layer
\(i+1\). intercepts_ is a list of bias vectors, where the vector
at index \(i\) represents the bias values added to layer \(i+1\).
The advantages of Multi-layer Perceptron are:
Capability to learn non-linear models.
Capability to learn models in real-time (on-line learning)
using partial_fit.
The disadvantages of Multi-layer Perceptron (MLP) include:
MLP with hidden layers have a non-convex loss function where there exists
more than one local minimum. Therefore different random weight
initializations can lead to different validation accuracy.
MLP requires tuning a number of hyperparameters such as the number of
hidden neurons, layers, and iterations.
MLP is sensitive to feature scaling.
Please see Tips on Practical Use section that addresses
some of these disadvantages.
- 1.17.2. Classification¶
Class MLPClassifier implements a multi-layer perceptron (MLP) algorithm
that trains using Backpropagation.
MLP trains on two arrays: array X of size (n_samples, n_features), which holds
the training samples represented as floating point feature vectors; and array
y of size (n_samples,), which holds the target values (class labels) for the
training samples:
>>> from sklearn.neural_network import MLPClassifier
>>> X = [[0., 0.], [1., 1.]]
>>> y = [0, 1]
>>> clf = MLPClassifier(solver='lbfgs', alpha=1e-5,
...                     hidden_layer_sizes=(5, 2), random_state=1)
...
>>> clf.fit(X, y)
MLPClassifier(alpha=1e-05, hidden_layer_sizes=(5, 2), random_state=1,
solver='lbfgs')
After fitting (training), the model can predict labels for new samples:
>>> clf.predict([[2., 2.], [-1., -2.]])
array([1, 0])
MLP can fit a non-linear model to the training data. clf.coefs_
contains the weight matrices that constitute the model parameters:
>>> [coef.shape for coef in clf.coefs_]
[(2, 5), (5, 2), (2, 1)]
Currently, MLPClassifier supports only the
Cross-Entropy loss function, which allows probability estimates by running the
predict_proba method.
MLP trains using Backpropagation. More precisely, it trains using some form of
gradient descent and the gradients are calculated using Backpropagation. For
classification, it minimizes the Cross-Entropy loss function, giving a vector
of probability estimates \(P(y|x)\) per sample \(x\):
>>> clf.predict_proba([[2., 2.], [1., 2.]])
array([[1.967...e-04, 9.998...-01],
[1.967...e-04, 9.998...-01]])
MLPClassifier supports multi-class classification by
applying Softmax
as the output function.
Further, the model supports multi-label classification
in which a sample can belong to more than one class. For each class, the raw
output passes through the logistic function. Values larger or equal to 0.5
are rounded to 1, otherwise to 0. For a predicted output of a sample, the
indices where the value is 1 represents the assigned classes of that sample:
>>> X = [[0., 0.], [1., 1.]]
>>> y = [[0, 1], [1, 1]]
>>> clf = MLPClassifier(solver='lbfgs', alpha=1e-5,
...                     hidden_layer_sizes=(15,), random_state=1)
...
>>> clf.fit(X, y)
MLPClassifier(alpha=1e-05, hidden_layer_sizes=(15,), random_state=1,
solver='lbfgs')
>>> clf.predict([[1., 2.]])
array([[1, 1]])
>>> clf.predict([[0., 0.]])
array([[0, 1]])
See the examples below and the docstring of
MLPClassifier.fit for further information.
Examples:
Compare Stochastic learning strategies for MLPClassifier
Visualization of MLP weights on MNIST
- 1.17.3. Regression¶
Class MLPRegressor implements a multi-layer perceptron (MLP) that
trains using backpropagation with no activation function in the output layer,
which can also be seen as using the identity function as activation function.
Therefore, it uses the square error as the loss function, and the output is a
set of continuous values.
MLPRegressor also supports multi-output regression, in
which a sample can have more than one target.
- 1.17.4. Regularization¶
Both MLPRegressor and MLPClassifier use parameter alpha
for regularization (L2 regularization) term which helps in avoiding overfitting
by penalizing weights with large magnitudes. Following plot displays varying
decision function with value of alpha.
See the examples below for further information.
Examples:
Varying regularization in Multi-layer Perceptron
- 1.17.5. Algorithms¶
MLP trains using Stochastic Gradient Descent,
Adam, or
L-BFGS.
Stochastic Gradient Descent (SGD) updates parameters using the gradient of the
loss function with respect to a parameter that needs adaptation, i.e.
\[w \leftarrow w - \eta (\alpha \frac{\partial R(w)}{\partial w}
+ \frac{\partial Loss}{\partial w})\]
where \(\eta\) is the learning rate which controls the step-size in
the parameter space search.  \(Loss\) is the loss function used
for the network.
More details can be found in the documentation of
SGD
Adam is similar to SGD in a sense that it is a stochastic optimizer, but it can
automatically adjust the amount to update parameters based on adaptive estimates
of lower-order moments.
With SGD or Adam, training supports online and mini-batch learning.
L-BFGS is a solver that approximates the Hessian matrix which represents the
second-order partial derivative of a function. Further it approximates the
inverse of the Hessian matrix to perform parameter updates. The implementation
uses the Scipy version of L-BFGS.
If the selected solver is ‘L-BFGS’, training does not support online nor
mini-batch learning.
- 1.17.6. Complexity¶
Suppose there are \(n\) training samples, \(m\) features, \(k\)
hidden layers, each containing \(h\) neurons - for simplicity, and \(o\)
output neurons.  The time complexity of backpropagation is
\(O(n\cdot m \cdot h^k \cdot o \cdot i)\), where \(i\) is the number
of iterations. Since backpropagation has a high time complexity, it is advisable
to start with smaller number of hidden neurons and few hidden layers for
training.
- 1.17.7. Mathematical formulation¶
Given a set of training examples \((x_1, y_1), (x_2, y_2), \ldots, (x_n, y_n)\)
where \(x_i \in \mathbf{R}^n\) and \(y_i \in \{0, 1\}\), a one hidden
layer one hidden neuron MLP learns the function \(f(x) = W_2 g(W_1^T x + b_1) + b_2\)
where \(W_1 \in \mathbf{R}^m\) and \(W_2, b_1, b_2 \in \mathbf{R}\) are
model parameters. \(W_1, W_2\) represent the weights of the input layer and
hidden layer, respectively; and \(b_1, b_2\) represent the bias added to
the hidden layer and the output layer, respectively.
\(g(\cdot) : R \rightarrow R\) is the activation function, set by default as
the hyperbolic tan. It is given as,
\[g(z)= \frac{e^z-e^{-z}}{e^z+e^{-z}}\]
For binary classification, \(f(x)\) passes through the logistic function
\(g(z)=1/(1+e^{-z})\) to obtain output values between zero and one. A
threshold, set to 0.5, would assign samples of outputs larger or equal 0.5
to the positive class, and the rest to the negative class.
If there are more than two classes, \(f(x)\) itself would be a vector of
size (n_classes,). Instead of passing through logistic function, it passes
through the softmax function, which is written as,
\[\text{softmax}(z)_i = \frac{\exp(z_i)}{\sum_{l=1}^k\exp(z_l)}\]
where \(z_i\) represents the \(i\) th element of the input to softmax,
which corresponds to class \(i\), and \(K\) is the number of classes.
The result is a vector containing the probabilities that sample \(x\)
belong to each class. The output is the class with the highest probability.
In regression, the output remains as \(f(x)\); therefore, output activation
function is just the identity function.
MLP uses different loss functions depending on the problem type. The loss
function for classification is Average Cross-Entropy, which in binary case is
given as,
\[Loss(\hat{y},y,W) = -\dfrac{1}{n}\sum_{i=0}^n(y_i \ln {\hat{y_i}} + (1-y_i) \ln{(1-\hat{y_i})}) + \dfrac{\alpha}{2n} ||W||_2^2\]
where \(\alpha ||W||_2^2\) is an L2-regularization term (aka penalty)
that penalizes complex models; and \(\alpha > 0\) is a non-negative
hyperparameter that controls the magnitude of the penalty.
For regression, MLP uses the Mean Square Error loss function; written as,
\[Loss(\hat{y},y,W) = \frac{1}{2n}\sum_{i=0}^n||\hat{y}_i - y_i ||_2^2 + \frac{\alpha}{2n} ||W||_2^2\]
Starting from initial random weights, multi-layer perceptron (MLP) minimizes
the loss function by repeatedly updating these weights. After computing the
loss, a backward pass propagates it from the output layer to the previous
layers, providing each weight parameter with an update value meant to decrease
the loss.
In gradient descent, the gradient \(\nabla Loss_{W}\) of the loss with respect
to the weights is computed and deducted from \(W\).
More formally, this is expressed as,
\[W^{i+1} = W^i - \epsilon \nabla {Loss}_{W}^{i}\]
where \(i\) is the iteration step, and \(\epsilon\) is the learning rate
with a value larger than 0.
The algorithm stops when it reaches a preset maximum number of iterations; or
when the improvement in loss is below a certain, small number.
- 1.17.8. Tips on Practical Use¶
Multi-layer Perceptron is sensitive to feature scaling, so it
is highly recommended to scale your data. For example, scale each
attribute on the input vector X to [0, 1] or [-1, +1], or standardize
it to have mean 0 and variance 1. Note that you must apply the same
scaling to the test set for meaningful results.
You can use StandardScaler for standardization.
>>> from sklearn.preprocessing import StandardScaler
>>> scaler = StandardScaler()
>>> # Don't cheat - fit only on training data
>>> scaler.fit(X_train)
>>> X_train = scaler.transform(X_train)
>>> # apply same transformation to test data
>>> X_test = scaler.transform(X_test)
An alternative and recommended approach is to use
StandardScaler in a
Pipeline
Finding a reasonable regularization parameter \(\alpha\) is best done
using GridSearchCV, usually in the range
10.0 ** -np.arange(1, 7).
Empirically, we observed that L-BFGS converges faster and
with better solutions on small datasets. For relatively large
datasets, however, Adam is very robust. It usually converges
quickly and gives pretty good performance. SGD with momentum or
nesterov’s momentum, on the other hand, can perform better than
those two algorithms if learning rate is correctly tuned.
- 1.17.9. More control with warm_start¶
If you want more control over stopping criteria or learning rate in SGD,
or want to do additional monitoring, using warm_start=True and
max_iter=1 and iterating yourself can be helpful:
>>> X = [[0., 0.], [1., 1.]]
>>> y = [0, 1]
>>> clf = MLPClassifier(hidden_layer_sizes=(15,), random_state=1, max_iter=1, warm_start=True)
>>> for i in range(10):
...     clf.fit(X, y)
...     # additional monitoring / inspection
MLPClassifier(...
References:
“Learning representations by back-propagating errors.”
Rumelhart, David E., Geoffrey E. Hinton, and Ronald J. Williams.
“Stochastic Gradient Descent” L. Bottou - Website, 2010.
“Backpropagation”
Andrew Ng, Jiquan Ngiam, Chuan Yu Foo, Yifan Mai, Caroline Suen - Website, 2011.
“Efficient BackProp”
Y. LeCun, L. Bottou, G. Orr, K. Müller - In Neural Networks: Tricks
of the Trade 1998.
“Adam: A method for stochastic optimization.”
Kingma, Diederik, and Jimmy Ba (2014)
© 2007 - 2024, scikit-learn developers (BSD License).
Show this page source
### 2.1. Gaussian mixture models — scikit-learn 1.4.2 documentation

Install
User Guide
API
Examples
Community
Getting Started
Tutorial
What's new
Glossary
Development
FAQ
Support
Related packages
Roadmap
Governance
About us
GitHub
Other Versions and Download
More
Getting Started
Tutorial
What's new
Glossary
Development
FAQ
Support
Related packages
Roadmap
Governance
About us
GitHub
Other Versions and Download
Toggle Menu
PrevUp
Next
scikit-learn 1.4.2
Other versions
Please cite us if you use the software.
### 2.1. Gaussian mixture models

- 2.1.1. Gaussian Mixture
- 2.1.2. Variational Bayesian Gaussian Mixture
2.1.2.1. The Dirichlet Process
### 2.1. Gaussian mixture models¶

sklearn.mixture is a package which enables one to learn
Gaussian Mixture Models (diagonal, spherical, tied and full covariance
matrices supported), sample them, and estimate them from
data. Facilities to help determine the appropriate number of
components are also provided.
Two-component Gaussian mixture model: data points, and equi-probability
surfaces of the model.¶
A Gaussian mixture model is a probabilistic model that assumes all the
data points are generated from a mixture of a finite number of
Gaussian distributions with unknown parameters. One can think of
mixture models as generalizing k-means clustering to incorporate
information about the covariance structure of the data as well as the
centers of the latent Gaussians.
Scikit-learn implements different classes to estimate Gaussian
mixture models, that correspond to different estimation strategies,
detailed below.
- 2.1.1. Gaussian Mixture¶
The GaussianMixture object implements the
expectation-maximization (EM)
algorithm for fitting mixture-of-Gaussian models. It can also draw
confidence ellipsoids for multivariate models, and compute the
Bayesian Information Criterion to assess the number of clusters in the
data. A GaussianMixture.fit method is provided that learns a Gaussian
Mixture Model from train data. Given test data, it can assign to each
sample the Gaussian it most probably belongs to using
the GaussianMixture.predict method.
The GaussianMixture comes with different options to constrain the
covariance of the difference classes estimated: spherical, diagonal, tied or
full covariance.
Examples:
See GMM covariances for an example of
using the Gaussian mixture as clustering on the iris dataset.
See Density Estimation for a Gaussian mixture for an example on plotting the
density estimation.
Pros and cons of class GaussianMixture
Click for more details
¶
Pros:
Speed:
It is the fastest algorithm for learning mixture models
Agnostic:
As this algorithm maximizes only the likelihood, it
will not bias the means towards zero, or bias the cluster sizes to
have specific structures that might or might not apply.
Cons:
Singularities:
When one has insufficiently many points per
mixture, estimating the covariance matrices becomes difficult,
and the algorithm is known to diverge and find solutions with
infinite likelihood unless one regularizes the covariances artificially.
Number of components:
This algorithm will always use all the
components it has access to, needing held-out data
or information theoretical criteria to decide how many components to use
in the absence of external cues.
Selecting the number of components in a classical Gaussian Mixture model
Click for more details
¶
The BIC criterion can be used to select the number of components in a Gaussian
Mixture in an efficient way. In theory, it recovers the true number of
components only in the asymptotic regime (i.e. if much data is available and
assuming that the data was actually generated i.i.d. from a mixture of Gaussian
distribution). Note that using a Variational Bayesian Gaussian mixture
avoids the specification of the number of components for a Gaussian mixture
model.
Examples:
See Gaussian Mixture Model Selection for an example
of model selection performed with classical Gaussian mixture.
Estimation algorithm expectation-maximization
Click for more details
¶
The main difficulty in learning Gaussian mixture models from unlabeled
data is that one usually doesn’t know which points came from
which latent component (if one has access to this information it gets
very easy to fit a separate Gaussian distribution to each set of
points). Expectation-maximization
is a well-founded statistical
algorithm to get around this problem by an iterative process. First
one assumes random components (randomly centered on data points,
learned from k-means, or even just normally distributed around the
origin) and computes for each point a probability of being generated by
each component of the model. Then, one tweaks the
parameters to maximize the likelihood of the data given those
assignments. Repeating this process is guaranteed to always converge
to a local optimum.
Choice of the Initialization method
Click for more details
¶
There is a choice of four initialization methods (as well as inputting user defined
initial means) to generate the initial centers for the model components:
k-means (default)This applies a traditional k-means clustering algorithm.
This can be computationally expensive compared to other initialization methods.
k-means++This uses the initialization method of k-means clustering: k-means++.
This will pick the first center at random from the data. Subsequent centers will be
chosen from a weighted distribution of the data favouring points further away from
existing centers. k-means++ is the default initialization for k-means so will be
quicker than running a full k-means but can still take a significant amount of
time for large data sets with many components.
random_from_dataThis will pick random data points from the input data as the initial
centers. This is a very fast method of initialization but can produce non-convergent
results if the chosen points are too close to each other.
randomCenters are chosen as a small perturbation away from the mean of all data.
This method is simple but can lead to the model taking longer to converge.
Examples:
See GMM Initialization Methods for an example of
using different initializations in Gaussian Mixture.
- 2.1.2. Variational Bayesian Gaussian Mixture¶
The BayesianGaussianMixture object implements a variant of the
Gaussian mixture model with variational inference algorithms. The API is
similar to the one defined by GaussianMixture.
Estimation algorithm: variational inference
Variational inference is an extension of expectation-maximization that
maximizes a lower bound on model evidence (including
priors) instead of data likelihood. The principle behind
variational methods is the same as expectation-maximization (that is
both are iterative algorithms that alternate between finding the
probabilities for each point to be generated by each mixture and
fitting the mixture to these assigned points), but variational
methods add regularization by integrating information from prior
distributions. This avoids the singularities often found in
expectation-maximization solutions but introduces some subtle biases
to the model. Inference is often notably slower, but not usually as
much so as to render usage unpractical.
Due to its Bayesian nature, the variational algorithm needs more hyperparameters
than expectation-maximization, the most important of these being the
concentration parameter weight_concentration_prior. Specifying a low value
for the concentration prior will make the model put most of the weight on a few
components and set the remaining components’ weights very close to zero. High
values of the concentration prior will allow a larger number of components to
be active in the mixture.
The parameters implementation of the BayesianGaussianMixture class
proposes two types of prior for the weights distribution: a finite mixture model
with Dirichlet distribution and an infinite mixture model with the Dirichlet
Process. In practice Dirichlet Process inference algorithm is approximated and
uses a truncated distribution with a fixed maximum number of components (called
the Stick-breaking representation). The number of components actually used
almost always depends on the data.
The next figure compares the results obtained for the different type of the
weight concentration prior (parameter weight_concentration_prior_type)
for different values of weight_concentration_prior.
Here, we can see the value of the weight_concentration_prior parameter
has a strong impact on the effective number of active components obtained. We
can also notice that large values for the concentration weight prior lead to
more uniform weights when the type of prior is ‘dirichlet_distribution’ while
this is not necessarily the case for the ‘dirichlet_process’ type (used by
default).
The examples below compare Gaussian mixture models with a fixed number of
components, to the variational Gaussian mixture models with a Dirichlet process
prior. Here, a classical Gaussian mixture is fitted with 5 components on a
dataset composed of 2 clusters. We can see that the variational Gaussian mixture
with a Dirichlet process prior is able to limit itself to only 2 components
whereas the Gaussian mixture fits the data with a fixed number of components
that has to be set a priori by the user. In this case the user has selected
n_components=5 which does not match the true generative distribution of this
toy dataset. Note that with very little observations, the variational Gaussian
mixture models with a Dirichlet process prior can take a conservative stand, and
fit only one component.
On the following figure we are fitting a dataset not well-depicted by a
Gaussian mixture. Adjusting the weight_concentration_prior, parameter of the
BayesianGaussianMixture controls the number of components used to fit
this data. We also present on the last two plots a random sampling generated
from the two resulting mixtures.
Examples:
See Gaussian Mixture Model Ellipsoids for an example on
plotting the confidence ellipsoids for both GaussianMixture
and BayesianGaussianMixture.
Gaussian Mixture Model Sine Curve shows using
GaussianMixture and BayesianGaussianMixture to fit a
sine wave.
See Concentration Prior Type Analysis of Variation Bayesian Gaussian Mixture
for an example plotting the confidence ellipsoids for the
BayesianGaussianMixture with different
weight_concentration_prior_type for different values of the parameter
weight_concentration_prior.
Pros and cons of variational inference with BayesianGaussianMixture
Click for more details
¶
Pros:
Automatic selection:
when weight_concentration_prior is small enough and
n_components is larger than what is found necessary by the model, the
Variational Bayesian mixture model has a natural tendency to set some mixture
weights values close to zero. This makes it possible to let the model choose
a suitable number of effective components automatically. Only an upper bound
of this number needs to be provided. Note however that the “ideal” number of
active components is very application specific and is typically ill-defined
in a data exploration setting.
Less sensitivity to the number of parameters:
unlike finite models, which will
almost always use all components as much as they can, and hence will produce
wildly different solutions for different numbers of components, the
variational inference with a Dirichlet process prior
(weight_concentration_prior_type='dirichlet_process') won’t change much
with changes to the parameters, leading to more stability and less tuning.
Regularization:
due to the incorporation of prior information,
variational solutions have less pathological special cases than
expectation-maximization solutions.
Cons:
Speed:
the extra parametrization necessary for variational inference makes
inference slower, although not by much.
Hyperparameters:
this algorithm needs an extra hyperparameter
that might need experimental tuning via cross-validation.
Bias:
there are many implicit biases in the inference algorithms (and also in
the Dirichlet process if used), and whenever there is a mismatch between
these biases and the data it might be possible to fit better models using a
finite mixture.
2.1.2.1. The Dirichlet Process¶
Here we describe variational inference algorithms on Dirichlet process
mixture. The Dirichlet process is a prior probability distribution on
clusterings with an infinite, unbounded, number of partitions.
Variational techniques let us incorporate this prior structure on
Gaussian mixture models at almost no penalty in inference time, comparing
with a finite Gaussian mixture model.
An important question is how can the Dirichlet process use an infinite,
unbounded number of clusters and still be consistent. While a full explanation
doesn’t fit this manual, one can think of its stick breaking process
analogy to help understanding it. The stick breaking process is a generative
story for the Dirichlet process. We start with a unit-length stick and in each
step we break off a portion of the remaining stick. Each time, we associate the
length of the piece of the stick to the proportion of points that falls into a
group of the mixture. At the end, to represent the infinite mixture, we
associate the last remaining piece of the stick to the proportion of points
that don’t fall into all the other groups. The length of each piece is a random
variable with probability proportional to the concentration parameter. Smaller
values of the concentration will divide the unit-length into larger pieces of
the stick (defining more concentrated distribution). Larger concentration
values will create smaller pieces of the stick (increasing the number of
components with non zero weights).
Variational inference techniques for the Dirichlet process still work
with a finite approximation to this infinite mixture model, but
instead of having to specify a priori how many components one wants to
use, one just specifies the concentration parameter and an upper bound
on the number of mixture components (this upper bound, assuming it is
higher than the “true” number of components, affects only algorithmic
complexity, not the actual number of components used).
© 2007 - 2024, scikit-learn developers (BSD License).
Show this page source
### 2.2. Manifold learning — scikit-learn 1.4.2 documentation

Install
User Guide
API
Examples
Community
Getting Started
Tutorial
What's new
Glossary
Development
FAQ
Support
Related packages
Roadmap
Governance
About us
GitHub
Other Versions and Download
More
Getting Started
Tutorial
What's new
Glossary
Development
FAQ
Support
Related packages
Roadmap
Governance
About us
GitHub
Other Versions and Download
Toggle Menu
PrevUp
Next
scikit-learn 1.4.2
Other versions
Please cite us if you use the software.
### 2.2. Manifold learning

- 2.2.1. Introduction
- 2.2.2. Isomap
- 2.2.3. Locally Linear Embedding
- 2.2.4. Modified Locally Linear Embedding
- 2.2.5. Hessian Eigenmapping
- 2.2.6. Spectral Embedding
- 2.2.7. Local Tangent Space Alignment
- 2.2.8. Multi-dimensional Scaling (MDS)
- 2.2.9. t-distributed Stochastic Neighbor Embedding (t-SNE)
- 2.2.10. Tips on practical use
### 2.2. Manifold learning¶

Look for the bare necessities
The simple bare necessities
Forget about your worries and your strife
I mean the bare necessities
Old Mother Nature’s recipes
That bring the bare necessities of life
– Baloo’s song [The Jungle Book]
Manifold learning is an approach to non-linear dimensionality reduction.
Algorithms for this task are based on the idea that the dimensionality of
many data sets is only artificially high.
- 2.2.1. Introduction¶
High-dimensional datasets can be very difficult to visualize.  While data
in two or three dimensions can be plotted to show the inherent
structure of the data, equivalent high-dimensional plots are much less
intuitive.  To aid visualization of the structure of a dataset, the
dimension must be reduced in some way.
The simplest way to accomplish this dimensionality reduction is by taking
a random projection of the data.  Though this allows some degree of
visualization of the data structure, the randomness of the choice leaves much
to be desired.  In a random projection, it is likely that the more
interesting structure within the data will be lost.
To address this concern, a number of supervised and unsupervised linear
dimensionality reduction frameworks have been designed, such as Principal
Component Analysis (PCA), Independent Component Analysis, Linear
Discriminant Analysis, and others.  These algorithms define specific
rubrics to choose an “interesting” linear projection of the data.
These methods can be powerful, but often miss important non-linear
structure in the data.
Manifold Learning can be thought of as an attempt to generalize linear
frameworks like PCA to be sensitive to non-linear structure in data. Though
supervised variants exist, the typical manifold learning problem is
unsupervised: it learns the high-dimensional structure of the data
from the data itself, without the use of predetermined classifications.
Examples:
See Manifold learning on handwritten digits: Locally Linear Embedding, Isomap… for an example of
dimensionality reduction on handwritten digits.
See Comparison of Manifold Learning methods for an example of
dimensionality reduction on a toy “S-curve” dataset.
The manifold learning implementations available in scikit-learn are
summarized below
- 2.2.2. Isomap¶
One of the earliest approaches to manifold learning is the Isomap
algorithm, short for Isometric Mapping.  Isomap can be viewed as an
extension of Multi-dimensional Scaling (MDS) or Kernel PCA.
Isomap seeks a lower-dimensional embedding which maintains geodesic
distances between all points.  Isomap can be performed with the object
Isomap.
Complexity
Click for more details
¶
The Isomap algorithm comprises three stages:
Nearest neighbor search.  Isomap uses
BallTree for efficient neighbor search.
The cost is approximately \(O[D \log(k) N \log(N)]\), for \(k\)
nearest neighbors of \(N\) points in \(D\) dimensions.
Shortest-path graph search.  The most efficient known algorithms
for this are Dijkstra’s Algorithm, which is approximately
\(O[N^2(k + \log(N))]\), or the Floyd-Warshall algorithm, which
is \(O[N^3]\).  The algorithm can be selected by the user with
the path_method keyword of Isomap.  If unspecified, the code
attempts to choose the best algorithm for the input data.
Partial eigenvalue decomposition.  The embedding is encoded in the
eigenvectors corresponding to the \(d\) largest eigenvalues of the
\(N \times N\) isomap kernel.  For a dense solver, the cost is
approximately \(O[d N^2]\).  This cost can often be improved using
the ARPACK solver.  The eigensolver can be specified by the user
with the eigen_solver keyword of Isomap.  If unspecified, the
code attempts to choose the best algorithm for the input data.
The overall complexity of Isomap is
\(O[D \log(k) N \log(N)] + O[N^2(k + \log(N))] + O[d N^2]\).
\(N\) : number of training data points
\(D\) : input dimension
\(k\) : number of nearest neighbors
\(d\) : output dimension
References:
“A global geometric framework for nonlinear dimensionality reduction”
Tenenbaum, J.B.; De Silva, V.; & Langford, J.C.  Science 290 (5500)
- 2.2.3. Locally Linear Embedding¶
Locally linear embedding (LLE) seeks a lower-dimensional projection of the data
which preserves distances within local neighborhoods.  It can be thought
of as a series of local Principal Component Analyses which are globally
compared to find the best non-linear embedding.
Locally linear embedding can be performed with function
locally_linear_embedding or its object-oriented counterpart
LocallyLinearEmbedding.
Complexity
Click for more details
¶
The standard LLE algorithm comprises three stages:
Nearest Neighbors Search.  See discussion under Isomap above.
Weight Matrix Construction. \(O[D N k^3]\).
The construction of the LLE weight matrix involves the solution of a
\(k \times k\) linear equation for each of the \(N\) local
neighborhoods
Partial Eigenvalue Decomposition. See discussion under Isomap above.
The overall complexity of standard LLE is
\(O[D \log(k) N \log(N)] + O[D N k^3] + O[d N^2]\).
\(N\) : number of training data points
\(D\) : input dimension
\(k\) : number of nearest neighbors
\(d\) : output dimension
References:
“Nonlinear dimensionality reduction by locally linear embedding”
Roweis, S. & Saul, L.  Science 290:2323 (2000)
- 2.2.4. Modified Locally Linear Embedding¶
One well-known issue with LLE is the regularization problem.  When the number
of neighbors is greater than the number of input dimensions, the matrix
defining each local neighborhood is rank-deficient.  To address this, standard
LLE applies an arbitrary regularization parameter \(r\), which is chosen
relative to the trace of the local weight matrix.  Though it can be shown
formally that as \(r \to 0\), the solution converges to the desired
embedding, there is no guarantee that the optimal solution will be found
for \(r > 0\).  This problem manifests itself in embeddings which distort
the underlying geometry of the manifold.
One method to address the regularization problem is to use multiple weight
vectors in each neighborhood.  This is the essence of modified locally
linear embedding (MLLE).  MLLE can be  performed with function
locally_linear_embedding or its object-oriented counterpart
LocallyLinearEmbedding, with the keyword method = 'modified'.
It requires n_neighbors > n_components.
Complexity
Click for more details
¶
The MLLE algorithm comprises three stages:
Nearest Neighbors Search.  Same as standard LLE
Weight Matrix Construction. Approximately
\(O[D N k^3] + O[N (k-D) k^2]\).  The first term is exactly equivalent
to that of standard LLE.  The second term has to do with constructing the
weight matrix from multiple weights.  In practice, the added cost of
constructing the MLLE weight matrix is relatively small compared to the
cost of stages 1 and 3.
Partial Eigenvalue Decomposition. Same as standard LLE
The overall complexity of MLLE is
\(O[D \log(k) N \log(N)] + O[D N k^3] + O[N (k-D) k^2] + O[d N^2]\).
\(N\) : number of training data points
\(D\) : input dimension
\(k\) : number of nearest neighbors
\(d\) : output dimension
References:
“MLLE: Modified Locally Linear Embedding Using Multiple Weights”
Zhang, Z. & Wang, J.
- 2.2.5. Hessian Eigenmapping¶
Hessian Eigenmapping (also known as Hessian-based LLE: HLLE) is another method
of solving the regularization problem of LLE.  It revolves around a
hessian-based quadratic form at each neighborhood which is used to recover
the locally linear structure.  Though other implementations note its poor
scaling with data size, sklearn implements some algorithmic
improvements which make its cost comparable to that of other LLE variants
for small output dimension.  HLLE can be  performed with function
locally_linear_embedding or its object-oriented counterpart
LocallyLinearEmbedding, with the keyword method = 'hessian'.
It requires n_neighbors > n_components * (n_components + 3) / 2.
Complexity
Click for more details
¶
The HLLE algorithm comprises three stages:
Nearest Neighbors Search.  Same as standard LLE
Weight Matrix Construction. Approximately
\(O[D N k^3] + O[N d^6]\).  The first term reflects a similar
cost to that of standard LLE.  The second term comes from a QR
decomposition of the local hessian estimator.
Partial Eigenvalue Decomposition. Same as standard LLE
The overall complexity of standard HLLE is
\(O[D \log(k) N \log(N)] + O[D N k^3] + O[N d^6] + O[d N^2]\).
\(N\) : number of training data points
\(D\) : input dimension
\(k\) : number of nearest neighbors
\(d\) : output dimension
References:
“Hessian Eigenmaps: Locally linear embedding techniques for
high-dimensional data”
Donoho, D. & Grimes, C. Proc Natl Acad Sci USA. 100:5591 (2003)
- 2.2.6. Spectral Embedding¶
Spectral Embedding is an approach to calculating a non-linear embedding.
Scikit-learn implements Laplacian Eigenmaps, which finds a low dimensional
representation of the data using a spectral decomposition of the graph
Laplacian. The graph generated can be considered as a discrete approximation of
the low dimensional manifold in the high dimensional space. Minimization of a
cost function based on the graph ensures that points close to each other on
the manifold are mapped close to each other in the low dimensional space,
preserving local distances. Spectral embedding can be  performed with the
function spectral_embedding or its object-oriented counterpart
SpectralEmbedding.
Complexity
Click for more details
¶
The Spectral Embedding (Laplacian Eigenmaps) algorithm comprises three stages:
Weighted Graph Construction. Transform the raw input data into
graph representation using affinity (adjacency) matrix representation.
Graph Laplacian Construction. unnormalized Graph Laplacian
is constructed as \(L = D - A\) for and normalized one as
\(L = D^{-\frac{1}{2}} (D - A) D^{-\frac{1}{2}}\).
Partial Eigenvalue Decomposition. Eigenvalue decomposition is
done on graph Laplacian
The overall complexity of spectral embedding is
\(O[D \log(k) N \log(N)] + O[D N k^3] + O[d N^2]\).
\(N\) : number of training data points
\(D\) : input dimension
\(k\) : number of nearest neighbors
\(d\) : output dimension
References:
“Laplacian Eigenmaps for Dimensionality Reduction
and Data Representation”
M. Belkin, P. Niyogi, Neural Computation, June 2003; 15 (6):1373-1396
- 2.2.7. Local Tangent Space Alignment¶
Though not technically a variant of LLE, Local tangent space alignment (LTSA)
is algorithmically similar enough to LLE that it can be put in this category.
Rather than focusing on preserving neighborhood distances as in LLE, LTSA
seeks to characterize the local geometry at each neighborhood via its
tangent space, and performs a global optimization to align these local
tangent spaces to learn the embedding.  LTSA can be performed with function
locally_linear_embedding or its object-oriented counterpart
LocallyLinearEmbedding, with the keyword method = 'ltsa'.
Complexity
Click for more details
¶
The LTSA algorithm comprises three stages:
Nearest Neighbors Search.  Same as standard LLE
Weight Matrix Construction. Approximately
\(O[D N k^3] + O[k^2 d]\).  The first term reflects a similar
cost to that of standard LLE.
Partial Eigenvalue Decomposition. Same as standard LLE
The overall complexity of standard LTSA is
\(O[D \log(k) N \log(N)] + O[D N k^3] + O[k^2 d] + O[d N^2]\).
\(N\) : number of training data points
\(D\) : input dimension
\(k\) : number of nearest neighbors
\(d\) : output dimension
References:
“Principal manifolds and nonlinear dimensionality reduction via
tangent space alignment”
Zhang, Z. & Zha, H. Journal of Shanghai Univ. 8:406 (2004)
- 2.2.8. Multi-dimensional Scaling (MDS)¶
Multidimensional scaling
(MDS) seeks a low-dimensional
representation of the data in which the distances respect well the
distances in the original high-dimensional space.
In general, MDS is a technique used for analyzing similarity or
dissimilarity data. It attempts to model similarity or dissimilarity data as
distances in a geometric spaces. The data can be ratings of similarity between
objects, interaction frequencies of molecules, or trade indices between
countries.
There exists two types of MDS algorithm: metric and non metric. In
scikit-learn, the class MDS implements both. In Metric MDS, the input
similarity matrix arises from a metric (and thus respects the triangular
inequality), the distances between output two points are then set to be as
close as possible to the similarity or dissimilarity data. In the non-metric
version, the algorithms will try to preserve the order of the distances, and
hence seek for a monotonic relationship between the distances in the embedded
space and the similarities/dissimilarities.
Let \(S\) be the similarity matrix, and \(X\) the coordinates of the
\(n\) input points. Disparities \(\hat{d}_{ij}\) are transformation of
the similarities chosen in some optimal ways. The objective, called the
stress, is then defined by \(\sum_{i < j} d_{ij}(X) - \hat{d}_{ij}(X)\)
Metric MDS
Click for more details
¶
The simplest metric MDS model, called absolute MDS, disparities are defined by
\(\hat{d}_{ij} = S_{ij}\). With absolute MDS, the value \(S_{ij}\)
should then correspond exactly to the distance between point \(i\) and
\(j\) in the embedding point.
Most commonly, disparities are set to \(\hat{d}_{ij} = b S_{ij}\).
Nonmetric MDS
Click for more details
¶
Non metric MDS focuses on the ordination of the data. If
\(S_{ij} > S_{jk}\), then the embedding should enforce \(d_{ij} <
d_{jk}\). For this reason, we discuss it in terms of dissimilarities
(\(\delta_{ij}\)) instead of similarities (\(S_{ij}\)). Note that
dissimilarities can easily be obtained from similarities through a simple
transform, e.g. \(\delta_{ij}=c_1-c_2 S_{ij}\) for some real constants
\(c_1, c_2\). A simple algorithm to enforce proper ordination is to use a
monotonic regression of \(d_{ij}\) on \(\delta_{ij}\), yielding
disparities \(\hat{d}_{ij}\) in the same order as \(\delta_{ij}\).
A trivial solution to this problem is to set all the points on the origin. In
order to avoid that, the disparities \(\hat{d}_{ij}\) are normalized. Note
that since we only care about relative ordering, our objective should be
invariant to simple translation and scaling, however the stress used in metric
MDS is sensitive to scaling. To address this, non-metric MDS may use a
normalized stress, known as Stress-1 defined as
\[\sqrt{\frac{\sum_{i < j} (d_{ij} - \hat{d}_{ij})^2}{\sum_{i < j} d_{ij}^2}}.\]
The use of normalized Stress-1 can be enabled by setting normalized_stress=True,
however it is only compatible with the non-metric MDS problem and will be ignored
in the metric case.
References:
“Modern Multidimensional Scaling - Theory and Applications”
Borg, I.; Groenen P. Springer Series in Statistics (1997)
“Nonmetric multidimensional scaling: a numerical method”
Kruskal, J. Psychometrika, 29 (1964)
“Multidimensional scaling by optimizing goodness of fit to a nonmetric hypothesis”
Kruskal, J. Psychometrika, 29, (1964)
- 2.2.9. t-distributed Stochastic Neighbor Embedding (t-SNE)¶
t-SNE (TSNE) converts affinities of data points to probabilities.
The affinities in the original space are represented by Gaussian joint
probabilities and the affinities in the embedded space are represented by
Student’s t-distributions. This allows t-SNE to be particularly sensitive
to local structure and has a few other advantages over existing techniques:
Revealing the structure at many scales on a single map
Revealing data that lie in multiple, different, manifolds or clusters
Reducing the tendency to crowd points together at the center
While Isomap, LLE and variants are best suited to unfold a single continuous
low dimensional manifold, t-SNE will focus on the local structure of the data
and will tend to extract clustered local groups of samples as highlighted on
the S-curve example. This ability to group samples based on the local structure
might be beneficial to visually disentangle a dataset that comprises several
manifolds at once as is the case in the digits dataset.
The Kullback-Leibler (KL) divergence of the joint
probabilities in the original space and the embedded space will be minimized
by gradient descent. Note that the KL divergence is not convex, i.e.
multiple restarts with different initializations will end up in local minima
of the KL divergence. Hence, it is sometimes useful to try different seeds
and select the embedding with the lowest KL divergence.
The disadvantages to using t-SNE are roughly:
t-SNE is computationally expensive, and can take several hours on million-sample
datasets where PCA will finish in seconds or minutes
The Barnes-Hut t-SNE method is limited to two or three dimensional embeddings.
The algorithm is stochastic and multiple restarts with different seeds can
yield different embeddings. However, it is perfectly legitimate to pick the
embedding with the least error.
Global structure is not explicitly preserved. This problem is mitigated by
initializing points with PCA (using init='pca').
Optimizing t-SNE
Click for more details
¶
The main purpose of t-SNE is visualization of high-dimensional data. Hence,
it works best when the data will be embedded on two or three dimensions.
Optimizing the KL divergence can be a little bit tricky sometimes. There are
five parameters that control the optimization of t-SNE and therefore possibly
the quality of the resulting embedding:
perplexity
early exaggeration factor
learning rate
maximum number of iterations
angle (not used in the exact method)
The perplexity is defined as \(k=2^{(S)}\) where \(S\) is the Shannon
entropy of the conditional probability distribution. The perplexity of a
\(k\)-sided die is \(k\), so that \(k\) is effectively the number of
nearest neighbors t-SNE considers when generating the conditional probabilities.
Larger perplexities lead to more nearest neighbors and less sensitive to small
structure. Conversely a lower perplexity considers a smaller number of
neighbors, and thus ignores more global information in favour of the
local neighborhood. As dataset sizes get larger more points will be
required to get a reasonable sample of the local neighborhood, and hence
larger perplexities may be required. Similarly noisier datasets will require
larger perplexity values to encompass enough local neighbors to see beyond
the background noise.
The maximum number of iterations is usually high enough and does not need
any tuning. The optimization consists of two phases: the early exaggeration
phase and the final optimization. During early exaggeration the joint
probabilities in the original space will be artificially increased by
multiplication with a given factor. Larger factors result in larger gaps
between natural clusters in the data. If the factor is too high, the KL
divergence could increase during this phase. Usually it does not have to be
tuned. A critical parameter is the learning rate. If it is too low gradient
descent will get stuck in a bad local minimum. If it is too high the KL
divergence will increase during optimization. A heuristic suggested in
Belkina et al. (2019) is to set the learning rate to the sample size
divided by the early exaggeration factor. We implement this heuristic
as learning_rate='auto' argument. More tips can be found in
Laurens van der Maaten’s FAQ (see references). The last parameter, angle,
is a tradeoff between performance and accuracy. Larger angles imply that we
can approximate larger regions by a single point, leading to better speed
but less accurate results.
“How to Use t-SNE Effectively”
provides a good discussion of the effects of the various parameters, as well
as interactive plots to explore the effects of different parameters.
Barnes-Hut t-SNE
Click for more details
¶
The Barnes-Hut t-SNE that has been implemented here is usually much slower than
other manifold learning algorithms. The optimization is quite difficult
and the computation of the gradient is \(O[d N log(N)]\), where \(d\)
is the number of output dimensions and \(N\) is the number of samples. The
Barnes-Hut method improves on the exact method where t-SNE complexity is
\(O[d N^2]\), but has several other notable differences:
The Barnes-Hut implementation only works when the target dimensionality is 3
or less. The 2D case is typical when building visualizations.
Barnes-Hut only works with dense input data. Sparse data matrices can only be
embedded with the exact method or can be approximated by a dense low rank
projection for instance using PCA
Barnes-Hut is an approximation of the exact method. The approximation is
parameterized with the angle parameter, therefore the angle parameter is
unused when method=”exact”
Barnes-Hut is significantly more scalable. Barnes-Hut can be used to embed
hundred of thousands of data points while the exact method can handle
thousands of samples before becoming computationally intractable
For visualization purpose (which is the main use case of t-SNE), using the
Barnes-Hut method is strongly recommended. The exact t-SNE method is useful
for checking the theoretically properties of the embedding possibly in higher
dimensional space but limit to small datasets due to computational constraints.
Also note that the digits labels roughly match the natural grouping found by
t-SNE while the linear 2D projection of the PCA model yields a representation
where label regions largely overlap. This is a strong clue that this data can
be well separated by non linear methods that focus on the local structure (e.g.
an SVM with a Gaussian RBF kernel). However, failing to visualize well
separated homogeneously labeled groups with t-SNE in 2D does not necessarily
imply that the data cannot be correctly classified by a supervised model. It
might be the case that 2 dimensions are not high enough to accurately represent
the internal structure of the data.
References:
“Visualizing High-Dimensional Data Using t-SNE”
van der Maaten, L.J.P.; Hinton, G. Journal of Machine Learning Research
(2008)
“t-Distributed Stochastic Neighbor Embedding”
van der Maaten, L.J.P.
“Accelerating t-SNE using Tree-Based Algorithms”
van der Maaten, L.J.P.; Journal of Machine Learning Research 15(Oct):3221-3245, 2014.
“Automated optimized parameters for T-distributed stochastic neighbor
embedding improve visualization and analysis of large datasets”
Belkina, A.C., Ciccolella, C.O., Anno, R., Halpert, R., Spidlen, J.,
Snyder-Cappione, J.E., Nature Communications 10, 5415 (2019).
- 2.2.10. Tips on practical use¶
Make sure the same scale is used over all features. Because manifold
learning methods are based on a nearest-neighbor search, the algorithm
may perform poorly otherwise.  See StandardScaler
for convenient ways of scaling heterogeneous data.
The reconstruction error computed by each routine can be used to choose
the optimal output dimension.  For a \(d\)-dimensional manifold embedded
in a \(D\)-dimensional parameter space, the reconstruction error will
decrease as n_components is increased until n_components == d.
Note that noisy data can “short-circuit” the manifold, in essence acting
as a bridge between parts of the manifold that would otherwise be
well-separated.  Manifold learning on noisy and/or incomplete data is
an active area of research.
Certain input configurations can lead to singular weight matrices, for
example when more than two points in the dataset are identical, or when
the data is split into disjointed groups.  In this case, solver='arpack'
will fail to find the null space.  The easiest way to address this is to
use solver='dense' which will work on a singular matrix, though it may
be very slow depending on the number of input points.  Alternatively, one
can attempt to understand the source of the singularity: if it is due to
disjoint sets, increasing n_neighbors may help.  If it is due to
identical points in the dataset, removing these points may help.
See also
Totally Random Trees Embedding can also be useful to derive non-linear
representations of feature space, also it does not perform
dimensionality reduction.
### 2.3. Clustering — scikit-learn 1.4.2 documentation

Install
User Guide
API
Examples
Community
Getting Started
Tutorial
What's new
Glossary
Development
FAQ
Support
Related packages
Roadmap
Governance
About us
GitHub
Other Versions and Download
More
Getting Started
Tutorial
What's new
Glossary
Development
FAQ
Support
Related packages
Roadmap
Governance
About us
GitHub
Other Versions and Download
Toggle Menu
PrevUp
Next
scikit-learn 1.4.2
Other versions
Please cite us if you use the software.
### 2.3. Clustering

- 2.3.1. Overview of clustering methods
- 2.3.2. K-means
2.3.2.1. Low-level parallelism
2.3.2.2. Mini Batch K-Means
- 2.3.3. Affinity Propagation
- 2.3.4. Mean Shift
- 2.3.5. Spectral clustering
2.3.5.1. Different label assignment strategies
2.3.5.2. Spectral Clustering Graphs
- 2.3.6. Hierarchical clustering
2.3.6.1. Different linkage type: Ward, complete, average, and single linkage
2.3.6.2. Visualization of cluster hierarchy
2.3.6.3. Adding connectivity constraints
2.3.6.4. Varying the metric
2.3.6.5. Bisecting K-Means
- 2.3.7. DBSCAN
- 2.3.8. HDBSCAN
2.3.8.1. Mutual Reachability Graph
2.3.8.2. Hierarchical Clustering
- 2.3.9. OPTICS
- 2.3.10. BIRCH
- 2.3.11. Clustering performance evaluation
2.3.11.1. Rand index
2.3.11.1.1. Advantages
2.3.11.1.2. Drawbacks
2.3.11.1.3. Mathematical formulation
2.3.11.2. Mutual Information based scores
2.3.11.2.1. Advantages
2.3.11.2.2. Drawbacks
2.3.11.2.3. Mathematical formulation
2.3.11.3. Homogeneity, completeness and V-measure
2.3.11.3.1. Advantages
2.3.11.3.2. Drawbacks
2.3.11.3.3. Mathematical formulation
2.3.11.4. Fowlkes-Mallows scores
2.3.11.4.1. Advantages
2.3.11.4.2. Drawbacks
2.3.11.5. Silhouette Coefficient
2.3.11.5.1. Advantages
2.3.11.5.2. Drawbacks
2.3.11.6. Calinski-Harabasz Index
2.3.11.6.1. Advantages
2.3.11.6.2. Drawbacks
2.3.11.6.3. Mathematical formulation
2.3.11.7. Davies-Bouldin Index
2.3.11.7.1. Advantages
2.3.11.7.2. Drawbacks
2.3.11.7.3. Mathematical formulation
2.3.11.8. Contingency Matrix
2.3.11.8.1. Advantages
2.3.11.8.2. Drawbacks
2.3.11.9. Pair Confusion Matrix
### 2.3. Clustering¶

Clustering of
unlabeled data can be performed with the module sklearn.cluster.
Each clustering algorithm comes in two variants: a class, that implements
the fit method to learn the clusters on train data, and a function,
that, given train data, returns an array of integer labels corresponding
to the different clusters. For the class, the labels over the training
data can be found in the labels_ attribute.
Input data
One important thing to note is that the algorithms implemented in
this module can take different kinds of matrix as input. All the
methods accept standard data matrices of shape (n_samples, n_features).
These can be obtained from the classes in the sklearn.feature_extraction
module. For AffinityPropagation, SpectralClustering
and DBSCAN one can also input similarity matrices of shape
(n_samples, n_samples). These can be obtained from the functions
in the sklearn.metrics.pairwise module.
- 2.3.1. Overview of clustering methods¶
A comparison of the clustering algorithms in scikit-learn¶
Method name
Parameters
Scalability
Usecase
Geometry (metric used)
K-Means
number of clusters
Very large n_samples, medium n_clusters with
MiniBatch code
General-purpose, even cluster size, flat geometry,
not too many clusters, inductive
Distances between points
Affinity propagation
damping, sample preference
Not scalable with n_samples
Many clusters, uneven cluster size, non-flat geometry, inductive
Graph distance (e.g. nearest-neighbor graph)
Mean-shift
bandwidth
Not scalable with n_samples
Many clusters, uneven cluster size, non-flat geometry, inductive
Distances between points
Spectral clustering
number of clusters
Medium n_samples, small n_clusters
Few clusters, even cluster size, non-flat geometry, transductive
Graph distance (e.g. nearest-neighbor graph)
Ward hierarchical clustering
number of clusters or distance threshold
Large n_samples and n_clusters
Many clusters, possibly connectivity constraints, transductive
Distances between points
Agglomerative clustering
number of clusters or distance threshold, linkage type, distance
Large n_samples and n_clusters
Many clusters, possibly connectivity constraints, non Euclidean
distances, transductive
Any pairwise distance
DBSCAN
neighborhood size
Very large n_samples, medium n_clusters
Non-flat geometry, uneven cluster sizes, outlier removal,
transductive
Distances between nearest points
HDBSCAN
minimum cluster membership, minimum point neighbors
large n_samples, medium n_clusters
Non-flat geometry, uneven cluster sizes, outlier removal,
transductive, hierarchical, variable cluster density
Distances between nearest points
OPTICS
minimum cluster membership
Very large n_samples, large n_clusters
Non-flat geometry, uneven cluster sizes, variable cluster density,
outlier removal, transductive
Distances between points
Gaussian mixtures
many
Not scalable
Flat geometry, good for density estimation, inductive
Mahalanobis distances to  centers
BIRCH
branching factor, threshold, optional global clusterer.
Large n_clusters and n_samples
Large dataset, outlier removal, data reduction, inductive
Euclidean distance between points
Bisecting K-Means
number of clusters
Very large n_samples, medium n_clusters
General-purpose, even cluster size, flat geometry,
no empty clusters, inductive, hierarchical
Distances between points
Non-flat geometry clustering is useful when the clusters have a specific
shape, i.e. a non-flat manifold, and the standard euclidean distance is
not the right metric. This case arises in the two top rows of the figure
above.
Gaussian mixture models, useful for clustering, are described in
another chapter of the documentation dedicated to
mixture models. KMeans can be seen as a special case of Gaussian mixture
model with equal covariance per component.
Transductive clustering methods (in contrast to
inductive clustering methods) are not designed to be applied to new,
unseen data.
- 2.3.2. K-means¶
The KMeans algorithm clusters data by trying to separate samples in n
groups of equal variance, minimizing a criterion known as the inertia or
within-cluster sum-of-squares (see below). This algorithm requires the number
of clusters to be specified. It scales well to large numbers of samples and has
been used across a large range of application areas in many different fields.
The k-means algorithm divides a set of \(N\) samples \(X\) into
\(K\) disjoint clusters \(C\), each described by the mean \(\mu_j\)
of the samples in the cluster. The means are commonly called the cluster
“centroids”; note that they are not, in general, points from \(X\),
although they live in the same space.
The K-means algorithm aims to choose centroids that minimise the inertia,
or within-cluster sum-of-squares criterion:
\[\sum_{i=0}^{n}\min_{\mu_j \in C}(||x_i - \mu_j||^2)\]
Inertia can be recognized as a measure of how internally coherent clusters are.
It suffers from various drawbacks:
Inertia makes the assumption that clusters are convex and isotropic,
which is not always the case. It responds poorly to elongated clusters,
or manifolds with irregular shapes.
Inertia is not a normalized metric: we just know that lower values are
better and zero is optimal. But in very high-dimensional spaces, Euclidean
distances tend to become inflated
(this is an instance of the so-called “curse of dimensionality”).
Running a dimensionality reduction algorithm such as Principal component analysis (PCA) prior to
k-means clustering can alleviate this problem and speed up the
computations.
For more detailed descriptions of the issues shown above and how to address them,
refer to the examples Demonstration of k-means assumptions
and Selecting the number of clusters with silhouette analysis on KMeans clustering.
K-means is often referred to as Lloyd’s algorithm. In basic terms, the
algorithm has three steps. The first step chooses the initial centroids, with
the most basic method being to choose \(k\) samples from the dataset
\(X\). After initialization, K-means consists of looping between the
two other steps. The first step assigns each sample to its nearest centroid.
The second step creates new centroids by taking the mean value of all of the
samples assigned to each previous centroid. The difference between the old
and the new centroids are computed and the algorithm repeats these last two
steps until this value is less than a threshold. In other words, it repeats
until the centroids do not move significantly.
K-means is equivalent to the expectation-maximization algorithm
with a small, all-equal, diagonal covariance matrix.
The algorithm can also be understood through the concept of Voronoi diagrams. First the Voronoi diagram of
the points is calculated using the current centroids. Each segment in the
Voronoi diagram becomes a separate cluster. Secondly, the centroids are updated
to the mean of each segment. The algorithm then repeats this until a stopping
criterion is fulfilled. Usually, the algorithm stops when the relative decrease
in the objective function between iterations is less than the given tolerance
value. This is not the case in this implementation: iteration stops when
centroids move less than the tolerance.
Given enough time, K-means will always converge, however this may be to a local
minimum. This is highly dependent on the initialization of the centroids.
As a result, the computation is often done several times, with different
initializations of the centroids. One method to help address this issue is the
k-means++ initialization scheme, which has been implemented in scikit-learn
(use the init='k-means++' parameter). This initializes the centroids to be
(generally) distant from each other, leading to probably better results than
random initialization, as shown in the reference. For a detailed example of
comaparing different initialization schemes, refer to
A demo of K-Means clustering on the handwritten digits data.
K-means++ can also be called independently to select seeds for other
clustering algorithms, see sklearn.cluster.kmeans_plusplus for details
and example usage.
The algorithm supports sample weights, which can be given by a parameter
sample_weight. This allows to assign more weight to some samples when
computing cluster centers and values of inertia. For example, assigning a
weight of 2 to a sample is equivalent to adding a duplicate of that sample
to the dataset \(X\).
K-means can be used for vector quantization. This is achieved using the
transform method of a trained model of KMeans. For an example of
performing vector quantization on an image refer to
Color Quantization using K-Means.
Examples:
K-means Clustering: Example usage of
KMeans using the iris dataset
Clustering text documents using k-means: Document clustering
using KMeans and MiniBatchKMeans based on sparse data
2.3.2.1. Low-level parallelism¶
KMeans benefits from OpenMP based parallelism through Cython. Small
chunks of data (256 samples) are processed in parallel, which in addition
yields a low memory footprint. For more details on how to control the number of
threads, please refer to our Parallelism notes.
Examples:
Demonstration of k-means assumptions: Demonstrating when
k-means performs intuitively and when it does not
A demo of K-Means clustering on the handwritten digits data: Clustering handwritten digits
References:
“k-means++: The advantages of careful seeding”
Arthur, David, and Sergei Vassilvitskii,
Proceedings of the eighteenth annual ACM-SIAM symposium on Discrete
algorithms, Society for Industrial and Applied Mathematics (2007)
2.3.2.2. Mini Batch K-Means¶
The MiniBatchKMeans is a variant of the KMeans algorithm
which uses mini-batches to reduce the computation time, while still attempting
to optimise the same objective function. Mini-batches are subsets of the input
data, randomly sampled in each training iteration. These mini-batches
drastically reduce the amount of computation required to converge to a local
solution. In contrast to other algorithms that reduce the convergence time of
k-means, mini-batch k-means produces results that are generally only slightly
worse than the standard algorithm.
The algorithm iterates between two major steps, similar to vanilla k-means.
In the first step, \(b\) samples are drawn randomly from the dataset, to form
a mini-batch. These are then assigned to the nearest centroid. In the second
step, the centroids are updated. In contrast to k-means, this is done on a
per-sample basis. For each sample in the mini-batch, the assigned centroid
is updated by taking the streaming average of the sample and all previous
samples assigned to that centroid. This has the effect of decreasing the
rate of change for a centroid over time. These steps are performed until
convergence or a predetermined number of iterations is reached.
MiniBatchKMeans converges faster than KMeans, but the quality
of the results is reduced. In practice this difference in quality can be quite
small, as shown in the example and cited reference.
Examples:
Comparison of the K-Means and MiniBatchKMeans clustering algorithms: Comparison of
KMeans and MiniBatchKMeans
Clustering text documents using k-means: Document clustering
using KMeans and MiniBatchKMeans based on sparse data
Online learning of a dictionary of parts of faces
References:
“Web Scale K-Means clustering”
D. Sculley, Proceedings of the 19th international conference on World
wide web (2010)
- 2.3.3. Affinity Propagation¶
AffinityPropagation creates clusters by sending messages between
pairs of samples until convergence. A dataset is then described using a small
number of exemplars, which are identified as those most representative of other
samples. The messages sent between pairs represent the suitability for one
sample to be the exemplar of the other, which is updated in response to the
values from other pairs. This updating happens iteratively until convergence,
at which point the final exemplars are chosen, and hence the final clustering
is given.
Affinity Propagation can be interesting as it chooses the number of
clusters based on the data provided. For this purpose, the two important
parameters are the preference, which controls how many exemplars are
used, and the damping factor which damps the responsibility and
availability messages to avoid numerical oscillations when updating these
messages.
The main drawback of Affinity Propagation is its complexity. The
algorithm has a time complexity of the order \(O(N^2 T)\), where \(N\)
is the number of samples and \(T\) is the number of iterations until
convergence. Further, the memory complexity is of the order
\(O(N^2)\) if a dense similarity matrix is used, but reducible if a
sparse similarity matrix is used. This makes Affinity Propagation most
appropriate for small to medium sized datasets.
Examples:
Demo of affinity propagation clustering algorithm: Affinity
Propagation on a synthetic 2D datasets with 3 classes.
Visualizing the stock market structure Affinity Propagation on
Financial time series to find groups of companies
Algorithm description:
The messages sent between points belong to one of two categories. The first is
the responsibility \(r(i, k)\),
which is the accumulated evidence that sample \(k\)
should be the exemplar for sample \(i\).
The second is the availability \(a(i, k)\)
which is the accumulated evidence that sample \(i\)
should choose sample \(k\) to be its exemplar,
and considers the values for all other samples that \(k\) should
be an exemplar. In this way, exemplars are chosen by samples if they are (1)
similar enough to many samples and (2) chosen by many samples to be
representative of themselves.
More formally, the responsibility of a sample \(k\)
to be the exemplar of sample \(i\) is given by:
\[r(i, k) \leftarrow s(i, k) - max [ a(i, k') + s(i, k') \forall k' \neq k ]\]
Where \(s(i, k)\) is the similarity between samples \(i\) and \(k\).
The availability of sample \(k\)
to be the exemplar of sample \(i\) is given by:
\[a(i, k) \leftarrow min [0, r(k, k) + \sum_{i'~s.t.~i' \notin \{i, k\}}{r(i', k)}]\]
To begin with, all values for \(r\) and \(a\) are set to zero,
and the calculation of each iterates until convergence.
As discussed above, in order to avoid numerical oscillations when updating the
messages, the damping factor \(\lambda\) is introduced to iteration process:
\[r_{t+1}(i, k) = \lambda\cdot r_{t}(i, k) + (1-\lambda)\cdot r_{t+1}(i, k)\]
\[a_{t+1}(i, k) = \lambda\cdot a_{t}(i, k) + (1-\lambda)\cdot a_{t+1}(i, k)\]
where \(t\) indicates the iteration times.
- 2.3.4. Mean Shift¶
MeanShift clustering aims to discover blobs in a smooth density of
samples. It is a centroid based algorithm, which works by updating candidates
for centroids to be the mean of the points within a given region. These
candidates are then filtered in a post-processing stage to eliminate
near-duplicates to form the final set of centroids.
The position of centroid candidates is iteratively adjusted using a technique called hill
climbing, which finds local maxima of the estimated probability density.
Given a candidate centroid \(x\) for iteration \(t\), the candidate
is updated according to the following equation:
\[x^{t+1} = x^t + m(x^t)\]
Where \(m\) is the mean shift vector that is computed for each
centroid that points towards a region of the maximum increase in the density of points.
To compute \(m\) we define \(N(x)\) as the neighborhood of samples within
a given distance around \(x\). Then \(m\) is computed using the following
equation, effectively updating a centroid to be the mean of the samples within
its neighborhood:
\[m(x) = \frac{1}{|N(x)|} \sum_{x_j \in N(x)}x_j - x\]
In general, the equation for \(m\) depends on a kernel used for density estimation.
The generic formula is:
\[m(x) = \frac{\sum_{x_j \in N(x)}K(x_j - x)x_j}{\sum_{x_j \in N(x)}K(x_j - x)} - x\]
In our implementation, \(K(x)\) is equal to 1 if \(x\) is small enough and is
equal to 0 otherwise. Effectively \(K(y - x)\) indicates whether \(y\) is in
the neighborhood of \(x\).
The algorithm automatically sets the number of clusters, instead of relying on a
parameter bandwidth, which dictates the size of the region to search through.
This parameter can be set manually, but can be estimated using the provided
estimate_bandwidth function, which is called if the bandwidth is not set.
The algorithm is not highly scalable, as it requires multiple nearest neighbor
searches during the execution of the algorithm. The algorithm is guaranteed to
converge, however the algorithm will stop iterating when the change in centroids
is small.
Labelling a new sample is performed by finding the nearest centroid for a
given sample.
Examples:
A demo of the mean-shift clustering algorithm: Mean Shift clustering
on a synthetic 2D datasets with 3 classes.
References:
“Mean shift: A robust approach toward feature space analysis”
D. Comaniciu and P. Meer, IEEE Transactions on Pattern Analysis and Machine Intelligence (2002)
- 2.3.5. Spectral clustering¶
SpectralClustering performs a low-dimension embedding of the
affinity matrix between samples, followed by clustering, e.g., by KMeans,
of the components of the eigenvectors in the low dimensional space.
It is especially computationally efficient if the affinity matrix is sparse
and the amg solver is used for the eigenvalue problem (Note, the amg solver
requires that the pyamg module is installed.)
The present version of SpectralClustering requires the number of clusters
to be specified in advance. It works well for a small number of clusters,
but is not advised for many clusters.
For two clusters, SpectralClustering solves a convex relaxation of the
normalized cuts
problem on the similarity graph: cutting the graph in two so that the weight of
the edges cut is small compared to the weights of the edges inside each
cluster. This criteria is especially interesting when working on images, where
graph vertices are pixels, and weights of the edges of the similarity graph are
computed using a function of a gradient of the image.
Warning
Transforming distance to well-behaved similarities
Note that if the values of your similarity matrix are not well
distributed, e.g. with negative values or with a distance matrix
rather than a similarity, the spectral problem will be singular and
the problem not solvable. In which case it is advised to apply a
transformation to the entries of the matrix. For instance, in the
case of a signed distance matrix, is common to apply a heat kernel:
similarity = np.exp(-beta * distance / distance.std())
See the examples for such an application.
Examples:
Spectral clustering for image segmentation: Segmenting objects
from a noisy background using spectral clustering.
Segmenting the picture of greek coins in regions: Spectral clustering
to split the image of coins in regions.
2.3.5.1. Different label assignment strategies¶
Different label assignment strategies can be used, corresponding to the
assign_labels parameter of SpectralClustering.
"kmeans" strategy can match finer details, but can be unstable.
In particular, unless you control the random_state, it may not be
reproducible from run-to-run, as it depends on random initialization.
The alternative "discretize" strategy is 100% reproducible, but tends
to create parcels of fairly even and geometrical shape.
The recently added "cluster_qr" option is a deterministic alternative that
tends to create the visually best partitioning on the example application
below.
assign_labels="kmeans"
assign_labels="discretize"
assign_labels="cluster_qr"
References:
“Multiclass spectral clustering”
Stella X. Yu, Jianbo Shi, 2003
“Simple, direct, and efficient multi-way spectral clustering”
Anil Damle, Victor Minden, Lexing Ying, 2019
2.3.5.2. Spectral Clustering Graphs¶
Spectral Clustering can also be used to partition graphs via their spectral
embeddings.  In this case, the affinity matrix is the adjacency matrix of the
graph, and SpectralClustering is initialized with affinity='precomputed':
>>> from sklearn.cluster import SpectralClustering
>>> sc = SpectralClustering(3, affinity='precomputed', n_init=100,
...                         assign_labels='discretize')
>>> sc.fit_predict(adjacency_matrix)
References:
“A Tutorial on Spectral Clustering”
Ulrike von Luxburg, 2007
“Normalized cuts and image segmentation”
Jianbo Shi, Jitendra Malik, 2000
“A Random Walks View of Spectral Segmentation”
Marina Meila, Jianbo Shi, 2001
“On Spectral Clustering: Analysis and an algorithm”
Andrew Y. Ng, Michael I. Jordan, Yair Weiss, 2001
“Preconditioned Spectral Clustering for Stochastic
Block Partition Streaming Graph Challenge”
David Zhuzhunashvili, Andrew Knyazev
- 2.3.6. Hierarchical clustering¶
Hierarchical clustering is a general family of clustering algorithms that
build nested clusters by merging or splitting them successively. This
hierarchy of clusters is represented as a tree (or dendrogram). The root of the
tree is the unique cluster that gathers all the samples, the leaves being the
clusters with only one sample. See the Wikipedia page for more details.
The AgglomerativeClustering object performs a hierarchical clustering
using a bottom up approach: each observation starts in its own cluster, and
clusters are successively merged together. The linkage criteria determines the
metric used for the merge strategy:
Ward minimizes the sum of squared differences within all clusters. It is a
variance-minimizing approach and in this sense is similar to the k-means
objective function but tackled with an agglomerative hierarchical
approach.
Maximum or complete linkage minimizes the maximum distance between
observations of pairs of clusters.
Average linkage minimizes the average of the distances between all
observations of pairs of clusters.
Single linkage minimizes the distance between the closest
observations of pairs of clusters.
AgglomerativeClustering can also scale to large number of samples
when it is used jointly with a connectivity matrix, but is computationally
expensive when no connectivity constraints are added between samples: it
considers at each step all the possible merges.
FeatureAgglomeration
The FeatureAgglomeration uses agglomerative clustering to
group together features that look very similar, thus decreasing the
number of features. It is a dimensionality reduction tool, see
Unsupervised dimensionality reduction.
2.3.6.1. Different linkage type: Ward, complete, average, and single linkage¶
AgglomerativeClustering supports Ward, single, average, and complete
linkage strategies.
Agglomerative cluster has a “rich get richer” behavior that leads to
uneven cluster sizes. In this regard, single linkage is the worst
strategy, and Ward gives the most regular sizes. However, the affinity
(or distance used in clustering) cannot be varied with Ward, thus for non
Euclidean metrics, average linkage is a good alternative. Single linkage,
while not robust to noisy data, can be computed very efficiently and can
therefore be useful to provide hierarchical clustering of larger datasets.
Single linkage can also perform well on non-globular data.
Examples:
Various Agglomerative Clustering on a 2D embedding of digits: exploration of the
different linkage strategies in a real dataset.
2.3.6.2. Visualization of cluster hierarchy¶
It’s possible to visualize the tree representing the hierarchical merging of clusters
as a dendrogram. Visual inspection can often be useful for understanding the structure
of the data, though more so in the case of small sample sizes.
2.3.6.3. Adding connectivity constraints¶
An interesting aspect of AgglomerativeClustering is that
connectivity constraints can be added to this algorithm (only adjacent
clusters can be merged together), through a connectivity matrix that defines
for each sample the neighboring samples following a given structure of the
data. For instance, in the swiss-roll example below, the connectivity
constraints forbid the merging of points that are not adjacent on the swiss
roll, and thus avoid forming clusters that extend across overlapping folds of
the roll.
These constraint are useful to impose a certain local structure, but they
also make the algorithm faster, especially when the number of the samples
is high.
The connectivity constraints are imposed via an connectivity matrix: a
scipy sparse matrix that has elements only at the intersection of a row
and a column with indices of the dataset that should be connected. This
matrix can be constructed from a-priori information: for instance, you
may wish to cluster web pages by only merging pages with a link pointing
from one to another. It can also be learned from the data, for instance
using sklearn.neighbors.kneighbors_graph to restrict
merging to nearest neighbors as in this example, or
using sklearn.feature_extraction.image.grid_to_graph to
enable only merging of neighboring pixels on an image, as in the
coin example.
Examples:
A demo of structured Ward hierarchical clustering on an image of coins: Ward clustering
to split the image of coins in regions.
Hierarchical clustering: structured vs unstructured ward: Example of
Ward algorithm on a swiss-roll, comparison of structured approaches
versus unstructured approaches.
Feature agglomeration vs. univariate selection:
Example of dimensionality reduction with feature agglomeration based on
Ward hierarchical clustering.
Agglomerative clustering with and without structure
Warning
Connectivity constraints with single, average and complete linkage
Connectivity constraints and single, complete or average linkage can enhance
the ‘rich getting richer’ aspect of agglomerative clustering,
particularly so if they are built with
sklearn.neighbors.kneighbors_graph. In the limit of a small
number of clusters, they tend to give a few macroscopically occupied
clusters and almost empty ones. (see the discussion in
Agglomerative clustering with and without structure).
Single linkage is the most brittle linkage option with regard to this issue.
2.3.6.4. Varying the metric¶
Single, average and complete linkage can be used with a variety of distances (or
affinities), in particular Euclidean distance (l2), Manhattan distance
(or Cityblock, or l1), cosine distance, or any precomputed affinity
matrix.
l1 distance is often good for sparse features, or sparse noise: i.e.
many of the features are zero, as in text mining using occurrences of
rare words.
cosine distance is interesting because it is invariant to global
scalings of the signal.
The guidelines for choosing a metric is to use one that maximizes the
distance between samples in different classes, and minimizes that within
each class.
Examples:
Agglomerative clustering with different metrics
2.3.6.5. Bisecting K-Means¶
The BisectingKMeans is an iterative variant of KMeans, using
divisive hierarchical clustering. Instead of creating all centroids at once, centroids
are picked progressively based on a previous clustering: a cluster is split into two
new clusters repeatedly until the target number of clusters is reached.
BisectingKMeans is more efficient than KMeans when the number of
clusters is large since it only works on a subset of the data at each bisection
while KMeans always works on the entire dataset.
Although BisectingKMeans can’t benefit from the advantages of the "k-means++"
initialization by design, it will still produce comparable results than
KMeans(init="k-means++") in terms of inertia at cheaper computational costs, and will
likely produce better results than KMeans with a random initialization.
This variant is more efficient to agglomerative clustering if the number of clusters is
small compared to the number of data points.
This variant also does not produce empty clusters.
There exist two strategies for selecting the cluster to split:
bisecting_strategy="largest_cluster" selects the cluster having the most points
bisecting_strategy="biggest_inertia" selects the cluster with biggest inertia
(cluster with biggest Sum of Squared Errors within)
Picking by largest amount of data points in most cases produces result as
accurate as picking by inertia and is faster (especially for larger amount of data
points, where calculating error may be costly).
Picking by largest amount of data points will also likely produce clusters of similar
sizes while KMeans is known to produce clusters of different sizes.
Difference between Bisecting K-Means and regular K-Means can be seen on example
Bisecting K-Means and Regular K-Means Performance Comparison.
While the regular K-Means algorithm tends to create non-related clusters,
clusters from Bisecting K-Means are well ordered and create quite a visible hierarchy.
References:
“A Comparison of Document Clustering Techniques”
Michael Steinbach, George Karypis and Vipin Kumar,
Department of Computer Science and Egineering, University of Minnesota
(June 2000)
“Performance Analysis of K-Means and Bisecting K-Means Algorithms in Weblog Data”
K.Abirami and Dr.P.Mayilvahanan,
International Journal of Emerging Technologies in Engineering Research (IJETER)
Volume 4, Issue 8, (August 2016)
“Bisecting K-means Algorithm Based on K-valued Self-determining
and Clustering Center Optimization”
Jian Di, Xinyue Gou
School of Control and Computer Engineering,North China Electric Power University,
Baoding, Hebei, China (August 2017)
- 2.3.7. DBSCAN¶
The DBSCAN algorithm views clusters as areas of high density
separated by areas of low density. Due to this rather generic view, clusters
found by DBSCAN can be any shape, as opposed to k-means which assumes that
clusters are convex shaped. The central component to the DBSCAN is the concept
of core samples, which are samples that are in areas of high density. A
cluster is therefore a set of core samples, each close to each other
(measured by some distance measure)
and a set of non-core samples that are close to a core sample (but are not
themselves core samples). There are two parameters to the algorithm,
min_samples and eps,
which define formally what we mean when we say dense.
Higher min_samples or lower eps
indicate higher density necessary to form a cluster.
More formally, we define a core sample as being a sample in the dataset such
that there exist min_samples other samples within a distance of
eps, which are defined as neighbors of the core sample. This tells
us that the core sample is in a dense area of the vector space. A cluster
is a set of core samples that can be built by recursively taking a core
sample, finding all of its neighbors that are core samples, finding all of
their neighbors that are core samples, and so on. A cluster also has a
set of non-core samples, which are samples that are neighbors of a core sample
in the cluster but are not themselves core samples. Intuitively, these samples
are on the fringes of a cluster.
Any core sample is part of a cluster, by definition. Any sample that is not a
core sample, and is at least eps in distance from any core sample, is
considered an outlier by the algorithm.
While the parameter min_samples primarily controls how tolerant the
algorithm is towards noise (on noisy and large data sets it may be desirable
to increase this parameter), the parameter eps is crucial to choose
appropriately for the data set and distance function and usually cannot be
left at the default value. It controls the local neighborhood of the points.
When chosen too small, most data will not be clustered at all (and labeled
as -1 for “noise”). When chosen too large, it causes close clusters to
be merged into one cluster, and eventually the entire data set to be returned
as a single cluster. Some heuristics for choosing this parameter have been
discussed in the literature, for example based on a knee in the nearest neighbor
distances plot (as discussed in the references below).
In the figure below, the color indicates cluster membership, with large circles
indicating core samples found by the algorithm. Smaller circles are non-core
samples that are still part of a cluster. Moreover, the outliers are indicated
by black points below.
Examples:
Demo of DBSCAN clustering algorithm
Implementation
The DBSCAN algorithm is deterministic, always generating the same clusters
when given the same data in the same order.  However, the results can differ when
data is provided in a different order. First, even though the core samples
will always be assigned to the same clusters, the labels of those clusters
will depend on the order in which those samples are encountered in the data.
Second and more importantly, the clusters to which non-core samples are assigned
can differ depending on the data order.  This would happen when a non-core sample
has a distance lower than eps to two core samples in different clusters. By the
triangular inequality, those two core samples must be more distant than
eps from each other, or they would be in the same cluster. The non-core
sample is assigned to whichever cluster is generated first in a pass
through the data, and so the results will depend on the data ordering.
The current implementation uses ball trees and kd-trees
to determine the neighborhood of points,
which avoids calculating the full distance matrix
(as was done in scikit-learn versions before 0.14).
The possibility to use custom metrics is retained;
for details, see NearestNeighbors.
Memory consumption for large sample sizes
This implementation is by default not memory efficient because it constructs
a full pairwise similarity matrix in the case where kd-trees or ball-trees cannot
be used (e.g., with sparse matrices). This matrix will consume \(n^2\) floats.
A couple of mechanisms for getting around this are:
Use OPTICS clustering in conjunction with the
extract_dbscan method. OPTICS clustering also calculates the full
pairwise matrix, but only keeps one row in memory at a time (memory
complexity n).
A sparse radius neighborhood graph (where missing entries are presumed to
be out of eps) can be precomputed in a memory-efficient way and dbscan
can be run over this with metric='precomputed'.  See
sklearn.neighbors.NearestNeighbors.radius_neighbors_graph.
The dataset can be compressed, either by removing exact duplicates if
these occur in your data, or by using BIRCH. Then you only have a
relatively small number of representatives for a large number of points.
You can then provide a sample_weight when fitting DBSCAN.
References:
“A Density-Based Algorithm for Discovering Clusters in Large Spatial Databases
with Noise”
Ester, M., H. P. Kriegel, J. Sander, and X. Xu,
In Proceedings of the 2nd International Conference on Knowledge Discovery
and Data Mining, Portland, OR, AAAI Press, pp. 226–231. 1996
“DBSCAN revisited, revisited: why and how you should (still) use DBSCAN.”
Schubert, E., Sander, J., Ester, M., Kriegel, H. P., & Xu, X. (2017).
In ACM Transactions on Database Systems (TODS), 42(3), 19.
- 2.3.8. HDBSCAN¶
The HDBSCAN algorithm can be seen as an extension of DBSCAN
and OPTICS. Specifically, DBSCAN assumes that the clustering
criterion (i.e. density requirement) is globally homogeneous.
In other words, DBSCAN may struggle to successfully capture clusters
with different densities.
HDBSCAN alleviates this assumption and explores all possible density
scales by building an alternative representation of the clustering problem.
Note
This implementation is adapted from the original implementation of HDBSCAN,
scikit-learn-contrib/hdbscan based on [LJ2017].
2.3.8.1. Mutual Reachability Graph¶
HDBSCAN first defines \(d_c(x_p)\), the core distance of a sample \(x_p\), as the
distance to its min_samples th-nearest neighbor, counting itself. For example,
if min_samples=5 and \(x_*\) is the 5th-nearest neighbor of \(x_p\)
then the core distance is:
\[d_c(x_p)=d(x_p, x_*).\]
Next it defines \(d_m(x_p, x_q)\), the mutual reachability distance of two points
\(x_p, x_q\), as:
\[d_m(x_p, x_q) = \max\{d_c(x_p), d_c(x_q), d(x_p, x_q)\}\]
These two notions allow us to construct the mutual reachability graph
\(G_{ms}\) defined for a fixed choice of min_samples by associating each
sample \(x_p\) with a vertex of the graph, and thus edges between points
\(x_p, x_q\) are the mutual reachability distance \(d_m(x_p, x_q)\)
between them. We may build subsets of this graph, denoted as
\(G_{ms,\varepsilon}\), by removing any edges with value greater than \(\varepsilon\):
from the original graph. Any points whose core distance is less than \(\varepsilon\):
are at this staged marked as noise. The remaining points are then clustered by
finding the connected components of this trimmed graph.
Note
Taking the connected components of a trimmed graph \(G_{ms,\varepsilon}\) is
equivalent to running DBSCAN* with min_samples and \(\varepsilon\). DBSCAN* is a
slightly modified version of DBSCAN mentioned in [CM2013].
2.3.8.2. Hierarchical Clustering¶
HDBSCAN can be seen as an algorithm which performs DBSCAN* clustering across all
values of \(\varepsilon\). As mentioned prior, this is equivalent to finding the connected
components of the mutual reachability graphs for all values of \(\varepsilon\). To do this
efficiently, HDBSCAN first extracts a minimum spanning tree (MST) from the fully
-connected mutual reachability graph, then greedily cuts the edges with highest
weight. An outline of the HDBSCAN algorithm is as follows:
Extract the MST of \(G_{ms}\).
Extend the MST by adding a “self edge” for each vertex, with weight equal
to the core distance of the underlying sample.
Initialize a single cluster and label for the MST.
Remove the edge with the greatest weight from the MST (ties are
removed simultaneously).
Assign cluster labels to the connected components which contain the
end points of the now-removed edge. If the component does not have at least
one edge it is instead assigned a “null” label marking it as noise.
Repeat 4-5 until there are no more connected components.
HDBSCAN is therefore able to obtain all possible partitions achievable by
DBSCAN* for a fixed choice of min_samples in a hierarchical fashion.
Indeed, this allows HDBSCAN to perform clustering across multiple densities
and as such it no longer needs \(\varepsilon\) to be given as a hyperparameter. Instead
it relies solely on the choice of min_samples, which tends to be a more robust
hyperparameter.
HDBSCAN can be smoothed with an additional hyperparameter min_cluster_size
which specifies that during the hierarchical clustering, components with fewer
than minimum_cluster_size many samples are considered noise. In practice, one
can set minimum_cluster_size = min_samples to couple the parameters and
simplify the hyperparameter space.
References:
[CM2013]
Campello, R.J.G.B., Moulavi, D., Sander, J. (2013). Density-Based Clustering
Based on Hierarchical Density Estimates. In: Pei, J., Tseng, V.S., Cao, L.,
Motoda, H., Xu, G. (eds) Advances in Knowledge Discovery and Data Mining.
PAKDD 2013. Lecture Notes in Computer Science(), vol 7819. Springer, Berlin,
Heidelberg.
Density-Based Clustering Based on Hierarchical Density Estimates
[LJ2017]
L. McInnes and J. Healy, (2017). Accelerated Hierarchical Density Based
Clustering. In: IEEE International Conference on Data Mining Workshops (ICDMW),
2017, pp. 33-42.
Accelerated Hierarchical Density Based Clustering
- 2.3.9. OPTICS¶
The OPTICS algorithm shares many similarities with the DBSCAN
algorithm, and can be considered a generalization of DBSCAN that relaxes the
eps requirement from a single value to a value range. The key difference
between DBSCAN and OPTICS is that the OPTICS algorithm builds a reachability
graph, which assigns each sample both a reachability_ distance, and a spot
within the cluster ordering_ attribute; these two attributes are assigned
when the model is fitted, and are used to determine cluster membership. If
OPTICS is run with the default value of inf set for max_eps, then DBSCAN
style cluster extraction can be performed repeatedly in linear time for any
given eps value using the cluster_optics_dbscan method. Setting
max_eps to a lower value will result in shorter run times, and can be
thought of as the maximum neighborhood radius from each point to find other
potential reachable points.
The reachability distances generated by OPTICS allow for variable density
extraction of clusters within a single data set. As shown in the above plot,
combining reachability distances and data set ordering_ produces a
reachability plot, where point density is represented on the Y-axis, and
points are ordered such that nearby points are adjacent. ‘Cutting’ the
reachability plot at a single value produces DBSCAN like results; all points
above the ‘cut’ are classified as noise, and each time that there is a break
when reading from left to right signifies a new cluster. The default cluster
extraction with OPTICS looks at the steep slopes within the graph to find
clusters, and the user can define what counts as a steep slope using the
parameter xi. There are also other possibilities for analysis on the graph
itself, such as generating hierarchical representations of the data through
reachability-plot dendrograms, and the hierarchy of clusters detected by the
algorithm can be accessed through the cluster_hierarchy_ parameter. The
plot above has been color-coded so that cluster colors in planar space match
the linear segment clusters of the reachability plot. Note that the blue and
red clusters are adjacent in the reachability plot, and can be hierarchically
represented as children of a larger parent cluster.
Examples:
Demo of OPTICS clustering algorithm
Comparison with DBSCAN
The results from OPTICS cluster_optics_dbscan method and DBSCAN are
very similar, but not always identical; specifically, labeling of periphery
and noise points. This is in part because the first samples of each dense
area processed by OPTICS have a large reachability value while being close
to other points in their area, and will thus sometimes be marked as noise
rather than periphery. This affects adjacent points when they are
considered as candidates for being marked as either periphery or noise.
Note that for any single value of eps, DBSCAN will tend to have a
shorter run time than OPTICS; however, for repeated runs at varying eps
values, a single run of OPTICS may require less cumulative runtime than
DBSCAN. It is also important to note that OPTICS’ output is close to
DBSCAN’s only if eps and max_eps are close.
Computational Complexity
Spatial indexing trees are used to avoid calculating the full distance
matrix, and allow for efficient memory usage on large sets of samples.
Different distance metrics can be supplied via the metric keyword.
For large datasets, similar (but not identical) results can be obtained via
HDBSCAN. The HDBSCAN implementation is
multithreaded, and has better algorithmic runtime complexity than OPTICS,
at the cost of worse memory scaling. For extremely large datasets that
exhaust system memory using HDBSCAN, OPTICS will maintain \(n\) (as opposed
to \(n^2\)) memory scaling; however, tuning of the max_eps parameter
will likely need to be used to give a solution in a reasonable amount of
wall time.
References:
“OPTICS: ordering points to identify the clustering structure.”
Ankerst, Mihael, Markus M. Breunig, Hans-Peter Kriegel, and Jörg Sander.
In ACM Sigmod Record, vol. 28, no. 2, pp. 49-60. ACM, 1999.
- 2.3.10. BIRCH¶
The Birch builds a tree called the Clustering Feature Tree (CFT)
for the given data. The data is essentially lossy compressed to a set of
Clustering Feature nodes (CF Nodes). The CF Nodes have a number of
subclusters called Clustering Feature subclusters (CF Subclusters)
and these CF Subclusters located in the non-terminal CF Nodes
can have CF Nodes as children.
The CF Subclusters hold the necessary information for clustering which prevents
the need to hold the entire input data in memory. This information includes:
Number of samples in a subcluster.
Linear Sum - An n-dimensional vector holding the sum of all samples
Squared Sum - Sum of the squared L2 norm of all samples.
Centroids - To avoid recalculation linear sum / n_samples.
Squared norm of the centroids.
The BIRCH algorithm has two parameters, the threshold and the branching factor.
The branching factor limits the number of subclusters in a node and the
threshold limits the distance between the entering sample and the existing
subclusters.
This algorithm can be viewed as an instance or data reduction method,
since it reduces the input data to a set of subclusters which are obtained directly
from the leaves of the CFT. This reduced data can be further processed by feeding
it into a global clusterer. This global clusterer can be set by n_clusters.
If n_clusters is set to None, the subclusters from the leaves are directly
read off, otherwise a global clustering step labels these subclusters into global
clusters (labels) and the samples are mapped to the global label of the nearest subcluster.
Algorithm description:
A new sample is inserted into the root of the CF Tree which is a CF Node.
It is then merged with the subcluster of the root, that has the smallest
radius after merging, constrained by the threshold and branching factor conditions.
If the subcluster has any child node, then this is done repeatedly till it reaches
a leaf. After finding the nearest subcluster in the leaf, the properties of this
subcluster and the parent subclusters are recursively updated.
If the radius of the subcluster obtained by merging the new sample and the
nearest subcluster is greater than the square of the threshold and if the
number of subclusters is greater than the branching factor, then a space is temporarily
allocated to this new sample. The two farthest subclusters are taken and
the subclusters are divided into two groups on the basis of the distance
between these subclusters.
If this split node has a parent subcluster and there is room
for a new subcluster, then the parent is split into two. If there is no room,
then this node is again split into two and the process is continued
recursively, till it reaches the root.
BIRCH or MiniBatchKMeans?
BIRCH does not scale very well to high dimensional data. As a rule of thumb if
n_features is greater than twenty, it is generally better to use MiniBatchKMeans.
If the number of instances of data needs to be reduced, or if one wants a
large number of subclusters either as a preprocessing step or otherwise,
BIRCH is more useful than MiniBatchKMeans.
How to use partial_fit?
To avoid the computation of global clustering, for every call of partial_fit
the user is advised
To set n_clusters=None initially
Train all data by multiple calls to partial_fit.
Set n_clusters to a required value using
brc.set_params(n_clusters=n_clusters).
Call partial_fit finally with no arguments, i.e. brc.partial_fit()
which performs the global clustering.
References:
Tian Zhang, Raghu Ramakrishnan, Maron Livny
BIRCH: An efficient data clustering method for large databases.
https://www.cs.sfu.ca/CourseCentral/459/han/papers/zhang96.pdf
Roberto Perdisci
JBirch - Java implementation of BIRCH clustering algorithm
https://code.google.com/archive/p/jbirch
- 2.3.11. Clustering performance evaluation¶
Evaluating the performance of a clustering algorithm is not as trivial as
counting the number of errors or the precision and recall of a supervised
classification algorithm. In particular any evaluation metric should not
take the absolute values of the cluster labels into account but rather
if this clustering define separations of the data similar to some ground
truth set of classes or satisfying some assumption such that members
belong to the same class are more similar than members of different
classes according to some similarity metric.
2.3.11.1. Rand index¶
Given the knowledge of the ground truth class assignments
labels_true and our clustering algorithm assignments of the same
samples labels_pred, the (adjusted or unadjusted) Rand index
is a function that measures the similarity of the two assignments,
ignoring permutations:
>>> from sklearn import metrics
>>> labels_true = [0, 0, 0, 1, 1, 1]
>>> labels_pred = [0, 0, 1, 1, 2, 2]
>>> metrics.rand_score(labels_true, labels_pred)
0.66...
The Rand index does not ensure to obtain a value close to 0.0 for a
random labelling. The adjusted Rand index corrects for chance and
will give such a baseline.
>>> metrics.adjusted_rand_score(labels_true, labels_pred)
0.24...
As with all clustering metrics, one can permute 0 and 1 in the predicted
labels, rename 2 to 3, and get the same score:
>>> labels_pred = [1, 1, 0, 0, 3, 3]
>>> metrics.rand_score(labels_true, labels_pred)
0.66...
>>> metrics.adjusted_rand_score(labels_true, labels_pred)
0.24...
Furthermore, both rand_score adjusted_rand_score are
symmetric: swapping the argument does not change the scores. They can
thus be used as consensus measures:
>>> metrics.rand_score(labels_pred, labels_true)
0.66...
>>> metrics.adjusted_rand_score(labels_pred, labels_true)
0.24...
Perfect labeling is scored 1.0:
>>> labels_pred = labels_true[:]
>>> metrics.rand_score(labels_true, labels_pred)
1.0
>>> metrics.adjusted_rand_score(labels_true, labels_pred)
1.0
Poorly agreeing labels (e.g. independent labelings) have lower scores,
and for the adjusted Rand index the score will be negative or close to
zero. However, for the unadjusted Rand index the score, while lower,
will not necessarily be close to zero.:
>>> labels_true = [0, 0, 0, 0, 0, 0, 1, 1]
>>> labels_pred = [0, 1, 2, 3, 4, 5, 5, 6]
>>> metrics.rand_score(labels_true, labels_pred)
0.39...
>>> metrics.adjusted_rand_score(labels_true, labels_pred)
-0.07...
2.3.11.1.1. Advantages¶
Interpretability: The unadjusted Rand index is proportional
to the number of sample pairs whose labels are the same in both
labels_pred and labels_true, or are different in both.
Random (uniform) label assignments have an adjusted Rand index
score close to 0.0 for any value of n_clusters and
n_samples (which is not the case for the unadjusted Rand index
or the V-measure for instance).
Bounded range: Lower values indicate different labelings,
similar clusterings have a high (adjusted or unadjusted) Rand index,
1.0 is the perfect match score. The score range is [0, 1] for the
unadjusted Rand index and [-1, 1] for the adjusted Rand index.
No assumption is made on the cluster structure: The (adjusted or
unadjusted) Rand index can be used to compare all kinds of
clustering algorithms, and can be used to compare clustering
algorithms such as k-means which assumes isotropic blob shapes with
results of spectral clustering algorithms which can find cluster
with “folded” shapes.
2.3.11.1.2. Drawbacks¶
Contrary to inertia, the (adjusted or unadjusted) Rand index
requires knowledge of the ground truth classes which is almost
never available in practice or requires manual assignment by human
annotators (as in the supervised learning setting).
However (adjusted or unadjusted) Rand index can also be useful in a
purely unsupervised setting as a building block for a Consensus
Index that can be used for clustering model selection (TODO).
The unadjusted Rand index is often close to 1.0 even if the
clusterings themselves differ significantly. This can be understood
when interpreting the Rand index as the accuracy of element pair
labeling resulting from the clusterings: In practice there often is
a majority of element pairs that are assigned the different pair
label under both the predicted and the ground truth clustering
resulting in a high proportion of pair labels that agree, which
leads subsequently to a high score.
Examples:
Adjustment for chance in clustering performance evaluation:
Analysis of the impact of the dataset size on the value of
clustering measures for random assignments.
2.3.11.1.3. Mathematical formulation¶
If C is a ground truth class assignment and K the clustering, let us
define \(a\) and \(b\) as:
\(a\), the number of pairs of elements that are in the same set
in C and in the same set in K
\(b\), the number of pairs of elements that are in different sets
in C and in different sets in K
The unadjusted Rand index is then given by:
\[\text{RI} = \frac{a + b}{C_2^{n_{samples}}}\]
where \(C_2^{n_{samples}}\) is the total number of possible pairs
in the dataset. It does not matter if the calculation is performed on
ordered pairs or unordered pairs as long as the calculation is
performed consistently.
However, the Rand index does not guarantee that random label assignments
will get a value close to zero (esp. if the number of clusters is in
the same order of magnitude as the number of samples).
To counter this effect we can discount the expected RI \(E[\text{RI}]\) of
random labelings by defining the adjusted Rand index as follows:
\[\text{ARI} = \frac{\text{RI} - E[\text{RI}]}{\max(\text{RI}) - E[\text{RI}]}\]
References
Comparing Partitions
L. Hubert and P. Arabie, Journal of Classification 1985
Properties of the Hubert-Arabie adjusted Rand index
D. Steinley, Psychological Methods 2004
Wikipedia entry for the Rand index
Wikipedia entry for the adjusted Rand index
2.3.11.2. Mutual Information based scores¶
Given the knowledge of the ground truth class assignments labels_true and
our clustering algorithm assignments of the same samples labels_pred, the
Mutual Information is a function that measures the agreement of the two
assignments, ignoring permutations.  Two different normalized versions of this
measure are available, Normalized Mutual Information (NMI) and Adjusted
Mutual Information (AMI). NMI is often used in the literature, while AMI was
proposed more recently and is normalized against chance:
>>> from sklearn import metrics
>>> labels_true = [0, 0, 0, 1, 1, 1]
>>> labels_pred = [0, 0, 1, 1, 2, 2]
>>> metrics.adjusted_mutual_info_score(labels_true, labels_pred)
0.22504...
One can permute 0 and 1 in the predicted labels, rename 2 to 3 and get
the same score:
>>> labels_pred = [1, 1, 0, 0, 3, 3]
>>> metrics.adjusted_mutual_info_score(labels_true, labels_pred)
0.22504...
All, mutual_info_score, adjusted_mutual_info_score and
normalized_mutual_info_score are symmetric: swapping the argument does
not change the score. Thus they can be used as a consensus measure:
>>> metrics.adjusted_mutual_info_score(labels_pred, labels_true)
0.22504...
Perfect labeling is scored 1.0:
>>> labels_pred = labels_true[:]
>>> metrics.adjusted_mutual_info_score(labels_true, labels_pred)
1.0
>>> metrics.normalized_mutual_info_score(labels_true, labels_pred)
1.0
This is not true for mutual_info_score, which is therefore harder to judge:
>>> metrics.mutual_info_score(labels_true, labels_pred)
0.69...
Bad (e.g. independent labelings) have non-positive scores:
>>> labels_true = [0, 1, 2, 0, 3, 4, 5, 1]
>>> labels_pred = [1, 1, 0, 0, 2, 2, 2, 2]
>>> metrics.adjusted_mutual_info_score(labels_true, labels_pred)
-0.10526...
2.3.11.2.1. Advantages¶
Random (uniform) label assignments have a AMI score close to 0.0
for any value of n_clusters and n_samples (which is not the
case for raw Mutual Information or the V-measure for instance).
Upper bound  of 1:  Values close to zero indicate two label
assignments that are largely independent, while values close to one
indicate significant agreement. Further, an AMI of exactly 1 indicates
that the two label assignments are equal (with or without permutation).
2.3.11.2.2. Drawbacks¶
Contrary to inertia, MI-based measures require the knowledge
of the ground truth classes while almost never available in practice or
requires manual assignment by human annotators (as in the supervised learning
setting).
However MI-based measures can also be useful in purely unsupervised setting as a
building block for a Consensus Index that can be used for clustering
model selection.
NMI and MI are not adjusted against chance.
Examples:
Adjustment for chance in clustering performance evaluation: Analysis of
the impact of the dataset size on the value of clustering measures
for random assignments. This example also includes the Adjusted Rand
Index.
2.3.11.2.3. Mathematical formulation¶
Assume two label assignments (of the same N objects), \(U\) and \(V\).
Their entropy is the amount of uncertainty for a partition set, defined by:
\[H(U) = - \sum_{i=1}^{|U|}P(i)\log(P(i))\]
where \(P(i) = |U_i| / N\) is the probability that an object picked at
random from \(U\) falls into class \(U_i\). Likewise for \(V\):
\[H(V) = - \sum_{j=1}^{|V|}P'(j)\log(P'(j))\]
With \(P'(j) = |V_j| / N\). The mutual information (MI) between \(U\)
and \(V\) is calculated by:
\[\text{MI}(U, V) = \sum_{i=1}^{|U|}\sum_{j=1}^{|V|}P(i, j)\log\left(\frac{P(i,j)}{P(i)P'(j)}\right)\]
where \(P(i, j) = |U_i \cap V_j| / N\) is the probability that an object
picked at random falls into both classes \(U_i\) and \(V_j\).
It also can be expressed in set cardinality formulation:
\[\text{MI}(U, V) = \sum_{i=1}^{|U|} \sum_{j=1}^{|V|} \frac{|U_i \cap V_j|}{N}\log\left(\frac{N|U_i \cap V_j|}{|U_i||V_j|}\right)\]
The normalized mutual information is defined as
\[\text{NMI}(U, V) = \frac{\text{MI}(U, V)}{\text{mean}(H(U), H(V))}\]
This value of the mutual information and also the normalized variant is not
adjusted for chance and will tend to increase as the number of different labels
(clusters) increases, regardless of the actual amount of “mutual information”
between the label assignments.
The expected value for the mutual information can be calculated using the
following equation [VEB2009]. In this equation,
\(a_i = |U_i|\) (the number of elements in \(U_i\)) and
\(b_j = |V_j|\) (the number of elements in \(V_j\)).
\[E[\text{MI}(U,V)]=\sum_{i=1}^{|U|} \sum_{j=1}^{|V|} \sum_{n_{ij}=(a_i+b_j-N)^+
}^{\min(a_i, b_j)} \frac{n_{ij}}{N}\log \left( \frac{ N.n_{ij}}{a_i b_j}\right)
\frac{a_i!b_j!(N-a_i)!(N-b_j)!}{N!n_{ij}!(a_i-n_{ij})!(b_j-n_{ij})!
(N-a_i-b_j+n_{ij})!}\]
Using the expected value, the adjusted mutual information can then be
calculated using a similar form to that of the adjusted Rand index:
\[\text{AMI} = \frac{\text{MI} - E[\text{MI}]}{\text{mean}(H(U), H(V)) - E[\text{MI}]}\]
For normalized mutual information and adjusted mutual information, the normalizing
value is typically some generalized mean of the entropies of each clustering.
Various generalized means exist, and no firm rules exist for preferring one over the
others.  The decision is largely a field-by-field basis; for instance, in community
detection, the arithmetic mean is most common. Each
normalizing method provides “qualitatively similar behaviours” [YAT2016]. In our
implementation, this is controlled by the average_method parameter.
Vinh et al. (2010) named variants of NMI and AMI by their averaging method [VEB2010]. Their
‘sqrt’ and ‘sum’ averages are the geometric and arithmetic means; we use these
more broadly common names.
References
Strehl, Alexander, and Joydeep Ghosh (2002). “Cluster ensembles – a
knowledge reuse framework for combining multiple partitions”. Journal of
Machine Learning Research 3: 583–617.
doi:10.1162/153244303321897735.
Wikipedia entry for the (normalized) Mutual Information
Wikipedia entry for the Adjusted Mutual Information
[VEB2009]
Vinh, Epps, and Bailey, (2009). “Information theoretic measures
for clusterings comparison”. Proceedings of the 26th Annual International
Conference on Machine Learning - ICML ‘09.
doi:10.1145/1553374.1553511.
ISBN 9781605585161.
[VEB2010]
Vinh, Epps, and Bailey, (2010). “Information Theoretic Measures for
Clusterings Comparison: Variants, Properties, Normalization and
Correction for Chance”. JMLR
<https://jmlr.csail.mit.edu/papers/volume11/vinh10a/vinh10a.pdf>
[YAT2016]
Yang, Algesheimer, and Tessone, (2016). “A comparative analysis of
community
detection algorithms on artificial networks”. Scientific Reports 6: 30750.
doi:10.1038/srep30750.
2.3.11.3. Homogeneity, completeness and V-measure¶
Given the knowledge of the ground truth class assignments of the samples,
it is possible to define some intuitive metric using conditional entropy
analysis.
In particular Rosenberg and Hirschberg (2007) define the following two
desirable objectives for any cluster assignment:
homogeneity: each cluster contains only members of a single class.
completeness: all members of a given class are assigned to the same
cluster.
We can turn those concept as scores homogeneity_score and
completeness_score. Both are bounded below by 0.0 and above by
1.0 (higher is better):
>>> from sklearn import metrics
>>> labels_true = [0, 0, 0, 1, 1, 1]
>>> labels_pred = [0, 0, 1, 1, 2, 2]
>>> metrics.homogeneity_score(labels_true, labels_pred)
0.66...
>>> metrics.completeness_score(labels_true, labels_pred)
0.42...
Their harmonic mean called V-measure is computed by
v_measure_score:
>>> metrics.v_measure_score(labels_true, labels_pred)
0.51...
This function’s formula is as follows:
\[v = \frac{(1 + \beta) \times \text{homogeneity} \times \text{completeness}}{(\beta \times \text{homogeneity} + \text{completeness})}\]
beta defaults to a value of 1.0, but for using a value less than 1 for beta:
>>> metrics.v_measure_score(labels_true, labels_pred, beta=0.6)
0.54...
more weight will be attributed to homogeneity, and using a value greater than 1:
>>> metrics.v_measure_score(labels_true, labels_pred, beta=1.8)
0.48...
more weight will be attributed to completeness.
The V-measure is actually equivalent to the mutual information (NMI)
discussed above, with the aggregation function being the arithmetic mean [B2011].
Homogeneity, completeness and V-measure can be computed at once using
homogeneity_completeness_v_measure as follows:
>>> metrics.homogeneity_completeness_v_measure(labels_true, labels_pred)
(0.66..., 0.42..., 0.51...)
The following clustering assignment is slightly better, since it is
homogeneous but not complete:
>>> labels_pred = [0, 0, 0, 1, 2, 2]
>>> metrics.homogeneity_completeness_v_measure(labels_true, labels_pred)
(1.0, 0.68..., 0.81...)
Note
v_measure_score is symmetric: it can be used to evaluate
the agreement of two independent assignments on the same dataset.
This is not the case for completeness_score and
homogeneity_score: both are bound by the relationship:
homogeneity_score(a, b) == completeness_score(b, a)
2.3.11.3.1. Advantages¶
Bounded scores: 0.0 is as bad as it can be, 1.0 is a perfect score.
Intuitive interpretation: clustering with bad V-measure can be
qualitatively analyzed in terms of homogeneity and completeness
to better feel what ‘kind’ of mistakes is done by the assignment.
No assumption is made on the cluster structure: can be used
to compare clustering algorithms such as k-means which assumes isotropic
blob shapes with results of spectral clustering algorithms which can
find cluster with “folded” shapes.
2.3.11.3.2. Drawbacks¶
The previously introduced metrics are not normalized with regards to
random labeling: this means that depending on the number of samples,
clusters and ground truth classes, a completely random labeling will
not always yield the same values for homogeneity, completeness and
hence v-measure. In particular random labeling won’t yield zero
scores especially when the number of clusters is large.
This problem can safely be ignored when the number of samples is more
than a thousand and the number of clusters is less than 10. For
smaller sample sizes or larger number of clusters it is safer to use
an adjusted index such as the Adjusted Rand Index (ARI).
These metrics require the knowledge of the ground truth classes while
almost never available in practice or requires manual assignment by
human annotators (as in the supervised learning setting).
Examples:
Adjustment for chance in clustering performance evaluation: Analysis of
the impact of the dataset size on the value of clustering measures
for random assignments.
2.3.11.3.3. Mathematical formulation¶
Homogeneity and completeness scores are formally given by:
\[h = 1 - \frac{H(C|K)}{H(C)}\]
\[c = 1 - \frac{H(K|C)}{H(K)}\]
where \(H(C|K)\) is the conditional entropy of the classes given
the cluster assignments and is given by:
\[H(C|K) = - \sum_{c=1}^{|C|} \sum_{k=1}^{|K|} \frac{n_{c,k}}{n}
\cdot \log\left(\frac{n_{c,k}}{n_k}\right)\]
and \(H(C)\) is the entropy of the classes and is given by:
\[H(C) = - \sum_{c=1}^{|C|} \frac{n_c}{n} \cdot \log\left(\frac{n_c}{n}\right)\]
with \(n\) the total number of samples, \(n_c\) and \(n_k\)
the number of samples respectively belonging to class \(c\) and
cluster \(k\), and finally \(n_{c,k}\) the number of samples
from class \(c\) assigned to cluster \(k\).
The conditional entropy of clusters given class \(H(K|C)\) and the
entropy of clusters \(H(K)\) are defined in a symmetric manner.
Rosenberg and Hirschberg further define V-measure as the harmonic
mean of homogeneity and completeness:
\[v = 2 \cdot \frac{h \cdot c}{h + c}\]
References
V-Measure: A conditional entropy-based external cluster evaluation
measure
Andrew Rosenberg and Julia Hirschberg, 2007
[B2011]
Identification and Characterization of Events in Social Media, Hila
Becker, PhD Thesis.
2.3.11.4. Fowlkes-Mallows scores¶
The Fowlkes-Mallows index (sklearn.metrics.fowlkes_mallows_score) can be
used when the ground truth class assignments of the samples is known. The
Fowlkes-Mallows score FMI is defined as the geometric mean of the
pairwise precision and recall:
\[\text{FMI} = \frac{\text{TP}}{\sqrt{(\text{TP} + \text{FP}) (\text{TP} + \text{FN})}}\]
Where TP is the number of True Positive (i.e. the number of pair
of points that belong to the same clusters in both the true labels and the
predicted labels), FP is the number of False Positive (i.e. the number
of pair of points that belong to the same clusters in the true labels and not
in the predicted labels) and FN is the number of False Negative (i.e. the
number of pair of points that belongs in the same clusters in the predicted
labels and not in the true labels).
The score ranges from 0 to 1. A high value indicates a good similarity
between two clusters.
>>> from sklearn import metrics
>>> labels_true = [0, 0, 0, 1, 1, 1]
>>> labels_pred = [0, 0, 1, 1, 2, 2]
>>> metrics.fowlkes_mallows_score(labels_true, labels_pred)
0.47140...
One can permute 0 and 1 in the predicted labels, rename 2 to 3 and get
the same score:
>>> labels_pred = [1, 1, 0, 0, 3, 3]
>>> metrics.fowlkes_mallows_score(labels_true, labels_pred)
0.47140...
Perfect labeling is scored 1.0:
>>> labels_pred = labels_true[:]
>>> metrics.fowlkes_mallows_score(labels_true, labels_pred)
1.0
Bad (e.g. independent labelings) have zero scores:
>>> labels_true = [0, 1, 2, 0, 3, 4, 5, 1]
>>> labels_pred = [1, 1, 0, 0, 2, 2, 2, 2]
>>> metrics.fowlkes_mallows_score(labels_true, labels_pred)
0.0
2.3.11.4.1. Advantages¶
Random (uniform) label assignments have a FMI score close to 0.0
for any value of n_clusters and n_samples (which is not the
case for raw Mutual Information or the V-measure for instance).
Upper-bounded at 1:  Values close to zero indicate two label
assignments that are largely independent, while values close to one
indicate significant agreement. Further, values of exactly 0 indicate
purely independent label assignments and a FMI of exactly 1 indicates
that the two label assignments are equal (with or without permutation).
No assumption is made on the cluster structure: can be used
to compare clustering algorithms such as k-means which assumes isotropic
blob shapes with results of spectral clustering algorithms which can
find cluster with “folded” shapes.
2.3.11.4.2. Drawbacks¶
Contrary to inertia, FMI-based measures require the knowledge
of the ground truth classes while almost never available in practice or
requires manual assignment by human annotators (as in the supervised learning
setting).
References
E. B. Fowkles and C. L. Mallows, 1983. “A method for comparing two
hierarchical clusterings”. Journal of the American Statistical Association.
https://www.tandfonline.com/doi/abs/10.1080/01621459.1983.10478008
Wikipedia entry for the Fowlkes-Mallows Index
2.3.11.5. Silhouette Coefficient¶
If the ground truth labels are not known, evaluation must be performed using
the model itself. The Silhouette Coefficient
(sklearn.metrics.silhouette_score)
is an example of such an evaluation, where a
higher Silhouette Coefficient score relates to a model with better defined
clusters. The Silhouette Coefficient is defined for each sample and is composed
of two scores:
a: The mean distance between a sample and all other points in the same
class.
b: The mean distance between a sample and all other points in the next
nearest cluster.
The Silhouette Coefficient s for a single sample is then given as:
\[s = \frac{b - a}{max(a, b)}\]
The Silhouette Coefficient for a set of samples is given as the mean of the
Silhouette Coefficient for each sample.
>>> from sklearn import metrics
>>> from sklearn.metrics import pairwise_distances
>>> from sklearn import datasets
>>> X, y = datasets.load_iris(return_X_y=True)
In normal usage, the Silhouette Coefficient is applied to the results of a
cluster analysis.
>>> import numpy as np
>>> from sklearn.cluster import KMeans
>>> kmeans_model = KMeans(n_clusters=3, random_state=1).fit(X)
>>> labels = kmeans_model.labels_
>>> metrics.silhouette_score(X, labels, metric='euclidean')
0.55...
References
Peter J. Rousseeuw (1987). “Silhouettes: a Graphical Aid to the
Interpretation and Validation of Cluster Analysis”
. Computational and Applied Mathematics 20: 53–65.
2.3.11.5.1. Advantages¶
The score is bounded between -1 for incorrect clustering and +1 for highly
dense clustering. Scores around zero indicate overlapping clusters.
The score is higher when clusters are dense and well separated, which relates
to a standard concept of a cluster.
2.3.11.5.2. Drawbacks¶
The Silhouette Coefficient is generally higher for convex clusters than other
concepts of clusters, such as density based clusters like those obtained
through DBSCAN.
Examples:
Selecting the number of clusters with silhouette analysis on KMeans clustering : In this example
the silhouette analysis is used to choose an optimal value for n_clusters.
2.3.11.6. Calinski-Harabasz Index¶
If the ground truth labels are not known, the Calinski-Harabasz index
(sklearn.metrics.calinski_harabasz_score) - also known as the Variance
Ratio Criterion - can be used to evaluate the model, where a higher
Calinski-Harabasz score relates to a model with better defined clusters.
The index is the ratio of the sum of between-clusters dispersion and of
within-cluster dispersion for all clusters (where dispersion is defined as the
sum of distances squared):
>>> from sklearn import metrics
>>> from sklearn.metrics import pairwise_distances
>>> from sklearn import datasets
>>> X, y = datasets.load_iris(return_X_y=True)
In normal usage, the Calinski-Harabasz index is applied to the results of a
cluster analysis:
>>> import numpy as np
>>> from sklearn.cluster import KMeans
>>> kmeans_model = KMeans(n_clusters=3, random_state=1).fit(X)
>>> labels = kmeans_model.labels_
>>> metrics.calinski_harabasz_score(X, labels)
561.59...
2.3.11.6.1. Advantages¶
The score is higher when clusters are dense and well separated, which relates
to a standard concept of a cluster.
The score is fast to compute.
2.3.11.6.2. Drawbacks¶
The Calinski-Harabasz index is generally higher for convex clusters than other
concepts of clusters, such as density based clusters like those obtained
through DBSCAN.
2.3.11.6.3. Mathematical formulation¶
For a set of data \(E\) of size \(n_E\) which has been clustered into
\(k\) clusters, the Calinski-Harabasz score \(s\) is defined as the
ratio of the between-clusters dispersion mean and the within-cluster dispersion:
\[s = \frac{\mathrm{tr}(B_k)}{\mathrm{tr}(W_k)} \times \frac{n_E - k}{k - 1}\]
where \(\mathrm{tr}(B_k)\) is trace of the between group dispersion matrix
and \(\mathrm{tr}(W_k)\) is the trace of the within-cluster dispersion
matrix defined by:
\[W_k = \sum_{q=1}^k \sum_{x \in C_q} (x - c_q) (x - c_q)^T\]
\[B_k = \sum_{q=1}^k n_q (c_q - c_E) (c_q - c_E)^T\]
with \(C_q\) the set of points in cluster \(q\), \(c_q\) the center
of cluster \(q\), \(c_E\) the center of \(E\), and \(n_q\) the
number of points in cluster \(q\).
References
Caliński, T., & Harabasz, J. (1974).
“A Dendrite Method for Cluster Analysis”.
Communications in Statistics-theory and Methods 3: 1-27.
2.3.11.7. Davies-Bouldin Index¶
If the ground truth labels are not known, the Davies-Bouldin index
(sklearn.metrics.davies_bouldin_score) can be used to evaluate the
model, where a lower Davies-Bouldin index relates to a model with better
separation between the clusters.
This index signifies the average ‘similarity’ between clusters, where the
similarity is a measure that compares the distance between clusters with the
size of the clusters themselves.
Zero is the lowest possible score. Values closer to zero indicate a better
partition.
In normal usage, the Davies-Bouldin index is applied to the results of a
cluster analysis as follows:
>>> from sklearn import datasets
>>> iris = datasets.load_iris()
>>> X = iris.data
>>> from sklearn.cluster import KMeans
>>> from sklearn.metrics import davies_bouldin_score
>>> kmeans = KMeans(n_clusters=3, random_state=1).fit(X)
>>> labels = kmeans.labels_
>>> davies_bouldin_score(X, labels)
0.666...
2.3.11.7.1. Advantages¶
The computation of Davies-Bouldin is simpler than that of Silhouette scores.
The index is solely based on quantities and features inherent to the dataset
as its computation only uses point-wise distances.
2.3.11.7.2. Drawbacks¶
The Davies-Boulding index is generally higher for convex clusters than other
concepts of clusters, such as density based clusters like those obtained from
DBSCAN.
The usage of centroid distance limits the distance metric to Euclidean space.
2.3.11.7.3. Mathematical formulation¶
The index is defined as the average similarity between each cluster \(C_i\)
for \(i=1, ..., k\) and its most similar one \(C_j\). In the context of
this index, similarity is defined as a measure \(R_{ij}\) that trades off:
\(s_i\), the average distance between each point of cluster \(i\) and
the centroid of that cluster – also know as cluster diameter.
\(d_{ij}\), the distance between cluster centroids \(i\) and \(j\).
A simple choice to construct \(R_{ij}\) so that it is nonnegative and
symmetric is:
\[R_{ij} = \frac{s_i + s_j}{d_{ij}}\]
Then the Davies-Bouldin index is defined as:
\[DB = \frac{1}{k} \sum_{i=1}^k \max_{i \neq j} R_{ij}\]
References
Davies, David L.; Bouldin, Donald W. (1979).
“A Cluster Separation Measure”
IEEE Transactions on Pattern Analysis and Machine Intelligence.
PAMI-1 (2): 224-227.
Halkidi, Maria; Batistakis, Yannis; Vazirgiannis, Michalis (2001).
“On Clustering Validation Techniques”
Journal of Intelligent Information Systems, 17(2-3), 107-145.
Wikipedia entry for Davies-Bouldin index.
2.3.11.8. Contingency Matrix¶
Contingency matrix (sklearn.metrics.cluster.contingency_matrix)
reports the intersection cardinality for every true/predicted cluster pair.
The contingency matrix provides sufficient statistics for all clustering
metrics where the samples are independent and identically distributed and
one doesn’t need to account for some instances not being clustered.
Here is an example:
>>> from sklearn.metrics.cluster import contingency_matrix
>>> x = ["a", "a", "a", "b", "b", "b"]
>>> y = [0, 0, 1, 1, 2, 2]
>>> contingency_matrix(x, y)
array([[2, 1, 0],
[0, 1, 2]])
The first row of output array indicates that there are three samples whose
true cluster is “a”. Of them, two are in predicted cluster 0, one is in 1,
and none is in 2. And the second row indicates that there are three samples
whose true cluster is “b”. Of them, none is in predicted cluster 0, one is in
1 and two are in 2.
A confusion matrix for classification is a square
contingency matrix where the order of rows and columns correspond to a list
of classes.
2.3.11.8.1. Advantages¶
Allows to examine the spread of each true cluster across predicted
clusters and vice versa.
The contingency table calculated is typically utilized in the calculation
of a similarity statistic (like the others listed in this document) between
the two clusterings.
2.3.11.8.2. Drawbacks¶
Contingency matrix is easy to interpret for a small number of clusters, but
becomes very hard to interpret for a large number of clusters.
It doesn’t give a single metric to use as an objective for clustering
optimisation.
References
Wikipedia entry for contingency matrix
2.3.11.9. Pair Confusion Matrix¶
The pair confusion matrix
(sklearn.metrics.cluster.pair_confusion_matrix) is a 2x2
similarity matrix
\[\begin{split}C = \left[\begin{matrix}
C_{00} & C_{01} \\
C_{10} & C_{11}
\end{matrix}\right]\end{split}\]
between two clusterings computed by considering all pairs of samples and
counting pairs that are assigned into the same or into different clusters
under the true and predicted clusterings.
It has the following entries:
\(C_{00}\) : number of pairs with both clusterings having the samples
not clustered together
\(C_{10}\) : number of pairs with the true label clustering having the
samples clustered together but the other clustering not having the samples
clustered together
\(C_{01}\) : number of pairs with the true label clustering not having
the samples clustered together but the other clustering having the samples
clustered together
\(C_{11}\) : number of pairs with both clusterings having the samples
clustered together
Considering a pair of samples that is clustered together a positive pair,
then as in binary classification the count of true negatives is
\(C_{00}\), false negatives is \(C_{10}\), true positives is
\(C_{11}\) and false positives is \(C_{01}\).
Perfectly matching labelings have all non-zero entries on the
diagonal regardless of actual label values:
>>> from sklearn.metrics.cluster import pair_confusion_matrix
>>> pair_confusion_matrix([0, 0, 1, 1], [0, 0, 1, 1])
array([[8, 0],
[0, 4]])
>>> pair_confusion_matrix([0, 0, 1, 1], [1, 1, 0, 0])
array([[8, 0],
[0, 4]])
Labelings that assign all classes members to the same clusters
are complete but may not always be pure, hence penalized, and
have some off-diagonal non-zero entries:
>>> pair_confusion_matrix([0, 0, 1, 2], [0, 0, 1, 1])
array([[8, 2],
[0, 2]])
The matrix is not symmetric:
>>> pair_confusion_matrix([0, 0, 1, 1], [0, 0, 1, 2])
array([[8, 0],
[2, 2]])
If classes members are completely split across different clusters, the
assignment is totally incomplete, hence the matrix has all zero
diagonal entries:
>>> pair_confusion_matrix([0, 0, 0, 0], [0, 1, 2, 3])
array([[ 0,  0],
[12,  0]])
References
“Comparing Partitions”
L. Hubert and P. Arabie, Journal of Classification 1985
© 2007 - 2024, scikit-learn developers (BSD License).
Show this page source
### 2.4. Biclustering — scikit-learn 1.4.2 documentation

Install
User Guide
API
Examples
Community
Getting Started
Tutorial
What's new
Glossary
Development
FAQ
Support
Related packages
Roadmap
Governance
About us
GitHub
Other Versions and Download
More
Getting Started
Tutorial
What's new
Glossary
Development
FAQ
Support
Related packages
Roadmap
Governance
About us
GitHub
Other Versions and Download
Toggle Menu
PrevUp
Next
scikit-learn 1.4.2
Other versions
Please cite us if you use the software.
### 2.4. Biclustering

- 2.4.1. Spectral Co-Clustering
2.4.1.1. Mathematical formulation
- 2.4.2. Spectral Biclustering
2.4.2.1. Mathematical formulation
- 2.4.3. Biclustering evaluation
### 2.4. Biclustering¶

Biclustering algorithms simultaneously
cluster rows and columns of a data matrix. These clusters of rows and
columns are known as biclusters. Each determines a submatrix of the
original data matrix with some desired properties.
For instance, given a matrix of shape (10, 10), one possible bicluster
with three rows and two columns induces a submatrix of shape (3, 2):
>>> import numpy as np
>>> data = np.arange(100).reshape(10, 10)
>>> rows = np.array([0, 2, 3])[:, np.newaxis]
>>> columns = np.array([1, 2])
>>> data[rows, columns]
array([[ 1,  2],
[21, 22],
[31, 32]])
For visualization purposes, given a bicluster, the rows and columns of
the data matrix may be rearranged to make the bicluster contiguous.
Algorithms differ in how they define biclusters. Some of the
common types include:
constant values, constant rows, or constant columns
unusually high or low values
submatrices with low variance
correlated rows or columns
Algorithms also differ in how rows and columns may be assigned to
biclusters, which leads to different bicluster structures. Block
diagonal or checkerboard structures occur when rows and columns are
divided into partitions.
If each row and each column belongs to exactly one bicluster, then
rearranging the rows and columns of the data matrix reveals the
biclusters on the diagonal. Here is an example of this structure
where biclusters have higher average values than the other rows and
columns:
An example of biclusters formed by partitioning rows and columns.¶
In the checkerboard case, each row belongs to all column clusters, and
each column belongs to all row clusters. Here is an example of this
structure where the variance of the values within each bicluster is
small:
An example of checkerboard biclusters.¶
After fitting a model, row and column cluster membership can be found
in the rows_ and columns_ attributes. rows_[i] is a binary vector
with nonzero entries corresponding to rows that belong to bicluster
i. Similarly, columns_[i] indicates which columns belong to
bicluster i.
Some models also have row_labels_ and column_labels_ attributes.
These models partition the rows and columns, such as in the block
diagonal and checkerboard bicluster structures.
Note
Biclustering has many other names in different fields including
co-clustering, two-mode clustering, two-way clustering, block
clustering, coupled two-way clustering, etc. The names of some
algorithms, such as the Spectral Co-Clustering algorithm, reflect
these alternate names.
- 2.4.1. Spectral Co-Clustering¶
The SpectralCoclustering algorithm finds biclusters with
values higher than those in the corresponding other rows and columns.
Each row and each column belongs to exactly one bicluster, so
rearranging the rows and columns to make partitions contiguous reveals
these high values along the diagonal:
Note
The algorithm treats the input data matrix as a bipartite graph: the
rows and columns of the matrix correspond to the two sets of vertices,
and each entry corresponds to an edge between a row and a column. The
algorithm approximates the normalized cut of this graph to find heavy
subgraphs.
2.4.1.1. Mathematical formulation¶
An approximate solution to the optimal normalized cut may be found via
the generalized eigenvalue decomposition of the Laplacian of the
graph. Usually this would mean working directly with the Laplacian
matrix. If the original data matrix \(A\) has shape \(m
\times n\), the Laplacian matrix for the corresponding bipartite graph
has shape \((m + n) \times (m + n)\). However, in this case it is
possible to work directly with \(A\), which is smaller and more
efficient.
The input matrix \(A\) is preprocessed as follows:
\[A_n = R^{-1/2} A C^{-1/2}\]
Where \(R\) is the diagonal matrix with entry \(i\) equal to
\(\sum_{j} A_{ij}\) and \(C\) is the diagonal matrix with
entry \(j\) equal to \(\sum_{i} A_{ij}\).
The singular value decomposition, \(A_n = U \Sigma V^\top\),
provides the partitions of the rows and columns of \(A\). A subset
of the left singular vectors gives the row partitions, and a subset
of the right singular vectors gives the column partitions.
The \(\ell = \lceil \log_2 k \rceil\) singular vectors, starting
from the second, provide the desired partitioning information. They
are used to form the matrix \(Z\):
\[\begin{split}Z = \begin{bmatrix} R^{-1/2} U \\\\
C^{-1/2} V
\end{bmatrix}\end{split}\]
where the columns of \(U\) are \(u_2, \dots, u_{\ell +
1}\), and similarly for \(V\).
Then the rows of \(Z\) are clustered using k-means. The first n_rows labels provide the row partitioning,
and the remaining n_columns labels provide the column partitioning.
Examples:
A demo of the Spectral Co-Clustering algorithm: A simple example
showing how to generate a data matrix with biclusters and apply
this method to it.
Biclustering documents with the Spectral Co-clustering algorithm: An example of finding
biclusters in the twenty newsgroup dataset.
References:
Dhillon, Inderjit S, 2001. Co-clustering documents and words using
bipartite spectral graph partitioning
- 2.4.2. Spectral Biclustering¶
The SpectralBiclustering algorithm assumes that the input
data matrix has a hidden checkerboard structure. The rows and columns
of a matrix with this structure may be partitioned so that the entries
of any bicluster in the Cartesian product of row clusters and column
clusters are approximately constant. For instance, if there are two
row partitions and three column partitions, each row will belong to
three biclusters, and each column will belong to two biclusters.
The algorithm partitions the rows and columns of a matrix so that a
corresponding blockwise-constant checkerboard matrix provides a good
approximation to the original matrix.
2.4.2.1. Mathematical formulation¶
The input matrix \(A\) is first normalized to make the
checkerboard pattern more obvious. There are three possible methods:
Independent row and column normalization, as in Spectral
Co-Clustering. This method makes the rows sum to a constant and the
columns sum to a different constant.
Bistochastization: repeated row and column normalization until
convergence. This method makes both rows and columns sum to the
same constant.
Log normalization: the log of the data matrix is computed: \(L =
\log A\). Then the column mean \(\overline{L_{i \cdot}}\), row mean
\(\overline{L_{\cdot j}}\), and overall mean \(\overline{L_{\cdot
\cdot}}\) of \(L\) are computed. The final matrix is computed
according to the formula
\[K_{ij} = L_{ij} - \overline{L_{i \cdot}} - \overline{L_{\cdot
j}} + \overline{L_{\cdot \cdot}}\]
After normalizing, the first few singular vectors are computed, just
as in the Spectral Co-Clustering algorithm.
If log normalization was used, all the singular vectors are
meaningful. However, if independent normalization or bistochastization
were used, the first singular vectors, \(u_1\) and \(v_1\).
are discarded. From now on, the “first” singular vectors refers to
\(u_2 \dots u_{p+1}\) and \(v_2 \dots v_{p+1}\) except in the
case of log normalization.
Given these singular vectors, they are ranked according to which can
be best approximated by a piecewise-constant vector. The
approximations for each vector are found using one-dimensional k-means
and scored using the Euclidean distance. Some subset of the best left
and right singular vector are selected. Next, the data is projected to
this best subset of singular vectors and clustered.
For instance, if \(p\) singular vectors were calculated, the
\(q\) best are found as described, where \(q<p\). Let
\(U\) be the matrix with columns the \(q\) best left singular
vectors, and similarly \(V\) for the right. To partition the rows,
the rows of \(A\) are projected to a \(q\) dimensional space:
\(A * V\). Treating the \(m\) rows of this \(m \times q\)
matrix as samples and clustering using k-means yields the row labels.
Similarly, projecting the columns to \(A^{\top} * U\) and
clustering this \(n \times q\) matrix yields the column labels.
Examples:
A demo of the Spectral Biclustering algorithm: a simple example
showing how to generate a checkerboard matrix and bicluster it.
References:
Kluger, Yuval, et. al., 2003. Spectral biclustering of microarray
data: coclustering genes and conditions
- 2.4.3. Biclustering evaluation¶
There are two ways of evaluating a biclustering result: internal and
external. Internal measures, such as cluster stability, rely only on
the data and the result themselves. Currently there are no internal
bicluster measures in scikit-learn. External measures refer to an
external source of information, such as the true solution. When
working with real data the true solution is usually unknown, but
biclustering artificial data may be useful for evaluating algorithms
precisely because the true solution is known.
To compare a set of found biclusters to the set of true biclusters,
two similarity measures are needed: a similarity measure for
individual biclusters, and a way to combine these individual
similarities into an overall score.
To compare individual biclusters, several measures have been used. For
now, only the Jaccard index is implemented:
\[J(A, B) = \frac{|A \cap B|}{|A| + |B| - |A \cap B|}\]
where \(A\) and \(B\) are biclusters, \(|A \cap B|\) is
the number of elements in their intersection. The Jaccard index
achieves its minimum of 0 when the biclusters to not overlap at all
and its maximum of 1 when they are identical.
Several methods have been developed to compare two sets of biclusters.
For now, only consensus_score (Hochreiter et. al., 2010) is
available:
Compute bicluster similarities for pairs of biclusters, one in each
set, using the Jaccard index or a similar measure.
Assign biclusters from one set to another in a one-to-one fashion
to maximize the sum of their similarities. This step is performed
using the Hungarian algorithm.
The final sum of similarities is divided by the size of the larger
set.
The minimum consensus score, 0, occurs when all pairs of biclusters
are totally dissimilar. The maximum score, 1, occurs when both sets
are identical.
References:
Hochreiter, Bodenhofer, et. al., 2010. FABIA: factor analysis
for bicluster acquisition.
### 2.5. Decomposing signals in components (matrix factorization problems) — scikit-learn 1.4.2 documentation

Install
User Guide
API
Examples
Community
Getting Started
Tutorial
What's new
Glossary
Development
FAQ
Support
Related packages
Roadmap
Governance
About us
GitHub
Other Versions and Download
More
Getting Started
Tutorial
What's new
Glossary
Development
FAQ
Support
Related packages
Roadmap
Governance
About us
GitHub
Other Versions and Download
Toggle Menu
PrevUp
Next
scikit-learn 1.4.2
Other versions
Please cite us if you use the software.
### 2.5. Decomposing signals in components (matrix factorization problems)

- 2.5.1. Principal component analysis (PCA)
2.5.1.1. Exact PCA and probabilistic interpretation
2.5.1.2. Incremental PCA
2.5.1.3. PCA using randomized SVD
2.5.1.4. Sparse principal components analysis (SparsePCA and MiniBatchSparsePCA)
- 2.5.2. Kernel Principal Component Analysis (kPCA)
2.5.2.1. Exact Kernel PCA
2.5.2.2. Choice of solver for Kernel PCA
- 2.5.3. Truncated singular value decomposition and latent semantic analysis
- 2.5.4. Dictionary Learning
2.5.4.1. Sparse coding with a precomputed dictionary
2.5.4.2. Generic dictionary learning
2.5.4.3. Mini-batch dictionary learning
- 2.5.5. Factor Analysis
- 2.5.6. Independent component analysis (ICA)
- 2.5.7. Non-negative matrix factorization (NMF or NNMF)
2.5.7.1. NMF with the Frobenius norm
2.5.7.2. NMF with a beta-divergence
2.5.7.3. Mini-batch Non Negative Matrix Factorization
- 2.5.8. Latent Dirichlet Allocation (LDA)
### 2.5. Decomposing signals in components (matrix factorization problems)¶

- 2.5.1. Principal component analysis (PCA)¶
2.5.1.1. Exact PCA and probabilistic interpretation¶
PCA is used to decompose a multivariate dataset in a set of successive
orthogonal components that explain a maximum amount of the variance. In
scikit-learn, PCA is implemented as a transformer object
that learns \(n\) components in its fit method, and can be used on new
data to project it on these components.
PCA centers but does not scale the input data for each feature before
applying the SVD. The optional parameter whiten=True makes it
possible to project the data onto the singular space while scaling each
component to unit variance. This is often useful if the models down-stream make
strong assumptions on the isotropy of the signal: this is for example the case
for Support Vector Machines with the RBF kernel and the K-Means clustering
algorithm.
Below is an example of the iris dataset, which is comprised of 4
features, projected on the 2 dimensions that explain most variance:
The PCA object also provides a
probabilistic interpretation of the PCA that can give a likelihood of
data based on the amount of variance it explains. As such it implements a
score method that can be used in cross-validation:
Examples:
PCA example with Iris Data-set
Comparison of LDA and PCA 2D projection of Iris dataset
Model selection with Probabilistic PCA and Factor Analysis (FA)
2.5.1.2. Incremental PCA¶
The PCA object is very useful, but has certain limitations for
large datasets. The biggest limitation is that PCA only supports
batch processing, which means all of the data to be processed must fit in main
memory. The IncrementalPCA object uses a different form of
processing and allows for partial computations which almost
exactly match the results of PCA while processing the data in a
minibatch fashion. IncrementalPCA makes it possible to implement
out-of-core Principal Component Analysis either by:
Using its partial_fit method on chunks of data fetched sequentially
from the local hard drive or a network database.
Calling its fit method on a memory mapped file using
numpy.memmap.
IncrementalPCA only stores estimates of component and noise variances,
in order update explained_variance_ratio_ incrementally. This is why
memory usage depends on the number of samples per batch, rather than the
number of samples to be processed in the dataset.
As in PCA, IncrementalPCA centers but does not scale the
input data for each feature before applying the SVD.
Examples:
Incremental PCA
2.5.1.3. PCA using randomized SVD¶
It is often interesting to project data to a lower-dimensional
space that preserves most of the variance, by dropping the singular vector
of components associated with lower singular values.
For instance, if we work with 64x64 pixel gray-level pictures
for face recognition,
the dimensionality of the data is 4096 and it is slow to train an
RBF support vector machine on such wide data. Furthermore we know that
the intrinsic dimensionality of the data is much lower than 4096 since all
pictures of human faces look somewhat alike.
The samples lie on a manifold of much lower
dimension (say around 200 for instance). The PCA algorithm can be used
to linearly transform the data while both reducing the dimensionality
and preserve most of the explained variance at the same time.
The class PCA used with the optional parameter
svd_solver='randomized' is very useful in that case: since we are going
to drop most of the singular vectors it is much more efficient to limit the
computation to an approximated estimate of the singular vectors we will keep
to actually perform the transform.
For instance, the following shows 16 sample portraits (centered around
0.0) from the Olivetti dataset. On the right hand side are the first 16
singular vectors reshaped as portraits. Since we only require the top
16 singular vectors of a dataset with size \(n_{samples} = 400\)
and \(n_{features} = 64 \times 64 = 4096\), the computation time is
less than 1s:
If we note \(n_{\max} = \max(n_{\mathrm{samples}}, n_{\mathrm{features}})\) and
\(n_{\min} = \min(n_{\mathrm{samples}}, n_{\mathrm{features}})\), the time complexity
of the randomized PCA is \(O(n_{\max}^2 \cdot n_{\mathrm{components}})\)
instead of \(O(n_{\max}^2 \cdot n_{\min})\) for the exact method
implemented in PCA.
The memory footprint of randomized PCA is also proportional to
\(2 \cdot n_{\max} \cdot n_{\mathrm{components}}\) instead of \(n_{\max}
\cdot n_{\min}\) for the exact method.
Note: the implementation of inverse_transform in PCA with
svd_solver='randomized' is not the exact inverse transform of
transform even when whiten=False (default).
Examples:
Faces recognition example using eigenfaces and SVMs
Faces dataset decompositions
References:
Algorithm 4.3 in
“Finding structure with randomness: Stochastic algorithms for
constructing approximate matrix decompositions”
Halko, et al., 2009
“An implementation of a randomized algorithm for principal component
analysis” A. Szlam et al. 2014
2.5.1.4. Sparse principal components analysis (SparsePCA and MiniBatchSparsePCA)¶
SparsePCA is a variant of PCA, with the goal of extracting the
set of sparse components that best reconstruct the data.
Mini-batch sparse PCA (MiniBatchSparsePCA) is a variant of
SparsePCA that is faster but less accurate. The increased speed is
reached by iterating over small chunks of the set of features, for a given
number of iterations.
Principal component analysis (PCA) has the disadvantage that the
components extracted by this method have exclusively dense expressions, i.e.
they have non-zero coefficients when expressed as linear combinations of the
original variables. This can make interpretation difficult. In many cases,
the real underlying components can be more naturally imagined as sparse
vectors; for example in face recognition, components might naturally map to
parts of faces.
Sparse principal components yields a more parsimonious, interpretable
representation, clearly emphasizing which of the original features contribute
to the differences between samples.
The following example illustrates 16 components extracted using sparse PCA from
the Olivetti faces dataset.  It can be seen how the regularization term induces
many zeros. Furthermore, the natural structure of the data causes the non-zero
coefficients to be vertically adjacent. The model does not enforce this
mathematically: each component is a vector \(h \in \mathbf{R}^{4096}\), and
there is no notion of vertical adjacency except during the human-friendly
visualization as 64x64 pixel images. The fact that the components shown below
appear local is the effect of the inherent structure of the data, which makes
such local patterns minimize reconstruction error. There exist sparsity-inducing
norms that take into account adjacency and different kinds of structure; see
[Jen09] for a review of such methods.
For more details on how to use Sparse PCA, see the Examples section, below.
Note that there are many different formulations for the Sparse PCA
problem. The one implemented here is based on [Mrl09] . The optimization
problem solved is a PCA problem (dictionary learning) with an
\(\ell_1\) penalty on the components:
\[\begin{split}(U^*, V^*) = \underset{U, V}{\operatorname{arg\,min\,}} & \frac{1}{2}
||X-UV||_{\text{Fro}}^2+\alpha||V||_{1,1} \\
\text{subject to } & ||U_k||_2 <= 1 \text{ for all }
0 \leq k < n_{components}\end{split}\]
\(||.||_{\text{Fro}}\) stands for the Frobenius norm and \(||.||_{1,1}\)
stands for the entry-wise matrix norm which is the sum of the absolute values
of all the entries in the matrix.
The sparsity-inducing \(||.||_{1,1}\) matrix norm also prevents learning
components from noise when few training samples are available. The degree
of penalization (and thus sparsity) can be adjusted through the
hyperparameter alpha. Small values lead to a gently regularized
factorization, while larger values shrink many coefficients to zero.
Note
While in the spirit of an online algorithm, the class
MiniBatchSparsePCA does not implement partial_fit because
the algorithm is online along the features direction, not the samples
direction.
Examples:
Faces dataset decompositions
References:
[Mrl09]
“Online Dictionary Learning for Sparse Coding”
J. Mairal, F. Bach, J. Ponce, G. Sapiro, 2009
[Jen09]
“Structured Sparse Principal Component Analysis”
R. Jenatton, G. Obozinski, F. Bach, 2009
- 2.5.2. Kernel Principal Component Analysis (kPCA)¶
2.5.2.1. Exact Kernel PCA¶
KernelPCA is an extension of PCA which achieves non-linear
dimensionality reduction through the use of kernels (see Pairwise metrics, Affinities and Kernels) [Scholkopf1997]. It
has many applications including denoising, compression and structured
prediction (kernel dependency estimation). KernelPCA supports both
transform and inverse_transform.
Note
KernelPCA.inverse_transform relies on a kernel ridge to learn the
function mapping samples from the PCA basis into the original feature
space [Bakir2003]. Thus, the reconstruction obtained with
KernelPCA.inverse_transform is an approximation. See the example
linked below for more details.
Examples:
Kernel PCA
References:
[Scholkopf1997]
Schölkopf, Bernhard, Alexander Smola, and Klaus-Robert Müller.
“Kernel principal component analysis.”
International conference on artificial neural networks.
Springer, Berlin, Heidelberg, 1997.
[Bakir2003]
Bakır, Gökhan H., Jason Weston, and Bernhard Schölkopf.
“Learning to find pre-images.”
Advances in neural information processing systems 16 (2003): 449-456.
2.5.2.2. Choice of solver for Kernel PCA¶
While in PCA the number of components is bounded by the number of
features, in KernelPCA the number of components is bounded by the
number of samples. Many real-world datasets have large number of samples! In
these cases finding all the components with a full kPCA is a waste of
computation time, as data is mostly described by the first few components
(e.g. n_components<=100). In other words, the centered Gram matrix that
is eigendecomposed in the Kernel PCA fitting process has an effective rank that
is much smaller than its size. This is a situation where approximate
eigensolvers can provide speedup with very low precision loss.
Eigensolvers
Click for more details
¶
The optional parameter eigen_solver='randomized' can be used to
significantly reduce the computation time when the number of requested
n_components is small compared with the number of samples. It relies on
randomized decomposition methods to find an approximate solution in a shorter
time.
The time complexity of the randomized KernelPCA is
\(O(n_{\mathrm{samples}}^2 \cdot n_{\mathrm{components}})\)
instead of \(O(n_{\mathrm{samples}}^3)\) for the exact method
implemented with eigen_solver='dense'.
The memory footprint of randomized KernelPCA is also proportional to
\(2 \cdot n_{\mathrm{samples}} \cdot n_{\mathrm{components}}\) instead of
\(n_{\mathrm{samples}}^2\) for the exact method.
Note: this technique is the same as in PCA using randomized SVD.
In addition to the above two solvers, eigen_solver='arpack' can be used as
an alternate way to get an approximate decomposition. In practice, this method
only provides reasonable execution times when the number of components to find
is extremely small. It is enabled by default when the desired number of
components is less than 10 (strict) and the number of samples is more than 200
(strict). See KernelPCA for details.
References:
dense solver:
scipy.linalg.eigh documentation
randomized solver:
Algorithm 4.3 in
“Finding structure with randomness: Stochastic
algorithms for constructing approximate matrix decompositions”
Halko, et al. (2009)
“An implementation of a randomized algorithm
for principal component analysis”
A. Szlam et al. (2014)
arpack solver:
scipy.sparse.linalg.eigsh documentation
R. B. Lehoucq, D. C. Sorensen, and C. Yang, (1998)
- 2.5.3. Truncated singular value decomposition and latent semantic analysis¶
TruncatedSVD implements a variant of singular value decomposition
(SVD) that only computes the \(k\) largest singular values,
where \(k\) is a user-specified parameter.
TruncatedSVD is very similar to PCA, but differs
in that the matrix \(X\) does not need to be centered.
When the columnwise (per-feature) means of \(X\)
are subtracted from the feature values,
truncated SVD on the resulting matrix is equivalent to PCA.
About truncated SVD and latent semantic analysis (LSA)
Click for more details
¶
When truncated SVD is applied to term-document matrices
(as returned by CountVectorizer or
TfidfVectorizer),
this transformation is known as
latent semantic analysis
(LSA), because it transforms such matrices
to a “semantic” space of low dimensionality.
In particular, LSA is known to combat the effects of synonymy and polysemy
(both of which roughly mean there are multiple meanings per word),
which cause term-document matrices to be overly sparse
and exhibit poor similarity under measures such as cosine similarity.
Note
LSA is also known as latent semantic indexing, LSI,
though strictly that refers to its use in persistent indexes
for information retrieval purposes.
Mathematically, truncated SVD applied to training samples \(X\)
produces a low-rank approximation \(X\):
\[X \approx X_k = U_k \Sigma_k V_k^\top\]
After this operation, \(U_k \Sigma_k\)
is the transformed training set with \(k\) features
(called n_components in the API).
To also transform a test set \(X\), we multiply it with \(V_k\):
\[X' = X V_k\]
Note
Most treatments of LSA in the natural language processing (NLP)
and information retrieval (IR) literature
swap the axes of the matrix \(X\) so that it has shape
n_features × n_samples.
We present LSA in a different way that matches the scikit-learn API better,
but the singular values found are the same.
While the TruncatedSVD transformer
works with any feature matrix,
using it on tf–idf matrices is recommended over raw frequency counts
in an LSA/document processing setting.
In particular, sublinear scaling and inverse document frequency
should be turned on (sublinear_tf=True, use_idf=True)
to bring the feature values closer to a Gaussian distribution,
compensating for LSA’s erroneous assumptions about textual data.
Examples:
Clustering text documents using k-means
References:
Christopher D. Manning, Prabhakar Raghavan and Hinrich Schütze (2008),
Introduction to Information Retrieval, Cambridge University Press,
chapter 18: Matrix decompositions & latent semantic indexing
- 2.5.4. Dictionary Learning¶
2.5.4.1. Sparse coding with a precomputed dictionary¶
The SparseCoder object is an estimator that can be used to transform signals
into sparse linear combination of atoms from a fixed, precomputed dictionary
such as a discrete wavelet basis. This object therefore does not
implement a fit method. The transformation amounts
to a sparse coding problem: finding a representation of the data as a linear
combination of as few dictionary atoms as possible. All variations of
dictionary learning implement the following transform methods, controllable via
the transform_method initialization parameter:
Orthogonal matching pursuit (Orthogonal Matching Pursuit (OMP))
Least-angle regression (Least Angle Regression)
Lasso computed by least-angle regression
Lasso using coordinate descent (Lasso)
Thresholding
Thresholding is very fast but it does not yield accurate reconstructions.
They have been shown useful in literature for classification tasks. For image
reconstruction tasks, orthogonal matching pursuit yields the most accurate,
unbiased reconstruction.
The dictionary learning objects offer, via the split_code parameter, the
possibility to separate the positive and negative values in the results of
sparse coding. This is useful when dictionary learning is used for extracting
features that will be used for supervised learning, because it allows the
learning algorithm to assign different weights to negative loadings of a
particular atom, from to the corresponding positive loading.
The split code for a single sample has length 2 * n_components
and is constructed using the following rule: First, the regular code of length
n_components is computed. Then, the first n_components entries of the
split_code are
filled with the positive part of the regular code vector. The second half of
the split code is filled with the negative part of the code vector, only with
a positive sign. Therefore, the split_code is non-negative.
Examples:
Sparse coding with a precomputed dictionary
2.5.4.2. Generic dictionary learning¶
Dictionary learning (DictionaryLearning) is a matrix factorization
problem that amounts to finding a (usually overcomplete) dictionary that will
perform well at sparsely encoding the fitted data.
Representing data as sparse combinations of atoms from an overcomplete
dictionary is suggested to be the way the mammalian primary visual cortex works.
Consequently, dictionary learning applied on image patches has been shown to
give good results in image processing tasks such as image completion,
inpainting and denoising, as well as for supervised recognition tasks.
Dictionary learning is an optimization problem solved by alternatively updating
the sparse code, as a solution to multiple Lasso problems, considering the
dictionary fixed, and then updating the dictionary to best fit the sparse code.
\[\begin{split}(U^*, V^*) = \underset{U, V}{\operatorname{arg\,min\,}} & \frac{1}{2}
||X-UV||_{\text{Fro}}^2+\alpha||U||_{1,1} \\
\text{subject to } & ||V_k||_2 <= 1 \text{ for all }
0 \leq k < n_{\mathrm{atoms}}\end{split}\]
\(||.||_{\text{Fro}}\) stands for the Frobenius norm and \(||.||_{1,1}\)
stands for the entry-wise matrix norm which is the sum of the absolute values
of all the entries in the matrix.
After using such a procedure to fit the dictionary, the transform is simply a
sparse coding step that shares the same implementation with all dictionary
learning objects (see Sparse coding with a precomputed dictionary).
It is also possible to constrain the dictionary and/or code to be positive to
match constraints that may be present in the data. Below are the faces with
different positivity constraints applied. Red indicates negative values, blue
indicates positive values, and white represents zeros.
The following image shows how a dictionary learned from 4x4 pixel image patches
extracted from part of the image of a raccoon face looks like.
Examples:
Image denoising using dictionary learning
References:
“Online dictionary learning for sparse coding”
J. Mairal, F. Bach, J. Ponce, G. Sapiro, 2009
2.5.4.3. Mini-batch dictionary learning¶
MiniBatchDictionaryLearning implements a faster, but less accurate
version of the dictionary learning algorithm that is better suited for large
datasets.
By default, MiniBatchDictionaryLearning divides the data into
mini-batches and optimizes in an online manner by cycling over the mini-batches
for the specified number of iterations. However, at the moment it does not
implement a stopping condition.
The estimator also implements partial_fit, which updates the dictionary by
iterating only once over a mini-batch. This can be used for online learning
when the data is not readily available from the start, or for when the data
does not fit into the memory.
Clustering for dictionary learning
Note that when using dictionary learning to extract a representation
(e.g. for sparse coding) clustering can be a good proxy to learn the
dictionary. For instance the MiniBatchKMeans estimator is
computationally efficient and implements on-line learning with a
partial_fit method.
Example: Online learning of a dictionary of parts of faces
- 2.5.5. Factor Analysis¶
In unsupervised learning we only have a dataset \(X = \{x_1, x_2, \dots, x_n
\}\). How can this dataset be described mathematically? A very simple
continuous latent variable model for \(X\) is
\[x_i = W h_i + \mu + \epsilon\]
The vector \(h_i\) is called “latent” because it is unobserved. \(\epsilon\) is
considered a noise term distributed according to a Gaussian with mean 0 and
covariance \(\Psi\) (i.e. \(\epsilon \sim \mathcal{N}(0, \Psi)\)), \(\mu\) is some
arbitrary offset vector. Such a model is called “generative” as it describes
how \(x_i\) is generated from \(h_i\). If we use all the \(x_i\)’s as columns to form
a matrix \(\mathbf{X}\) and all the \(h_i\)’s as columns of a matrix \(\mathbf{H}\)
then we can write (with suitably defined \(\mathbf{M}\) and \(\mathbf{E}\)):
\[\mathbf{X} = W \mathbf{H} + \mathbf{M} + \mathbf{E}\]
In other words, we decomposed matrix \(\mathbf{X}\).
If \(h_i\) is given, the above equation automatically implies the following
probabilistic interpretation:
\[p(x_i|h_i) = \mathcal{N}(Wh_i + \mu, \Psi)\]
For a complete probabilistic model we also need a prior distribution for the
latent variable \(h\). The most straightforward assumption (based on the nice
properties of the Gaussian distribution) is \(h \sim \mathcal{N}(0,
\mathbf{I})\).  This yields a Gaussian as the marginal distribution of \(x\):
\[p(x) = \mathcal{N}(\mu, WW^T + \Psi)\]
Now, without any further assumptions the idea of having a latent variable \(h\)
would be superfluous – \(x\) can be completely modelled with a mean
and a covariance. We need to impose some more specific structure on one
of these two parameters. A simple additional assumption regards the
structure of the error covariance \(\Psi\):
\(\Psi = \sigma^2 \mathbf{I}\): This assumption leads to
the probabilistic model of PCA.
\(\Psi = \mathrm{diag}(\psi_1, \psi_2, \dots, \psi_n)\): This model is called
FactorAnalysis, a classical statistical model. The matrix W is
sometimes called the “factor loading matrix”.
Both models essentially estimate a Gaussian with a low-rank covariance matrix.
Because both models are probabilistic they can be integrated in more complex
models, e.g. Mixture of Factor Analysers. One gets very different models (e.g.
FastICA) if non-Gaussian priors on the latent variables are assumed.
Factor analysis can produce similar components (the columns of its loading
matrix) to PCA. However, one can not make any general statements
about these components (e.g. whether they are orthogonal):
The main advantage for Factor Analysis over PCA is that
it can model the variance in every direction of the input space independently
(heteroscedastic noise):
This allows better model selection than probabilistic PCA in the presence
of heteroscedastic noise:
Factor Analysis is often followed by a rotation of the factors (with the
parameter rotation), usually to improve interpretability. For example,
Varimax rotation maximizes the sum of the variances of the squared loadings,
i.e., it tends to produce sparser factors, which are influenced by only a few
features each (the “simple structure”). See e.g., the first example below.
Examples:
Factor Analysis (with rotation) to visualize patterns
Model selection with Probabilistic PCA and Factor Analysis (FA)
- 2.5.6. Independent component analysis (ICA)¶
Independent component analysis separates a multivariate signal into
additive subcomponents that are maximally independent. It is
implemented in scikit-learn using the Fast ICA
algorithm. Typically, ICA is not used for reducing dimensionality but
for separating superimposed signals. Since the ICA model does not include
a noise term, for the model to be correct, whitening must be applied.
This can be done internally using the whiten argument or manually using one
of the PCA variants.
It is classically used to separate mixed signals (a problem known as
blind source separation), as in the example below:
ICA can also be used as yet another non linear decomposition that finds
components with some sparsity:
Examples:
Blind source separation using FastICA
FastICA on 2D point clouds
Faces dataset decompositions
- 2.5.7. Non-negative matrix factorization (NMF or NNMF)¶
2.5.7.1. NMF with the Frobenius norm¶
NMF [1] is an alternative approach to decomposition that assumes that the
data and the components are non-negative. NMF can be plugged in
instead of PCA or its variants, in the cases where the data matrix
does not contain negative values. It finds a decomposition of samples
\(X\) into two matrices \(W\) and \(H\) of non-negative elements,
by optimizing the distance \(d\) between \(X\) and the matrix product
\(WH\). The most widely used distance function is the squared Frobenius
norm, which is an obvious extension of the Euclidean norm to matrices:
\[d_{\mathrm{Fro}}(X, Y) = \frac{1}{2} ||X - Y||_{\mathrm{Fro}}^2 = \frac{1}{2} \sum_{i,j} (X_{ij} - {Y}_{ij})^2\]
Unlike PCA, the representation of a vector is obtained in an additive
fashion, by superimposing the components, without subtracting. Such additive
models are efficient for representing images and text.
It has been observed in [Hoyer, 2004] [2] that, when carefully constrained,
NMF can produce a parts-based representation of the dataset,
resulting in interpretable models. The following example displays 16
sparse components found by NMF from the images in the Olivetti
faces dataset, in comparison with the PCA eigenfaces.
The init attribute determines the initialization method applied, which
has a great impact on the performance of the method. NMF implements the
method Nonnegative Double Singular Value Decomposition. NNDSVD [4] is based on
two SVD processes, one approximating the data matrix, the other approximating
positive sections of the resulting partial SVD factors utilizing an algebraic
property of unit rank matrices. The basic NNDSVD algorithm is better fit for
sparse factorization. Its variants NNDSVDa (in which all zeros are set equal to
the mean of all elements of the data), and NNDSVDar (in which the zeros are set
to random perturbations less than the mean of the data divided by 100) are
recommended in the dense case.
Note that the Multiplicative Update (‘mu’) solver cannot update zeros present in
the initialization, so it leads to poorer results when used jointly with the
basic NNDSVD algorithm which introduces a lot of zeros; in this case, NNDSVDa or
NNDSVDar should be preferred.
NMF can also be initialized with correctly scaled random non-negative
matrices by setting init="random". An integer seed or a
RandomState can also be passed to random_state to control
reproducibility.
In NMF, L1 and L2 priors can be added to the loss function in order to
regularize the model. The L2 prior uses the Frobenius norm, while the L1 prior
uses an elementwise L1 norm. As in ElasticNet,
we control the combination of L1 and L2 with the l1_ratio (\(\rho\))
parameter, and the intensity of the regularization with the alpha_W and
alpha_H (\(\alpha_W\) and \(\alpha_H\)) parameters. The priors are
scaled by the number of samples (\(n\_samples\)) for H and the number of
features (\(n\_features\)) for W to keep their impact balanced with
respect to one another and to the data fit term as independent as possible of
the size of the training set. Then the priors terms are:
\[(\alpha_W \rho ||W||_1 + \frac{\alpha_W(1-\rho)}{2} ||W||_{\mathrm{Fro}} ^ 2) * n\_features
+ (\alpha_H \rho ||H||_1 + \frac{\alpha_H(1-\rho)}{2} ||H||_{\mathrm{Fro}} ^ 2) * n\_samples\]
and the regularized objective function is:
\[d_{\mathrm{Fro}}(X, WH)
+ (\alpha_W \rho ||W||_1 + \frac{\alpha_W(1-\rho)}{2} ||W||_{\mathrm{Fro}} ^ 2) * n\_features
+ (\alpha_H \rho ||H||_1 + \frac{\alpha_H(1-\rho)}{2} ||H||_{\mathrm{Fro}} ^ 2) * n\_samples\]
2.5.7.2. NMF with a beta-divergence¶
As described previously, the most widely used distance function is the squared
Frobenius norm, which is an obvious extension of the Euclidean norm to
matrices:
\[d_{\mathrm{Fro}}(X, Y) = \frac{1}{2} ||X - Y||_{Fro}^2 = \frac{1}{2} \sum_{i,j} (X_{ij} - {Y}_{ij})^2\]
Other distance functions can be used in NMF as, for example, the (generalized)
Kullback-Leibler (KL) divergence, also referred as I-divergence:
\[d_{KL}(X, Y) = \sum_{i,j} (X_{ij} \log(\frac{X_{ij}}{Y_{ij}}) - X_{ij} + Y_{ij})\]
Or, the Itakura-Saito (IS) divergence:
\[d_{IS}(X, Y) = \sum_{i,j} (\frac{X_{ij}}{Y_{ij}} - \log(\frac{X_{ij}}{Y_{ij}}) - 1)\]
These three distances are special cases of the beta-divergence family, with
\(\beta = 2, 1, 0\) respectively [6]. The beta-divergence are
defined by :
\[d_{\beta}(X, Y) = \sum_{i,j} \frac{1}{\beta(\beta - 1)}(X_{ij}^\beta + (\beta-1)Y_{ij}^\beta - \beta X_{ij} Y_{ij}^{\beta - 1})\]
Note that this definition is not valid if \(\beta \in (0; 1)\), yet it can
be continuously extended to the definitions of \(d_{KL}\) and \(d_{IS}\)
respectively.
NMF implemented solvers
Click for more details
¶
NMF implements two solvers, using Coordinate Descent (‘cd’) [5], and
Multiplicative Update (‘mu’) [6]. The ‘mu’ solver can optimize every
beta-divergence, including of course the Frobenius norm (\(\beta=2\)), the
(generalized) Kullback-Leibler divergence (\(\beta=1\)) and the
Itakura-Saito divergence (\(\beta=0\)). Note that for
\(\beta \in (1; 2)\), the ‘mu’ solver is significantly faster than for other
values of \(\beta\). Note also that with a negative (or 0, i.e.
‘itakura-saito’) \(\beta\), the input matrix cannot contain zero values.
The ‘cd’ solver can only optimize the Frobenius norm. Due to the
underlying non-convexity of NMF, the different solvers may converge to
different minima, even when optimizing the same distance function.
NMF is best used with the fit_transform method, which returns the matrix W.
The matrix H is stored into the fitted model in the components_ attribute;
the method transform will decompose a new matrix X_new based on these
stored components:
>>> import numpy as np
>>> X = np.array([[1, 1], [2, 1], [3, 1.2], [4, 1], [5, 0.8], [6, 1]])
>>> from sklearn.decomposition import NMF
>>> model = NMF(n_components=2, init='random', random_state=0)
>>> W = model.fit_transform(X)
>>> H = model.components_
>>> X_new = np.array([[1, 0], [1, 6.1], [1, 0], [1, 4], [3.2, 1], [0, 4]])
>>> W_new = model.transform(X_new)
Examples:
Faces dataset decompositions
Topic extraction with Non-negative Matrix Factorization and Latent Dirichlet Allocation
2.5.7.3. Mini-batch Non Negative Matrix Factorization¶
MiniBatchNMF [7] implements a faster, but less accurate version of the
non negative matrix factorization (i.e. NMF),
better suited for large datasets.
By default, MiniBatchNMF divides the data into mini-batches and
optimizes the NMF model in an online manner by cycling over the mini-batches
for the specified number of iterations. The batch_size parameter controls
the size of the batches.
In order to speed up the mini-batch algorithm it is also possible to scale
past batches, giving them less importance than newer batches. This is done
introducing a so-called forgetting factor controlled by the forget_factor
parameter.
The estimator also implements partial_fit, which updates H by iterating
only once over a mini-batch. This can be used for online learning when the data
is not readily available from the start, or when the data does not fit into memory.
References:
[1]
“Learning the parts of objects by non-negative matrix factorization”
D. Lee, S. Seung, 1999
[2]
“Non-negative Matrix Factorization with Sparseness Constraints”
P. Hoyer, 2004
[4]
“SVD based initialization: A head start for nonnegative
matrix factorization”
C. Boutsidis, E. Gallopoulos, 2008
[5]
“Fast local algorithms for large scale nonnegative matrix and tensor
factorizations.”
A. Cichocki, A. Phan, 2009
[6]
(1,2)
“Algorithms for nonnegative matrix factorization with
the beta-divergence”
C. Fevotte, J. Idier, 2011
[7]
“Online algorithms for nonnegative matrix factorization with the
Itakura-Saito divergence”
A. Lefevre, F. Bach, C. Fevotte, 2011
- 2.5.8. Latent Dirichlet Allocation (LDA)¶
Latent Dirichlet Allocation is a generative probabilistic model for collections of
discrete dataset such as text corpora. It is also a topic model that is used for
discovering abstract topics from a collection of documents.
The graphical model of LDA is a three-level generative model:
Note on notations presented in the graphical model above, which can be found in
Hoffman et al. (2013):
The corpus is a collection of \(D\) documents.
A document is a sequence of \(N\) words.
There are \(K\) topics in the corpus.
The boxes represent repeated sampling.
In the graphical model, each node is a random variable and has a role in the
generative process. A shaded node indicates an observed variable and an unshaded
node indicates a hidden (latent) variable. In this case, words in the corpus are
the only data that we observe. The latent variables determine the random mixture
of topics in the corpus and the distribution of words in the documents.
The goal of LDA is to use the observed words to infer the hidden topic
structure.
Details on modeling text corpora
Click for more details
¶
When modeling text corpora, the model assumes the following generative process
for a corpus with \(D\) documents and \(K\) topics, with \(K\)
corresponding to n_components in the API:
For each topic \(k \in K\), draw \(\beta_k \sim
\mathrm{Dirichlet}(\eta)\). This provides a distribution over the words,
i.e. the probability of a word appearing in topic \(k\).
\(\eta\) corresponds to topic_word_prior.
For each document \(d \in D\), draw the topic proportions
\(\theta_d \sim \mathrm{Dirichlet}(\alpha)\). \(\alpha\)
corresponds to doc_topic_prior.
For each word \(i\) in document \(d\):
Draw the topic assignment \(z_{di} \sim \mathrm{Multinomial}
(\theta_d)\)
Draw the observed word \(w_{ij} \sim \mathrm{Multinomial}
(\beta_{z_{di}})\)
For parameter estimation, the posterior distribution is:
\[p(z, \theta, \beta |w, \alpha, \eta) =
\frac{p(z, \theta, \beta|\alpha, \eta)}{p(w|\alpha, \eta)}\]
Since the posterior is intractable, variational Bayesian method
uses a simpler distribution \(q(z,\theta,\beta | \lambda, \phi, \gamma)\)
to approximate it, and those variational parameters \(\lambda\),
\(\phi\), \(\gamma\) are optimized to maximize the Evidence
Lower Bound (ELBO):
\[\log\: P(w | \alpha, \eta) \geq L(w,\phi,\gamma,\lambda) \overset{\triangle}{=}
E_{q}[\log\:p(w,z,\theta,\beta|\alpha,\eta)] - E_{q}[\log\:q(z, \theta, \beta)]\]
Maximizing ELBO is equivalent to minimizing the Kullback-Leibler(KL) divergence
between \(q(z,\theta,\beta)\) and the true posterior
\(p(z, \theta, \beta |w, \alpha, \eta)\).
LatentDirichletAllocation implements the online variational Bayes
algorithm and supports both online and batch update methods.
While the batch method updates variational variables after each full pass through
the data, the online method updates variational variables from mini-batch data
points.
Note
Although the online method is guaranteed to converge to a local optimum point, the quality of
the optimum point and the speed of convergence may depend on mini-batch size and
attributes related to learning rate setting.
When LatentDirichletAllocation is applied on a “document-term” matrix, the matrix
will be decomposed into a “topic-term” matrix and a “document-topic” matrix. While
“topic-term” matrix is stored as components_ in the model, “document-topic” matrix
can be calculated from transform method.
LatentDirichletAllocation also implements partial_fit method. This is used
when data can be fetched sequentially.
Examples:
Topic extraction with Non-negative Matrix Factorization and Latent Dirichlet Allocation
References:
“Latent Dirichlet Allocation”
D. Blei, A. Ng, M. Jordan, 2003
“Online Learning for Latent Dirichlet Allocation”
M. Hoffman, D. Blei, F. Bach, 2010
“Stochastic Variational Inference”
M. Hoffman, D. Blei, C. Wang, J. Paisley, 2013
“The varimax criterion for analytic rotation in factor analysis”
H. F. Kaiser, 1958
See also Dimensionality reduction for dimensionality reduction with
Neighborhood Components Analysis.
### 2.6. Covariance estimation — scikit-learn 1.4.2 documentation

Install
User Guide
API
Examples
Community
Getting Started
Tutorial
What's new
Glossary
Development
FAQ
Support
Related packages
Roadmap
Governance
About us
GitHub
Other Versions and Download
More
Getting Started
Tutorial
What's new
Glossary
Development
FAQ
Support
Related packages
Roadmap
Governance
About us
GitHub
Other Versions and Download
Toggle Menu
PrevUp
Next
scikit-learn 1.4.2
Other versions
Please cite us if you use the software.
### 2.6. Covariance estimation

- 2.6.1. Empirical covariance
- 2.6.2. Shrunk Covariance
2.6.2.1. Basic shrinkage
2.6.2.2. Ledoit-Wolf shrinkage
2.6.2.3. Oracle Approximating Shrinkage
- 2.6.3. Sparse inverse covariance
- 2.6.4. Robust Covariance Estimation
2.6.4.1. Minimum Covariance Determinant
### 2.6. Covariance estimation¶

Many statistical problems require the estimation of a
population’s covariance matrix, which can be seen as an estimation of
data set scatter plot shape. Most of the time, such an estimation has
to be done on a sample whose properties (size, structure, homogeneity)
have a large influence on the estimation’s quality. The
sklearn.covariance package provides tools for accurately estimating
a population’s covariance matrix under various settings.
We assume that the observations are independent and identically
distributed (i.i.d.).
- 2.6.1. Empirical covariance¶
The covariance matrix of a data set is known to be well approximated
by the classical maximum likelihood estimator (or “empirical
covariance”), provided the number of observations is large enough
compared to the number of features (the variables describing the
observations). More precisely, the Maximum Likelihood Estimator of a
sample is an asymptotically unbiased estimator of the corresponding
population’s covariance matrix.
The empirical covariance matrix of a sample can be computed using the
empirical_covariance function of the package, or by fitting an
EmpiricalCovariance object to the data sample with the
EmpiricalCovariance.fit method. Be careful that results depend
on whether the data are centered, so one may want to use the
assume_centered parameter accurately. More precisely, if
assume_centered=False, then the test set is supposed to have the
same mean vector as the training set. If not, both should be centered
by the user, and assume_centered=True should be used.
Examples:
See Shrinkage covariance estimation: LedoitWolf vs OAS and max-likelihood for
an example on how to fit an EmpiricalCovariance object
to data.
- 2.6.2. Shrunk Covariance¶
2.6.2.1. Basic shrinkage¶
Despite being an asymptotically unbiased estimator of the covariance matrix,
the Maximum Likelihood Estimator is not a good estimator of the
eigenvalues of the covariance matrix, so the precision matrix obtained
from its inversion is not accurate. Sometimes, it even occurs that the
empirical covariance matrix cannot be inverted for numerical
reasons. To avoid such an inversion problem, a transformation of the
empirical covariance matrix has been introduced: the shrinkage.
In scikit-learn, this transformation (with a user-defined shrinkage
coefficient) can be directly applied to a pre-computed covariance with
the shrunk_covariance method. Also, a shrunk estimator of the
covariance can be fitted to data with a ShrunkCovariance object
and its ShrunkCovariance.fit method. Again, results depend on
whether the data are centered, so one may want to use the
assume_centered parameter accurately.
Mathematically, this shrinkage consists in reducing the ratio between the
smallest and the largest eigenvalues of the empirical covariance matrix.
It can be done by simply shifting every eigenvalue according to a given
offset, which is equivalent of finding the l2-penalized Maximum
Likelihood Estimator of the covariance matrix. In practice, shrinkage
boils down to a simple a convex transformation : \(\Sigma_{\rm
shrunk} = (1-\alpha)\hat{\Sigma} + \alpha\frac{{\rm
Tr}\hat{\Sigma}}{p}\rm Id\).
Choosing the amount of shrinkage, \(\alpha\) amounts to setting a
bias/variance trade-off, and is discussed below.
Examples:
See Shrinkage covariance estimation: LedoitWolf vs OAS and max-likelihood for
an example on how to fit a ShrunkCovariance object
to data.
2.6.2.2. Ledoit-Wolf shrinkage¶
In their 2004 paper [1], O. Ledoit and M. Wolf propose a formula
to compute the optimal shrinkage coefficient \(\alpha\) that
minimizes the Mean Squared Error between the estimated and the real
covariance matrix.
The Ledoit-Wolf estimator of the covariance matrix can be computed on
a sample with the ledoit_wolf function of the
sklearn.covariance package, or it can be otherwise obtained by
fitting a LedoitWolf object to the same sample.
Note
Case when population covariance matrix is isotropic
It is important to note that when the number of samples is much larger than
the number of features, one would expect that no shrinkage would be
necessary. The intuition behind this is that if the population covariance
is full rank, when the number of sample grows, the sample covariance will
also become positive definite. As a result, no shrinkage would necessary
and the method should automatically do this.
This, however, is not the case in the Ledoit-Wolf procedure when the
population covariance happens to be a multiple of the identity matrix. In
this case, the Ledoit-Wolf shrinkage estimate approaches 1 as the number of
samples increases. This indicates that the optimal estimate of the
covariance matrix in the Ledoit-Wolf sense is multiple of the identity.
Since the population covariance is already a multiple of the identity
matrix, the Ledoit-Wolf solution is indeed a reasonable estimate.
Examples:
See Shrinkage covariance estimation: LedoitWolf vs OAS and max-likelihood for
an example on how to fit a LedoitWolf object to data and
for visualizing the performances of the Ledoit-Wolf estimator in
terms of likelihood.
References:
[1]
O. Ledoit and M. Wolf, “A Well-Conditioned Estimator for Large-Dimensional
Covariance Matrices”, Journal of Multivariate Analysis, Volume 88, Issue 2,
February 2004, pages 365-411.
2.6.2.3. Oracle Approximating Shrinkage¶
Under the assumption that the data are Gaussian distributed, Chen et
al. [2] derived a formula aimed at choosing a shrinkage coefficient that
yields a smaller Mean Squared Error than the one given by Ledoit and
Wolf’s formula. The resulting estimator is known as the Oracle
Shrinkage Approximating estimator of the covariance.
The OAS estimator of the covariance matrix can be computed on a sample
with the oas function of the sklearn.covariance
package, or it can be otherwise obtained by fitting an OAS
object to the same sample.
Bias-variance trade-off when setting the shrinkage: comparing the
choices of Ledoit-Wolf and OAS estimators¶
References:
[2]
“Shrinkage algorithms for MMSE covariance estimation.”,
Chen, Y., Wiesel, A., Eldar, Y. C., & Hero, A. O.
IEEE Transactions on Signal Processing, 58(10), 5016-5029, 2010.
Examples:
See Shrinkage covariance estimation: LedoitWolf vs OAS and max-likelihood for
an example on how to fit an OAS object
to data.
See Ledoit-Wolf vs OAS estimation to visualize the
Mean Squared Error difference between a LedoitWolf and
an OAS estimator of the covariance.
- 2.6.3. Sparse inverse covariance¶
The matrix inverse of the covariance matrix, often called the precision
matrix, is proportional to the partial correlation matrix. It gives the
partial independence relationship. In other words, if two features are
independent conditionally on the others, the corresponding coefficient in
the precision matrix will be zero. This is why it makes sense to
estimate a sparse precision matrix: the estimation of the covariance
matrix is better conditioned by learning independence relations from
the data. This is known as covariance selection.
In the small-samples situation, in which n_samples is on the order
of n_features or smaller, sparse inverse covariance estimators tend to work
better than shrunk covariance estimators. However, in the opposite
situation, or for very correlated data, they can be numerically unstable.
In addition, unlike shrinkage estimators, sparse estimators are able to
recover off-diagonal structure.
The GraphicalLasso estimator uses an l1 penalty to enforce sparsity on
the precision matrix: the higher its alpha parameter, the more sparse
the precision matrix. The corresponding GraphicalLassoCV object uses
cross-validation to automatically set the alpha parameter.
A comparison of maximum likelihood, shrinkage and sparse estimates of
the covariance and precision matrix in the very small samples
settings.¶
Note
Structure recovery
Recovering a graphical structure from correlations in the data is a
challenging thing. If you are interested in such recovery keep in mind
that:
Recovery is easier from a correlation matrix than a covariance
matrix: standardize your observations before running GraphicalLasso
If the underlying graph has nodes with much more connections than
the average node, the algorithm will miss some of these connections.
If your number of observations is not large compared to the number
of edges in your underlying graph, you will not recover it.
Even if you are in favorable recovery conditions, the alpha
parameter chosen by cross-validation (e.g. using the
GraphicalLassoCV object) will lead to selecting too many edges.
However, the relevant edges will have heavier weights than the
irrelevant ones.
The mathematical formulation is the following:
\[\hat{K} = \mathrm{argmin}_K \big(
\mathrm{tr} S K - \mathrm{log} \mathrm{det} K
+ \alpha \|K\|_1
\big)\]
Where \(K\) is the precision matrix to be estimated, and \(S\) is the
sample covariance matrix. \(\|K\|_1\) is the sum of the absolute values of
off-diagonal coefficients of \(K\). The algorithm employed to solve this
problem is the GLasso algorithm, from the Friedman 2008 Biostatistics
paper. It is the same algorithm as in the R glasso package.
Examples:
Sparse inverse covariance estimation: example on synthetic
data showing some recovery of a structure, and comparing to other
covariance estimators.
Visualizing the stock market structure: example on real
stock market data, finding which symbols are most linked.
References:
Friedman et al, “Sparse inverse covariance estimation with the
graphical lasso”,
Biostatistics 9, pp 432, 2008
- 2.6.4. Robust Covariance Estimation¶
Real data sets are often subject to measurement or recording
errors. Regular but uncommon observations may also appear for a variety
of reasons. Observations which are very uncommon are called
outliers.
The empirical covariance estimator and the shrunk covariance
estimators presented above are very sensitive to the presence of
outliers in the data. Therefore, one should use robust
covariance estimators to estimate the covariance of its real data
sets. Alternatively, robust covariance estimators can be used to
perform outlier detection and discard/downweight some observations
according to further processing of the data.
The sklearn.covariance package implements a robust estimator of covariance,
the Minimum Covariance Determinant [3].
2.6.4.1. Minimum Covariance Determinant¶
The Minimum Covariance Determinant estimator is a robust estimator of
a data set’s covariance introduced by P.J. Rousseeuw in [3].  The idea
is to find a given proportion (h) of “good” observations which are not
outliers and compute their empirical covariance matrix.  This
empirical covariance matrix is then rescaled to compensate the
performed selection of observations (“consistency step”).  Having
computed the Minimum Covariance Determinant estimator, one can give
weights to observations according to their Mahalanobis distance,
leading to a reweighted estimate of the covariance matrix of the data
set (“reweighting step”).
Rousseeuw and Van Driessen [4] developed the FastMCD algorithm in order
to compute the Minimum Covariance Determinant. This algorithm is used
in scikit-learn when fitting an MCD object to data. The FastMCD
algorithm also computes a robust estimate of the data set location at
the same time.
Raw estimates can be accessed as raw_location_ and raw_covariance_
attributes of a MinCovDet robust covariance estimator object.
References:
[3]
(1,2)
P. J. Rousseeuw. Least median of squares regression.
J. Am Stat Ass, 79:871, 1984.
[4]
A Fast Algorithm for the Minimum Covariance Determinant Estimator,
1999, American Statistical Association and the American Society
for Quality, TECHNOMETRICS.
Examples:
See Robust vs Empirical covariance estimate for
an example on how to fit a MinCovDet object to data and see how
the estimate remains accurate despite the presence of outliers.
See Robust covariance estimation and Mahalanobis distances relevance to
visualize the difference between EmpiricalCovariance and
MinCovDet covariance estimators in terms of Mahalanobis distance
(so we get a better estimate of the precision matrix too).
Influence of outliers on location and covariance estimates
Separating inliers from outliers using a Mahalanobis distance
### 2.7. Novelty and Outlier Detection — scikit-learn 1.4.2 documentation

Install
User Guide
API
Examples
Community
Getting Started
Tutorial
What's new
Glossary
Development
FAQ
Support
Related packages
Roadmap
Governance
About us
GitHub
Other Versions and Download
More
Getting Started
Tutorial
What's new
Glossary
Development
FAQ
Support
Related packages
Roadmap
Governance
About us
GitHub
Other Versions and Download
Toggle Menu
PrevUp
Next
scikit-learn 1.4.2
Other versions
Please cite us if you use the software.
### 2.7. Novelty and Outlier Detection

- 2.7.1. Overview of outlier detection methods
- 2.7.2. Novelty Detection
2.7.2.1. Scaling up the One-Class SVM
- 2.7.3. Outlier Detection
2.7.3.1. Fitting an elliptic envelope
2.7.3.2. Isolation Forest
2.7.3.3. Local Outlier Factor
- 2.7.4. Novelty detection with Local Outlier Factor
### 2.7. Novelty and Outlier Detection¶

Many applications require being able to decide whether a new observation
belongs to the same distribution as existing observations (it is an
inlier), or should be considered as different (it is an outlier).
Often, this ability is used to clean real data sets. Two important
distinctions must be made:
outlier detection:
The training data contains outliers which are defined as observations that
are far from the others. Outlier detection estimators thus try to fit the
regions where the training data is the most concentrated, ignoring the
deviant observations.
novelty detection:
The training data is not polluted by outliers and we are interested in
detecting whether a new observation is an outlier. In this context an
outlier is also called a novelty.
Outlier detection and novelty detection are both used for anomaly
detection, where one is interested in detecting abnormal or unusual
observations. Outlier detection is then also known as unsupervised anomaly
detection and novelty detection as semi-supervised anomaly detection. In the
context of outlier detection, the outliers/anomalies cannot form a
dense cluster as available estimators assume that the outliers/anomalies are
located in low density regions. On the contrary, in the context of novelty
detection, novelties/anomalies can form a dense cluster as long as they are in
a low density region of the training data, considered as normal in this
context.
The scikit-learn project provides a set of machine learning tools that
can be used both for novelty or outlier detection. This strategy is
implemented with objects learning in an unsupervised way from the data:
estimator.fit(X_train)
new observations can then be sorted as inliers or outliers with a
predict method:
estimator.predict(X_test)
Inliers are labeled 1, while outliers are labeled -1. The predict method
makes use of a threshold on the raw scoring function computed by the
estimator. This scoring function is accessible through the score_samples
method, while the threshold can be controlled by the contamination
parameter.
The decision_function method is also defined from the scoring function,
in such a way that negative values are outliers and non-negative ones are
inliers:
estimator.decision_function(X_test)
Note that neighbors.LocalOutlierFactor does not support
predict, decision_function and score_samples methods by default
but only a fit_predict method, as this estimator was originally meant to
be applied for outlier detection. The scores of abnormality of the training
samples are accessible through the negative_outlier_factor_ attribute.
If you really want to use neighbors.LocalOutlierFactor for novelty
detection, i.e. predict labels or compute the score of abnormality of new
unseen data, you can instantiate the estimator with the novelty parameter
set to True before fitting the estimator. In this case, fit_predict is
not available.
Warning
Novelty detection with Local Outlier Factor
When novelty is set to True be aware that you must only use
predict, decision_function and score_samples on new unseen data
and not on the training samples as this would lead to wrong results.
I.e., the result of predict will not be the same as fit_predict.
The scores of abnormality of the training samples are always accessible
through the negative_outlier_factor_ attribute.
The behavior of neighbors.LocalOutlierFactor is summarized in the
following table.
Method
Outlier detection
Novelty detection
fit_predict
OK
Not available
predict
Not available
Use only on new data
decision_function
Not available
Use only on new data
score_samples
Use negative_outlier_factor_
Use only on new data
negative_outlier_factor_
OK
OK
- 2.7.1. Overview of outlier detection methods¶
A comparison of the outlier detection algorithms in scikit-learn. Local
Outlier Factor (LOF) does not show a decision boundary in black as it
has no predict method to be applied on new data when it is used for outlier
detection.
ensemble.IsolationForest and neighbors.LocalOutlierFactor
perform reasonably well on the data sets considered here.
The svm.OneClassSVM is known to be sensitive to outliers and thus
does not perform very well for outlier detection. That being said, outlier
detection in high-dimension, or without any assumptions on the distribution
of the inlying data is very challenging. svm.OneClassSVM may still
be used with outlier detection but requires fine-tuning of its hyperparameter
nu to handle outliers and prevent overfitting.
linear_model.SGDOneClassSVM provides an implementation of a
linear One-Class SVM with a linear complexity in the number of samples. This
implementation is here used with a kernel approximation technique to obtain
results similar to svm.OneClassSVM which uses a Gaussian kernel
by default. Finally, covariance.EllipticEnvelope assumes the data is
Gaussian and learns an ellipse. For more details on the different estimators
refer to the example
Comparing anomaly detection algorithms for outlier detection on toy datasets and the
sections hereunder.
Examples:
See Comparing anomaly detection algorithms for outlier detection on toy datasets
for a comparison of the svm.OneClassSVM, the
ensemble.IsolationForest, the
neighbors.LocalOutlierFactor and
covariance.EllipticEnvelope.
See Evaluation of outlier detection estimators
for an example showing how to evaluate outlier detection estimators,
the neighbors.LocalOutlierFactor and the
ensemble.IsolationForest, using ROC curves from
metrics.RocCurveDisplay.
- 2.7.2. Novelty Detection¶
Consider a data set of \(n\) observations from the same
distribution described by \(p\) features.  Consider now that we
add one more observation to that data set. Is the new observation so
different from the others that we can doubt it is regular? (i.e. does
it come from the same distribution?) Or on the contrary, is it so
similar to the other that we cannot distinguish it from the original
observations? This is the question addressed by the novelty detection
tools and methods.
In general, it is about to learn a rough, close frontier delimiting
the contour of the initial observations distribution, plotted in
embedding \(p\)-dimensional space. Then, if further observations
lay within the frontier-delimited subspace, they are considered as
coming from the same population than the initial
observations. Otherwise, if they lay outside the frontier, we can say
that they are abnormal with a given confidence in our assessment.
The One-Class SVM has been introduced by Schölkopf et al. for that purpose
and implemented in the Support Vector Machines module in the
svm.OneClassSVM object. It requires the choice of a
kernel and a scalar parameter to define a frontier.  The RBF kernel is
usually chosen although there exists no exact formula or algorithm to
set its bandwidth parameter. This is the default in the scikit-learn
implementation. The nu parameter, also known as the margin of
the One-Class SVM, corresponds to the probability of finding a new,
but regular, observation outside the frontier.
References:
Estimating the support of a high-dimensional distribution
Schölkopf, Bernhard, et al. Neural computation 13.7 (2001): 1443-1471.
Examples:
See One-class SVM with non-linear kernel (RBF) for visualizing the
frontier learned around some data by a
svm.OneClassSVM object.
Species distribution modeling
2.7.2.1. Scaling up the One-Class SVM¶
An online linear version of the One-Class SVM is implemented in
linear_model.SGDOneClassSVM. This implementation scales linearly with
the number of samples and can be used with a kernel approximation to
approximate the solution of a kernelized svm.OneClassSVM whose
complexity is at best quadratic in the number of samples. See section
Online One-Class SVM for more details.
Examples:
See One-Class SVM versus One-Class SVM using Stochastic Gradient Descent
for an illustration of the approximation of a kernelized One-Class SVM
with the linear_model.SGDOneClassSVM combined with kernel approximation.
- 2.7.3. Outlier Detection¶
Outlier detection is similar to novelty detection in the sense that
the goal is to separate a core of regular observations from some
polluting ones, called outliers. Yet, in the case of outlier
detection, we don’t have a clean data set representing the population
of regular observations that can be used to train any tool.
2.7.3.1. Fitting an elliptic envelope¶
One common way of performing outlier detection is to assume that the
regular data come from a known distribution (e.g. data are Gaussian
distributed). From this assumption, we generally try to define the
“shape” of the data, and can define outlying observations as
observations which stand far enough from the fit shape.
The scikit-learn provides an object
covariance.EllipticEnvelope that fits a robust covariance
estimate to the data, and thus fits an ellipse to the central data
points, ignoring points outside the central mode.
For instance, assuming that the inlier data are Gaussian distributed, it
will estimate the inlier location and covariance in a robust way (i.e.
without being influenced by outliers). The Mahalanobis distances
obtained from this estimate is used to derive a measure of outlyingness.
This strategy is illustrated below.
Examples:
See Robust covariance estimation and Mahalanobis distances relevance for
an illustration of the difference between using a standard
(covariance.EmpiricalCovariance) or a robust estimate
(covariance.MinCovDet) of location and covariance to
assess the degree of outlyingness of an observation.
References:
Rousseeuw, P.J., Van Driessen, K. “A fast algorithm for the minimum
covariance determinant estimator” Technometrics 41(3), 212 (1999)
2.7.3.2. Isolation Forest¶
One efficient way of performing outlier detection in high-dimensional datasets
is to use random forests.
The ensemble.IsolationForest ‘isolates’ observations by randomly selecting
a feature and then randomly selecting a split value between the maximum and
minimum values of the selected feature.
Since recursive partitioning can be represented by a tree structure, the
number of splittings required to isolate a sample is equivalent to the path
length from the root node to the terminating node.
This path length, averaged over a forest of such random trees, is a
measure of normality and our decision function.
Random partitioning produces noticeably shorter paths for anomalies.
Hence, when a forest of random trees collectively produce shorter path
lengths for particular samples, they are highly likely to be anomalies.
The implementation of ensemble.IsolationForest is based on an ensemble
of tree.ExtraTreeRegressor. Following Isolation Forest original paper,
the maximum depth of each tree is set to \(\lceil \log_2(n) \rceil\) where
\(n\) is the number of samples used to build the tree (see (Liu et al.,
2008) for more details).
This algorithm is illustrated below.
The ensemble.IsolationForest supports warm_start=True which
allows you to add more trees to an already fitted model:
>>> from sklearn.ensemble import IsolationForest
>>> import numpy as np
>>> X = np.array([[-1, -1], [-2, -1], [-3, -2], [0, 0], [-20, 50], [3, 5]])
>>> clf = IsolationForest(n_estimators=10, warm_start=True)
>>> clf.fit(X)  # fit 10 trees
>>> clf.set_params(n_estimators=20)  # add 10 more trees
>>> clf.fit(X)  # fit the added trees
Examples:
See IsolationForest example for
an illustration of the use of IsolationForest.
See Comparing anomaly detection algorithms for outlier detection on toy datasets
for a comparison of ensemble.IsolationForest with
neighbors.LocalOutlierFactor,
svm.OneClassSVM (tuned to perform like an outlier detection
method), linear_model.SGDOneClassSVM, and a covariance-based
outlier detection with covariance.EllipticEnvelope.
References:
Liu, Fei Tony, Ting, Kai Ming and Zhou, Zhi-Hua. “Isolation forest.”
Data Mining, 2008. ICDM’08. Eighth IEEE International Conference on.
2.7.3.3. Local Outlier Factor¶
Another efficient way to perform outlier detection on moderately high dimensional
datasets is to use the Local Outlier Factor (LOF) algorithm.
The neighbors.LocalOutlierFactor (LOF) algorithm computes a score
(called local outlier factor) reflecting the degree of abnormality of the
observations.
It measures the local density deviation of a given data point with respect to
its neighbors. The idea is to detect the samples that have a substantially
lower density than their neighbors.
In practice the local density is obtained from the k-nearest neighbors.
The LOF score of an observation is equal to the ratio of the
average local density of its k-nearest neighbors, and its own local density:
a normal instance is expected to have a local density similar to that of its
neighbors, while abnormal data are expected to have much smaller local density.
The number k of neighbors considered, (alias parameter n_neighbors) is typically
chosen 1) greater than the minimum number of objects a cluster has to contain,
so that other objects can be local outliers relative to this cluster, and 2)
smaller than the maximum number of close by objects that can potentially be
local outliers.
In practice, such information is generally not available, and taking
n_neighbors=20 appears to work well in general.
When the proportion of outliers is high (i.e. greater than 10 %, as in the
example below), n_neighbors should be greater (n_neighbors=35 in the example
below).
The strength of the LOF algorithm is that it takes both local and global
properties of datasets into consideration: it can perform well even in datasets
where abnormal samples have different underlying densities.
The question is not, how isolated the sample is, but how isolated it is
with respect to the surrounding neighborhood.
When applying LOF for outlier detection, there are no predict,
decision_function and score_samples methods but only a fit_predict
method. The scores of abnormality of the training samples are accessible
through the negative_outlier_factor_ attribute.
Note that predict, decision_function and score_samples can be used
on new unseen data when LOF is applied for novelty detection, i.e. when the
novelty parameter is set to True, but the result of predict may
differ from that of fit_predict. See Novelty detection with Local Outlier Factor.
This strategy is illustrated below.
Examples:
See Outlier detection with Local Outlier Factor (LOF)
for an illustration of the use of neighbors.LocalOutlierFactor.
See Comparing anomaly detection algorithms for outlier detection on toy datasets
for a comparison with other anomaly detection methods.
References:
Breunig, Kriegel, Ng, and Sander (2000)
LOF: identifying density-based local outliers.
Proc. ACM SIGMOD
- 2.7.4. Novelty detection with Local Outlier Factor¶
To use neighbors.LocalOutlierFactor for novelty detection, i.e.
predict labels or compute the score of abnormality of new unseen data, you
need to instantiate the estimator with the novelty parameter
set to True before fitting the estimator:
lof = LocalOutlierFactor(novelty=True)
lof.fit(X_train)
Note that fit_predict is not available in this case to avoid inconsistencies.
Warning
Novelty detection with Local Outlier Factor`
When novelty is set to True be aware that you must only use
predict, decision_function and score_samples on new unseen data
and not on the training samples as this would lead to wrong results.
I.e., the result of predict will not be the same as fit_predict.
The scores of abnormality of the training samples are always accessible
through the negative_outlier_factor_ attribute.
Novelty detection with Local Outlier Factor is illustrated below.
### 2.8. Density Estimation — scikit-learn 1.4.2 documentation

Install
User Guide
API
Examples
Community
Getting Started
Tutorial
What's new
Glossary
Development
FAQ
Support
Related packages
Roadmap
Governance
About us
GitHub
Other Versions and Download
More
Getting Started
Tutorial
What's new
Glossary
Development
FAQ
Support
Related packages
Roadmap
Governance
About us
GitHub
Other Versions and Download
Toggle Menu
PrevUp
Next
scikit-learn 1.4.2
Other versions
Please cite us if you use the software.
### 2.8. Density Estimation

- 2.8.1. Density Estimation: Histograms
- 2.8.2. Kernel Density Estimation
### 2.8. Density Estimation¶

Density estimation walks the line between unsupervised learning, feature
engineering, and data modeling.  Some of the most popular and useful
density estimation techniques are mixture models such as
Gaussian Mixtures (GaussianMixture), and
neighbor-based approaches such as the kernel density estimate
(KernelDensity).
Gaussian Mixtures are discussed more fully in the context of
clustering, because the technique is also useful as
an unsupervised clustering scheme.
Density estimation is a very simple concept, and most people are already
familiar with one common density estimation technique: the histogram.
- 2.8.1. Density Estimation: Histograms¶
A histogram is a simple visualization of data where bins are defined, and the
number of data points within each bin is tallied.  An example of a histogram
can be seen in the upper-left panel of the following figure:
A major problem with histograms, however, is that the choice of binning can
have a disproportionate effect on the resulting visualization.  Consider the
upper-right panel of the above figure.  It shows a histogram over the same
data, with the bins shifted right.  The results of the two visualizations look
entirely different, and might lead to different interpretations of the data.
Intuitively, one can also think of a histogram as a stack of blocks, one block
per point.  By stacking the blocks in the appropriate grid space, we recover
the histogram.  But what if, instead of stacking the blocks on a regular grid,
we center each block on the point it represents, and sum the total height at
each location?  This idea leads to the lower-left visualization.  It is perhaps
not as clean as a histogram, but the fact that the data drive the block
locations mean that it is a much better representation of the underlying
data.
This visualization is an example of a kernel density estimation, in this case
with a top-hat kernel (i.e. a square block at each point).  We can recover a
smoother distribution by using a smoother kernel.  The bottom-right plot shows
a Gaussian kernel density estimate, in which each point contributes a Gaussian
curve to the total.  The result is a smooth density estimate which is derived
from the data, and functions as a powerful non-parametric model of the
distribution of points.
- 2.8.2. Kernel Density Estimation¶
Kernel density estimation in scikit-learn is implemented in the
KernelDensity estimator, which uses the
Ball Tree or KD Tree for efficient queries (see Nearest Neighbors for
a discussion of these).  Though the above example
uses a 1D data set for simplicity, kernel density estimation can be
performed in any number of dimensions, though in practice the curse of
dimensionality causes its performance to degrade in high dimensions.
In the following figure, 100 points are drawn from a bimodal distribution,
and the kernel density estimates are shown for three choices of kernels:
It’s clear how the kernel shape affects the smoothness of the resulting
distribution.  The scikit-learn kernel density estimator can be used as
follows:
>>> from sklearn.neighbors import KernelDensity
>>> import numpy as np
>>> X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])
>>> kde = KernelDensity(kernel='gaussian', bandwidth=0.2).fit(X)
>>> kde.score_samples(X)
array([-0.41075698, -0.41075698, -0.41076071, -0.41075698, -0.41075698,
-0.41076071])
Here we have used kernel='gaussian', as seen above.
Mathematically, a kernel is a positive function \(K(x;h)\)
which is controlled by the bandwidth parameter \(h\).
Given this kernel form, the density estimate at a point \(y\) within
a group of points \(x_i; i=1\cdots N\) is given by:
\[\rho_K(y) = \sum_{i=1}^{N} K(y - x_i; h)\]
The bandwidth here acts as a smoothing parameter, controlling the tradeoff
between bias and variance in the result.  A large bandwidth leads to a very
smooth (i.e. high-bias) density distribution.  A small bandwidth leads
to an unsmooth (i.e. high-variance) density distribution.
The parameter bandwidth controls this smoothing. One can either set
manually this parameter or use Scott’s and Silvermann’s estimation
methods.
KernelDensity implements several common kernel
forms, which are shown in the following figure:
kernels’ mathematical expressions
Click for more details
¶
The form of these kernels is as follows:
Gaussian kernel (kernel = 'gaussian')
\(K(x; h) \propto \exp(- \frac{x^2}{2h^2} )\)
Tophat kernel (kernel = 'tophat')
\(K(x; h) \propto 1\) if \(x < h\)
Epanechnikov kernel (kernel = 'epanechnikov')
\(K(x; h) \propto 1 - \frac{x^2}{h^2}\)
Exponential kernel (kernel = 'exponential')
\(K(x; h) \propto \exp(-x/h)\)
Linear kernel (kernel = 'linear')
\(K(x; h) \propto 1 - x/h\) if \(x < h\)
Cosine kernel (kernel = 'cosine')
\(K(x; h) \propto \cos(\frac{\pi x}{2h})\) if \(x < h\)
The kernel density estimator can be used with any of the valid distance
metrics (see DistanceMetric for a list of
available metrics), though the results are properly normalized only
for the Euclidean metric.  One particularly useful metric is the
Haversine distance
which measures the angular distance between points on a sphere.  Here
is an example of using a kernel density estimate for a visualization
of geospatial data, in this case the distribution of observations of two
different species on the South American continent:
One other useful application of kernel density estimation is to learn a
non-parametric generative model of a dataset in order to efficiently
draw new samples from this generative model.
Here is an example of using this process to
create a new set of hand-written digits, using a Gaussian kernel learned
on a PCA projection of the data:
The “new” data consists of linear combinations of the input data, with weights
probabilistically drawn given the KDE model.
Examples:
Simple 1D Kernel Density Estimation: computation of simple kernel
density estimates in one dimension.
Kernel Density Estimation: an example of using
Kernel Density estimation to learn a generative model of the hand-written
digits data, and drawing new samples from this model.
Kernel Density Estimate of Species Distributions: an example of Kernel Density
estimation using the Haversine distance metric to visualize geospatial data
### 2.9. Neural network models (unsupervised) — scikit-learn 1.4.2 documentation

Install
User Guide
API
Examples
Community
Getting Started
Tutorial
What's new
Glossary
Development
FAQ
Support
Related packages
Roadmap
Governance
About us
GitHub
Other Versions and Download
More
Getting Started
Tutorial
What's new
Glossary
Development
FAQ
Support
Related packages
Roadmap
Governance
About us
GitHub
Other Versions and Download
Toggle Menu
PrevUp
Next
scikit-learn 1.4.2
Other versions
Please cite us if you use the software.
### 2.9. Neural network models (unsupervised)

- 2.9.1. Restricted Boltzmann machines
2.9.1.1. Graphical model and parametrization
2.9.1.2. Bernoulli Restricted Boltzmann machines
2.9.1.3. Stochastic Maximum Likelihood learning
### 2.9. Neural network models (unsupervised)¶

- 2.9.1. Restricted Boltzmann machines¶
Restricted Boltzmann machines (RBM) are unsupervised nonlinear feature learners
based on a probabilistic model. The features extracted by an RBM or a hierarchy
of RBMs often give good results when fed into a linear classifier such as a
linear SVM or a perceptron.
The model makes assumptions regarding the distribution of inputs. At the moment,
scikit-learn only provides BernoulliRBM, which assumes the inputs are
either binary values or values between 0 and 1, each encoding the probability
that the specific feature would be turned on.
The RBM tries to maximize the likelihood of the data using a particular
graphical model. The parameter learning algorithm used (Stochastic
Maximum Likelihood) prevents the representations from straying far
from the input data, which makes them capture interesting regularities, but
makes the model less useful for small datasets, and usually not useful for
density estimation.
The method gained popularity for initializing deep neural networks with the
weights of independent RBMs. This method is known as unsupervised pre-training.
Examples:
Restricted Boltzmann Machine features for digit classification
2.9.1.1. Graphical model and parametrization¶
The graphical model of an RBM is a fully-connected bipartite graph.
The nodes are random variables whose states depend on the state of the other
nodes they are connected to. The model is therefore parameterized by the
weights of the connections, as well as one intercept (bias) term for each
visible and hidden unit, omitted from the image for simplicity.
The energy function measures the quality of a joint assignment:
\[E(\mathbf{v}, \mathbf{h}) = -\sum_i \sum_j w_{ij}v_ih_j - \sum_i b_iv_i
- \sum_j c_jh_j\]
In the formula above, \(\mathbf{b}\) and \(\mathbf{c}\) are the
intercept vectors for the visible and hidden layers, respectively. The
joint probability of the model is defined in terms of the energy:
\[P(\mathbf{v}, \mathbf{h}) = \frac{e^{-E(\mathbf{v}, \mathbf{h})}}{Z}\]
The word restricted refers to the bipartite structure of the model, which
prohibits direct interaction between hidden units, or between visible units.
This means that the following conditional independencies are assumed:
\[\begin{split}h_i \bot h_j | \mathbf{v} \\
v_i \bot v_j | \mathbf{h}\end{split}\]
The bipartite structure allows for the use of efficient block Gibbs sampling for
inference.
2.9.1.2. Bernoulli Restricted Boltzmann machines¶
In the BernoulliRBM, all units are binary stochastic units. This
means that the input data should either be binary, or real-valued between 0 and
1 signifying the probability that the visible unit would turn on or off. This
is a good model for character recognition, where the interest is on which
pixels are active and which aren’t. For images of natural scenes it no longer
fits because of background, depth and the tendency of neighbouring pixels to
take the same values.
The conditional probability distribution of each unit is given by the
logistic sigmoid activation function of the input it receives:
\[\begin{split}P(v_i=1|\mathbf{h}) = \sigma(\sum_j w_{ij}h_j + b_i) \\
P(h_i=1|\mathbf{v}) = \sigma(\sum_i w_{ij}v_i + c_j)\end{split}\]
where \(\sigma\) is the logistic sigmoid function:
\[\sigma(x) = \frac{1}{1 + e^{-x}}\]
2.9.1.3. Stochastic Maximum Likelihood learning¶
The training algorithm implemented in BernoulliRBM is known as
Stochastic Maximum Likelihood (SML) or Persistent Contrastive Divergence
(PCD). Optimizing maximum likelihood directly is infeasible because of
the form of the data likelihood:
\[\log P(v) = \log \sum_h e^{-E(v, h)} - \log \sum_{x, y} e^{-E(x, y)}\]
For simplicity the equation above is written for a single training example.
The gradient with respect to the weights is formed of two terms corresponding to
the ones above. They are usually known as the positive gradient and the negative
gradient, because of their respective signs.  In this implementation, the
gradients are estimated over mini-batches of samples.
In maximizing the log-likelihood, the positive gradient makes the model prefer
hidden states that are compatible with the observed training data. Because of
the bipartite structure of RBMs, it can be computed efficiently. The
negative gradient, however, is intractable. Its goal is to lower the energy of
joint states that the model prefers, therefore making it stay true to the data.
It can be approximated by Markov chain Monte Carlo using block Gibbs sampling by
iteratively sampling each of \(v\) and \(h\) given the other, until the
chain mixes. Samples generated in this way are sometimes referred as fantasy
particles. This is inefficient and it is difficult to determine whether the
Markov chain mixes.
The Contrastive Divergence method suggests to stop the chain after a small
number of iterations, \(k\), usually even 1. This method is fast and has
low variance, but the samples are far from the model distribution.
Persistent Contrastive Divergence addresses this. Instead of starting a new
chain each time the gradient is needed, and performing only one Gibbs sampling
step, in PCD we keep a number of chains (fantasy particles) that are updated
\(k\) Gibbs steps after each weight update. This allows the particles to
explore the space more thoroughly.
References:
“A fast learning algorithm for deep belief nets”
G. Hinton, S. Osindero, Y.-W. Teh, 2006
“Training Restricted Boltzmann Machines using Approximations to
the Likelihood Gradient”
T. Tieleman, 2008
### 3.1. Cross-validation: evaluating estimator performance — scikit-learn 1.4.2 documentation

Install
User Guide
API
Examples
Community
Getting Started
Tutorial
What's new
Glossary
Development
FAQ
Support
Related packages
Roadmap
Governance
About us
GitHub
Other Versions and Download
More
Getting Started
Tutorial
What's new
Glossary
Development
FAQ
Support
Related packages
Roadmap
Governance
About us
GitHub
Other Versions and Download
Toggle Menu
PrevUp
Next
scikit-learn 1.4.2
Other versions
Please cite us if you use the software.
### 3.1. Cross-validation: evaluating estimator performance

- 3.1.1. Computing cross-validated metrics
3.1.1.1. The cross_validate function and multiple metric evaluation
3.1.1.2. Obtaining predictions by cross-validation
- 3.1.2. Cross validation iterators
3.1.2.1. Cross-validation iterators for i.i.d. data
3.1.2.1.1. K-fold
3.1.2.1.2. Repeated K-Fold
3.1.2.1.3. Leave One Out (LOO)
3.1.2.1.4. Leave P Out (LPO)
3.1.2.1.5. Random permutations cross-validation a.k.a. Shuffle & Split
3.1.2.2. Cross-validation iterators with stratification based on class labels
3.1.2.2.1. Stratified k-fold
3.1.2.2.2. Stratified Shuffle Split
3.1.2.3. Cross-validation iterators for grouped data
3.1.2.3.1. Group k-fold
3.1.2.3.2. StratifiedGroupKFold
3.1.2.3.3. Leave One Group Out
3.1.2.3.4. Leave P Groups Out
3.1.2.3.5. Group Shuffle Split
3.1.2.4. Predefined fold-splits / Validation-sets
3.1.2.5. Using cross-validation iterators to split train and test
3.1.2.6. Cross validation of time series data
3.1.2.6.1. Time Series Split
- 3.1.3. A note on shuffling
- 3.1.4. Cross validation and model selection
- 3.1.5. Permutation test score
### 3.1. Cross-validation: evaluating estimator performance¶

Learning the parameters of a prediction function and testing it on the
same data is a methodological mistake: a model that would just repeat
the labels of the samples that it has just seen would have a perfect
score but would fail to predict anything useful on yet-unseen data.
This situation is called overfitting.
To avoid it, it is common practice when performing
a (supervised) machine learning experiment
to hold out part of the available data as a test set X_test, y_test.
Note that the word “experiment” is not intended
to denote academic use only,
because even in commercial settings
machine learning usually starts out experimentally.
Here is a flowchart of typical cross validation workflow in model training.
The best parameters can be determined by
grid search techniques.
In scikit-learn a random split into training and test sets
can be quickly computed with the train_test_split helper function.
Let’s load the iris data set to fit a linear support vector machine on it:
>>> import numpy as np
>>> from sklearn.model_selection import train_test_split
>>> from sklearn import datasets
>>> from sklearn import svm
>>> X, y = datasets.load_iris(return_X_y=True)
>>> X.shape, y.shape
((150, 4), (150,))
We can now quickly sample a training set while holding out 40% of the
data for testing (evaluating) our classifier:
>>> X_train, X_test, y_train, y_test = train_test_split(
...     X, y, test_size=0.4, random_state=0)
>>> X_train.shape, y_train.shape
((90, 4), (90,))
>>> X_test.shape, y_test.shape
((60, 4), (60,))
>>> clf = svm.SVC(kernel='linear', C=1).fit(X_train, y_train)
>>> clf.score(X_test, y_test)
0.96...
When evaluating different settings (“hyperparameters”) for estimators,
such as the C setting that must be manually set for an SVM,
there is still a risk of overfitting on the test set
because the parameters can be tweaked until the estimator performs optimally.
This way, knowledge about the test set can “leak” into the model
and evaluation metrics no longer report on generalization performance.
To solve this problem, yet another part of the dataset can be held out
as a so-called “validation set”: training proceeds on the training set,
after which evaluation is done on the validation set,
and when the experiment seems to be successful,
final evaluation can be done on the test set.
However, by partitioning the available data into three sets,
we drastically reduce the number of samples
which can be used for learning the model,
and the results can depend on a particular random choice for the pair of
(train, validation) sets.
A solution to this problem is a procedure called
cross-validation
(CV for short).
A test set should still be held out for final evaluation,
but the validation set is no longer needed when doing CV.
In the basic approach, called k-fold CV,
the training set is split into k smaller sets
(other approaches are described below,
but generally follow the same principles).
The following procedure is followed for each of the k “folds”:
A model is trained using \(k-1\) of the folds as training data;
the resulting model is validated on the remaining part of the data
(i.e., it is used as a test set to compute a performance measure
such as accuracy).
The performance measure reported by k-fold cross-validation
is then the average of the values computed in the loop.
This approach can be computationally expensive,
but does not waste too much data
(as is the case when fixing an arbitrary validation set),
which is a major advantage in problems such as inverse inference
where the number of samples is very small.
- 3.1.1. Computing cross-validated metrics¶
The simplest way to use cross-validation is to call the
cross_val_score helper function on the estimator and the dataset.
The following example demonstrates how to estimate the accuracy of a linear
kernel support vector machine on the iris dataset by splitting the data, fitting
a model and computing the score 5 consecutive times (with different splits each
time):
>>> from sklearn.model_selection import cross_val_score
>>> clf = svm.SVC(kernel='linear', C=1, random_state=42)
>>> scores = cross_val_score(clf, X, y, cv=5)
>>> scores
array([0.96..., 1. , 0.96..., 0.96..., 1. ])
The mean score and the standard deviation are hence given by:
>>> print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))
0.98 accuracy with a standard deviation of 0.02
By default, the score computed at each CV iteration is the score
method of the estimator. It is possible to change this by using the
scoring parameter:
>>> from sklearn import metrics
>>> scores = cross_val_score(
...     clf, X, y, cv=5, scoring='f1_macro')
>>> scores
array([0.96..., 1.  ..., 0.96..., 0.96..., 1.        ])
See The scoring parameter: defining model evaluation rules for details.
In the case of the Iris dataset, the samples are balanced across target
classes hence the accuracy and the F1-score are almost equal.
When the cv argument is an integer, cross_val_score uses the
KFold or StratifiedKFold strategies by default, the latter
being used if the estimator derives from ClassifierMixin.
It is also possible to use other cross validation strategies by passing a cross
validation iterator instead, for instance:
>>> from sklearn.model_selection import ShuffleSplit
>>> n_samples = X.shape[0]
>>> cv = ShuffleSplit(n_splits=5, test_size=0.3, random_state=0)
>>> cross_val_score(clf, X, y, cv=cv)
array([0.977..., 0.977..., 1.  ..., 0.955..., 1.        ])
Another option is to use an iterable yielding (train, test) splits as arrays of
indices, for example:
>>> def custom_cv_2folds(X):
...     n = X.shape[0]
...     i = 1
...     while i <= 2:
...         idx = np.arange(n * (i - 1) / 2, n * i / 2, dtype=int)
...         yield idx, idx
...         i += 1
...
>>> custom_cv = custom_cv_2folds(X)
>>> cross_val_score(clf, X, y, cv=custom_cv)
array([1.        , 0.973...])
Data transformation with held out data
Just as it is important to test a predictor on data held-out from
training, preprocessing (such as standardization, feature selection, etc.)
and similar data transformations similarly should
be learnt from a training set and applied to held-out data for prediction:
>>> from sklearn import preprocessing
>>> X_train, X_test, y_train, y_test = train_test_split(
...     X, y, test_size=0.4, random_state=0)
>>> scaler = preprocessing.StandardScaler().fit(X_train)
>>> X_train_transformed = scaler.transform(X_train)
>>> clf = svm.SVC(C=1).fit(X_train_transformed, y_train)
>>> X_test_transformed = scaler.transform(X_test)
>>> clf.score(X_test_transformed, y_test)
0.9333...
A Pipeline makes it easier to compose
estimators, providing this behavior under cross-validation:
>>> from sklearn.pipeline import make_pipeline
>>> clf = make_pipeline(preprocessing.StandardScaler(), svm.SVC(C=1))
>>> cross_val_score(clf, X, y, cv=cv)
array([0.977..., 0.933..., 0.955..., 0.933..., 0.977...])
See Pipelines and composite estimators.
3.1.1.1. The cross_validate function and multiple metric evaluation¶
The cross_validate function differs from cross_val_score in
two ways:
It allows specifying multiple metrics for evaluation.
It returns a dict containing fit-times, score-times
(and optionally training scores, fitted estimators, train-test split indices)
in addition to the test score.
For single metric evaluation, where the scoring parameter is a string,
callable or None, the keys will be - ['test_score', 'fit_time', 'score_time']
And for multiple metric evaluation, the return value is a dict with the
following keys -
['test_<scorer1_name>', 'test_<scorer2_name>', 'test_<scorer...>', 'fit_time', 'score_time']
return_train_score is set to False by default to save computation time.
To evaluate the scores on the training set as well you need to set it to
True. You may also retain the estimator fitted on each training set by
setting return_estimator=True. Similarly, you may set
return_indices=True to retain the training and testing indices used to split
the dataset into train and test sets for each cv split.
The multiple metrics can be specified either as a list, tuple or set of
predefined scorer names:
>>> from sklearn.model_selection import cross_validate
>>> from sklearn.metrics import recall_score
>>> scoring = ['precision_macro', 'recall_macro']
>>> clf = svm.SVC(kernel='linear', C=1, random_state=0)
>>> scores = cross_validate(clf, X, y, scoring=scoring)
>>> sorted(scores.keys())
['fit_time', 'score_time', 'test_precision_macro', 'test_recall_macro']
>>> scores['test_recall_macro']
array([0.96..., 1.  ..., 0.96..., 0.96..., 1.        ])
Or as a dict mapping scorer name to a predefined or custom scoring function:
>>> from sklearn.metrics import make_scorer
>>> scoring = {'prec_macro': 'precision_macro',
...            'rec_macro': make_scorer(recall_score, average='macro')}
>>> scores = cross_validate(clf, X, y, scoring=scoring,
...                         cv=5, return_train_score=True)
>>> sorted(scores.keys())
['fit_time', 'score_time', 'test_prec_macro', 'test_rec_macro',
'train_prec_macro', 'train_rec_macro']
>>> scores['train_rec_macro']
array([0.97..., 0.97..., 0.99..., 0.98..., 0.98...])
Here is an example of cross_validate using a single metric:
>>> scores = cross_validate(clf, X, y,
...                         scoring='precision_macro', cv=5,
...                         return_estimator=True)
>>> sorted(scores.keys())
['estimator', 'fit_time', 'score_time', 'test_score']
3.1.1.2. Obtaining predictions by cross-validation¶
The function cross_val_predict has a similar interface to
cross_val_score, but returns, for each element in the input, the
prediction that was obtained for that element when it was in the test set. Only
cross-validation strategies that assign all elements to a test set exactly once
can be used (otherwise, an exception is raised).
Warning
Note on inappropriate usage of cross_val_predict
The result of cross_val_predict may be different from those
obtained using cross_val_score as the elements are grouped in
different ways. The function cross_val_score takes an average
over cross-validation folds, whereas cross_val_predict simply
returns the labels (or probabilities) from several distinct models
undistinguished. Thus, cross_val_predict is not an appropriate
measure of generalization error.
The function cross_val_predict is appropriate for:
Visualization of predictions obtained from different models.
Model blending: When predictions of one supervised estimator are used to
train another estimator in ensemble methods.
The available cross validation iterators are introduced in the following
section.
Examples
Receiver Operating Characteristic (ROC) with cross validation,
Recursive feature elimination with cross-validation,
Custom refit strategy of a grid search with cross-validation,
Sample pipeline for text feature extraction and evaluation,
Plotting Cross-Validated Predictions,
Nested versus non-nested cross-validation.
- 3.1.2. Cross validation iterators¶
The following sections list utilities to generate indices
that can be used to generate dataset splits according to different cross
validation strategies.
3.1.2.1. Cross-validation iterators for i.i.d. data¶
Assuming that some data is Independent and Identically Distributed (i.i.d.) is
making the assumption that all samples stem from the same generative process
and that the generative process is assumed to have no memory of past generated
samples.
The following cross-validators can be used in such cases.
Note
While i.i.d. data is a common assumption in machine learning theory, it rarely
holds in practice. If one knows that the samples have been generated using a
time-dependent process, it is safer to
use a time-series aware cross-validation scheme.
Similarly, if we know that the generative process has a group structure
(samples collected from different subjects, experiments, measurement
devices), it is safer to use group-wise cross-validation.
3.1.2.1.1. K-fold¶
KFold divides all the samples in \(k\) groups of samples,
called folds (if \(k = n\), this is equivalent to the Leave One
Out strategy), of equal sizes (if possible). The prediction function is
learned using \(k - 1\) folds, and the fold left out is used for test.
Example of 2-fold cross-validation on a dataset with 4 samples:
>>> import numpy as np
>>> from sklearn.model_selection import KFold
>>> X = ["a", "b", "c", "d"]
>>> kf = KFold(n_splits=2)
>>> for train, test in kf.split(X):
...     print("%s %s" % (train, test))
[2 3] [0 1]
[0 1] [2 3]
Here is a visualization of the cross-validation behavior. Note that
KFold is not affected by classes or groups.
Each fold is constituted by two arrays: the first one is related to the
training set, and the second one to the test set.
Thus, one can create the training/test sets using numpy indexing:
>>> X = np.array([[0., 0.], [1., 1.], [-1., -1.], [2., 2.]])
>>> y = np.array([0, 1, 0, 1])
>>> X_train, X_test, y_train, y_test = X[train], X[test], y[train], y[test]
3.1.2.1.2. Repeated K-Fold¶
RepeatedKFold repeats K-Fold n times. It can be used when one
requires to run KFold n times, producing different splits in
each repetition.
Example of 2-fold K-Fold repeated 2 times:
>>> import numpy as np
>>> from sklearn.model_selection import RepeatedKFold
>>> X = np.array([[1, 2], [3, 4], [1, 2], [3, 4]])
>>> random_state = 12883823
>>> rkf = RepeatedKFold(n_splits=2, n_repeats=2, random_state=random_state)
>>> for train, test in rkf.split(X):
...     print("%s %s" % (train, test))
...
[2 3] [0 1]
[0 1] [2 3]
[0 2] [1 3]
[1 3] [0 2]
Similarly, RepeatedStratifiedKFold repeats Stratified K-Fold n times
with different randomization in each repetition.
3.1.2.1.3. Leave One Out (LOO)¶
LeaveOneOut (or LOO) is a simple cross-validation. Each learning
set is created by taking all the samples except one, the test set being
the sample left out. Thus, for \(n\) samples, we have \(n\) different
training sets and \(n\) different tests set. This cross-validation
procedure does not waste much data as only one sample is removed from the
training set:
>>> from sklearn.model_selection import LeaveOneOut
>>> X = [1, 2, 3, 4]
>>> loo = LeaveOneOut()
>>> for train, test in loo.split(X):
...     print("%s %s" % (train, test))
[1 2 3] [0]
[0 2 3] [1]
[0 1 3] [2]
[0 1 2] [3]
Potential users of LOO for model selection should weigh a few known caveats.
When compared with \(k\)-fold cross validation, one builds \(n\) models
from \(n\) samples instead of \(k\) models, where \(n > k\).
Moreover, each is trained on \(n - 1\) samples rather than
\((k-1) n / k\). In both ways, assuming \(k\) is not too large
and \(k < n\), LOO is more computationally expensive than \(k\)-fold
cross validation.
In terms of accuracy, LOO often results in high variance as an estimator for the
test error. Intuitively, since \(n - 1\) of
the \(n\) samples are used to build each model, models constructed from
folds are virtually identical to each other and to the model built from the
entire training set.
However, if the learning curve is steep for the training size in question,
then 5- or 10- fold cross validation can overestimate the generalization error.
As a general rule, most authors, and empirical evidence, suggest that 5- or 10-
fold cross validation should be preferred to LOO.
References:
http://www.faqs.org/faqs/ai-faq/neural-nets/part3/section-12.html;
T. Hastie, R. Tibshirani, J. Friedman,  The Elements of Statistical Learning, Springer 2009
L. Breiman, P. Spector Submodel selection and evaluation in regression: The X-random case, International Statistical Review 1992;
R. Kohavi, A Study of Cross-Validation and Bootstrap for Accuracy Estimation and Model Selection, Intl. Jnt. Conf. AI
R. Bharat Rao, G. Fung, R. Rosales, On the Dangers of Cross-Validation. An Experimental Evaluation, SIAM 2008;
G. James, D. Witten, T. Hastie, R Tibshirani, An Introduction to
Statistical Learning, Springer 2013.
3.1.2.1.4. Leave P Out (LPO)¶
LeavePOut is very similar to LeaveOneOut as it creates all
the possible training/test sets by removing \(p\) samples from the complete
set. For \(n\) samples, this produces \({n \choose p}\) train-test
pairs. Unlike LeaveOneOut and KFold, the test sets will
overlap for \(p > 1\).
Example of Leave-2-Out on a dataset with 4 samples:
>>> from sklearn.model_selection import LeavePOut
>>> X = np.ones(4)
>>> lpo = LeavePOut(p=2)
>>> for train, test in lpo.split(X):
...     print("%s %s" % (train, test))
[2 3] [0 1]
[1 3] [0 2]
[1 2] [0 3]
[0 3] [1 2]
[0 2] [1 3]
[0 1] [2 3]
3.1.2.1.5. Random permutations cross-validation a.k.a. Shuffle & Split¶
The ShuffleSplit iterator will generate a user defined number of
independent train / test dataset splits. Samples are first shuffled and
then split into a pair of train and test sets.
It is possible to control the randomness for reproducibility of the
results by explicitly seeding the random_state pseudo random number
generator.
Here is a usage example:
>>> from sklearn.model_selection import ShuffleSplit
>>> X = np.arange(10)
>>> ss = ShuffleSplit(n_splits=5, test_size=0.25, random_state=0)
>>> for train_index, test_index in ss.split(X):
...     print("%s %s" % (train_index, test_index))
[9 1 6 7 3 0 5] [2 8 4]
[2 9 8 0 6 7 4] [3 5 1]
[4 5 1 0 6 9 7] [2 3 8]
[2 7 5 8 0 3 4] [6 1 9]
[4 1 0 6 8 9 3] [5 2 7]
Here is a visualization of the cross-validation behavior. Note that
ShuffleSplit is not affected by classes or groups.
ShuffleSplit is thus a good alternative to KFold cross
validation that allows a finer control on the number of iterations and
the proportion of samples on each side of the train / test split.
3.1.2.2. Cross-validation iterators with stratification based on class labels¶
Some classification problems can exhibit a large imbalance in the distribution
of the target classes: for instance there could be several times more negative
samples than positive samples. In such cases it is recommended to use
stratified sampling as implemented in StratifiedKFold and
StratifiedShuffleSplit to ensure that relative class frequencies is
approximately preserved in each train and validation fold.
3.1.2.2.1. Stratified k-fold¶
StratifiedKFold is a variation of k-fold which returns stratified
folds: each set contains approximately the same percentage of samples of each
target class as the complete set.
Here is an example of stratified 3-fold cross-validation on a dataset with 50 samples from
two unbalanced classes.  We show the number of samples in each class and compare with
KFold.
>>> from sklearn.model_selection import StratifiedKFold, KFold
>>> import numpy as np
>>> X, y = np.ones((50, 1)), np.hstack(([0] * 45, [1] * 5))
>>> skf = StratifiedKFold(n_splits=3)
>>> for train, test in skf.split(X, y):
...     print('train -  {}   |   test -  {}'.format(
...         np.bincount(y[train]), np.bincount(y[test])))
train -  [30  3]   |   test -  [15  2]
train -  [30  3]   |   test -  [15  2]
train -  [30  4]   |   test -  [15  1]
>>> kf = KFold(n_splits=3)
>>> for train, test in kf.split(X, y):
...     print('train -  {}   |   test -  {}'.format(
...         np.bincount(y[train]), np.bincount(y[test])))
train -  [28  5]   |   test -  [17]
train -  [28  5]   |   test -  [17]
train -  [34]   |   test -  [11  5]
We can see that StratifiedKFold preserves the class ratios
(approximately 1 / 10) in both train and test dataset.
Here is a visualization of the cross-validation behavior.
RepeatedStratifiedKFold can be used to repeat Stratified K-Fold n times
with different randomization in each repetition.
3.1.2.2.2. Stratified Shuffle Split¶
StratifiedShuffleSplit is a variation of ShuffleSplit, which returns
stratified splits, i.e which creates splits by preserving the same
percentage for each target class as in the complete set.
Here is a visualization of the cross-validation behavior.
3.1.2.3. Cross-validation iterators for grouped data¶
The i.i.d. assumption is broken if the underlying generative process yield
groups of dependent samples.
Such a grouping of data is domain specific. An example would be when there is
medical data collected from multiple patients, with multiple samples taken from
each patient. And such data is likely to be dependent on the individual group.
In our example, the patient id for each sample will be its group identifier.
In this case we would like to know if a model trained on a particular set of
groups generalizes well to the unseen groups. To measure this, we need to
ensure that all the samples in the validation fold come from groups that are
not represented at all in the paired training fold.
The following cross-validation splitters can be used to do that.
The grouping identifier for the samples is specified via the groups
parameter.
3.1.2.3.1. Group k-fold¶
GroupKFold is a variation of k-fold which ensures that the same group is
not represented in both testing and training sets. For example if the data is
obtained from different subjects with several samples per-subject and if the
model is flexible enough to learn from highly person specific features it
could fail to generalize to new subjects. GroupKFold makes it possible
to detect this kind of overfitting situations.
Imagine you have three subjects, each with an associated number from 1 to 3:
>>> from sklearn.model_selection import GroupKFold
>>> X = [0.1, 0.2, 2.2, 2.4, 2.3, 4.55, 5.8, 8.8, 9, 10]
>>> y = ["a", "b", "b", "b", "c", "c", "c", "d", "d", "d"]
>>> groups = [1, 1, 1, 2, 2, 2, 3, 3, 3, 3]
>>> gkf = GroupKFold(n_splits=3)
>>> for train, test in gkf.split(X, y, groups=groups):
...     print("%s %s" % (train, test))
[0 1 2 3 4 5] [6 7 8 9]
[0 1 2 6 7 8 9] [3 4 5]
[3 4 5 6 7 8 9] [0 1 2]
Each subject is in a different testing fold, and the same subject is never in
both testing and training. Notice that the folds do not have exactly the same
size due to the imbalance in the data. If class proportions must be balanced
across folds, StratifiedGroupKFold is a better option.
Here is a visualization of the cross-validation behavior.
Similar to KFold, the test sets from GroupKFold will form a
complete partition of all the data. Unlike KFold, GroupKFold
is not randomized at all, whereas KFold is randomized when
shuffle=True.
3.1.2.3.2. StratifiedGroupKFold¶
StratifiedGroupKFold is a cross-validation scheme that combines both
StratifiedKFold and GroupKFold. The idea is to try to
preserve the distribution of classes in each split while keeping each group
within a single split. That might be useful when you have an unbalanced
dataset so that using just GroupKFold might produce skewed splits.
Example:
>>> from sklearn.model_selection import StratifiedGroupKFold
>>> X = list(range(18))
>>> y = [1] * 6 + [0] * 12
>>> groups = [1, 2, 3, 3, 4, 4, 1, 1, 2, 2, 3, 4, 5, 5, 5, 6, 6, 6]
>>> sgkf = StratifiedGroupKFold(n_splits=3)
>>> for train, test in sgkf.split(X, y, groups=groups):
...     print("%s %s" % (train, test))
[ 0  2  3  4  5  6  7 10 11 15 16 17] [ 1  8  9 12 13 14]
[ 0  1  4  5  6  7  8  9 11 12 13 14] [ 2  3 10 15 16 17]
[ 1  2  3  8  9 10 12 13 14 15 16 17] [ 0  4  5  6  7 11]
Implementation notes:
With the current implementation full shuffle is not possible in most
scenarios. When shuffle=True, the following happens:
All groups are shuffled.
Groups are sorted by standard deviation of classes using stable sort.
Sorted groups are iterated over and assigned to folds.
That means that only groups with the same standard deviation of class
distribution will be shuffled, which might be useful when each group has only
a single class.
The algorithm greedily assigns each group to one of n_splits test sets,
choosing the test set that minimises the variance in class distribution
across test sets. Group assignment proceeds from groups with highest to
lowest variance in class frequency, i.e. large groups peaked on one or few
classes are assigned first.
This split is suboptimal in a sense that it might produce imbalanced splits
even if perfect stratification is possible. If you have relatively close
distribution of classes in each group, using GroupKFold is better.
Here is a visualization of cross-validation behavior for uneven groups:
3.1.2.3.3. Leave One Group Out¶
LeaveOneGroupOut is a cross-validation scheme where each split holds
out samples belonging to one specific group. Group information is
provided via an array that encodes the group of each sample.
Each training set is thus constituted by all the samples except the ones
related to a specific group. This is the same as LeavePGroupsOut with
n_groups=1 and the same as GroupKFold with n_splits equal to the
number of unique labels passed to the groups parameter.
For example, in the cases of multiple experiments, LeaveOneGroupOut
can be used to create a cross-validation based on the different experiments:
we create a training set using the samples of all the experiments except one:
>>> from sklearn.model_selection import LeaveOneGroupOut
>>> X = [1, 5, 10, 50, 60, 70, 80]
>>> y = [0, 1, 1, 2, 2, 2, 2]
>>> groups = [1, 1, 2, 2, 3, 3, 3]
>>> logo = LeaveOneGroupOut()
>>> for train, test in logo.split(X, y, groups=groups):
...     print("%s %s" % (train, test))
[2 3 4 5 6] [0 1]
[0 1 4 5 6] [2 3]
[0 1 2 3] [4 5 6]
Another common application is to use time information: for instance the
groups could be the year of collection of the samples and thus allow
for cross-validation against time-based splits.
3.1.2.3.4. Leave P Groups Out¶
LeavePGroupsOut is similar as LeaveOneGroupOut, but removes
samples related to \(P\) groups for each training/test set. All possible
combinations of \(P\) groups are left out, meaning test sets will overlap
for \(P>1\).
Example of Leave-2-Group Out:
>>> from sklearn.model_selection import LeavePGroupsOut
>>> X = np.arange(6)
>>> y = [1, 1, 1, 2, 2, 2]
>>> groups = [1, 1, 2, 2, 3, 3]
>>> lpgo = LeavePGroupsOut(n_groups=2)
>>> for train, test in lpgo.split(X, y, groups=groups):
...     print("%s %s" % (train, test))
[4 5] [0 1 2 3]
[2 3] [0 1 4 5]
[0 1] [2 3 4 5]
3.1.2.3.5. Group Shuffle Split¶
The GroupShuffleSplit iterator behaves as a combination of
ShuffleSplit and LeavePGroupsOut, and generates a
sequence of randomized partitions in which a subset of groups are held
out for each split. Each train/test split is performed independently meaning
there is no guaranteed relationship between successive test sets.
Here is a usage example:
>>> from sklearn.model_selection import GroupShuffleSplit
>>> X = [0.1, 0.2, 2.2, 2.4, 2.3, 4.55, 5.8, 0.001]
>>> y = ["a", "b", "b", "b", "c", "c", "c", "a"]
>>> groups = [1, 1, 2, 2, 3, 3, 4, 4]
>>> gss = GroupShuffleSplit(n_splits=4, test_size=0.5, random_state=0)
>>> for train, test in gss.split(X, y, groups=groups):
...     print("%s %s" % (train, test))
...
[0 1 2 3] [4 5 6 7]
[2 3 6 7] [0 1 4 5]
[2 3 4 5] [0 1 6 7]
[4 5 6 7] [0 1 2 3]
Here is a visualization of the cross-validation behavior.
This class is useful when the behavior of LeavePGroupsOut is
desired, but the number of groups is large enough that generating all
possible partitions with \(P\) groups withheld would be prohibitively
expensive. In such a scenario, GroupShuffleSplit provides
a random sample (with replacement) of the train / test splits
generated by LeavePGroupsOut.
3.1.2.4. Predefined fold-splits / Validation-sets¶
For some datasets, a pre-defined split of the data into training- and
validation fold or into several cross-validation folds already
exists. Using PredefinedSplit it is possible to use these folds
e.g. when searching for hyperparameters.
For example, when using a validation set, set the test_fold to 0 for all
samples that are part of the validation set, and to -1 for all other samples.
3.1.2.5. Using cross-validation iterators to split train and test¶
The above group cross-validation functions may also be useful for splitting a
dataset into training and testing subsets. Note that the convenience
function train_test_split is a wrapper around ShuffleSplit
and thus only allows for stratified splitting (using the class labels)
and cannot account for groups.
To perform the train and test split, use the indices for the train and test
subsets yielded by the generator output by the split() method of the
cross-validation splitter. For example:
>>> import numpy as np
>>> from sklearn.model_selection import GroupShuffleSplit
>>> X = np.array([0.1, 0.2, 2.2, 2.4, 2.3, 4.55, 5.8, 0.001])
>>> y = np.array(["a", "b", "b", "b", "c", "c", "c", "a"])
>>> groups = np.array([1, 1, 2, 2, 3, 3, 4, 4])
>>> train_indx, test_indx = next(
...     GroupShuffleSplit(random_state=7).split(X, y, groups)
... )
>>> X_train, X_test, y_train, y_test = \
...     X[train_indx], X[test_indx], y[train_indx], y[test_indx]
>>> X_train.shape, X_test.shape
((6,), (2,))
>>> np.unique(groups[train_indx]), np.unique(groups[test_indx])
(array([1, 2, 4]), array([3]))
3.1.2.6. Cross validation of time series data¶
Time series data is characterized by the correlation between observations
that are near in time (autocorrelation). However, classical
cross-validation techniques such as KFold and
ShuffleSplit assume the samples are independent and
identically distributed, and would result in unreasonable correlation
between training and testing instances (yielding poor estimates of
generalization error) on time series data. Therefore, it is very important
to evaluate our model for time series data on the “future” observations
least like those that are used to train the model. To achieve this, one
solution is provided by TimeSeriesSplit.
3.1.2.6.1. Time Series Split¶
TimeSeriesSplit is a variation of k-fold which
returns first \(k\) folds as train set and the \((k+1)\) th
fold as test set. Note that unlike standard cross-validation methods,
successive training sets are supersets of those that come before them.
Also, it adds all surplus data to the first training partition, which
is always used to train the model.
This class can be used to cross-validate time series data samples
that are observed at fixed time intervals.
Example of 3-split time series cross-validation on a dataset with 6 samples:
>>> from sklearn.model_selection import TimeSeriesSplit
>>> X = np.array([[1, 2], [3, 4], [1, 2], [3, 4], [1, 2], [3, 4]])
>>> y = np.array([1, 2, 3, 4, 5, 6])
>>> tscv = TimeSeriesSplit(n_splits=3)
>>> print(tscv)
TimeSeriesSplit(gap=0, max_train_size=None, n_splits=3, test_size=None)
>>> for train, test in tscv.split(X):
...     print("%s %s" % (train, test))
[0 1 2] [3]
[0 1 2 3] [4]
[0 1 2 3 4] [5]
Here is a visualization of the cross-validation behavior.
- 3.1.3. A note on shuffling¶
If the data ordering is not arbitrary (e.g. samples with the same class label
are contiguous), shuffling it first may be essential to get a meaningful cross-
validation result. However, the opposite may be true if the samples are not
independently and identically distributed. For example, if samples correspond
to news articles, and are ordered by their time of publication, then shuffling
the data will likely lead to a model that is overfit and an inflated validation
score: it will be tested on samples that are artificially similar (close in
time) to training samples.
Some cross validation iterators, such as KFold, have an inbuilt option
to shuffle the data indices before splitting them. Note that:
This consumes less memory than shuffling the data directly.
By default no shuffling occurs, including for the (stratified) K fold cross-
validation performed by specifying cv=some_integer to
cross_val_score, grid search, etc. Keep in mind that
train_test_split still returns a random split.
The random_state parameter defaults to None, meaning that the
shuffling will be different every time KFold(..., shuffle=True) is
iterated. However, GridSearchCV will use the same shuffling for each set
of parameters validated by a single call to its fit method.
To get identical results for each split, set random_state to an integer.
For more details on how to control the randomness of cv splitters and avoid
common pitfalls, see Controlling randomness.
- 3.1.4. Cross validation and model selection¶
Cross validation iterators can also be used to directly perform model
selection using Grid Search for the optimal hyperparameters of the
model. This is the topic of the next section: Tuning the hyper-parameters of an estimator.
- 3.1.5. Permutation test score¶
permutation_test_score offers another way
to evaluate the performance of classifiers. It provides a permutation-based
p-value, which represents how likely an observed performance of the
classifier would be obtained by chance. The null hypothesis in this test is
that the classifier fails to leverage any statistical dependency between the
features and the labels to make correct predictions on left out data.
permutation_test_score generates a null
distribution by calculating n_permutations different permutations of the
data. In each permutation the labels are randomly shuffled, thereby removing
any dependency between the features and the labels. The p-value output
is the fraction of permutations for which the average cross-validation score
obtained by the model is better than the cross-validation score obtained by
the model using the original data. For reliable results n_permutations
should typically be larger than 100 and cv between 3-10 folds.
A low p-value provides evidence that the dataset contains real dependency
between features and labels and the classifier was able to utilize this
to obtain good results. A high p-value could be due to a lack of dependency
between features and labels (there is no difference in feature values between
the classes) or because the classifier was not able to use the dependency in
the data. In the latter case, using a more appropriate classifier that
is able to utilize the structure in the data, would result in a lower
p-value.
Cross-validation provides information about how well a classifier generalizes,
specifically the range of expected errors of the classifier. However, a
classifier trained on a high dimensional dataset with no structure may still
perform better than expected on cross-validation, just by chance.
This can typically happen with small datasets with less than a few hundred
samples.
permutation_test_score provides information
on whether the classifier has found a real class structure and can help in
evaluating the performance of the classifier.
It is important to note that this test has been shown to produce low
p-values even if there is only weak structure in the data because in the
corresponding permutated datasets there is absolutely no structure. This
test is therefore only able to show when the model reliably outperforms
random guessing.
Finally, permutation_test_score is computed
using brute force and internally fits (n_permutations + 1) * n_cv models.
It is therefore only tractable with small datasets for which fitting an
individual model is very fast.
Examples
Test with permutations the significance of a classification score
References:
Ojala and Garriga. Permutation Tests for Studying Classifier Performance.
J. Mach. Learn. Res. 2010.
### 3.2. Tuning the hyper-parameters of an estimator — scikit-learn 1.4.2 documentation

Install
User Guide
API
Examples
Community
Getting Started
Tutorial
What's new
Glossary
Development
FAQ
Support
Related packages
Roadmap
Governance
About us
GitHub
Other Versions and Download
More
Getting Started
Tutorial
What's new
Glossary
Development
FAQ
Support
Related packages
Roadmap
Governance
About us
GitHub
Other Versions and Download
Toggle Menu
PrevUp
Next
scikit-learn 1.4.2
Other versions
Please cite us if you use the software.
### 3.2. Tuning the hyper-parameters of an estimator

- 3.2.1. Exhaustive Grid Search
- 3.2.2. Randomized Parameter Optimization
- 3.2.3. Searching for optimal parameters with successive halving
3.2.3.1. Choosing min_resources and the number of candidates
3.2.3.2. Amount of resource and number of candidates at each iteration
3.2.3.3. Choosing a resource
3.2.3.4. Exhausting the available resources
3.2.3.5. Aggressive elimination of candidates
3.2.3.6. Analyzing results with the cv_results_ attribute
- 3.2.4. Tips for parameter search
3.2.4.1. Specifying an objective metric
3.2.4.2. Specifying multiple metrics for evaluation
3.2.4.3. Composite estimators and parameter spaces
3.2.4.4. Model selection: development and evaluation
3.2.4.5. Parallelism
3.2.4.6. Robustness to failure
- 3.2.5. Alternatives to brute force parameter search
3.2.5.1. Model specific cross-validation
3.2.5.2. Information Criterion
3.2.5.3. Out of Bag Estimates
### 3.2. Tuning the hyper-parameters of an estimator¶

Hyper-parameters are parameters that are not directly learnt within estimators.
In scikit-learn they are passed as arguments to the constructor of the
estimator classes. Typical examples include C, kernel and gamma
for Support Vector Classifier, alpha for Lasso, etc.
It is possible and recommended to search the hyper-parameter space for the
best cross validation score.
Any parameter provided when constructing an estimator may be optimized in this
manner. Specifically, to find the names and current values for all parameters
for a given estimator, use:
estimator.get_params()
A search consists of:
an estimator (regressor or classifier such as sklearn.svm.SVC());
a parameter space;
a method for searching or sampling candidates;
a cross-validation scheme; and
a score function.
Two generic approaches to parameter search are provided in
scikit-learn: for given values, GridSearchCV exhaustively considers
all parameter combinations, while RandomizedSearchCV can sample a
given number of candidates from a parameter space with a specified
distribution. Both these tools have successive halving counterparts
HalvingGridSearchCV and HalvingRandomSearchCV, which can be
much faster at finding a good parameter combination.
After describing these tools we detail best practices applicable to these approaches. Some models allow for
specialized, efficient parameter search strategies, outlined in
Alternatives to brute force parameter search.
Note that it is common that a small subset of those parameters can have a large
impact on the predictive or computation performance of the model while others
can be left to their default values. It is recommended to read the docstring of
the estimator class to get a finer understanding of their expected behavior,
possibly by reading the enclosed reference to the literature.
- 3.2.1. Exhaustive Grid Search¶
The grid search provided by GridSearchCV exhaustively generates
candidates from a grid of parameter values specified with the param_grid
parameter. For instance, the following param_grid:
param_grid = [
{'C': [1, 10, 100, 1000], 'kernel': ['linear']},
{'C': [1, 10, 100, 1000], 'gamma': [0.001, 0.0001], 'kernel': ['rbf']},
]
specifies that two grids should be explored: one with a linear kernel and
C values in [1, 10, 100, 1000], and the second one with an RBF kernel,
and the cross-product of C values ranging in [1, 10, 100, 1000] and gamma
values in [0.001, 0.0001].
The GridSearchCV instance implements the usual estimator API: when
“fitting” it on a dataset all the possible combinations of parameter values are
evaluated and the best combination is retained.
Examples:
See Custom refit strategy of a grid search with cross-validation for an example of
Grid Search computation on the digits dataset.
See Sample pipeline for text feature extraction and evaluation for an example
of Grid Search coupling parameters from a text documents feature
extractor (n-gram count vectorizer and TF-IDF transformer) with a
classifier (here a linear SVM trained with SGD with either elastic
net or L2 penalty) using a Pipeline instance.
See Nested versus non-nested cross-validation
for an example of Grid Search within a cross validation loop on the iris
dataset. This is the best practice for evaluating the performance of a
model with grid search.
See Demonstration of multi-metric evaluation on cross_val_score and GridSearchCV
for an example of GridSearchCV being used to evaluate multiple
metrics simultaneously.
See Balance model complexity and cross-validated score
for an example of using refit=callable interface in
GridSearchCV. The example shows how this interface adds certain
amount of flexibility in identifying the “best” estimator. This interface
can also be used in multiple metrics evaluation.
See Statistical comparison of models using grid search
for an example of how to do a statistical comparison on the outputs of
GridSearchCV.
- 3.2.2. Randomized Parameter Optimization¶
While using a grid of parameter settings is currently the most widely used
method for parameter optimization, other search methods have more
favorable properties.
RandomizedSearchCV implements a randomized search over parameters,
where each setting is sampled from a distribution over possible parameter values.
This has two main benefits over an exhaustive search:
A budget can be chosen independent of the number of parameters and possible values.
Adding parameters that do not influence the performance does not decrease efficiency.
Specifying how parameters should be sampled is done using a dictionary, very
similar to specifying parameters for GridSearchCV. Additionally,
a computation budget, being the number of sampled candidates or sampling
iterations, is specified using the n_iter parameter.
For each parameter, either a distribution over possible values or a list of
discrete choices (which will be sampled uniformly) can be specified:
{'C': scipy.stats.expon(scale=100), 'gamma': scipy.stats.expon(scale=.1),
'kernel': ['rbf'], 'class_weight':['balanced', None]}
This example uses the scipy.stats module, which contains many useful
distributions for sampling parameters, such as expon, gamma,
uniform, loguniform or randint.
In principle, any function can be passed that provides a rvs (random
variate sample) method to sample a value. A call to the rvs function should
provide independent random samples from possible parameter values on
consecutive calls.
Warning
The distributions in scipy.stats prior to version scipy 0.16
do not allow specifying a random state. Instead, they use the global
numpy random state, that can be seeded via np.random.seed or set
using np.random.set_state. However, beginning scikit-learn 0.18,
the sklearn.model_selection module sets the random state provided
by the user if scipy >= 0.16 is also available.
For continuous parameters, such as C above, it is important to specify
a continuous distribution to take full advantage of the randomization. This way,
increasing n_iter will always lead to a finer search.
A continuous log-uniform random variable is the continuous version of
a log-spaced parameter. For example to specify the equivalent of C from above,
loguniform(1, 100) can be used instead of [1, 10, 100].
Mirroring the example above in grid search, we can specify a continuous random
variable that is log-uniformly distributed between 1e0 and 1e3:
from sklearn.utils.fixes import loguniform
{'C': loguniform(1e0, 1e3),
'gamma': loguniform(1e-4, 1e-3),
'kernel': ['rbf'],
'class_weight':['balanced', None]}
Examples:
Comparing randomized search and grid search for hyperparameter estimation compares the usage and efficiency
of randomized search and grid search.
References:
Bergstra, J. and Bengio, Y.,
Random search for hyper-parameter optimization,
The Journal of Machine Learning Research (2012)
- 3.2.3. Searching for optimal parameters with successive halving¶
Scikit-learn also provides the HalvingGridSearchCV and
HalvingRandomSearchCV estimators that can be used to
search a parameter space using successive halving [1] [2]. Successive
halving (SH) is like a tournament among candidate parameter combinations.
SH is an iterative selection process where all candidates (the
parameter combinations) are evaluated with a small amount of resources at
the first iteration. Only some of these candidates are selected for the next
iteration, which will be allocated more resources. For parameter tuning, the
resource is typically the number of training samples, but it can also be an
arbitrary numeric parameter such as n_estimators in a random forest.
As illustrated in the figure below, only a subset of candidates
‘survive’ until the last iteration. These are the candidates that have
consistently ranked among the top-scoring candidates across all iterations.
Each iteration is allocated an increasing amount of resources per candidate,
here the number of samples.
We here briefly describe the main parameters, but each parameter and their
interactions are described in more details in the sections below. The
factor (> 1) parameter controls the rate at which the resources grow, and
the rate at which the number of candidates decreases. In each iteration, the
number of resources per candidate is multiplied by factor and the number
of candidates is divided by the same factor. Along with resource and
min_resources, factor is the most important parameter to control the
search in our implementation, though a value of 3 usually works well.
factor effectively controls the number of iterations in
HalvingGridSearchCV and the number of candidates (by default) and
iterations in HalvingRandomSearchCV. aggressive_elimination=True
can also be used if the number of available resources is small. More control
is available through tuning the min_resources parameter.
These estimators are still experimental: their predictions
and their API might change without any deprecation cycle. To use them, you
need to explicitly import enable_halving_search_cv:
>>> # explicitly require this experimental feature
>>> from sklearn.experimental import enable_halving_search_cv  # noqa
>>> # now you can import normally from model_selection
>>> from sklearn.model_selection import HalvingGridSearchCV
>>> from sklearn.model_selection import HalvingRandomSearchCV
Examples:
Comparison between grid search and successive halving
Successive Halving Iterations
3.2.3.1. Choosing min_resources and the number of candidates¶
Beside factor, the two main parameters that influence the behaviour of a
successive halving search are the min_resources parameter, and the
number of candidates (or parameter combinations) that are evaluated.
min_resources is the amount of resources allocated at the first
iteration for each candidate. The number of candidates is specified directly
in HalvingRandomSearchCV, and is determined from the param_grid
parameter of HalvingGridSearchCV.
Consider a case where the resource is the number of samples, and where we
have 1000 samples. In theory, with min_resources=10 and factor=2, we
are able to run at most 7 iterations with the following number of
samples: [10, 20, 40, 80, 160, 320, 640].
But depending on the number of candidates, we might run less than 7
iterations: if we start with a small number of candidates, the last
iteration might use less than 640 samples, which means not using all the
available resources (samples). For example if we start with 5 candidates, we
only need 2 iterations: 5 candidates for the first iteration, then
5 // 2 = 2 candidates at the second iteration, after which we know which
candidate performs the best (so we don’t need a third one). We would only be
using at most 20 samples which is a waste since we have 1000 samples at our
disposal. On the other hand, if we start with a high number of
candidates, we might end up with a lot of candidates at the last iteration,
which may not always be ideal: it means that many candidates will run with
the full resources, basically reducing the procedure to standard search.
In the case of HalvingRandomSearchCV, the number of candidates is set
by default such that the last iteration uses as much of the available
resources as possible. For HalvingGridSearchCV, the number of
candidates is determined by the param_grid parameter. Changing the value of
min_resources will impact the number of possible iterations, and as a
result will also have an effect on the ideal number of candidates.
Another consideration when choosing min_resources is whether or not it
is easy to discriminate between good and bad candidates with a small amount
of resources. For example, if you need a lot of samples to distinguish
between good and bad parameters, a high min_resources is recommended. On
the other hand if the distinction is clear even with a small amount of
samples, then a small min_resources may be preferable since it would
speed up the computation.
Notice in the example above that the last iteration does not use the maximum
amount of resources available: 1000 samples are available, yet only 640 are
used, at most. By default, both HalvingRandomSearchCV and
HalvingGridSearchCV try to use as many resources as possible in the
last iteration, with the constraint that this amount of resources must be a
multiple of both min_resources and factor (this constraint will be clear
in the next section). HalvingRandomSearchCV achieves this by
sampling the right amount of candidates, while HalvingGridSearchCV
achieves this by properly setting min_resources. Please see
Exhausting the available resources for details.
3.2.3.2. Amount of resource and number of candidates at each iteration¶
At any iteration i, each candidate is allocated a given amount of resources
which we denote n_resources_i. This quantity is controlled by the
parameters factor and min_resources as follows (factor is strictly
greater than 1):
n_resources_i = factor**i * min_resources,
or equivalently:
n_resources_{i+1} = n_resources_i * factor
where min_resources == n_resources_0 is the amount of resources used at
the first iteration. factor also defines the proportions of candidates
that will be selected for the next iteration:
n_candidates_i = n_candidates // (factor ** i)
or equivalently:
n_candidates_0 = n_candidates
n_candidates_{i+1} = n_candidates_i // factor
So in the first iteration, we use min_resources resources
n_candidates times. In the second iteration, we use min_resources *
factor resources n_candidates // factor times. The third again
multiplies the resources per candidate and divides the number of candidates.
This process stops when the maximum amount of resource per candidate is
reached, or when we have identified the best candidate. The best candidate
is identified at the iteration that is evaluating factor or less candidates
(see just below for an explanation).
Here is an example with min_resources=3 and factor=2, starting with
70 candidates:
n_resources_i
n_candidates_i
3 (=min_resources)
70 (=n_candidates)
3 * 2 = 6
70 // 2 = 35
6 * 2 = 12
35 // 2 = 17
12 * 2 = 24
17 // 2 = 8
24 * 2 = 48
8 // 2 = 4
48 * 2 = 96
4 // 2 = 2
We can note that:
the process stops at the first iteration which evaluates factor=2
candidates: the best candidate is the best out of these 2 candidates. It
is not necessary to run an additional iteration, since it would only
evaluate one candidate (namely the best one, which we have already
identified). For this reason, in general, we want the last iteration to
run at most factor candidates. If the last iteration evaluates more
than factor candidates, then this last iteration reduces to a regular
search (as in RandomizedSearchCV or GridSearchCV).
each n_resources_i is a multiple of both factor and
min_resources (which is confirmed by its definition above).
The amount of resources that is used at each iteration can be found in the
n_resources_ attribute.
3.2.3.3. Choosing a resource¶
By default, the resource is defined in terms of number of samples. That is,
each iteration will use an increasing amount of samples to train on. You can
however manually specify a parameter to use as the resource with the
resource parameter. Here is an example where the resource is defined in
terms of the number of estimators of a random forest:
>>> from sklearn.datasets import make_classification
>>> from sklearn.ensemble import RandomForestClassifier
>>> from sklearn.experimental import enable_halving_search_cv  # noqa
>>> from sklearn.model_selection import HalvingGridSearchCV
>>> import pandas as pd
>>>
>>> param_grid = {'max_depth': [3, 5, 10],
...               'min_samples_split': [2, 5, 10]}
>>> base_estimator = RandomForestClassifier(random_state=0)
>>> X, y = make_classification(n_samples=1000, random_state=0)
>>> sh = HalvingGridSearchCV(base_estimator, param_grid, cv=5,
...                          factor=2, resource='n_estimators',
...                          max_resources=30).fit(X, y)
>>> sh.best_estimator_
RandomForestClassifier(max_depth=5, n_estimators=24, random_state=0)
Note that it is not possible to budget on a parameter that is part of the
parameter grid.
3.2.3.4. Exhausting the available resources¶
As mentioned above, the number of resources that is used at each iteration
depends on the min_resources parameter.
If you have a lot of resources available but start with a low number of
resources, some of them might be wasted (i.e. not used):
>>> from sklearn.datasets import make_classification
>>> from sklearn.svm import SVC
>>> from sklearn.experimental import enable_halving_search_cv  # noqa
>>> from sklearn.model_selection import HalvingGridSearchCV
>>> import pandas as pd
>>> param_grid= {'kernel': ('linear', 'rbf'),
...              'C': [1, 10, 100]}
>>> base_estimator = SVC(gamma='scale')
>>> X, y = make_classification(n_samples=1000)
>>> sh = HalvingGridSearchCV(base_estimator, param_grid, cv=5,
...                          factor=2, min_resources=20).fit(X, y)
>>> sh.n_resources_
[20, 40, 80]
The search process will only use 80 resources at most, while our maximum
amount of available resources is n_samples=1000. Here, we have
min_resources = r_0 = 20.
For HalvingGridSearchCV, by default, the min_resources parameter
is set to ‘exhaust’. This means that min_resources is automatically set
such that the last iteration can use as many resources as possible, within
the max_resources limit:
>>> sh = HalvingGridSearchCV(base_estimator, param_grid, cv=5,
...                          factor=2, min_resources='exhaust').fit(X, y)
>>> sh.n_resources_
[250, 500, 1000]
min_resources was here automatically set to 250, which results in the last
iteration using all the resources. The exact value that is used depends on
the number of candidate parameter, on max_resources and on factor.
For HalvingRandomSearchCV, exhausting the resources can be done in 2
ways:
by setting min_resources='exhaust', just like for
HalvingGridSearchCV;
by setting n_candidates='exhaust'.
Both options are mutually exclusive: using min_resources='exhaust' requires
knowing the number of candidates, and symmetrically n_candidates='exhaust'
requires knowing min_resources.
In general, exhausting the total number of resources leads to a better final
candidate parameter, and is slightly more time-intensive.
3.2.3.5. Aggressive elimination of candidates¶
Ideally, we want the last iteration to evaluate factor candidates (see
Amount of resource and number of candidates at each iteration). We then just have to
pick the best one. When the number of available resources is small with
respect to the number of candidates, the last iteration may have to evaluate
more than factor candidates:
>>> from sklearn.datasets import make_classification
>>> from sklearn.svm import SVC
>>> from sklearn.experimental import enable_halving_search_cv  # noqa
>>> from sklearn.model_selection import HalvingGridSearchCV
>>> import pandas as pd
>>>
>>>
>>> param_grid = {'kernel': ('linear', 'rbf'),
...               'C': [1, 10, 100]}
>>> base_estimator = SVC(gamma='scale')
>>> X, y = make_classification(n_samples=1000)
>>> sh = HalvingGridSearchCV(base_estimator, param_grid, cv=5,
...                          factor=2, max_resources=40,
...                          aggressive_elimination=False).fit(X, y)
>>> sh.n_resources_
[20, 40]
>>> sh.n_candidates_
[6, 3]
Since we cannot use more than max_resources=40 resources, the process
has to stop at the second iteration which evaluates more than factor=2
candidates.
Using the aggressive_elimination parameter, you can force the search
process to end up with less than factor candidates at the last
iteration. To do this, the process will eliminate as many candidates as
necessary using min_resources resources:
>>> sh = HalvingGridSearchCV(base_estimator, param_grid, cv=5,
...                            factor=2,
...                            max_resources=40,
...                            aggressive_elimination=True,
...                            ).fit(X, y)
>>> sh.n_resources_
[20, 20,  40]
>>> sh.n_candidates_
[6, 3, 2]
Notice that we end with 2 candidates at the last iteration since we have
eliminated enough candidates during the first iterations, using n_resources =
min_resources = 20.
3.2.3.6. Analyzing results with the cv_results_ attribute¶
The cv_results_ attribute contains useful information for analyzing the
results of a search. It can be converted to a pandas dataframe with df =
pd.DataFrame(est.cv_results_). The cv_results_ attribute of
HalvingGridSearchCV and HalvingRandomSearchCV is similar
to that of GridSearchCV and RandomizedSearchCV, with
additional information related to the successive halving process.
Here is an example with some of the columns of a (truncated) dataframe:
iter
n_resources
mean_test_score
params
0
0
125
0.983667
{‘criterion’: ‘log_loss’, ‘max_depth’: None, ‘max_features’: 9, ‘min_samples_split’: 5}
1
0
125
0.983667
{‘criterion’: ‘gini’, ‘max_depth’: None, ‘max_features’: 8, ‘min_samples_split’: 7}
2
0
125
0.983667
{‘criterion’: ‘gini’, ‘max_depth’: None, ‘max_features’: 10, ‘min_samples_split’: 10}
3
0
125
0.983667
{‘criterion’: ‘log_loss’, ‘max_depth’: None, ‘max_features’: 6, ‘min_samples_split’: 6}
…
…
…
…
…
15
2
500
0.951958
{‘criterion’: ‘log_loss’, ‘max_depth’: None, ‘max_features’: 9, ‘min_samples_split’: 10}
16
2
500
0.947958
{‘criterion’: ‘gini’, ‘max_depth’: None, ‘max_features’: 10, ‘min_samples_split’: 10}
17
2
500
0.951958
{‘criterion’: ‘gini’, ‘max_depth’: None, ‘max_features’: 10, ‘min_samples_split’: 4}
18
3
1000
0.961009
{‘criterion’: ‘log_loss’, ‘max_depth’: None, ‘max_features’: 9, ‘min_samples_split’: 10}
19
3
1000
0.955989
{‘criterion’: ‘gini’, ‘max_depth’: None, ‘max_features’: 10, ‘min_samples_split’: 4}
Each row corresponds to a given parameter combination (a candidate) and a given
iteration. The iteration is given by the iter column. The n_resources
column tells you how many resources were used.
In the example above, the best parameter combination is {'criterion':
'log_loss', 'max_depth': None, 'max_features': 9, 'min_samples_split': 10}
since it has reached the last iteration (3) with the highest score:
0.96.
References:
[1]
K. Jamieson, A. Talwalkar,
Non-stochastic Best Arm Identification and Hyperparameter
Optimization, in
proc. of Machine Learning Research, 2016.
[2]
L. Li, K. Jamieson, G. DeSalvo, A. Rostamizadeh, A. Talwalkar,
Hyperband: A Novel Bandit-Based Approach to Hyperparameter Optimization, in Machine Learning Research 18, 2018.
- 3.2.4. Tips for parameter search¶
3.2.4.1. Specifying an objective metric¶
By default, parameter search uses the score function of the estimator
to evaluate a parameter setting. These are the
sklearn.metrics.accuracy_score for classification and
sklearn.metrics.r2_score for regression.  For some applications,
other scoring functions are better suited (for example in unbalanced
classification, the accuracy score is often uninformative). An alternative
scoring function can be specified via the scoring parameter of most
parameter search tools. See The scoring parameter: defining model evaluation rules for more details.
3.2.4.2. Specifying multiple metrics for evaluation¶
GridSearchCV and RandomizedSearchCV allow specifying
multiple metrics for the scoring parameter.
Multimetric scoring can either be specified as a list of strings of predefined
scores names or a dict mapping the scorer name to the scorer function and/or
the predefined scorer name(s). See Using multiple metric evaluation for more details.
When specifying multiple metrics, the refit parameter must be set to the
metric (string) for which the best_params_ will be found and used to build
the best_estimator_ on the whole dataset. If the search should not be
refit, set refit=False. Leaving refit to the default value None will
result in an error when using multiple metrics.
See Demonstration of multi-metric evaluation on cross_val_score and GridSearchCV
for an example usage.
HalvingRandomSearchCV and HalvingGridSearchCV do not support
multimetric scoring.
3.2.4.3. Composite estimators and parameter spaces¶
GridSearchCV and RandomizedSearchCV allow searching over
parameters of composite or nested estimators such as
Pipeline,
ColumnTransformer,
VotingClassifier or
CalibratedClassifierCV using a dedicated
<estimator>__<parameter> syntax:
>>> from sklearn.model_selection import GridSearchCV
>>> from sklearn.calibration import CalibratedClassifierCV
>>> from sklearn.ensemble import RandomForestClassifier
>>> from sklearn.datasets import make_moons
>>> X, y = make_moons()
>>> calibrated_forest = CalibratedClassifierCV(
...    estimator=RandomForestClassifier(n_estimators=10))
>>> param_grid = {
...    'estimator__max_depth': [2, 4, 6, 8]}
>>> search = GridSearchCV(calibrated_forest, param_grid, cv=5)
>>> search.fit(X, y)
GridSearchCV(cv=5,
estimator=CalibratedClassifierCV(...),
param_grid={'estimator__max_depth': [2, 4, 6, 8]})
Here, <estimator> is the parameter name of the nested estimator,
in this case estimator.
If the meta-estimator is constructed as a collection of estimators as in
pipeline.Pipeline, then <estimator> refers to the name of the estimator,
see Access to nested parameters. In practice, there can be several
levels of nesting:
>>> from sklearn.pipeline import Pipeline
>>> from sklearn.feature_selection import SelectKBest
>>> pipe = Pipeline([
...    ('select', SelectKBest()),
...    ('model', calibrated_forest)])
>>> param_grid = {
...    'select__k': [1, 2],
...    'model__estimator__max_depth': [2, 4, 6, 8]}
>>> search = GridSearchCV(pipe, param_grid, cv=5).fit(X, y)
Please refer to Pipeline: chaining estimators for performing parameter searches over
pipelines.
3.2.4.4. Model selection: development and evaluation¶
Model selection by evaluating various parameter settings can be seen as a way
to use the labeled data to “train” the parameters of the grid.
When evaluating the resulting model it is important to do it on
held-out samples that were not seen during the grid search process:
it is recommended to split the data into a development set (to
be fed to the GridSearchCV instance) and an evaluation set
to compute performance metrics.
This can be done by using the train_test_split
utility function.
3.2.4.5. Parallelism¶
The parameter search tools evaluate each parameter combination on each data
fold independently. Computations can be run in parallel by using the keyword
n_jobs=-1. See function signature for more details, and also the Glossary
entry for n_jobs.
3.2.4.6. Robustness to failure¶
Some parameter settings may result in a failure to fit one or more folds
of the data.  By default, this will cause the entire search to fail, even if
some parameter settings could be fully evaluated. Setting error_score=0
(or =np.nan) will make the procedure robust to such failure, issuing a
warning and setting the score for that fold to 0 (or nan), but completing
the search.
- 3.2.5. Alternatives to brute force parameter search¶
3.2.5.1. Model specific cross-validation¶
Some models can fit data for a range of values of some parameter almost
as efficiently as fitting the estimator for a single value of the
parameter. This feature can be leveraged to perform a more efficient
cross-validation used for model selection of this parameter.
The most common parameter amenable to this strategy is the parameter
encoding the strength of the regularizer. In this case we say that we
compute the regularization path of the estimator.
Here is the list of such models:
linear_model.ElasticNetCV(*[, l1_ratio, ...])
Elastic Net model with iterative fitting along a regularization path.
linear_model.LarsCV(*[, fit_intercept, ...])
Cross-validated Least Angle Regression model.
linear_model.LassoCV(*[, eps, n_alphas, ...])
Lasso linear model with iterative fitting along a regularization path.
linear_model.LassoLarsCV(*[, fit_intercept, ...])
Cross-validated Lasso, using the LARS algorithm.
linear_model.LogisticRegressionCV(*[, Cs, ...])
Logistic Regression CV (aka logit, MaxEnt) classifier.
linear_model.MultiTaskElasticNetCV(*[, ...])
Multi-task L1/L2 ElasticNet with built-in cross-validation.
linear_model.MultiTaskLassoCV(*[, eps, ...])
Multi-task Lasso model trained with L1/L2 mixed-norm as regularizer.
linear_model.OrthogonalMatchingPursuitCV(*)
Cross-validated Orthogonal Matching Pursuit model (OMP).
linear_model.RidgeCV([alphas, ...])
Ridge regression with built-in cross-validation.
linear_model.RidgeClassifierCV([alphas, ...])
Ridge classifier with built-in cross-validation.
3.2.5.2. Information Criterion¶
Some models can offer an information-theoretic closed-form formula of the
optimal estimate of the regularization parameter by computing a single
regularization path (instead of several when using cross-validation).
Here is the list of models benefiting from the Akaike Information
Criterion (AIC) or the Bayesian Information Criterion (BIC) for automated
model selection:
linear_model.LassoLarsIC([criterion, ...])
Lasso model fit with Lars using BIC or AIC for model selection.
3.2.5.3. Out of Bag Estimates¶
When using ensemble methods base upon bagging, i.e. generating new
training sets using sampling with replacement, part of the training set
remains unused.  For each classifier in the ensemble, a different part
of the training set is left out.
This left out portion can be used to estimate the generalization error
without having to rely on a separate validation set.  This estimate
comes “for free” as no additional data is needed and can be used for
model selection.
This is currently implemented in the following classes:
ensemble.RandomForestClassifier([...])
A random forest classifier.
ensemble.RandomForestRegressor([...])
A random forest regressor.
ensemble.ExtraTreesClassifier([...])
An extra-trees classifier.
ensemble.ExtraTreesRegressor([n_estimators, ...])
An extra-trees regressor.
ensemble.GradientBoostingClassifier(*[, ...])
Gradient Boosting for classification.
ensemble.GradientBoostingRegressor(*[, ...])
Gradient Boosting for regression.
### 3.3. Metrics and scoring: quantifying the quality of predictions — scikit-learn 1.4.2 documentation

Install
User Guide
API
Examples
Community
Getting Started
Tutorial
What's new
Glossary
Development
FAQ
Support
Related packages
Roadmap
Governance
About us
GitHub
Other Versions and Download
More
Getting Started
Tutorial
What's new
Glossary
Development
FAQ
Support
Related packages
Roadmap
Governance
About us
GitHub
Other Versions and Download
Toggle Menu
PrevUp
Next
scikit-learn 1.4.2
Other versions
Please cite us if you use the software.
### 3.3. Metrics and scoring: quantifying the quality of predictions

- 3.3.1. The scoring parameter: defining model evaluation rules
3.3.1.1. Common cases: predefined values
3.3.1.2. Defining your scoring strategy from metric functions
3.3.1.3. Implementing your own scoring object
3.3.1.4. Using multiple metric evaluation
- 3.3.2. Classification metrics
3.3.2.1. From binary to multiclass and multilabel
3.3.2.2. Accuracy score
3.3.2.3. Top-k accuracy score
3.3.2.4. Balanced accuracy score
3.3.2.5. Cohen’s kappa
3.3.2.6. Confusion matrix
3.3.2.7. Classification report
3.3.2.8. Hamming loss
3.3.2.9. Precision, recall and F-measures
3.3.2.9.1. Binary classification
3.3.2.9.2. Multiclass and multilabel classification
3.3.2.10. Jaccard similarity coefficient score
3.3.2.11. Hinge loss
3.3.2.12. Log loss
3.3.2.13. Matthews correlation coefficient
3.3.2.14. Multi-label confusion matrix
3.3.2.15. Receiver operating characteristic (ROC)
3.3.2.15.1. Binary case
3.3.2.15.2. Multi-class case
3.3.2.15.3. Multi-label case
3.3.2.16. Detection error tradeoff (DET)
3.3.2.17. Zero one loss
3.3.2.18. Brier score loss
3.3.2.19. Class likelihood ratios
- 3.3.3. Multilabel ranking metrics
3.3.3.1. Coverage error
3.3.3.2. Label ranking average precision
3.3.3.3. Ranking loss
3.3.3.4. Normalized Discounted Cumulative Gain
- 3.3.4. Regression metrics
3.3.4.1. R² score, the coefficient of determination
3.3.4.2. Mean absolute error
3.3.4.3. Mean squared error
3.3.4.4. Mean squared logarithmic error
3.3.4.5. Mean absolute percentage error
3.3.4.6. Median absolute error
3.3.4.7. Max error
3.3.4.8. Explained variance score
3.3.4.9. Mean Poisson, Gamma, and Tweedie deviances
3.3.4.10. Pinball loss
3.3.4.11. D² score
3.3.4.11.1. D² Tweedie score
3.3.4.11.2. D² pinball score
3.3.4.11.3. D² absolute error score
3.3.4.12. Visual evaluation of regression models
- 3.3.5. Clustering metrics
- 3.3.6. Dummy estimators
### 3.3. Metrics and scoring: quantifying the quality of predictions¶

There are 3 different APIs for evaluating the quality of a model’s
predictions:
Estimator score method: Estimators have a score method providing a
default evaluation criterion for the problem they are designed to solve.
This is not discussed on this page, but in each estimator’s documentation.
Scoring parameter: Model-evaluation tools using
cross-validation (such as
model_selection.cross_val_score and
model_selection.GridSearchCV) rely on an internal scoring strategy.
This is discussed in the section The scoring parameter: defining model evaluation rules.
Metric functions: The sklearn.metrics module implements functions
assessing prediction error for specific purposes. These metrics are detailed
in sections on Classification metrics,
Multilabel ranking metrics, Regression metrics and
Clustering metrics.
Finally, Dummy estimators are useful to get a baseline
value of those metrics for random predictions.
See also
For “pairwise” metrics, between samples and not estimators or
predictions, see the Pairwise metrics, Affinities and Kernels section.
- 3.3.1. The scoring parameter: defining model evaluation rules¶
Model selection and evaluation using tools, such as
model_selection.GridSearchCV and
model_selection.cross_val_score, take a scoring parameter that
controls what metric they apply to the estimators evaluated.
3.3.1.1. Common cases: predefined values¶
For the most common use cases, you can designate a scorer object with the
scoring parameter; the table below shows all possible values.
All scorer objects follow the convention that higher return values are better
than lower return values.  Thus metrics which measure the distance between
the model and the data, like metrics.mean_squared_error, are
available as neg_mean_squared_error which return the negated value
of the metric.
Scoring
Function
Comment
Classification
‘accuracy’
metrics.accuracy_score
‘balanced_accuracy’
metrics.balanced_accuracy_score
‘top_k_accuracy’
metrics.top_k_accuracy_score
‘average_precision’
metrics.average_precision_score
‘neg_brier_score’
metrics.brier_score_loss
‘f1’
metrics.f1_score
for binary targets
‘f1_micro’
metrics.f1_score
micro-averaged
‘f1_macro’
metrics.f1_score
macro-averaged
‘f1_weighted’
metrics.f1_score
weighted average
‘f1_samples’
metrics.f1_score
by multilabel sample
‘neg_log_loss’
metrics.log_loss
requires predict_proba support
‘precision’ etc.
metrics.precision_score
suffixes apply as with ‘f1’
‘recall’ etc.
metrics.recall_score
suffixes apply as with ‘f1’
‘jaccard’ etc.
metrics.jaccard_score
suffixes apply as with ‘f1’
‘roc_auc’
metrics.roc_auc_score
‘roc_auc_ovr’
metrics.roc_auc_score
‘roc_auc_ovo’
metrics.roc_auc_score
‘roc_auc_ovr_weighted’
metrics.roc_auc_score
‘roc_auc_ovo_weighted’
metrics.roc_auc_score
Clustering
‘adjusted_mutual_info_score’
metrics.adjusted_mutual_info_score
‘adjusted_rand_score’
metrics.adjusted_rand_score
‘completeness_score’
metrics.completeness_score
‘fowlkes_mallows_score’
metrics.fowlkes_mallows_score
‘homogeneity_score’
metrics.homogeneity_score
‘mutual_info_score’
metrics.mutual_info_score
‘normalized_mutual_info_score’
metrics.normalized_mutual_info_score
‘rand_score’
metrics.rand_score
‘v_measure_score’
metrics.v_measure_score
Regression
‘explained_variance’
metrics.explained_variance_score
‘max_error’
metrics.max_error
‘neg_mean_absolute_error’
metrics.mean_absolute_error
‘neg_mean_squared_error’
metrics.mean_squared_error
‘neg_root_mean_squared_error’
metrics.root_mean_squared_error
‘neg_mean_squared_log_error’
metrics.mean_squared_log_error
‘neg_root_mean_squared_log_error’
metrics.root_mean_squared_log_error
‘neg_median_absolute_error’
metrics.median_absolute_error
‘r2’
metrics.r2_score
‘neg_mean_poisson_deviance’
metrics.mean_poisson_deviance
‘neg_mean_gamma_deviance’
metrics.mean_gamma_deviance
‘neg_mean_absolute_percentage_error’
metrics.mean_absolute_percentage_error
‘d2_absolute_error_score’
metrics.d2_absolute_error_score
‘d2_pinball_score’
metrics.d2_pinball_score
‘d2_tweedie_score’
metrics.d2_tweedie_score
Usage examples:
>>> from sklearn import svm, datasets
>>> from sklearn.model_selection import cross_val_score
>>> X, y = datasets.load_iris(return_X_y=True)
>>> clf = svm.SVC(random_state=0)
>>> cross_val_score(clf, X, y, cv=5, scoring='recall_macro')
array([0.96..., 0.96..., 0.96..., 0.93..., 1.        ])
Note
If a wrong scoring name is passed, an InvalidParameterError is raised.
You can retrieve the names of all available scorers by calling
get_scorer_names.
3.3.1.2. Defining your scoring strategy from metric functions¶
The module sklearn.metrics also exposes a set of simple functions
measuring a prediction error given ground truth and prediction:
functions ending with _score return a value to
maximize, the higher the better.
functions ending with _error or _loss return a
value to minimize, the lower the better.  When converting
into a scorer object using make_scorer, set
the greater_is_better parameter to False (True by default; see the
parameter description below).
Metrics available for various machine learning tasks are detailed in sections
below.
Many metrics are not given names to be used as scoring values,
sometimes because they require additional parameters, such as
fbeta_score. In such cases, you need to generate an appropriate
scoring object.  The simplest way to generate a callable object for scoring
is by using make_scorer. That function converts metrics
into callables that can be used for model evaluation.
One typical use case is to wrap an existing metric function from the library
with non-default values for its parameters, such as the beta parameter for
the fbeta_score function:
>>> from sklearn.metrics import fbeta_score, make_scorer
>>> ftwo_scorer = make_scorer(fbeta_score, beta=2)
>>> from sklearn.model_selection import GridSearchCV
>>> from sklearn.svm import LinearSVC
>>> grid = GridSearchCV(LinearSVC(dual="auto"), param_grid={'C': [1, 10]},
...                     scoring=ftwo_scorer, cv=5)
Custom scorer objects
Click for more details
¶
The second use case is to build a completely custom scorer object
from a simple python function using make_scorer, which can
take several parameters:
the python function you want to use (my_custom_loss_func
in the example below)
whether the python function returns a score (greater_is_better=True,
the default) or a loss (greater_is_better=False).  If a loss, the output
of the python function is negated by the scorer object, conforming to
the cross validation convention that scorers return higher values for better models.
for classification metrics only: whether the python function you provided requires
continuous decision certainties. If the scoring function only accepts probability
estimates (e.g. metrics.log_loss) then one needs to set the parameter
response_method, thus in this case response_method="predict_proba". Some scoring
function do not necessarily require probability estimates but rather non-thresholded
decision values (e.g. metrics.roc_auc_score). In this case, one provides a
list such as response_method=["decision_function", "predict_proba"]. In this case,
the scorer will use the first available method, in the order given in the list,
to compute the scores.
any additional parameters, such as beta or labels in f1_score.
Here is an example of building custom scorers, and of using the
greater_is_better parameter:
>>> import numpy as np
>>> def my_custom_loss_func(y_true, y_pred):
...     diff = np.abs(y_true - y_pred).max()
...     return np.log1p(diff)
...
>>> # score will negate the return value of my_custom_loss_func,
>>> # which will be np.log(2), 0.693, given the values for X
>>> # and y defined below.
>>> score = make_scorer(my_custom_loss_func, greater_is_better=False)
>>> X = [[1], [1]]
>>> y = [0, 1]
>>> from sklearn.dummy import DummyClassifier
>>> clf = DummyClassifier(strategy='most_frequent', random_state=0)
>>> clf = clf.fit(X, y)
>>> my_custom_loss_func(y, clf.predict(X))
0.69...
>>> score(clf, X, y)
-0.69...
3.3.1.3. Implementing your own scoring object¶
You can generate even more flexible model scorers by constructing your own
scoring object from scratch, without using the make_scorer factory.
How to build a scorer from scratch
Click for more details
¶
For a callable to be a scorer, it needs to meet the protocol specified by
the following two rules:
It can be called with parameters (estimator, X, y), where estimator
is the model that should be evaluated, X is validation data, and y is
the ground truth target for X (in the supervised case) or None (in the
unsupervised case).
It returns a floating point number that quantifies the
estimator prediction quality on X, with reference to y.
Again, by convention higher numbers are better, so if your scorer
returns loss, that value should be negated.
Advanced: If it requires extra metadata to be passed to it, it should expose
a get_metadata_routing method returning the requested metadata. The user
should be able to set the requested metadata via a set_score_request
method. Please see User Guide and Developer
Guide for
more details.
Note
Using custom scorers in functions where n_jobs > 1
While defining the custom scoring function alongside the calling function
should work out of the box with the default joblib backend (loky),
importing it from another module will be a more robust approach and work
independently of the joblib backend.
For example, to use n_jobs greater than 1 in the example below,
custom_scoring_function function is saved in a user-created module
(custom_scorer_module.py) and imported:
>>> from custom_scorer_module import custom_scoring_function
>>> cross_val_score(model,
...  X_train,
...  y_train,
...  scoring=make_scorer(custom_scoring_function, greater_is_better=False),
...  cv=5,
...  n_jobs=-1)
3.3.1.4. Using multiple metric evaluation¶
Scikit-learn also permits evaluation of multiple metrics in GridSearchCV,
RandomizedSearchCV and cross_validate.
There are three ways to specify multiple scoring metrics for the scoring
parameter:
As an iterable of string metrics::>>> scoring = ['accuracy', 'precision']
As a dict mapping the scorer name to the scoring function::>>> from sklearn.metrics import accuracy_score
>>> from sklearn.metrics import make_scorer
>>> scoring = {'accuracy': make_scorer(accuracy_score),
...            'prec': 'precision'}
Note that the dict values can either be scorer functions or one of the
predefined metric strings.
As a callable that returns a dictionary of scores:
>>> from sklearn.model_selection import cross_validate
>>> from sklearn.metrics import confusion_matrix
>>> # A sample toy binary classification dataset
>>> X, y = datasets.make_classification(n_classes=2, random_state=0)
>>> svm = LinearSVC(dual="auto", random_state=0)
>>> def confusion_matrix_scorer(clf, X, y):
...      y_pred = clf.predict(X)
...      cm = confusion_matrix(y, y_pred)
...      return {'tn': cm[0, 0], 'fp': cm[0, 1],
...              'fn': cm[1, 0], 'tp': cm[1, 1]}
>>> cv_results = cross_validate(svm, X, y, cv=5,
...                             scoring=confusion_matrix_scorer)
>>> # Getting the test set true positive scores
>>> print(cv_results['test_tp'])
[10  9  8  7  8]
>>> # Getting the test set false negative scores
>>> print(cv_results['test_fn'])
[0 1 2 3 2]
- 3.3.2. Classification metrics¶
The sklearn.metrics module implements several loss, score, and utility
functions to measure classification performance.
Some metrics might require probability estimates of the positive class,
confidence values, or binary decisions values.
Most implementations allow each sample to provide a weighted contribution
to the overall score, through the sample_weight parameter.
Some of these are restricted to the binary classification case:
precision_recall_curve(y_true, probas_pred, *)
Compute precision-recall pairs for different probability thresholds.
roc_curve(y_true, y_score, *[, pos_label, ...])
Compute Receiver operating characteristic (ROC).
class_likelihood_ratios(y_true, y_pred, *[, ...])
Compute binary classification positive and negative likelihood ratios.
det_curve(y_true, y_score[, pos_label, ...])
Compute error rates for different probability thresholds.
Others also work in the multiclass case:
balanced_accuracy_score(y_true, y_pred, *[, ...])
Compute the balanced accuracy.
cohen_kappa_score(y1, y2, *[, labels, ...])
Compute Cohen's kappa: a statistic that measures inter-annotator agreement.
confusion_matrix(y_true, y_pred, *[, ...])
Compute confusion matrix to evaluate the accuracy of a classification.
hinge_loss(y_true, pred_decision, *[, ...])
Average hinge loss (non-regularized).
matthews_corrcoef(y_true, y_pred, *[, ...])
Compute the Matthews correlation coefficient (MCC).
roc_auc_score(y_true, y_score, *[, average, ...])
Compute Area Under the Receiver Operating Characteristic Curve (ROC AUC)     from prediction scores.
top_k_accuracy_score(y_true, y_score, *[, ...])
Top-k Accuracy classification score.
Some also work in the multilabel case:
accuracy_score(y_true, y_pred, *[, ...])
Accuracy classification score.
classification_report(y_true, y_pred, *[, ...])
Build a text report showing the main classification metrics.
f1_score(y_true, y_pred, *[, labels, ...])
Compute the F1 score, also known as balanced F-score or F-measure.
fbeta_score(y_true, y_pred, *, beta[, ...])
Compute the F-beta score.
hamming_loss(y_true, y_pred, *[, sample_weight])
Compute the average Hamming loss.
jaccard_score(y_true, y_pred, *[, labels, ...])
Jaccard similarity coefficient score.
log_loss(y_true, y_pred, *[, eps, ...])
Log loss, aka logistic loss or cross-entropy loss.
multilabel_confusion_matrix(y_true, y_pred, *)
Compute a confusion matrix for each class or sample.
precision_recall_fscore_support(y_true, ...)
Compute precision, recall, F-measure and support for each class.
precision_score(y_true, y_pred, *[, labels, ...])
Compute the precision.
recall_score(y_true, y_pred, *[, labels, ...])
Compute the recall.
roc_auc_score(y_true, y_score, *[, average, ...])
Compute Area Under the Receiver Operating Characteristic Curve (ROC AUC)     from prediction scores.
zero_one_loss(y_true, y_pred, *[, ...])
Zero-one classification loss.
And some work with binary and multilabel (but not multiclass) problems:
average_precision_score(y_true, y_score, *)
Compute average precision (AP) from prediction scores.
In the following sub-sections, we will describe each of those functions,
preceded by some notes on common API and metric definition.
3.3.2.1. From binary to multiclass and multilabel¶
Some metrics are essentially defined for binary classification tasks (e.g.
f1_score, roc_auc_score). In these cases, by default
only the positive label is evaluated, assuming by default that the positive
class is labelled 1 (though this may be configurable through the
pos_label parameter).
In extending a binary metric to multiclass or multilabel problems, the data
is treated as a collection of binary problems, one for each class.
There are then a number of ways to average binary metric calculations across
the set of classes, each of which may be useful in some scenario.
Where available, you should select among these using the average parameter.
"macro" simply calculates the mean of the binary metrics,
giving equal weight to each class.  In problems where infrequent classes
are nonetheless important, macro-averaging may be a means of highlighting
their performance. On the other hand, the assumption that all classes are
equally important is often untrue, such that macro-averaging will
over-emphasize the typically low performance on an infrequent class.
"weighted" accounts for class imbalance by computing the average of
binary metrics in which each class’s score is weighted by its presence in the
true data sample.
"micro" gives each sample-class pair an equal contribution to the overall
metric (except as a result of sample-weight). Rather than summing the
metric per class, this sums the dividends and divisors that make up the
per-class metrics to calculate an overall quotient.
Micro-averaging may be preferred in multilabel settings, including
multiclass classification where a majority class is to be ignored.
"samples" applies only to multilabel problems. It does not calculate a
per-class measure, instead calculating the metric over the true and predicted
classes for each sample in the evaluation data, and returning their
(sample_weight-weighted) average.
Selecting average=None will return an array with the score for each
class.
While multiclass data is provided to the metric, like binary targets, as an
array of class labels, multilabel data is specified as an indicator matrix,
in which cell [i, j] has value 1 if sample i has label j and value
0 otherwise.
3.3.2.2. Accuracy score¶
The accuracy_score function computes the
accuracy, either the fraction
(default) or the count (normalize=False) of correct predictions.
In multilabel classification, the function returns the subset accuracy. If
the entire set of predicted labels for a sample strictly match with the true
set of labels, then the subset accuracy is 1.0; otherwise it is 0.0.
If \(\hat{y}_i\) is the predicted value of
the \(i\)-th sample and \(y_i\) is the corresponding true value,
then the fraction of correct predictions over \(n_\text{samples}\) is
defined as
\[\texttt{accuracy}(y, \hat{y}) = \frac{1}{n_\text{samples}} \sum_{i=0}^{n_\text{samples}-1} 1(\hat{y}_i = y_i)\]
where \(1(x)\) is the indicator function.
>>> import numpy as np
>>> from sklearn.metrics import accuracy_score
>>> y_pred = [0, 2, 1, 3]
>>> y_true = [0, 1, 2, 3]
>>> accuracy_score(y_true, y_pred)
0.5
>>> accuracy_score(y_true, y_pred, normalize=False)
2.0
In the multilabel case with binary label indicators:
>>> accuracy_score(np.array([[0, 1], [1, 1]]), np.ones((2, 2)))
0.5
Example:
See Test with permutations the significance of a classification score
for an example of accuracy score usage using permutations of
the dataset.
3.3.2.3. Top-k accuracy score¶
The top_k_accuracy_score function is a generalization of
accuracy_score. The difference is that a prediction is considered
correct as long as the true label is associated with one of the k highest
predicted scores. accuracy_score is the special case of k = 1.
The function covers the binary and multiclass classification cases but not the
multilabel case.
If \(\hat{f}_{i,j}\) is the predicted class for the \(i\)-th sample
corresponding to the \(j\)-th largest predicted score and \(y_i\) is the
corresponding true value, then the fraction of correct predictions over
\(n_\text{samples}\) is defined as
\[\texttt{top-k accuracy}(y, \hat{f}) = \frac{1}{n_\text{samples}} \sum_{i=0}^{n_\text{samples}-1} \sum_{j=1}^{k} 1(\hat{f}_{i,j} = y_i)\]
where \(k\) is the number of guesses allowed and \(1(x)\) is the
indicator function.
>>> import numpy as np
>>> from sklearn.metrics import top_k_accuracy_score
>>> y_true = np.array([0, 1, 2, 2])
>>> y_score = np.array([[0.5, 0.2, 0.2],
...                     [0.3, 0.4, 0.2],
...                     [0.2, 0.4, 0.3],
...                     [0.7, 0.2, 0.1]])
>>> top_k_accuracy_score(y_true, y_score, k=2)
0.75
>>> # Not normalizing gives the number of "correctly" classified samples
>>> top_k_accuracy_score(y_true, y_score, k=2, normalize=False)
3
3.3.2.4. Balanced accuracy score¶
The balanced_accuracy_score function computes the balanced accuracy, which avoids inflated
performance estimates on imbalanced datasets. It is the macro-average of recall
scores per class or, equivalently, raw accuracy where each sample is weighted
according to the inverse prevalence of its true class.
Thus for balanced datasets, the score is equal to accuracy.
In the binary case, balanced accuracy is equal to the arithmetic mean of
sensitivity
(true positive rate) and specificity (true negative
rate), or the area under the ROC curve with binary predictions rather than
scores:
\[\texttt{balanced-accuracy} = \frac{1}{2}\left( \frac{TP}{TP + FN} + \frac{TN}{TN + FP}\right )\]
If the classifier performs equally well on either class, this term reduces to
the conventional accuracy (i.e., the number of correct predictions divided by
the total number of predictions).
In contrast, if the conventional accuracy is above chance only because the
classifier takes advantage of an imbalanced test set, then the balanced
accuracy, as appropriate, will drop to \(\frac{1}{n\_classes}\).
The score ranges from 0 to 1, or when adjusted=True is used, it rescaled to
the range \(\frac{1}{1 - n\_classes}\) to 1, inclusive, with
performance at random scoring 0.
If \(y_i\) is the true value of the \(i\)-th sample, and \(w_i\)
is the corresponding sample weight, then we adjust the sample weight to:
\[\hat{w}_i = \frac{w_i}{\sum_j{1(y_j = y_i) w_j}}\]
where \(1(x)\) is the indicator function.
Given predicted \(\hat{y}_i\) for sample \(i\), balanced accuracy is
defined as:
\[\texttt{balanced-accuracy}(y, \hat{y}, w) = \frac{1}{\sum{\hat{w}_i}} \sum_i 1(\hat{y}_i = y_i) \hat{w}_i\]
With adjusted=True, balanced accuracy reports the relative increase from
\(\texttt{balanced-accuracy}(y, \mathbf{0}, w) =
\frac{1}{n\_classes}\).  In the binary case, this is also known as
*Youden’s J statistic*,
or informedness.
Note
The multiclass definition here seems the most reasonable extension of the
metric used in binary classification, though there is no certain consensus
in the literature:
Our definition: [Mosley2013], [Kelleher2015] and [Guyon2015], where
[Guyon2015] adopt the adjusted version to ensure that random predictions
have a score of \(0\) and perfect predictions have a score of \(1\)..
Class balanced accuracy as described in [Mosley2013]: the minimum between the precision
and the recall for each class is computed. Those values are then averaged over the total
number of classes to get the balanced accuracy.
Balanced Accuracy as described in [Urbanowicz2015]: the average of sensitivity and specificity
is computed for each class and then averaged over total number of classes.
References:
[Guyon2015]
(1,2)
I. Guyon, K. Bennett, G. Cawley, H.J. Escalante, S. Escalera, T.K. Ho, N. Macià,
B. Ray, M. Saeed, A.R. Statnikov, E. Viegas, Design of the 2015 ChaLearn AutoML Challenge,
IJCNN 2015.
[Mosley2013]
(1,2)
L. Mosley, A balanced approach to the multi-class imbalance problem,
IJCV 2010.
[Kelleher2015]
John. D. Kelleher, Brian Mac Namee, Aoife D’Arcy, Fundamentals of
Machine Learning for Predictive Data Analytics: Algorithms, Worked Examples,
and Case Studies,
2015.
[Urbanowicz2015]
Urbanowicz R.J.,  Moore, J.H. ExSTraCS 2.0: description
and evaluation of a scalable learning classifier
system, Evol. Intel. (2015) 8: 89.
3.3.2.5. Cohen’s kappa¶
The function cohen_kappa_score computes Cohen’s kappa statistic.
This measure is intended to compare labelings by different human annotators,
not a classifier versus a ground truth.
The kappa score (see docstring) is a number between -1 and 1.
Scores above .8 are generally considered good agreement;
zero or lower means no agreement (practically random labels).
Kappa scores can be computed for binary or multiclass problems,
but not for multilabel problems (except by manually computing a per-label score)
and not for more than two annotators.
>>> from sklearn.metrics import cohen_kappa_score
>>> y_true = [2, 0, 2, 2, 0, 1]
>>> y_pred = [0, 0, 2, 2, 0, 2]
>>> cohen_kappa_score(y_true, y_pred)
0.4285714285714286
3.3.2.6. Confusion matrix¶
The confusion_matrix function evaluates
classification accuracy by computing the confusion matrix with each row corresponding
to the true class (Wikipedia and other references may use different convention
for axes).
By definition, entry \(i, j\) in a confusion matrix is
the number of observations actually in group \(i\), but
predicted to be in group \(j\). Here is an example:
>>> from sklearn.metrics import confusion_matrix
>>> y_true = [2, 0, 2, 2, 0, 1]
>>> y_pred = [0, 0, 2, 2, 0, 2]
>>> confusion_matrix(y_true, y_pred)
array([[2, 0, 0],
[0, 0, 1],
[1, 0, 2]])
ConfusionMatrixDisplay can be used to visually represent a confusion
matrix as shown in the
Confusion matrix
example, which creates the following figure:
The parameter normalize allows to report ratios instead of counts. The
confusion matrix can be normalized in 3 different ways: 'pred', 'true',
and 'all' which will divide the counts by the sum of each columns, rows, or
the entire matrix, respectively.
>>> y_true = [0, 0, 0, 1, 1, 1, 1, 1]
>>> y_pred = [0, 1, 0, 1, 0, 1, 0, 1]
>>> confusion_matrix(y_true, y_pred, normalize='all')
array([[0.25 , 0.125],
[0.25 , 0.375]])
For binary problems, we can get counts of true negatives, false positives,
false negatives and true positives as follows:
>>> y_true = [0, 0, 0, 1, 1, 1, 1, 1]
>>> y_pred = [0, 1, 0, 1, 0, 1, 0, 1]
>>> tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()
>>> tn, fp, fn, tp
(2, 1, 2, 3)
Example:
See Confusion matrix
for an example of using a confusion matrix to evaluate classifier output
quality.
See Recognizing hand-written digits
for an example of using a confusion matrix to classify
hand-written digits.
See Classification of text documents using sparse features
for an example of using a confusion matrix to classify text
documents.
3.3.2.7. Classification report¶
The classification_report function builds a text report showing the
main classification metrics. Here is a small example with custom target_names
and inferred labels:
>>> from sklearn.metrics import classification_report
>>> y_true = [0, 1, 2, 2, 0]
>>> y_pred = [0, 0, 2, 1, 0]
>>> target_names = ['class 0', 'class 1', 'class 2']
>>> print(classification_report(y_true, y_pred, target_names=target_names))
precision    recall  f1-score   support
class 0       0.67      1.00      0.80         2
class 1       0.00      0.00      0.00         1
class 2       1.00      0.50      0.67         2
accuracy                           0.60         5
macro avg       0.56      0.50      0.49         5
weighted avg       0.67      0.60      0.59         5
Example:
See Recognizing hand-written digits
for an example of classification report usage for
hand-written digits.
See Custom refit strategy of a grid search with cross-validation
for an example of classification report usage for
grid search with nested cross-validation.
3.3.2.8. Hamming loss¶
The hamming_loss computes the average Hamming loss or Hamming
distance between two sets
of samples.
If \(\hat{y}_{i,j}\) is the predicted value for the \(j\)-th label of a
given sample \(i\), \(y_{i,j}\) is the corresponding true value,
\(n_\text{samples}\) is the number of samples and \(n_\text{labels}\)
is the number of labels, then the Hamming loss \(L_{Hamming}\) is defined
as:
\[L_{Hamming}(y, \hat{y}) = \frac{1}{n_\text{samples} * n_\text{labels}} \sum_{i=0}^{n_\text{samples}-1} \sum_{j=0}^{n_\text{labels} - 1} 1(\hat{y}_{i,j} \not= y_{i,j})\]
where \(1(x)\) is the indicator function.
The equation above does not hold true in the case of multiclass classification.
Please refer to the note below for more information.
>>> from sklearn.metrics import hamming_loss
>>> y_pred = [1, 2, 3, 4]
>>> y_true = [2, 2, 3, 4]
>>> hamming_loss(y_true, y_pred)
0.25
In the multilabel case with binary label indicators:
>>> hamming_loss(np.array([[0, 1], [1, 1]]), np.zeros((2, 2)))
0.75
Note
In multiclass classification, the Hamming loss corresponds to the Hamming
distance between y_true and y_pred which is similar to the
Zero one loss function.  However, while zero-one loss penalizes
prediction sets that do not strictly match true sets, the Hamming loss
penalizes individual labels.  Thus the Hamming loss, upper bounded by the zero-one
loss, is always between zero and one, inclusive; and predicting a proper subset
or superset of the true labels will give a Hamming loss between
zero and one, exclusive.
3.3.2.9. Precision, recall and F-measures¶
Intuitively, precision is the ability
of the classifier not to label as positive a sample that is negative, and
recall is the
ability of the classifier to find all the positive samples.
The  F-measure
(\(F_\beta\) and \(F_1\) measures) can be interpreted as a weighted
harmonic mean of the precision and recall. A
\(F_\beta\) measure reaches its best value at 1 and its worst score at 0.
With \(\beta = 1\),  \(F_\beta\) and
\(F_1\)  are equivalent, and the recall and the precision are equally important.
The precision_recall_curve computes a precision-recall curve
from the ground truth label and a score given by the classifier
by varying a decision threshold.
The average_precision_score function computes the
average precision
(AP) from prediction scores. The value is between 0 and 1 and higher is better.
AP is defined as
\[\text{AP} = \sum_n (R_n - R_{n-1}) P_n\]
where \(P_n\) and \(R_n\) are the precision and recall at the
nth threshold. With random predictions, the AP is the fraction of positive
samples.
References [Manning2008] and [Everingham2010] present alternative variants of
AP that interpolate the precision-recall curve. Currently,
average_precision_score does not implement any interpolated variant.
References [Davis2006] and [Flach2015] describe why a linear interpolation of
points on the precision-recall curve provides an overly-optimistic measure of
classifier performance. This linear interpolation is used when computing area
under the curve with the trapezoidal rule in auc.
Several functions allow you to analyze the precision, recall and F-measures
score:
average_precision_score(y_true, y_score, *)
Compute average precision (AP) from prediction scores.
f1_score(y_true, y_pred, *[, labels, ...])
Compute the F1 score, also known as balanced F-score or F-measure.
fbeta_score(y_true, y_pred, *, beta[, ...])
Compute the F-beta score.
precision_recall_curve(y_true, probas_pred, *)
Compute precision-recall pairs for different probability thresholds.
precision_recall_fscore_support(y_true, ...)
Compute precision, recall, F-measure and support for each class.
precision_score(y_true, y_pred, *[, labels, ...])
Compute the precision.
recall_score(y_true, y_pred, *[, labels, ...])
Compute the recall.
Note that the precision_recall_curve function is restricted to the
binary case. The average_precision_score function supports multiclass
and multilabel formats by computing each class score in a One-vs-the-rest (OvR)
fashion and averaging them or not depending of its average argument value.
The PrecisionRecallDisplay.from_estimator and
PrecisionRecallDisplay.from_predictions functions will plot the
precision-recall curve as follows.
Examples:
See Custom refit strategy of a grid search with cross-validation
for an example of precision_score and recall_score usage
to estimate parameters using grid search with nested cross-validation.
See Precision-Recall
for an example of precision_recall_curve usage to evaluate
classifier output quality.
References:
[Manning2008]
C.D. Manning, P. Raghavan, H. Schütze, Introduction to Information Retrieval,
2008.
[Everingham2010]
M. Everingham, L. Van Gool, C.K.I. Williams, J. Winn, A. Zisserman,
The Pascal Visual Object Classes (VOC) Challenge,
IJCV 2010.
[Davis2006]
J. Davis, M. Goadrich, The Relationship Between Precision-Recall and ROC Curves,
ICML 2006.
[Flach2015]
P.A. Flach, M. Kull, Precision-Recall-Gain Curves: PR Analysis Done Right,
NIPS 2015.
3.3.2.9.1. Binary classification¶
In a binary classification task, the terms ‘’positive’’ and ‘’negative’’ refer
to the classifier’s prediction, and the terms ‘’true’’ and ‘’false’’ refer to
whether that prediction corresponds to the external judgment (sometimes known
as the ‘’observation’’). Given these definitions, we can formulate the
following table:
Actual class (observation)
Predicted class
(expectation)
tp (true positive)
Correct result
fp (false positive)
Unexpected result
fn (false negative)
Missing result
tn (true negative)
Correct absence of result
In this context, we can define the notions of precision and recall:
\[\text{precision} = \frac{\text{tp}}{\text{tp} + \text{fp}},\]
\[\text{recall} = \frac{\text{tp}}{\text{tp} + \text{fn}},\]
(Sometimes recall is also called ‘’sensitivity’’)
F-measure is the weighted harmonic mean of precision and recall, with precision’s
contribution to the mean weighted by some parameter \(\beta\):
\[F_\beta = (1 + \beta^2) \frac{\text{precision} \times \text{recall}}{\beta^2 \text{precision} + \text{recall}}\]
To avoid division by zero when precision and recall are zero, Scikit-Learn calculates F-measure with this
otherwise-equivalent formula:
\[F_\beta = \frac{(1 + \beta^2) \text{tp}}{(1 + \beta^2) \text{tp} + \text{fp} + \beta^2 \text{fn}}\]
Note that this formula is still undefined when there are no true positives, false
positives, or false negatives. By default, F-1 for a set of exclusively true negatives
is calculated as 0, however this behavior can be changed using the zero_division
parameter.
Here are some small examples in binary classification:
>>> from sklearn import metrics
>>> y_pred = [0, 1, 0, 0]
>>> y_true = [0, 1, 0, 1]
>>> metrics.precision_score(y_true, y_pred)
1.0
>>> metrics.recall_score(y_true, y_pred)
0.5
>>> metrics.f1_score(y_true, y_pred)
0.66...
>>> metrics.fbeta_score(y_true, y_pred, beta=0.5)
0.83...
>>> metrics.fbeta_score(y_true, y_pred, beta=1)
0.66...
>>> metrics.fbeta_score(y_true, y_pred, beta=2)
0.55...
>>> metrics.precision_recall_fscore_support(y_true, y_pred, beta=0.5)
(array([0.66..., 1.        ]), array([1. , 0.5]), array([0.71..., 0.83...]), array([2, 2]))
>>> import numpy as np
>>> from sklearn.metrics import precision_recall_curve
>>> from sklearn.metrics import average_precision_score
>>> y_true = np.array([0, 0, 1, 1])
>>> y_scores = np.array([0.1, 0.4, 0.35, 0.8])
>>> precision, recall, threshold = precision_recall_curve(y_true, y_scores)
>>> precision
array([0.5       , 0.66..., 0.5       , 1.        , 1.        ])
>>> recall
array([1. , 1. , 0.5, 0.5, 0. ])
>>> threshold
array([0.1 , 0.35, 0.4 , 0.8 ])
>>> average_precision_score(y_true, y_scores)
0.83...
3.3.2.9.2. Multiclass and multilabel classification¶
In a multiclass and multilabel classification task, the notions of precision,
recall, and F-measures can be applied to each label independently.
There are a few ways to combine results across labels,
specified by the average argument to the
average_precision_score, f1_score,
fbeta_score, precision_recall_fscore_support,
precision_score and recall_score functions, as described
above. Note that if all labels are included, “micro”-averaging
in a multiclass setting will produce precision, recall and \(F\)
that are all identical to accuracy. Also note that “weighted” averaging may
produce an F-score that is not between precision and recall.
To make this more explicit, consider the following notation:
\(y\) the set of true \((sample, label)\) pairs
\(\hat{y}\) the set of predicted \((sample, label)\) pairs
\(L\) the set of labels
\(S\) the set of samples
\(y_s\) the subset of \(y\) with sample \(s\),
i.e. \(y_s := \left\{(s', l) \in y | s' = s\right\}\)
\(y_l\) the subset of \(y\) with label \(l\)
similarly, \(\hat{y}_s\) and \(\hat{y}_l\) are subsets of
\(\hat{y}\)
\(P(A, B) := \frac{\left| A \cap B \right|}{\left|B\right|}\) for some
sets \(A\) and \(B\)
\(R(A, B) := \frac{\left| A \cap B \right|}{\left|A\right|}\)
(Conventions vary on handling \(A = \emptyset\); this implementation uses
\(R(A, B):=0\), and similar for \(P\).)
\(F_\beta(A, B) := \left(1 + \beta^2\right) \frac{P(A, B) \times R(A, B)}{\beta^2 P(A, B) + R(A, B)}\)
Then the metrics are defined as:
average
Precision
Recall
F_beta
"micro"
\(P(y, \hat{y})\)
\(R(y, \hat{y})\)
\(F_\beta(y, \hat{y})\)
"samples"
\(\frac{1}{\left|S\right|} \sum_{s \in S} P(y_s, \hat{y}_s)\)
\(\frac{1}{\left|S\right|} \sum_{s \in S} R(y_s, \hat{y}_s)\)
\(\frac{1}{\left|S\right|} \sum_{s \in S} F_\beta(y_s, \hat{y}_s)\)
"macro"
\(\frac{1}{\left|L\right|} \sum_{l \in L} P(y_l, \hat{y}_l)\)
\(\frac{1}{\left|L\right|} \sum_{l \in L} R(y_l, \hat{y}_l)\)
\(\frac{1}{\left|L\right|} \sum_{l \in L} F_\beta(y_l, \hat{y}_l)\)
"weighted"
\(\frac{1}{\sum_{l \in L} \left|y_l\right|} \sum_{l \in L} \left|y_l\right| P(y_l, \hat{y}_l)\)
\(\frac{1}{\sum_{l \in L} \left|y_l\right|} \sum_{l \in L} \left|y_l\right| R(y_l, \hat{y}_l)\)
\(\frac{1}{\sum_{l \in L} \left|y_l\right|} \sum_{l \in L} \left|y_l\right| F_\beta(y_l, \hat{y}_l)\)
None
\(\langle P(y_l, \hat{y}_l) | l \in L \rangle\)
\(\langle R(y_l, \hat{y}_l) | l \in L \rangle\)
\(\langle F_\beta(y_l, \hat{y}_l) | l \in L \rangle\)
>>> from sklearn import metrics
>>> y_true = [0, 1, 2, 0, 1, 2]
>>> y_pred = [0, 2, 1, 0, 0, 1]
>>> metrics.precision_score(y_true, y_pred, average='macro')
0.22...
>>> metrics.recall_score(y_true, y_pred, average='micro')
0.33...
>>> metrics.f1_score(y_true, y_pred, average='weighted')
0.26...
>>> metrics.fbeta_score(y_true, y_pred, average='macro', beta=0.5)
0.23...
>>> metrics.precision_recall_fscore_support(y_true, y_pred, beta=0.5, average=None)
(array([0.66..., 0.        , 0.        ]), array([1., 0., 0.]), array([0.71..., 0.        , 0.        ]), array([2, 2, 2]...))
For multiclass classification with a “negative class”, it is possible to exclude some labels:
>>> metrics.recall_score(y_true, y_pred, labels=[1, 2], average='micro')
... # excluding 0, no labels were correctly recalled
0.0
Similarly, labels not present in the data sample may be accounted for in macro-averaging.
>>> metrics.precision_score(y_true, y_pred, labels=[0, 1, 2, 3], average='macro')
0.166...
3.3.2.10. Jaccard similarity coefficient score¶
The jaccard_score function computes the average of Jaccard similarity
coefficients, also called the
Jaccard index, between pairs of label sets.
The Jaccard similarity coefficient with a ground truth label set \(y\) and
predicted label set \(\hat{y}\), is defined as
\[J(y, \hat{y}) = \frac{|y \cap \hat{y}|}{|y \cup \hat{y}|}.\]
The jaccard_score (like precision_recall_fscore_support) applies
natively to binary targets. By computing it set-wise it can be extended to apply
to multilabel and multiclass through the use of average (see
above).
In the binary case:
>>> import numpy as np
>>> from sklearn.metrics import jaccard_score
>>> y_true = np.array([[0, 1, 1],
...                    [1, 1, 0]])
>>> y_pred = np.array([[1, 1, 1],
...                    [1, 0, 0]])
>>> jaccard_score(y_true[0], y_pred[0])
0.6666...
In the 2D comparison case (e.g. image similarity):
>>> jaccard_score(y_true, y_pred, average="micro")
0.6
In the multilabel case with binary label indicators:
>>> jaccard_score(y_true, y_pred, average='samples')
0.5833...
>>> jaccard_score(y_true, y_pred, average='macro')
0.6666...
>>> jaccard_score(y_true, y_pred, average=None)
array([0.5, 0.5, 1. ])
Multiclass problems are binarized and treated like the corresponding
multilabel problem:
>>> y_pred = [0, 2, 1, 2]
>>> y_true = [0, 1, 2, 2]
>>> jaccard_score(y_true, y_pred, average=None)
array([1. , 0. , 0.33...])
>>> jaccard_score(y_true, y_pred, average='macro')
0.44...
>>> jaccard_score(y_true, y_pred, average='micro')
0.33...
3.3.2.11. Hinge loss¶
The hinge_loss function computes the average distance between
the model and the data using
hinge loss, a one-sided metric
that considers only prediction errors. (Hinge
loss is used in maximal margin classifiers such as support vector machines.)
If the true label \(y_i\) of a binary classification task is encoded as
\(y_i=\left\{-1, +1\right\}\) for every sample \(i\); and \(w_i\)
is the corresponding predicted decision (an array of shape (n_samples,) as
output by the decision_function method), then the hinge loss is defined as:
\[L_\text{Hinge}(y, w) = \frac{1}{n_\text{samples}} \sum_{i=0}^{n_\text{samples}-1} \max\left\{1 - w_i y_i, 0\right\}\]
If there are more than two labels, hinge_loss uses a multiclass variant
due to Crammer & Singer.
Here is
the paper describing it.
In this case the predicted decision is an array of shape (n_samples,
n_labels). If \(w_{i, y_i}\) is the predicted decision for the true label
\(y_i\) of the \(i\)-th sample; and
\(\hat{w}_{i, y_i} = \max\left\{w_{i, y_j}~|~y_j \ne y_i \right\}\)
is the maximum of the
predicted decisions for all the other labels, then the multi-class hinge loss
is defined by:
\[L_\text{Hinge}(y, w) = \frac{1}{n_\text{samples}}
\sum_{i=0}^{n_\text{samples}-1} \max\left\{1 + \hat{w}_{i, y_i}
- w_{i, y_i}, 0\right\}\]
Here is a small example demonstrating the use of the hinge_loss function
with a svm classifier in a binary class problem:
>>> from sklearn import svm
>>> from sklearn.metrics import hinge_loss
>>> X = [[0], [1]]
>>> y = [-1, 1]
>>> est = svm.LinearSVC(dual="auto", random_state=0)
>>> est.fit(X, y)
LinearSVC(dual='auto', random_state=0)
>>> pred_decision = est.decision_function([[-2], [3], [0.5]])
>>> pred_decision
array([-2.18...,  2.36...,  0.09...])
>>> hinge_loss([-1, 1, 1], pred_decision)
0.3...
Here is an example demonstrating the use of the hinge_loss function
with a svm classifier in a multiclass problem:
>>> X = np.array([[0], [1], [2], [3]])
>>> Y = np.array([0, 1, 2, 3])
>>> labels = np.array([0, 1, 2, 3])
>>> est = svm.LinearSVC(dual="auto")
>>> est.fit(X, Y)
LinearSVC(dual='auto')
>>> pred_decision = est.decision_function([[-1], [2], [3]])
>>> y_true = [0, 2, 3]
>>> hinge_loss(y_true, pred_decision, labels=labels)
0.56...
3.3.2.12. Log loss¶
Log loss, also called logistic regression loss or
cross-entropy loss, is defined on probability estimates.  It is
commonly used in (multinomial) logistic regression and neural networks, as well
as in some variants of expectation-maximization, and can be used to evaluate the
probability outputs (predict_proba) of a classifier instead of its
discrete predictions.
For binary classification with a true label \(y \in \{0,1\}\)
and a probability estimate \(p = \operatorname{Pr}(y = 1)\),
the log loss per sample is the negative log-likelihood
of the classifier given the true label:
\[L_{\log}(y, p) = -\log \operatorname{Pr}(y|p) = -(y \log (p) + (1 - y) \log (1 - p))\]
This extends to the multiclass case as follows.
Let the true labels for a set of samples
be encoded as a 1-of-K binary indicator matrix \(Y\),
i.e., \(y_{i,k} = 1\) if sample \(i\) has label \(k\)
taken from a set of \(K\) labels.
Let \(P\) be a matrix of probability estimates,
with \(p_{i,k} = \operatorname{Pr}(y_{i,k} = 1)\).
Then the log loss of the whole set is
\[L_{\log}(Y, P) = -\log \operatorname{Pr}(Y|P) = - \frac{1}{N} \sum_{i=0}^{N-1} \sum_{k=0}^{K-1} y_{i,k} \log p_{i,k}\]
To see how this generalizes the binary log loss given above,
note that in the binary case,
\(p_{i,0} = 1 - p_{i,1}\) and \(y_{i,0} = 1 - y_{i,1}\),
so expanding the inner sum over \(y_{i,k} \in \{0,1\}\)
gives the binary log loss.
The log_loss function computes log loss given a list of ground-truth
labels and a probability matrix, as returned by an estimator’s predict_proba
method.
>>> from sklearn.metrics import log_loss
>>> y_true = [0, 0, 1, 1]
>>> y_pred = [[.9, .1], [.8, .2], [.3, .7], [.01, .99]]
>>> log_loss(y_true, y_pred)
0.1738...
The first [.9, .1] in y_pred denotes 90% probability that the first
sample has label 0.  The log loss is non-negative.
3.3.2.13. Matthews correlation coefficient¶
The matthews_corrcoef function computes the
Matthew’s correlation coefficient (MCC)
for binary classes.  Quoting Wikipedia:
“The Matthews correlation coefficient is used in machine learning as a
measure of the quality of binary (two-class) classifications. It takes
into account true and false positives and negatives and is generally
regarded as a balanced measure which can be used even if the classes are
of very different sizes. The MCC is in essence a correlation coefficient
value between -1 and +1. A coefficient of +1 represents a perfect
prediction, 0 an average random prediction and -1 an inverse prediction.
The statistic is also known as the phi coefficient.”
In the binary (two-class) case, \(tp\), \(tn\), \(fp\) and
\(fn\) are respectively the number of true positives, true negatives, false
positives and false negatives, the MCC is defined as
\[MCC = \frac{tp \times tn - fp \times fn}{\sqrt{(tp + fp)(tp + fn)(tn + fp)(tn + fn)}}.\]
In the multiclass case, the Matthews correlation coefficient can be defined in terms of a
confusion_matrix \(C\) for \(K\) classes.  To simplify the
definition consider the following intermediate variables:
\(t_k=\sum_{i}^{K} C_{ik}\) the number of times class \(k\) truly occurred,
\(p_k=\sum_{i}^{K} C_{ki}\) the number of times class \(k\) was predicted,
\(c=\sum_{k}^{K} C_{kk}\) the total number of samples correctly predicted,
\(s=\sum_{i}^{K} \sum_{j}^{K} C_{ij}\) the total number of samples.
Then the multiclass MCC is defined as:
\[MCC = \frac{
c \times s - \sum_{k}^{K} p_k \times t_k
}{\sqrt{
(s^2 - \sum_{k}^{K} p_k^2) \times
(s^2 - \sum_{k}^{K} t_k^2)
}}\]
When there are more than two labels, the value of the MCC will no longer range
between -1 and +1. Instead the minimum value will be somewhere between -1 and 0
depending on the number and distribution of ground true labels. The maximum
value is always +1.
Here is a small example illustrating the usage of the matthews_corrcoef
function:
>>> from sklearn.metrics import matthews_corrcoef
>>> y_true = [+1, +1, +1, -1]
>>> y_pred = [+1, -1, +1, +1]
>>> matthews_corrcoef(y_true, y_pred)
-0.33...
3.3.2.14. Multi-label confusion matrix¶
The multilabel_confusion_matrix function computes class-wise (default)
or sample-wise (samplewise=True) multilabel confusion matrix to evaluate
the accuracy of a classification. multilabel_confusion_matrix also treats
multiclass data as if it were multilabel, as this is a transformation commonly
applied to evaluate multiclass problems with binary classification metrics
(such as precision, recall, etc.).
When calculating class-wise multilabel confusion matrix \(C\), the
count of true negatives for class \(i\) is \(C_{i,0,0}\), false
negatives is \(C_{i,1,0}\), true positives is \(C_{i,1,1}\)
and false positives is \(C_{i,0,1}\).
Here is an example demonstrating the use of the
multilabel_confusion_matrix function with
multilabel indicator matrix input:
>>> import numpy as np
>>> from sklearn.metrics import multilabel_confusion_matrix
>>> y_true = np.array([[1, 0, 1],
...                    [0, 1, 0]])
>>> y_pred = np.array([[1, 0, 0],
...                    [0, 1, 1]])
>>> multilabel_confusion_matrix(y_true, y_pred)
array([[[1, 0],
[0, 1]],
[[1, 0],
[0, 1]],
[[0, 1],
[1, 0]]])
Or a confusion matrix can be constructed for each sample’s labels:
>>> multilabel_confusion_matrix(y_true, y_pred, samplewise=True)
array([[[1, 0],
[1, 1]],
[[1, 1],
[0, 1]]])
Here is an example demonstrating the use of the
multilabel_confusion_matrix function with
multiclass input:
>>> y_true = ["cat", "ant", "cat", "cat", "ant", "bird"]
>>> y_pred = ["ant", "ant", "cat", "cat", "ant", "cat"]
>>> multilabel_confusion_matrix(y_true, y_pred,
...                             labels=["ant", "bird", "cat"])
array([[[3, 1],
[0, 2]],
[[5, 0],
[1, 0]],
[[2, 1],
[1, 2]]])
Here are some examples demonstrating the use of the
multilabel_confusion_matrix function to calculate recall
(or sensitivity), specificity, fall out and miss rate for each class in a
problem with multilabel indicator matrix input.
Calculating
recall
(also called the true positive rate or the sensitivity) for each class:
>>> y_true = np.array([[0, 0, 1],
...                    [0, 1, 0],
...                    [1, 1, 0]])
>>> y_pred = np.array([[0, 1, 0],
...                    [0, 0, 1],
...                    [1, 1, 0]])
>>> mcm = multilabel_confusion_matrix(y_true, y_pred)
>>> tn = mcm[:, 0, 0]
>>> tp = mcm[:, 1, 1]
>>> fn = mcm[:, 1, 0]
>>> fp = mcm[:, 0, 1]
>>> tp / (tp + fn)
array([1. , 0.5, 0. ])
Calculating
specificity
(also called the true negative rate) for each class:
>>> tn / (tn + fp)
array([1. , 0. , 0.5])
Calculating fall out
(also called the false positive rate) for each class:
>>> fp / (fp + tn)
array([0. , 1. , 0.5])
Calculating miss rate
(also called the false negative rate) for each class:
>>> fn / (fn + tp)
array([0. , 0.5, 1. ])
3.3.2.15. Receiver operating characteristic (ROC)¶
The function roc_curve computes the
receiver operating characteristic curve, or ROC curve.
Quoting Wikipedia :
“A receiver operating characteristic (ROC), or simply ROC curve, is a
graphical plot which illustrates the performance of a binary classifier
system as its discrimination threshold is varied. It is created by plotting
the fraction of true positives out of the positives (TPR = true positive
rate) vs. the fraction of false positives out of the negatives (FPR = false
positive rate), at various threshold settings. TPR is also known as
sensitivity, and FPR is one minus the specificity or true negative rate.”
This function requires the true binary value and the target scores, which can
either be probability estimates of the positive class, confidence values, or
binary decisions. Here is a small example of how to use the roc_curve
function:
>>> import numpy as np
>>> from sklearn.metrics import roc_curve
>>> y = np.array([1, 1, 2, 2])
>>> scores = np.array([0.1, 0.4, 0.35, 0.8])
>>> fpr, tpr, thresholds = roc_curve(y, scores, pos_label=2)
>>> fpr
array([0. , 0. , 0.5, 0.5, 1. ])
>>> tpr
array([0. , 0.5, 0.5, 1. , 1. ])
>>> thresholds
array([ inf, 0.8 , 0.4 , 0.35, 0.1 ])
Compared to metrics such as the subset accuracy, the Hamming loss, or the
F1 score, ROC doesn’t require optimizing a threshold for each label.
The roc_auc_score function, denoted by ROC-AUC or AUROC, computes the
area under the ROC curve. By doing so, the curve information is summarized in
one number.
The following figure shows the ROC curve and ROC-AUC score for a classifier
aimed to distinguish the virginica flower from the rest of the species in the
Iris plants dataset:
For more information see the Wikipedia article on AUC.
3.3.2.15.1. Binary case¶
In the binary case, you can either provide the probability estimates, using
the classifier.predict_proba() method, or the non-thresholded decision values
given by the classifier.decision_function() method. In the case of providing
the probability estimates, the probability of the class with the
“greater label” should be provided. The “greater label” corresponds to
classifier.classes_[1] and thus classifier.predict_proba(X)[:, 1].
Therefore, the y_score parameter is of size (n_samples,).
>>> from sklearn.datasets import load_breast_cancer
>>> from sklearn.linear_model import LogisticRegression
>>> from sklearn.metrics import roc_auc_score
>>> X, y = load_breast_cancer(return_X_y=True)
>>> clf = LogisticRegression(solver="liblinear").fit(X, y)
>>> clf.classes_
array([0, 1])
We can use the probability estimates corresponding to clf.classes_[1].
>>> y_score = clf.predict_proba(X)[:, 1]
>>> roc_auc_score(y, y_score)
0.99...
Otherwise, we can use the non-thresholded decision values
>>> roc_auc_score(y, clf.decision_function(X))
0.99...
3.3.2.15.2. Multi-class case¶
The roc_auc_score function can also be used in multi-class
classification. Two averaging strategies are currently supported: the
one-vs-one algorithm computes the average of the pairwise ROC AUC scores, and
the one-vs-rest algorithm computes the average of the ROC AUC scores for each
class against all other classes. In both cases, the predicted labels are
provided in an array with values from 0 to n_classes, and the scores
correspond to the probability estimates that a sample belongs to a particular
class. The OvO and OvR algorithms support weighting uniformly
(average='macro') and by prevalence (average='weighted').
One-vs-one Algorithm: Computes the average AUC of all possible pairwise
combinations of classes. [HT2001] defines a multiclass AUC metric weighted
uniformly:
\[\frac{1}{c(c-1)}\sum_{j=1}^{c}\sum_{k > j}^c (\text{AUC}(j | k) +
\text{AUC}(k | j))\]
where \(c\) is the number of classes and \(\text{AUC}(j | k)\) is the
AUC with class \(j\) as the positive class and class \(k\) as the
negative class. In general,
\(\text{AUC}(j | k) \neq \text{AUC}(k | j))\) in the multiclass
case. This algorithm is used by setting the keyword argument multiclass
to 'ovo' and average to 'macro'.
The [HT2001] multiclass AUC metric can be extended to be weighted by the
prevalence:
\[\frac{1}{c(c-1)}\sum_{j=1}^{c}\sum_{k > j}^c p(j \cup k)(
\text{AUC}(j | k) + \text{AUC}(k | j))\]
where \(c\) is the number of classes. This algorithm is used by setting
the keyword argument multiclass to 'ovo' and average to
'weighted'. The 'weighted' option returns a prevalence-weighted average
as described in [FC2009].
One-vs-rest Algorithm: Computes the AUC of each class against the rest
[PD2000]. The algorithm is functionally the same as the multilabel case. To
enable this algorithm set the keyword argument multiclass to 'ovr'.
Additionally to 'macro' [F2006] and 'weighted' [F2001] averaging, OvR
supports 'micro' averaging.
In applications where a high false positive rate is not tolerable the parameter
max_fpr of roc_auc_score can be used to summarize the ROC curve up
to the given limit.
The following figure shows the micro-averaged ROC curve and its corresponding
ROC-AUC score for a classifier aimed to distinguish the different species in
the Iris plants dataset:
3.3.2.15.3. Multi-label case¶
In multi-label classification, the roc_auc_score function is
extended by averaging over the labels as above. In this case,
you should provide a y_score of shape (n_samples, n_classes). Thus, when
using the probability estimates, one needs to select the probability of the
class with the greater label for each output.
>>> from sklearn.datasets import make_multilabel_classification
>>> from sklearn.multioutput import MultiOutputClassifier
>>> X, y = make_multilabel_classification(random_state=0)
>>> inner_clf = LogisticRegression(solver="liblinear", random_state=0)
>>> clf = MultiOutputClassifier(inner_clf).fit(X, y)
>>> y_score = np.transpose([y_pred[:, 1] for y_pred in clf.predict_proba(X)])
>>> roc_auc_score(y, y_score, average=None)
array([0.82..., 0.86..., 0.94..., 0.85... , 0.94...])
And the decision values do not require such processing.
>>> from sklearn.linear_model import RidgeClassifierCV
>>> clf = RidgeClassifierCV().fit(X, y)
>>> y_score = clf.decision_function(X)
>>> roc_auc_score(y, y_score, average=None)
array([0.81..., 0.84... , 0.93..., 0.87..., 0.94...])
Examples:
See Multiclass Receiver Operating Characteristic (ROC)
for an example of using ROC to
evaluate the quality of the output of a classifier.
See Receiver Operating Characteristic (ROC) with cross validation
for an example of using ROC to
evaluate classifier output quality, using cross-validation.
See Species distribution modeling
for an example of using ROC to
model species distribution.
References:
[HT2001]
(1,2)
Hand, D.J. and Till, R.J., (2001). A simple generalisation
of the area under the ROC curve for multiple class classification problems.
Machine learning, 45(2), pp. 171-186.
[FC2009]
Ferri, Cèsar & Hernandez-Orallo, Jose & Modroiu, R. (2009).
An Experimental Comparison of Performance Measures for Classification.
Pattern Recognition Letters. 30. 27-38.
[PD2000]
Provost, F., Domingos, P. (2000). Well-trained PETs: Improving
probability estimation trees
(Section 6.2), CeDER Working Paper #IS-00-04, Stern School of Business,
New York University.
[F2006]
Fawcett, T., 2006. An introduction to ROC analysis.
Pattern Recognition Letters, 27(8), pp. 861-874.
[F2001]
Fawcett, T., 2001. Using rule sets to maximize
ROC performance
In Data Mining, 2001.
Proceedings IEEE International Conference, pp. 131-138.
3.3.2.16. Detection error tradeoff (DET)¶
The function det_curve computes the
detection error tradeoff curve (DET) curve [WikipediaDET2017].
Quoting Wikipedia:
“A detection error tradeoff (DET) graph is a graphical plot of error rates
for binary classification systems, plotting false reject rate vs. false
accept rate. The x- and y-axes are scaled non-linearly by their standard
normal deviates (or just by logarithmic transformation), yielding tradeoff
curves that are more linear than ROC curves, and use most of the image area
to highlight the differences of importance in the critical operating region.”
DET curves are a variation of receiver operating characteristic (ROC) curves
where False Negative Rate is plotted on the y-axis instead of True Positive
Rate.
DET curves are commonly plotted in normal deviate scale by transformation with
\(\phi^{-1}\) (with \(\phi\) being the cumulative distribution
function).
The resulting performance curves explicitly visualize the tradeoff of error
types for given classification algorithms.
See [Martin1997] for examples and further motivation.
This figure compares the ROC and DET curves of two example classifiers on the
same classification task:
Properties:
DET curves form a linear curve in normal deviate scale if the detection
scores are normally (or close-to normally) distributed.
It was shown by [Navratil2007] that the reverse is not necessarily true and
even more general distributions are able to produce linear DET curves.
The normal deviate scale transformation spreads out the points such that a
comparatively larger space of plot is occupied.
Therefore curves with similar classification performance might be easier to
distinguish on a DET plot.
With False Negative Rate being “inverse” to True Positive Rate the point
of perfection for DET curves is the origin (in contrast to the top left
corner for ROC curves).
Applications and limitations:
DET curves are intuitive to read and hence allow quick visual assessment of a
classifier’s performance.
Additionally DET curves can be consulted for threshold analysis and operating
point selection.
This is particularly helpful if a comparison of error types is required.
On the other hand DET curves do not provide their metric as a single number.
Therefore for either automated evaluation or comparison to other
classification tasks metrics like the derived area under ROC curve might be
better suited.
Examples:
See Detection error tradeoff (DET) curve
for an example comparison between receiver operating characteristic (ROC)
curves and Detection error tradeoff (DET) curves.
References:
[WikipediaDET2017]
Wikipedia contributors. Detection error tradeoff.
Wikipedia, The Free Encyclopedia. September 4, 2017, 23:33 UTC.
Available at: https://en.wikipedia.org/w/index.php?title=Detection_error_tradeoff&oldid=798982054.
Accessed February 19, 2018.
[Martin1997]
A. Martin, G. Doddington, T. Kamm, M. Ordowski, and M. Przybocki,
The DET Curve in Assessment of Detection Task Performance,
NIST 1997.
[Navratil2007]
J. Navractil and D. Klusacek,
“On Linear DETs,”
2007 IEEE International Conference on Acoustics,
Speech and Signal Processing - ICASSP ‘07, Honolulu,
HI, 2007, pp. IV-229-IV-232.
3.3.2.17. Zero one loss¶
The zero_one_loss function computes the sum or the average of the 0-1
classification loss (\(L_{0-1}\)) over \(n_{\text{samples}}\). By
default, the function normalizes over the sample. To get the sum of the
\(L_{0-1}\), set normalize to False.
In multilabel classification, the zero_one_loss scores a subset as
one if its labels strictly match the predictions, and as a zero if there
are any errors.  By default, the function returns the percentage of imperfectly
predicted subsets.  To get the count of such subsets instead, set
normalize to False
If \(\hat{y}_i\) is the predicted value of
the \(i\)-th sample and \(y_i\) is the corresponding true value,
then the 0-1 loss \(L_{0-1}\) is defined as:
\[L_{0-1}(y, \hat{y}) = \frac{1}{n_\text{samples}} \sum_{i=0}^{n_\text{samples}-1} 1(\hat{y}_i \not= y_i)\]
where \(1(x)\) is the indicator function. The zero one
loss can also be computed as \(zero-one loss = 1 - accuracy\).
>>> from sklearn.metrics import zero_one_loss
>>> y_pred = [1, 2, 3, 4]
>>> y_true = [2, 2, 3, 4]
>>> zero_one_loss(y_true, y_pred)
0.25
>>> zero_one_loss(y_true, y_pred, normalize=False)
1.0
In the multilabel case with binary label indicators, where the first label
set [0,1] has an error:
>>> zero_one_loss(np.array([[0, 1], [1, 1]]), np.ones((2, 2)))
0.5
>>> zero_one_loss(np.array([[0, 1], [1, 1]]), np.ones((2, 2)),  normalize=False)
1.0
Example:
See Recursive feature elimination with cross-validation
for an example of zero one loss usage to perform recursive feature
elimination with cross-validation.
3.3.2.18. Brier score loss¶
The brier_score_loss function computes the
Brier score
for binary classes [Brier1950]. Quoting Wikipedia:
“The Brier score is a proper score function that measures the accuracy of
probabilistic predictions. It is applicable to tasks in which predictions
must assign probabilities to a set of mutually exclusive discrete outcomes.”
This function returns the mean squared error of the actual outcome
\(y \in \{0,1\}\) and the predicted probability estimate
\(p = \operatorname{Pr}(y = 1)\) (predict_proba) as outputted by:
\[BS = \frac{1}{n_{\text{samples}}} \sum_{i=0}^{n_{\text{samples}} - 1}(y_i - p_i)^2\]
The Brier score loss is also between 0 to 1 and the lower the value (the mean
square difference is smaller), the more accurate the prediction is.
Here is a small example of usage of this function:
>>> import numpy as np
>>> from sklearn.metrics import brier_score_loss
>>> y_true = np.array([0, 1, 1, 0])
>>> y_true_categorical = np.array(["spam", "ham", "ham", "spam"])
>>> y_prob = np.array([0.1, 0.9, 0.8, 0.4])
>>> y_pred = np.array([0, 1, 1, 0])
>>> brier_score_loss(y_true, y_prob)
0.055
>>> brier_score_loss(y_true, 1 - y_prob, pos_label=0)
0.055
>>> brier_score_loss(y_true_categorical, y_prob, pos_label="ham")
0.055
>>> brier_score_loss(y_true, y_prob > 0.5)
0.0
The Brier score can be used to assess how well a classifier is calibrated.
However, a lower Brier score loss does not always mean a better calibration.
This is because, by analogy with the bias-variance decomposition of the mean
squared error, the Brier score loss can be decomposed as the sum of calibration
loss and refinement loss [Bella2012]. Calibration loss is defined as the mean
squared deviation from empirical probabilities derived from the slope of ROC
segments. Refinement loss can be defined as the expected optimal loss as
measured by the area under the optimal cost curve. Refinement loss can change
independently from calibration loss, thus a lower Brier score loss does not
necessarily mean a better calibrated model. “Only when refinement loss remains
the same does a lower Brier score loss always mean better calibration”
[Bella2012], [Flach2008].
Example:
See Probability calibration of classifiers
for an example of Brier score loss usage to perform probability
calibration of classifiers.
References:
[Brier1950]
G. Brier, Verification of forecasts expressed in terms of
probability,
Monthly weather review 78.1 (1950)
[Bella2012]
(1,2)
Bella, Ferri, Hernández-Orallo, and Ramírez-Quintana
“Calibration of Machine Learning Models”
in Khosrow-Pour, M. “Machine learning: concepts, methodologies, tools
and applications.” Hershey, PA: Information Science Reference (2012).
[Flach2008]
Flach, Peter, and Edson Matsubara. “On classification, ranking,
and probability estimation.”
Dagstuhl Seminar Proceedings. Schloss Dagstuhl-Leibniz-Zentrum fr Informatik (2008).
3.3.2.19. Class likelihood ratios¶
The class_likelihood_ratios function computes the positive and negative
likelihood ratios
\(LR_\pm\) for binary classes, which can be interpreted as the ratio of
post-test to pre-test odds as explained below. As a consequence, this metric is
invariant w.r.t. the class prevalence (the number of samples in the positive
class divided by the total number of samples) and can be extrapolated between
populations regardless of any possible class imbalance.
The \(LR_\pm\) metrics are therefore very useful in settings where the data
available to learn and evaluate a classifier is a study population with nearly
balanced classes, such as a case-control study, while the target application,
i.e. the general population, has very low prevalence.
The positive likelihood ratio \(LR_+\) is the probability of a classifier to
correctly predict that a sample belongs to the positive class divided by the
probability of predicting the positive class for a sample belonging to the
negative class:
\[LR_+ = \frac{\text{PR}(P+|T+)}{\text{PR}(P+|T-)}.\]
The notation here refers to predicted (\(P\)) or true (\(T\)) label and
the sign \(+\) and \(-\) refer to the positive and negative class,
respectively, e.g. \(P+\) stands for “predicted positive”.
Analogously, the negative likelihood ratio \(LR_-\) is the probability of a
sample of the positive class being classified as belonging to the negative class
divided by the probability of a sample of the negative class being correctly
classified:
\[LR_- = \frac{\text{PR}(P-|T+)}{\text{PR}(P-|T-)}.\]
For classifiers above chance \(LR_+\) above 1 higher is better, while
\(LR_-\) ranges from 0 to 1 and lower is better.
Values of \(LR_\pm\approx 1\) correspond to chance level.
Notice that probabilities differ from counts, for instance
\(\operatorname{PR}(P+|T+)\) is not equal to the number of true positive
counts tp (see the wikipedia page for
the actual formulas).
Interpretation across varying prevalence:
Both class likelihood ratios are interpretable in terms of an odds ratio
(pre-test and post-tests):
\[\text{post-test odds} = \text{Likelihood ratio} \times \text{pre-test odds}.\]
Odds are in general related to probabilities via
\[\text{odds} = \frac{\text{probability}}{1 - \text{probability}},\]
or equivalently
\[\text{probability} = \frac{\text{odds}}{1 + \text{odds}}.\]
On a given population, the pre-test probability is given by the prevalence. By
converting odds to probabilities, the likelihood ratios can be translated into a
probability of truly belonging to either class before and after a classifier
prediction:
\[\text{post-test odds} = \text{Likelihood ratio} \times
\frac{\text{pre-test probability}}{1 - \text{pre-test probability}},\]
\[\text{post-test probability} = \frac{\text{post-test odds}}{1 + \text{post-test odds}}.\]
Mathematical divergences:
The positive likelihood ratio is undefined when \(fp = 0\), which can be
interpreted as the classifier perfectly identifying positive cases. If \(fp
= 0\) and additionally \(tp = 0\), this leads to a zero/zero division. This
happens, for instance, when using a DummyClassifier that always predicts the
negative class and therefore the interpretation as a perfect classifier is lost.
The negative likelihood ratio is undefined when \(tn = 0\). Such divergence
is invalid, as \(LR_- > 1\) would indicate an increase in the odds of a
sample belonging to the positive class after being classified as negative, as if
the act of classifying caused the positive condition. This includes the case of
a DummyClassifier that always predicts the positive class (i.e. when
\(tn=fn=0\)).
Both class likelihood ratios are undefined when \(tp=fn=0\), which means
that no samples of the positive class were present in the testing set. This can
also happen when cross-validating highly imbalanced data.
In all the previous cases the class_likelihood_ratios function raises by
default an appropriate warning message and returns nan to avoid pollution when
averaging over cross-validation folds.
For a worked-out demonstration of the class_likelihood_ratios function,
see the example below.
Examples:
Class Likelihood Ratios to measure classification performance
References:
Wikipedia entry for Likelihood ratios in diagnostic testing
Brenner, H., & Gefeller, O. (1997).
Variation of sensitivity, specificity, likelihood ratios and predictive
values with disease prevalence.
Statistics in medicine, 16(9), 981-991.
- 3.3.3. Multilabel ranking metrics¶
In multilabel learning, each sample can have any number of ground truth labels
associated with it. The goal is to give high scores and better rank to
the ground truth labels.
3.3.3.1. Coverage error¶
The coverage_error function computes the average number of labels that
have to be included in the final prediction such that all true labels
are predicted. This is useful if you want to know how many top-scored-labels
you have to predict in average without missing any true one. The best value
of this metrics is thus the average number of true labels.
Note
Our implementation’s score is 1 greater than the one given in Tsoumakas
et al., 2010. This extends it to handle the degenerate case in which an
instance has 0 true labels.
Formally, given a binary indicator matrix of the ground truth labels
\(y \in \left\{0, 1\right\}^{n_\text{samples} \times n_\text{labels}}\) and the
score associated with each label
\(\hat{f} \in \mathbb{R}^{n_\text{samples} \times n_\text{labels}}\),
the coverage is defined as
\[coverage(y, \hat{f}) = \frac{1}{n_{\text{samples}}}
\sum_{i=0}^{n_{\text{samples}} - 1} \max_{j:y_{ij} = 1} \text{rank}_{ij}\]
with \(\text{rank}_{ij} = \left|\left\{k: \hat{f}_{ik} \geq \hat{f}_{ij} \right\}\right|\).
Given the rank definition, ties in y_scores are broken by giving the
maximal rank that would have been assigned to all tied values.
Here is a small example of usage of this function:
>>> import numpy as np
>>> from sklearn.metrics import coverage_error
>>> y_true = np.array([[1, 0, 0], [0, 0, 1]])
>>> y_score = np.array([[0.75, 0.5, 1], [1, 0.2, 0.1]])
>>> coverage_error(y_true, y_score)
2.5
3.3.3.2. Label ranking average precision¶
The label_ranking_average_precision_score function
implements label ranking average precision (LRAP). This metric is linked to
the average_precision_score function, but is based on the notion of
label ranking instead of precision and recall.
Label ranking average precision (LRAP) averages over the samples the answer to
the following question: for each ground truth label, what fraction of
higher-ranked labels were true labels? This performance measure will be higher
if you are able to give better rank to the labels associated with each sample.
The obtained score is always strictly greater than 0, and the best value is 1.
If there is exactly one relevant label per sample, label ranking average
precision is equivalent to the mean
reciprocal rank.
Formally, given a binary indicator matrix of the ground truth labels
\(y \in \left\{0, 1\right\}^{n_\text{samples} \times n_\text{labels}}\)
and the score associated with each label
\(\hat{f} \in \mathbb{R}^{n_\text{samples} \times n_\text{labels}}\),
the average precision is defined as
\[LRAP(y, \hat{f}) = \frac{1}{n_{\text{samples}}}
\sum_{i=0}^{n_{\text{samples}} - 1} \frac{1}{||y_i||_0}
\sum_{j:y_{ij} = 1} \frac{|\mathcal{L}_{ij}|}{\text{rank}_{ij}}\]
where
\(\mathcal{L}_{ij} = \left\{k: y_{ik} = 1, \hat{f}_{ik} \geq \hat{f}_{ij} \right\}\),
\(\text{rank}_{ij} = \left|\left\{k: \hat{f}_{ik} \geq \hat{f}_{ij} \right\}\right|\),
\(|\cdot|\) computes the cardinality of the set (i.e., the number of
elements in the set), and \(||\cdot||_0\) is the \(\ell_0\) “norm”
(which computes the number of nonzero elements in a vector).
Here is a small example of usage of this function:
>>> import numpy as np
>>> from sklearn.metrics import label_ranking_average_precision_score
>>> y_true = np.array([[1, 0, 0], [0, 0, 1]])
>>> y_score = np.array([[0.75, 0.5, 1], [1, 0.2, 0.1]])
>>> label_ranking_average_precision_score(y_true, y_score)
0.416...
3.3.3.3. Ranking loss¶
The label_ranking_loss function computes the ranking loss which
averages over the samples the number of label pairs that are incorrectly
ordered, i.e. true labels have a lower score than false labels, weighted by
the inverse of the number of ordered pairs of false and true labels.
The lowest achievable ranking loss is zero.
Formally, given a binary indicator matrix of the ground truth labels
\(y \in \left\{0, 1\right\}^{n_\text{samples} \times n_\text{labels}}\) and the
score associated with each label
\(\hat{f} \in \mathbb{R}^{n_\text{samples} \times n_\text{labels}}\),
the ranking loss is defined as
\[ranking\_loss(y, \hat{f}) =  \frac{1}{n_{\text{samples}}}
\sum_{i=0}^{n_{\text{samples}} - 1} \frac{1}{||y_i||_0(n_\text{labels} - ||y_i||_0)}
\left|\left\{(k, l): \hat{f}_{ik} \leq \hat{f}_{il}, y_{ik} = 1, y_{il} = 0 \right\}\right|\]
where \(|\cdot|\) computes the cardinality of the set (i.e., the number of
elements in the set) and \(||\cdot||_0\) is the \(\ell_0\) “norm”
(which computes the number of nonzero elements in a vector).
Here is a small example of usage of this function:
>>> import numpy as np
>>> from sklearn.metrics import label_ranking_loss
>>> y_true = np.array([[1, 0, 0], [0, 0, 1]])
>>> y_score = np.array([[0.75, 0.5, 1], [1, 0.2, 0.1]])
>>> label_ranking_loss(y_true, y_score)
0.75...
>>> # With the following prediction, we have perfect and minimal loss
>>> y_score = np.array([[1.0, 0.1, 0.2], [0.1, 0.2, 0.9]])
>>> label_ranking_loss(y_true, y_score)
0.0
References:
Tsoumakas, G., Katakis, I., & Vlahavas, I. (2010). Mining multi-label data. In
Data mining and knowledge discovery handbook (pp. 667-685). Springer US.
3.3.3.4. Normalized Discounted Cumulative Gain¶
Discounted Cumulative Gain (DCG) and Normalized Discounted Cumulative Gain
(NDCG) are ranking metrics implemented in dcg_score
and ndcg_score ; they compare a predicted order to
ground-truth scores, such as the relevance of answers to a query.
From the Wikipedia page for Discounted Cumulative Gain:
“Discounted cumulative gain (DCG) is a measure of ranking quality. In
information retrieval, it is often used to measure effectiveness of web search
engine algorithms or related applications. Using a graded relevance scale of
documents in a search-engine result set, DCG measures the usefulness, or gain,
of a document based on its position in the result list. The gain is accumulated
from the top of the result list to the bottom, with the gain of each result
discounted at lower ranks”
DCG orders the true targets (e.g. relevance of query answers) in the predicted
order, then multiplies them by a logarithmic decay and sums the result. The sum
can be truncated after the first \(K\) results, in which case we call it
DCG@K.
NDCG, or NDCG@K is DCG divided by the DCG obtained by a perfect prediction, so
that it is always between 0 and 1. Usually, NDCG is preferred to DCG.
Compared with the ranking loss, NDCG can take into account relevance scores,
rather than a ground-truth ranking. So if the ground-truth consists only of an
ordering, the ranking loss should be preferred; if the ground-truth consists of
actual usefulness scores (e.g. 0 for irrelevant, 1 for relevant, 2 for very
relevant), NDCG can be used.
For one sample, given the vector of continuous ground-truth values for each
target \(y \in \mathbb{R}^{M}\), where \(M\) is the number of outputs, and
the prediction \(\hat{y}\), which induces the ranking function \(f\), the
DCG score is
\[\sum_{r=1}^{\min(K, M)}\frac{y_{f(r)}}{\log(1 + r)}\]
and the NDCG score is the DCG score divided by the DCG score obtained for
\(y\).
References:
Wikipedia entry for Discounted Cumulative Gain
Jarvelin, K., & Kekalainen, J. (2002).
Cumulated gain-based evaluation of IR techniques. ACM Transactions on
Information Systems (TOIS), 20(4), 422-446.
Wang, Y., Wang, L., Li, Y., He, D., Chen, W., & Liu, T. Y. (2013, May).
A theoretical analysis of NDCG ranking measures. In Proceedings of the 26th
Annual Conference on Learning Theory (COLT 2013)
McSherry, F., & Najork, M. (2008, March). Computing information retrieval
performance measures efficiently in the presence of tied scores. In
European conference on information retrieval (pp. 414-421). Springer,
Berlin, Heidelberg.
- 3.3.4. Regression metrics¶
The sklearn.metrics module implements several loss, score, and utility
functions to measure regression performance. Some of those have been enhanced
to handle the multioutput case: mean_squared_error,
mean_absolute_error, r2_score,
explained_variance_score, mean_pinball_loss, d2_pinball_score
and d2_absolute_error_score.
These functions have a multioutput keyword argument which specifies the
way the scores or losses for each individual target should be averaged. The
default is 'uniform_average', which specifies a uniformly weighted mean
over outputs. If an ndarray of shape (n_outputs,) is passed, then its
entries are interpreted as weights and an according weighted average is
returned. If multioutput is 'raw_values', then all unaltered
individual scores or losses will be returned in an array of shape
(n_outputs,).
The r2_score and explained_variance_score accept an additional
value 'variance_weighted' for the multioutput parameter. This option
leads to a weighting of each individual score by the variance of the
corresponding target variable. This setting quantifies the globally captured
unscaled variance. If the target variables are of different scale, then this
score puts more importance on explaining the higher variance variables.
multioutput='variance_weighted' is the default value for r2_score
for backward compatibility. This will be changed to uniform_average in the
future.
3.3.4.1. R² score, the coefficient of determination¶
The r2_score function computes the coefficient of
determination,
usually denoted as \(R^2\).
It represents the proportion of variance (of y) that has been explained by the
independent variables in the model. It provides an indication of goodness of
fit and therefore a measure of how well unseen samples are likely to be
predicted by the model, through the proportion of explained variance.
As such variance is dataset dependent, \(R^2\) may not be meaningfully comparable
across different datasets. Best possible score is 1.0 and it can be negative
(because the model can be arbitrarily worse). A constant model that always
predicts the expected (average) value of y, disregarding the input features,
would get an \(R^2\) score of 0.0.
Note: when the prediction residuals have zero mean, the \(R^2\) score and
the Explained variance score are identical.
If \(\hat{y}_i\) is the predicted value of the \(i\)-th sample
and \(y_i\) is the corresponding true value for total \(n\) samples,
the estimated \(R^2\) is defined as:
\[R^2(y, \hat{y}) = 1 - \frac{\sum_{i=1}^{n} (y_i - \hat{y}_i)^2}{\sum_{i=1}^{n} (y_i - \bar{y})^2}\]
where \(\bar{y} = \frac{1}{n} \sum_{i=1}^{n} y_i\) and \(\sum_{i=1}^{n} (y_i - \hat{y}_i)^2 = \sum_{i=1}^{n} \epsilon_i^2\).
Note that r2_score calculates unadjusted \(R^2\) without correcting for
bias in sample variance of y.
In the particular case where the true target is constant, the \(R^2\) score is
not finite: it is either NaN (perfect predictions) or -Inf (imperfect
predictions). Such non-finite scores may prevent correct model optimization
such as grid-search cross-validation to be performed correctly. For this reason
the default behaviour of r2_score is to replace them with 1.0 (perfect
predictions) or 0.0 (imperfect predictions). If force_finite
is set to False, this score falls back on the original \(R^2\) definition.
Here is a small example of usage of the r2_score function:
>>> from sklearn.metrics import r2_score
>>> y_true = [3, -0.5, 2, 7]
>>> y_pred = [2.5, 0.0, 2, 8]
>>> r2_score(y_true, y_pred)
0.948...
>>> y_true = [[0.5, 1], [-1, 1], [7, -6]]
>>> y_pred = [[0, 2], [-1, 2], [8, -5]]
>>> r2_score(y_true, y_pred, multioutput='variance_weighted')
0.938...
>>> y_true = [[0.5, 1], [-1, 1], [7, -6]]
>>> y_pred = [[0, 2], [-1, 2], [8, -5]]
>>> r2_score(y_true, y_pred, multioutput='uniform_average')
0.936...
>>> r2_score(y_true, y_pred, multioutput='raw_values')
array([0.965..., 0.908...])
>>> r2_score(y_true, y_pred, multioutput=[0.3, 0.7])
0.925...
>>> y_true = [-2, -2, -2]
>>> y_pred = [-2, -2, -2]
>>> r2_score(y_true, y_pred)
1.0
>>> r2_score(y_true, y_pred, force_finite=False)
nan
>>> y_true = [-2, -2, -2]
>>> y_pred = [-2, -2, -2 + 1e-8]
>>> r2_score(y_true, y_pred)
0.0
>>> r2_score(y_true, y_pred, force_finite=False)
-inf
Example:
See L1-based models for Sparse Signals
for an example of R² score usage to
evaluate Lasso and Elastic Net on sparse signals.
3.3.4.2. Mean absolute error¶
The mean_absolute_error function computes mean absolute
error, a risk
metric corresponding to the expected value of the absolute error loss or
\(l1\)-norm loss.
If \(\hat{y}_i\) is the predicted value of the \(i\)-th sample,
and \(y_i\) is the corresponding true value, then the mean absolute error
(MAE) estimated over \(n_{\text{samples}}\) is defined as
\[\text{MAE}(y, \hat{y}) = \frac{1}{n_{\text{samples}}} \sum_{i=0}^{n_{\text{samples}}-1} \left| y_i - \hat{y}_i \right|.\]
Here is a small example of usage of the mean_absolute_error function:
>>> from sklearn.metrics import mean_absolute_error
>>> y_true = [3, -0.5, 2, 7]
>>> y_pred = [2.5, 0.0, 2, 8]
>>> mean_absolute_error(y_true, y_pred)
0.5
>>> y_true = [[0.5, 1], [-1, 1], [7, -6]]
>>> y_pred = [[0, 2], [-1, 2], [8, -5]]
>>> mean_absolute_error(y_true, y_pred)
0.75
>>> mean_absolute_error(y_true, y_pred, multioutput='raw_values')
array([0.5, 1. ])
>>> mean_absolute_error(y_true, y_pred, multioutput=[0.3, 0.7])
0.85...
3.3.4.3. Mean squared error¶
The mean_squared_error function computes mean square
error, a risk
metric corresponding to the expected value of the squared (quadratic) error or
loss.
If \(\hat{y}_i\) is the predicted value of the \(i\)-th sample,
and \(y_i\) is the corresponding true value, then the mean squared error
(MSE) estimated over \(n_{\text{samples}}\) is defined as
\[\text{MSE}(y, \hat{y}) = \frac{1}{n_\text{samples}} \sum_{i=0}^{n_\text{samples} - 1} (y_i - \hat{y}_i)^2.\]
Here is a small example of usage of the mean_squared_error
function:
>>> from sklearn.metrics import mean_squared_error
>>> y_true = [3, -0.5, 2, 7]
>>> y_pred = [2.5, 0.0, 2, 8]
>>> mean_squared_error(y_true, y_pred)
0.375
>>> y_true = [[0.5, 1], [-1, 1], [7, -6]]
>>> y_pred = [[0, 2], [-1, 2], [8, -5]]
>>> mean_squared_error(y_true, y_pred)
0.7083...
Examples:
See Gradient Boosting regression
for an example of mean squared error usage to
evaluate gradient boosting regression.
Taking the square root of the MSE, called the root mean squared error (RMSE), is another
common metric that provides a measure in the same units as the target variable. RSME is
available through the root_mean_squared_error function.
3.3.4.4. Mean squared logarithmic error¶
The mean_squared_log_error function computes a risk metric
corresponding to the expected value of the squared logarithmic (quadratic)
error or loss.
If \(\hat{y}_i\) is the predicted value of the \(i\)-th sample,
and \(y_i\) is the corresponding true value, then the mean squared
logarithmic error (MSLE) estimated over \(n_{\text{samples}}\) is
defined as
\[\text{MSLE}(y, \hat{y}) = \frac{1}{n_\text{samples}} \sum_{i=0}^{n_\text{samples} - 1} (\log_e (1 + y_i) - \log_e (1 + \hat{y}_i) )^2.\]
Where \(\log_e (x)\) means the natural logarithm of \(x\). This metric
is best to use when targets having exponential growth, such as population
counts, average sales of a commodity over a span of years etc. Note that this
metric penalizes an under-predicted estimate greater than an over-predicted
estimate.
Here is a small example of usage of the mean_squared_log_error
function:
>>> from sklearn.metrics import mean_squared_log_error
>>> y_true = [3, 5, 2.5, 7]
>>> y_pred = [2.5, 5, 4, 8]
>>> mean_squared_log_error(y_true, y_pred)
0.039...
>>> y_true = [[0.5, 1], [1, 2], [7, 6]]
>>> y_pred = [[0.5, 2], [1, 2.5], [8, 8]]
>>> mean_squared_log_error(y_true, y_pred)
0.044...
The root mean squared logarithmic error (RMSLE) is available through the
root_mean_squared_log_error function.
3.3.4.5. Mean absolute percentage error¶
The mean_absolute_percentage_error (MAPE), also known as mean absolute
percentage deviation (MAPD), is an evaluation metric for regression problems.
The idea of this metric is to be sensitive to relative errors. It is for example
not changed by a global scaling of the target variable.
If \(\hat{y}_i\) is the predicted value of the \(i\)-th sample
and \(y_i\) is the corresponding true value, then the mean absolute percentage
error (MAPE) estimated over \(n_{\text{samples}}\) is defined as
\[\text{MAPE}(y, \hat{y}) = \frac{1}{n_{\text{samples}}} \sum_{i=0}^{n_{\text{samples}}-1} \frac{{}\left| y_i - \hat{y}_i \right|}{\max(\epsilon, \left| y_i \right|)}\]
where \(\epsilon\) is an arbitrary small yet strictly positive number to
avoid undefined results when y is zero.
The mean_absolute_percentage_error function supports multioutput.
Here is a small example of usage of the mean_absolute_percentage_error
function:
>>> from sklearn.metrics import mean_absolute_percentage_error
>>> y_true = [1, 10, 1e6]
>>> y_pred = [0.9, 15, 1.2e6]
>>> mean_absolute_percentage_error(y_true, y_pred)
0.2666...
In above example, if we had used mean_absolute_error, it would have ignored
the small magnitude values and only reflected the error in prediction of highest
magnitude value. But that problem is resolved in case of MAPE because it calculates
relative percentage error with respect to actual output.
3.3.4.6. Median absolute error¶
The median_absolute_error is particularly interesting because it is
robust to outliers. The loss is calculated by taking the median of all absolute
differences between the target and the prediction.
If \(\hat{y}_i\) is the predicted value of the \(i\)-th sample
and \(y_i\) is the corresponding true value, then the median absolute error
(MedAE) estimated over \(n_{\text{samples}}\) is defined as
\[\text{MedAE}(y, \hat{y}) = \text{median}(\mid y_1 - \hat{y}_1 \mid, \ldots, \mid y_n - \hat{y}_n \mid).\]
The median_absolute_error does not support multioutput.
Here is a small example of usage of the median_absolute_error
function:
>>> from sklearn.metrics import median_absolute_error
>>> y_true = [3, -0.5, 2, 7]
>>> y_pred = [2.5, 0.0, 2, 8]
>>> median_absolute_error(y_true, y_pred)
0.5
3.3.4.7. Max error¶
The max_error function computes the maximum residual error , a metric
that captures the worst case error between the predicted value and
the true value. In a perfectly fitted single output regression
model, max_error would be 0 on the training set and though this
would be highly unlikely in the real world, this metric shows the
extent of error that the model had when it was fitted.
If \(\hat{y}_i\) is the predicted value of the \(i\)-th sample,
and \(y_i\) is the corresponding true value, then the max error is
defined as
\[\text{Max Error}(y, \hat{y}) = \max(| y_i - \hat{y}_i |)\]
Here is a small example of usage of the max_error function:
>>> from sklearn.metrics import max_error
>>> y_true = [3, 2, 7, 1]
>>> y_pred = [9, 2, 7, 1]
>>> max_error(y_true, y_pred)
6
The max_error does not support multioutput.
3.3.4.8. Explained variance score¶
The explained_variance_score computes the explained variance
regression score.
If \(\hat{y}\) is the estimated target output, \(y\) the corresponding
(correct) target output, and \(Var\) is Variance, the square of the standard deviation,
then the explained variance is estimated as follow:
\[explained\_{}variance(y, \hat{y}) = 1 - \frac{Var\{ y - \hat{y}\}}{Var\{y\}}\]
The best possible score is 1.0, lower values are worse.
Link to R² score, the coefficient of determination
The difference between the explained variance score and the R² score, the coefficient of determination
is that when the explained variance score does not account for
systematic offset in the prediction. For this reason, the
R² score, the coefficient of determination should be preferred in general.
In the particular case where the true target is constant, the Explained
Variance score is not finite: it is either NaN (perfect predictions) or
-Inf (imperfect predictions). Such non-finite scores may prevent correct
model optimization such as grid-search cross-validation to be performed
correctly. For this reason the default behaviour of
explained_variance_score is to replace them with 1.0 (perfect
predictions) or 0.0 (imperfect predictions). You can set the force_finite
parameter to False to prevent this fix from happening and fallback on the
original Explained Variance score.
Here is a small example of usage of the explained_variance_score
function:
>>> from sklearn.metrics import explained_variance_score
>>> y_true = [3, -0.5, 2, 7]
>>> y_pred = [2.5, 0.0, 2, 8]
>>> explained_variance_score(y_true, y_pred)
0.957...
>>> y_true = [[0.5, 1], [-1, 1], [7, -6]]
>>> y_pred = [[0, 2], [-1, 2], [8, -5]]
>>> explained_variance_score(y_true, y_pred, multioutput='raw_values')
array([0.967..., 1.        ])
>>> explained_variance_score(y_true, y_pred, multioutput=[0.3, 0.7])
0.990...
>>> y_true = [-2, -2, -2]
>>> y_pred = [-2, -2, -2]
>>> explained_variance_score(y_true, y_pred)
1.0
>>> explained_variance_score(y_true, y_pred, force_finite=False)
nan
>>> y_true = [-2, -2, -2]
>>> y_pred = [-2, -2, -2 + 1e-8]
>>> explained_variance_score(y_true, y_pred)
0.0
>>> explained_variance_score(y_true, y_pred, force_finite=False)
-inf
3.3.4.9. Mean Poisson, Gamma, and Tweedie deviances¶
The mean_tweedie_deviance function computes the mean Tweedie
deviance error
with a power parameter (\(p\)). This is a metric that elicits
predicted expectation values of regression targets.
Following special cases exist,
when power=0 it is equivalent to mean_squared_error.
when power=1 it is equivalent to mean_poisson_deviance.
when power=2 it is equivalent to mean_gamma_deviance.
If \(\hat{y}_i\) is the predicted value of the \(i\)-th sample,
and \(y_i\) is the corresponding true value, then the mean Tweedie
deviance error (D) for power \(p\), estimated over \(n_{\text{samples}}\)
is defined as
\[\begin{split}\text{D}(y, \hat{y}) = \frac{1}{n_\text{samples}}
\sum_{i=0}^{n_\text{samples} - 1}
\begin{cases}
(y_i-\hat{y}_i)^2, & \text{for }p=0\text{ (Normal)}\\
2(y_i \log(y_i/\hat{y}_i) + \hat{y}_i - y_i),  & \text{for }p=1\text{ (Poisson)}\\
2(\log(\hat{y}_i/y_i) + y_i/\hat{y}_i - 1),  & \text{for }p=2\text{ (Gamma)}\\
2\left(\frac{\max(y_i,0)^{2-p}}{(1-p)(2-p)}-
\frac{y_i\,\hat{y}_i^{1-p}}{1-p}+\frac{\hat{y}_i^{2-p}}{2-p}\right),
& \text{otherwise}
\end{cases}\end{split}\]
Tweedie deviance is a homogeneous function of degree 2-power.
Thus, Gamma distribution with power=2 means that simultaneously scaling
y_true and y_pred has no effect on the deviance. For Poisson
distribution power=1 the deviance scales linearly, and for Normal
distribution (power=0), quadratically.  In general, the higher
power the less weight is given to extreme deviations between true
and predicted targets.
For instance, let’s compare the two predictions 1.5 and 150 that are both
50% larger than their corresponding true value.
The mean squared error (power=0) is very sensitive to the
prediction difference of the second point,:
>>> from sklearn.metrics import mean_tweedie_deviance
>>> mean_tweedie_deviance([1.0], [1.5], power=0)
0.25
>>> mean_tweedie_deviance([100.], [150.], power=0)
2500.0
If we increase power to 1,:
>>> mean_tweedie_deviance([1.0], [1.5], power=1)
0.18...
>>> mean_tweedie_deviance([100.], [150.], power=1)
18.9...
the difference in errors decreases. Finally, by setting, power=2:
>>> mean_tweedie_deviance([1.0], [1.5], power=2)
0.14...
>>> mean_tweedie_deviance([100.], [150.], power=2)
0.14...
we would get identical errors. The deviance when power=2 is thus only
sensitive to relative errors.
3.3.4.10. Pinball loss¶
The mean_pinball_loss function is used to evaluate the predictive
performance of quantile regression models.
\[\text{pinball}(y, \hat{y}) = \frac{1}{n_{\text{samples}}} \sum_{i=0}^{n_{\text{samples}}-1}  \alpha \max(y_i - \hat{y}_i, 0) + (1 - \alpha) \max(\hat{y}_i - y_i, 0)\]
The value of pinball loss is equivalent to half of mean_absolute_error when the quantile
parameter alpha is set to 0.5.
Here is a small example of usage of the mean_pinball_loss function:
>>> from sklearn.metrics import mean_pinball_loss
>>> y_true = [1, 2, 3]
>>> mean_pinball_loss(y_true, [0, 2, 3], alpha=0.1)
0.03...
>>> mean_pinball_loss(y_true, [1, 2, 4], alpha=0.1)
0.3...
>>> mean_pinball_loss(y_true, [0, 2, 3], alpha=0.9)
0.3...
>>> mean_pinball_loss(y_true, [1, 2, 4], alpha=0.9)
0.03...
>>> mean_pinball_loss(y_true, y_true, alpha=0.1)
0.0
>>> mean_pinball_loss(y_true, y_true, alpha=0.9)
0.0
It is possible to build a scorer object with a specific choice of alpha:
>>> from sklearn.metrics import make_scorer
>>> mean_pinball_loss_95p = make_scorer(mean_pinball_loss, alpha=0.95)
Such a scorer can be used to evaluate the generalization performance of a
quantile regressor via cross-validation:
>>> from sklearn.datasets import make_regression
>>> from sklearn.model_selection import cross_val_score
>>> from sklearn.ensemble import GradientBoostingRegressor
>>>
>>> X, y = make_regression(n_samples=100, random_state=0)
>>> estimator = GradientBoostingRegressor(
...     loss="quantile",
...     alpha=0.95,
...     random_state=0,
... )
>>> cross_val_score(estimator, X, y, cv=5, scoring=mean_pinball_loss_95p)
array([13.6..., 9.7..., 23.3..., 9.5..., 10.4...])
It is also possible to build scorer objects for hyper-parameter tuning. The
sign of the loss must be switched to ensure that greater means better as
explained in the example linked below.
Example:
See Prediction Intervals for Gradient Boosting Regression
for an example of using the pinball loss to evaluate and tune the
hyper-parameters of quantile regression models on data with non-symmetric
noise and outliers.
3.3.4.11. D² score¶
The D² score computes the fraction of deviance explained.
It is a generalization of R², where the squared error is generalized and replaced
by a deviance of choice \(\text{dev}(y, \hat{y})\)
(e.g., Tweedie, pinball or mean absolute error). D² is a form of a skill score.
It is calculated as
\[D^2(y, \hat{y}) = 1 - \frac{\text{dev}(y, \hat{y})}{\text{dev}(y, y_{\text{null}})} \,.\]
Where \(y_{\text{null}}\) is the optimal prediction of an intercept-only model
(e.g., the mean of y_true for the Tweedie case, the median for absolute
error and the alpha-quantile for pinball loss).
Like R², the best possible score is 1.0 and it can be negative (because the
model can be arbitrarily worse). A constant model that always predicts
\(y_{\text{null}}\), disregarding the input features, would get a D² score
of 0.0.
3.3.4.11.1. D² Tweedie score¶
The d2_tweedie_score function implements the special case of D²
where \(\text{dev}(y, \hat{y})\) is the Tweedie deviance, see Mean Poisson, Gamma, and Tweedie deviances.
It is also known as D² Tweedie and is related to McFadden’s likelihood ratio index.
The argument power defines the Tweedie power as for
mean_tweedie_deviance. Note that for power=0,
d2_tweedie_score equals r2_score (for single targets).
A scorer object with a specific choice of power can be built by:
>>> from sklearn.metrics import d2_tweedie_score, make_scorer
>>> d2_tweedie_score_15 = make_scorer(d2_tweedie_score, power=1.5)
3.3.4.11.2. D² pinball score¶
The d2_pinball_score function implements the special case
of D² with the pinball loss, see Pinball loss, i.e.:
\[\text{dev}(y, \hat{y}) = \text{pinball}(y, \hat{y}).\]
The argument alpha defines the slope of the pinball loss as for
mean_pinball_loss (Pinball loss). It determines the
quantile level alpha for which the pinball loss and also D²
are optimal. Note that for alpha=0.5 (the default) d2_pinball_score
equals d2_absolute_error_score.
A scorer object with a specific choice of alpha can be built by:
>>> from sklearn.metrics import d2_pinball_score, make_scorer
>>> d2_pinball_score_08 = make_scorer(d2_pinball_score, alpha=0.8)
3.3.4.11.3. D² absolute error score¶
The d2_absolute_error_score function implements the special case of
the Mean absolute error:
\[\text{dev}(y, \hat{y}) = \text{MAE}(y, \hat{y}).\]
Here are some usage examples of the d2_absolute_error_score function:
>>> from sklearn.metrics import d2_absolute_error_score
>>> y_true = [3, -0.5, 2, 7]
>>> y_pred = [2.5, 0.0, 2, 8]
>>> d2_absolute_error_score(y_true, y_pred)
0.764...
>>> y_true = [1, 2, 3]
>>> y_pred = [1, 2, 3]
>>> d2_absolute_error_score(y_true, y_pred)
1.0
>>> y_true = [1, 2, 3]
>>> y_pred = [2, 2, 2]
>>> d2_absolute_error_score(y_true, y_pred)
0.0
3.3.4.12. Visual evaluation of regression models¶
Among methods to assess the quality of regression models, scikit-learn provides
the PredictionErrorDisplay class. It allows to
visually inspect the prediction errors of a model in two different manners.
The plot on the left shows the actual values vs predicted values. For a
noise-free regression task aiming to predict the (conditional) expectation of
y, a perfect regression model would display data points on the diagonal
defined by predicted equal to actual values. The further away from this optimal
line, the larger the error of the model. In a more realistic setting with
irreducible noise, that is, when not all the variations of y can be explained
by features in X, then the best model would lead to a cloud of points densely
arranged around the diagonal.
Note that the above only holds when the predicted values is the expected value
of y given X. This is typically the case for regression models that
minimize the mean squared error objective function or more generally the
mean Tweedie deviance for any value of its
“power” parameter.
When plotting the predictions of an estimator that predicts a quantile
of y given X, e.g. QuantileRegressor
or any other model minimizing the pinball loss, a
fraction of the points are either expected to lie above or below the diagonal
depending on the estimated quantile level.
All in all, while intuitive to read, this plot does not really inform us on
what to do to obtain a better model.
The right-hand side plot shows the residuals (i.e. the difference between the
actual and the predicted values) vs. the predicted values.
This plot makes it easier to visualize if the residuals follow and
homoscedastic or heteroschedastic
distribution.
In particular, if the true distribution of y|X is Poisson or Gamma
distributed, it is expected that the variance of the residuals of the optimal
model would grow with the predicted value of E[y|X] (either linearly for
Poisson or quadratically for Gamma).
When fitting a linear least squares regression model (see
LinearRegression and
Ridge), we can use this plot to check
if some of the model assumptions
are met, in particular that the residuals should be uncorrelated, their
expected value should be null and that their variance should be constant
(homoschedasticity).
If this is not the case, and in particular if the residuals plot show some
banana-shaped structure, this is a hint that the model is likely mis-specified
and that non-linear feature engineering or switching to a non-linear regression
model might be useful.
Refer to the example below to see a model evaluation that makes use of this
display.
Example:
See Effect of transforming the targets in regression model for
an example on how to use PredictionErrorDisplay
to visualize the prediction quality improvement of a regression model
obtained by transforming the target before learning.
- 3.3.5. Clustering metrics¶
The sklearn.metrics module implements several loss, score, and utility
functions. For more information see the Clustering performance evaluation
section for instance clustering, and Biclustering evaluation for
biclustering.
- 3.3.6. Dummy estimators¶
When doing supervised learning, a simple sanity check consists of comparing
one’s estimator against simple rules of thumb. DummyClassifier
implements several such simple strategies for classification:
stratified generates random predictions by respecting the training
set class distribution.
most_frequent always predicts the most frequent label in the training set.
prior always predicts the class that maximizes the class prior
(like most_frequent) and predict_proba returns the class prior.
uniform generates predictions uniformly at random.
constant always predicts a constant label that is provided by the user.A major motivation of this method is F1-scoring, when the positive class
is in the minority.
Note that with all these strategies, the predict method completely ignores
the input data!
To illustrate DummyClassifier, first let’s create an imbalanced
dataset:
>>> from sklearn.datasets import load_iris
>>> from sklearn.model_selection import train_test_split
>>> X, y = load_iris(return_X_y=True)
>>> y[y != 1] = -1
>>> X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)
Next, let’s compare the accuracy of SVC and most_frequent:
>>> from sklearn.dummy import DummyClassifier
>>> from sklearn.svm import SVC
>>> clf = SVC(kernel='linear', C=1).fit(X_train, y_train)
>>> clf.score(X_test, y_test)
0.63...
>>> clf = DummyClassifier(strategy='most_frequent', random_state=0)
>>> clf.fit(X_train, y_train)
DummyClassifier(random_state=0, strategy='most_frequent')
>>> clf.score(X_test, y_test)
0.57...
We see that SVC doesn’t do much better than a dummy classifier. Now, let’s
change the kernel:
>>> clf = SVC(kernel='rbf', C=1).fit(X_train, y_train)
>>> clf.score(X_test, y_test)
0.94...
We see that the accuracy was boosted to almost 100%.  A cross validation
strategy is recommended for a better estimate of the accuracy, if it
is not too CPU costly. For more information see the Cross-validation: evaluating estimator performance
section. Moreover if you want to optimize over the parameter space, it is highly
recommended to use an appropriate methodology; see the Tuning the hyper-parameters of an estimator
section for details.
More generally, when the accuracy of a classifier is too close to random, it
probably means that something went wrong: features are not helpful, a
hyperparameter is not correctly tuned, the classifier is suffering from class
imbalance, etc…
DummyRegressor also implements four simple rules of thumb for regression:
mean always predicts the mean of the training targets.
median always predicts the median of the training targets.
quantile always predicts a user provided quantile of the training targets.
constant always predicts a constant value that is provided by the user.
In all these strategies, the predict method completely ignores
the input data.
### 3.4. Validation curves: plotting scores to evaluate models — scikit-learn 1.4.2 documentation

Install
User Guide
API
Examples
Community
Getting Started
Tutorial
What's new
Glossary
Development
FAQ
Support
Related packages
Roadmap
Governance
About us
GitHub
Other Versions and Download
More
Getting Started
Tutorial
What's new
Glossary
Development
FAQ
Support
Related packages
Roadmap
Governance
About us
GitHub
Other Versions and Download
Toggle Menu
PrevUp
Next
scikit-learn 1.4.2
Other versions
Please cite us if you use the software.
### 3.4. Validation curves: plotting scores to evaluate models

- 3.4.1. Validation curve
- 3.4.2. Learning curve
### 3.4. Validation curves: plotting scores to evaluate models¶

Every estimator has its advantages and drawbacks. Its generalization error
can be decomposed in terms of bias, variance and noise. The bias of an
estimator is its average error for different training sets. The variance
of an estimator indicates how sensitive it is to varying training sets. Noise
is a property of the data.
In the following plot, we see a function \(f(x) = \cos (\frac{3}{2} \pi x)\)
and some noisy samples from that function. We use three different estimators
to fit the function: linear regression with polynomial features of degree 1,
4 and 15. We see that the first estimator can at best provide only a poor fit
to the samples and the true function because it is too simple (high bias),
the second estimator approximates it almost perfectly and the last estimator
approximates the training data perfectly but does not fit the true function
very well, i.e. it is very sensitive to varying training data (high variance).
Bias and variance are inherent properties of estimators and we usually have to
select learning algorithms and hyperparameters so that both bias and variance
are as low as possible (see Bias-variance dilemma). Another way to reduce
the variance of a model is to use more training data. However, you should only
collect more training data if the true function is too complex to be
approximated by an estimator with a lower variance.
In the simple one-dimensional problem that we have seen in the example it is
easy to see whether the estimator suffers from bias or variance. However, in
high-dimensional spaces, models can become very difficult to visualize. For
this reason, it is often helpful to use the tools described below.
Examples:
Underfitting vs. Overfitting
Plotting Validation Curves
Plotting Learning Curves and Checking Models’ Scalability
- 3.4.1. Validation curve¶
To validate a model we need a scoring function (see Metrics and scoring: quantifying the quality of predictions),
for example accuracy for classifiers. The proper way of choosing multiple
hyperparameters of an estimator is of course grid search or similar methods
(see Tuning the hyper-parameters of an estimator) that select the hyperparameter with the maximum score
on a validation set or multiple validation sets. Note that if we optimize
the hyperparameters based on a validation score the validation score is biased
and not a good estimate of the generalization any longer. To get a proper
estimate of the generalization we have to compute the score on another test
set.
However, it is sometimes helpful to plot the influence of a single
hyperparameter on the training score and the validation score to find out
whether the estimator is overfitting or underfitting for some hyperparameter
values.
The function validation_curve can help in this case:
>>> import numpy as np
>>> from sklearn.model_selection import validation_curve
>>> from sklearn.datasets import load_iris
>>> from sklearn.svm import SVC
>>> np.random.seed(0)
>>> X, y = load_iris(return_X_y=True)
>>> indices = np.arange(y.shape[0])
>>> np.random.shuffle(indices)
>>> X, y = X[indices], y[indices]
>>> train_scores, valid_scores = validation_curve(
...     SVC(kernel="linear"), X, y, param_name="C", param_range=np.logspace(-7, 3, 3),
... )
>>> train_scores
array([[0.90..., 0.94..., 0.91..., 0.89..., 0.92...],
[0.9... , 0.92..., 0.93..., 0.92..., 0.93...],
[0.97..., 1...   , 0.98..., 0.97..., 0.99...]])
>>> valid_scores
array([[0.9..., 0.9... , 0.9... , 0.96..., 0.9... ],
[0.9..., 0.83..., 0.96..., 0.96..., 0.93...],
[1.... , 0.93..., 1....  , 1....  , 0.9... ]])
If you intend to plot the validation curves only, the class
ValidationCurveDisplay is more direct than
using matplotlib manually on the results of a call to validation_curve.
You can use the method
from_estimator similarly
to validation_curve to generate and plot the validation curve:
from sklearn.datasets import load_iris
from sklearn.model_selection import ValidationCurveDisplay
from sklearn.svm import SVC
from sklearn.utils import shuffle
X, y = load_iris(return_X_y=True)
X, y = shuffle(X, y, random_state=0)
ValidationCurveDisplay.from_estimator(
SVC(kernel="linear"), X, y, param_name="C", param_range=np.logspace(-7, 3, 10)
)
If the training score and the validation score are both low, the estimator will
be underfitting. If the training score is high and the validation score is low,
the estimator is overfitting and otherwise it is working very well. A low
training score and a high validation score is usually not possible. Underfitting,
overfitting, and a working model are shown in the in the plot below where we vary
the parameter gamma of an SVM with an RBF kernel on the digits dataset.
- 3.4.2. Learning curve¶
A learning curve shows the validation and training score of an estimator
for varying numbers of training samples. It is a tool to find out how much
we benefit from adding more training data and whether the estimator suffers
more from a variance error or a bias error. Consider the following example
where we plot the learning curve of a naive Bayes classifier and an SVM.
For the naive Bayes, both the validation score and the training score
converge to a value that is quite low with increasing size of the training
set. Thus, we will probably not benefit much from more training data.
In contrast, for small amounts of data, the training score of the SVM is
much greater than the validation score. Adding more training samples will
most likely increase generalization.
We can use the function learning_curve to generate the values
that are required to plot such a learning curve (number of samples
that have been used, the average scores on the training sets and the
average scores on the validation sets):
>>> from sklearn.model_selection import learning_curve
>>> from sklearn.svm import SVC
>>> train_sizes, train_scores, valid_scores = learning_curve(
...     SVC(kernel='linear'), X, y, train_sizes=[50, 80, 110], cv=5)
>>> train_sizes
array([ 50, 80, 110])
>>> train_scores
array([[0.98..., 0.98 , 0.98..., 0.98..., 0.98...],
[0.98..., 1.   , 0.98..., 0.98..., 0.98...],
[0.98..., 1.   , 0.98..., 0.98..., 0.99...]])
>>> valid_scores
array([[1. ,  0.93...,  1. ,  1. ,  0.96...],
[1. ,  0.96...,  1. ,  1. ,  0.96...],
[1. ,  0.96...,  1. ,  1. ,  0.96...]])
If you intend to plot the learning curves only, the class
LearningCurveDisplay will be easier to use.
You can use the method
from_estimator similarly
to learning_curve to generate and plot the learning curve:
from sklearn.datasets import load_iris
from sklearn.model_selection import LearningCurveDisplay
from sklearn.svm import SVC
from sklearn.utils import shuffle
X, y = load_iris(return_X_y=True)
X, y = shuffle(X, y, random_state=0)
LearningCurveDisplay.from_estimator(
SVC(kernel="linear"), X, y, train_sizes=[50, 80, 110], cv=5)
### 4.1. Partial Dependence and Individual Conditional Expectation plots — scikit-learn 1.4.2 documentation

Install
User Guide
API
Examples
Community
Getting Started
Tutorial
What's new
Glossary
Development
FAQ
Support
Related packages
Roadmap
Governance
About us
GitHub
Other Versions and Download
More
Getting Started
Tutorial
What's new
Glossary
Development
FAQ
Support
Related packages
Roadmap
Governance
About us
GitHub
Other Versions and Download
Toggle Menu
PrevUp
Next
scikit-learn 1.4.2
Other versions
Please cite us if you use the software.
### 4.1. Partial Dependence and Individual Conditional Expectation plots

- 4.1.1. Partial dependence plots
- 4.1.2. Individual conditional expectation (ICE) plot
- 4.1.3. Mathematical Definition
- 4.1.4. Computation methods
### 4.1. Partial Dependence and Individual Conditional Expectation plots¶

Partial dependence plots (PDP) and individual conditional expectation (ICE)
plots can be used to visualize and analyze interaction between the target
response [1] and a set of input features of interest.
Both PDPs [H2009] and ICEs [G2015] assume that the input features of interest
are independent from the complement features, and this assumption is often
violated in practice. Thus, in the case of correlated features, we will
create absurd data points to compute the PDP/ICE [M2019].
- 4.1.1. Partial dependence plots¶
Partial dependence plots (PDP) show the dependence between the target response
and a set of input features of interest, marginalizing over the values
of all other input features (the ‘complement’ features). Intuitively, we can
interpret the partial dependence as the expected target response as a
function of the input features of interest.
Due to the limits of human perception, the size of the set of input features of
interest must be small (usually, one or two) thus the input features of interest
are usually chosen among the most important features.
The figure below shows two one-way and one two-way partial dependence plots for
the bike sharing dataset, with a
HistGradientBoostingRegressor:
One-way PDPs tell us about the interaction between the target response and an input
feature of interest (e.g. linear, non-linear). The left plot in the above figure
shows the effect of the temperature on the number of bike rentals; we can clearly see
that a higher temperature is related with a higher number of bike rentals. Similarly, we
could analyze the effect of the humidity on the number of bike rentals (middle plot).
Thus, these interpretations are marginal, considering a feature at a time.
PDPs with two input features of interest show the interactions among the two features.
For example, the two-variable PDP in the above figure shows the dependence of the number
of bike rentals on joint values of temperature and humidity. We can clearly see an
interaction between the two features: with a temperature higher than 20 degrees Celsius,
mainly the humidity has a strong impact on the number of bike rentals. For lower
temperatures, both the temperature and the humidity have an impact on the number of bike
rentals.
The sklearn.inspection module provides a convenience function
from_estimator to create one-way and two-way partial
dependence plots. In the below example we show how to create a grid of
partial dependence plots: two one-way PDPs for the features 0 and 1
and a two-way PDP between the two features:
>>> from sklearn.datasets import make_hastie_10_2
>>> from sklearn.ensemble import GradientBoostingClassifier
>>> from sklearn.inspection import PartialDependenceDisplay
>>> X, y = make_hastie_10_2(random_state=0)
>>> clf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,
...     max_depth=1, random_state=0).fit(X, y)
>>> features = [0, 1, (0, 1)]
>>> PartialDependenceDisplay.from_estimator(clf, X, features)
<...>
You can access the newly created figure and Axes objects using plt.gcf()
and plt.gca().
To make a partial dependence plot with categorical features, you need to specify
which features are categorical using the parameter categorical_features. This
parameter takes a list of indices, names of the categorical features or a boolean
mask. The graphical representation of partial dependence for categorical features is
a bar plot or a 2D heatmap.
PDPs for multi-class classification
Click for more details
¶
For multi-class classification, you need to set the class label for which
the PDPs should be created via the target argument:
>>> from sklearn.datasets import load_iris
>>> iris = load_iris()
>>> mc_clf = GradientBoostingClassifier(n_estimators=10,
...     max_depth=1).fit(iris.data, iris.target)
>>> features = [3, 2, (3, 2)]
>>> PartialDependenceDisplay.from_estimator(mc_clf, X, features, target=0)
<...>
The same parameter target is used to specify the target in multi-output
regression settings.
If you need the raw values of the partial dependence function rather than
the plots, you can use the
sklearn.inspection.partial_dependence function:
>>> from sklearn.inspection import partial_dependence
>>> results = partial_dependence(clf, X, [0])
>>> results["average"]
array([[ 2.466...,  2.466..., ...
>>> results["values"]
[array([-1.624..., -1.592..., ...
The values at which the partial dependence should be evaluated are directly
generated from X. For 2-way partial dependence, a 2D-grid of values is
generated. The values field returned by
sklearn.inspection.partial_dependence gives the actual values
used in the grid for each input feature of interest. They also correspond to
the axis of the plots.
- 4.1.2. Individual conditional expectation (ICE) plot¶
Similar to a PDP, an individual conditional expectation (ICE) plot
shows the dependence between the target function and an input feature of
interest. However, unlike a PDP, which shows the average effect of the input
feature, an ICE plot visualizes the dependence of the prediction on a
feature for each sample separately with one line per sample.
Due to the limits of human perception, only one input feature of interest is
supported for ICE plots.
The figures below show two ICE plots for the bike sharing dataset,
with a HistGradientBoostingRegressor:.
The figures plot the corresponding PD line overlaid on ICE lines.
While the PDPs are good at showing the average effect of the target features,
they can obscure a heterogeneous relationship created by interactions.
When interactions are present the ICE plot will provide many more insights.
For example, we see that the ICE for the temperature feature gives us some
additional information: Some of the ICE lines are flat while some others
shows a decrease of the dependence for temperature above 35 degrees Celsius.
We observe a similar pattern for the humidity feature: some of the ICE
lines show a sharp decrease when the humidity is above 80%.
The sklearn.inspection module’s PartialDependenceDisplay.from_estimator
convenience function can be used to create ICE plots by setting
kind='individual'. In the example below, we show how to create a grid of
ICE plots:
>>> from sklearn.datasets import make_hastie_10_2
>>> from sklearn.ensemble import GradientBoostingClassifier
>>> from sklearn.inspection import PartialDependenceDisplay
>>> X, y = make_hastie_10_2(random_state=0)
>>> clf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,
...     max_depth=1, random_state=0).fit(X, y)
>>> features = [0, 1]
>>> PartialDependenceDisplay.from_estimator(clf, X, features,
...     kind='individual')
<...>
In ICE plots it might not be easy to see the average effect of the input
feature of interest. Hence, it is recommended to use ICE plots alongside
PDPs. They can be plotted together with
kind='both'.
>>> PartialDependenceDisplay.from_estimator(clf, X, features,
...     kind='both')
<...>
If there are too many lines in an ICE plot, it can be difficult to see
differences between individual samples and interpret the model. Centering the
ICE at the first value on the x-axis, produces centered Individual Conditional
Expectation (cICE) plots [G2015]. This puts emphasis on the divergence of
individual conditional expectations from the mean line, thus making it easier
to explore heterogeneous relationships. cICE plots can be plotted by setting
centered=True:
>>> PartialDependenceDisplay.from_estimator(clf, X, features,
...     kind='both', centered=True)
<...>
- 4.1.3. Mathematical Definition¶
Let \(X_S\) be the set of input features of interest (i.e. the features
parameter) and let \(X_C\) be its complement.
The partial dependence of the response \(f\) at a point \(x_S\) is
defined as:
\[\begin{split}pd_{X_S}(x_S) &\overset{def}{=} \mathbb{E}_{X_C}\left[ f(x_S, X_C) \right]\\
&= \int f(x_S, x_C) p(x_C) dx_C,\end{split}\]
where \(f(x_S, x_C)\) is the response function (predict,
predict_proba or decision_function) for a given sample whose
values are defined by \(x_S\) for the features in \(X_S\), and by
\(x_C\) for the features in \(X_C\). Note that \(x_S\) and
\(x_C\) may be tuples.
Computing this integral for various values of \(x_S\) produces a PDP plot
as above. An ICE line is defined as a single \(f(x_{S}, x_{C}^{(i)})\)
evaluated at \(x_{S}\).
- 4.1.4. Computation methods¶
There are two main methods to approximate the integral above, namely the
‘brute’ and ‘recursion’ methods. The method parameter controls which method
to use.
The ‘brute’ method is a generic method that works with any estimator. Note that
computing ICE plots is only supported with the ‘brute’ method. It
approximates the above integral by computing an average over the data X:
\[pd_{X_S}(x_S) \approx \frac{1}{n_\text{samples}} \sum_{i=1}^n f(x_S, x_C^{(i)}),\]
where \(x_C^{(i)}\) is the value of the i-th sample for the features in
\(X_C\). For each value of \(x_S\), this method requires a full pass
over the dataset X which is computationally intensive.
Each of the \(f(x_{S}, x_{C}^{(i)})\) corresponds to one ICE line evaluated
at \(x_{S}\). Computing this for multiple values of \(x_{S}\), one
obtains a full ICE line. As one can see, the average of the ICE lines
correspond to the partial dependence line.
The ‘recursion’ method is faster than the ‘brute’ method, but it is only
supported for PDP plots by some tree-based estimators. It is computed as
follows. For a given point \(x_S\), a weighted tree traversal is performed:
if a split node involves an input feature of interest, the corresponding left
or right branch is followed; otherwise both branches are followed, each branch
being weighted by the fraction of training samples that entered that branch.
Finally, the partial dependence is given by a weighted average of all the
visited leaves values.
With the ‘brute’ method, the parameter X is used both for generating the
grid of values \(x_S\) and the complement feature values \(x_C\).
However with the ‘recursion’ method, X is only used for the grid values:
implicitly, the \(x_C\) values are those of the training data.
By default, the ‘recursion’ method is used for plotting PDPs on tree-based
estimators that support it, and ‘brute’ is used for the rest.
Note
While both methods should be close in general, they might differ in some
specific settings. The ‘brute’ method assumes the existence of the
data points \((x_S, x_C^{(i)})\). When the features are correlated,
such artificial samples may have a very low probability mass. The ‘brute’
and ‘recursion’ methods will likely disagree regarding the value of the
partial dependence, because they will treat these unlikely
samples differently. Remember, however, that the primary assumption for
interpreting PDPs is that the features should be independent.
Examples:
Partial Dependence and Individual Conditional Expectation Plots
Footnotes
[1]
For classification, the target response may be the probability of a
class (the positive class for binary classification), or the decision
function.
References
[H2009]
T. Hastie, R. Tibshirani and J. Friedman,
The Elements of Statistical Learning,
Second Edition, Section 10.13.2, Springer, 2009.
[M2019]
C. Molnar,
Interpretable Machine Learning,
Section 5.1, 2019.
[G2015]
(1,2)
A. Goldstein, A. Kapelner, J. Bleich, and E. Pitkin,
“Peeking Inside the Black Box: Visualizing Statistical
Learning With Plots of Individual Conditional Expectation”
Journal of Computational and Graphical Statistics,
24(1): 44-65, Springer, 2015.
### 4.2. Permutation feature importance — scikit-learn 1.4.2 documentation

Install
User Guide
API
Examples
Community
Getting Started
Tutorial
What's new
Glossary
Development
FAQ
Support
Related packages
Roadmap
Governance
About us
GitHub
Other Versions and Download
More
Getting Started
Tutorial
What's new
Glossary
Development
FAQ
Support
Related packages
Roadmap
Governance
About us
GitHub
Other Versions and Download
Toggle Menu
PrevUp
Next
scikit-learn 1.4.2
Other versions
Please cite us if you use the software.
### 4.2. Permutation feature importance

- 4.2.1. Outline of the permutation importance algorithm
- 4.2.2. Relation to impurity-based importance in trees
- 4.2.3. Misleading values on strongly correlated features
### 4.2. Permutation feature importance¶

Permutation feature importance is a model inspection technique that measures the
contribution of each feature to a fitted model’s statistical performance
on a given tabular dataset. This technique is particularly useful for non-linear
or opaque estimators, and involves randomly shuffling the values of a
single feature and observing the resulting degradation of the model’s score
[1]. By breaking the relationship between the feature and the target, we
determine how much the model relies on such particular feature.
In the following figures, we observe the effect of permuting features on the correlation
between the feature and the target and consequently on the model statistical
performance.
On the top figure, we observe that permuting a predictive feature breaks the
correlation between the feature and the target, and consequently the model
statistical performance decreases. On the bottom figure, we observe that permuting
a non-predictive feature does not significantly degrade the model statistical performance.
One key advantage of permutation feature importance is that it is
model-agnostic, i.e. it can be applied to any fitted estimator. Moreover, it can
be calculated multiple times with different permutations of the feature, further
providing a measure of the variance in the estimated feature importances for the
specific trained model.
The figure below shows the permutation feature importance of a
RandomForestClassifier trained on an augmented
version of the titanic dataset that contains a random_cat and a random_num
features, i.e. a categrical and a numerical feature that are not correlated in
any way with the target variable:
Warning
Features that are deemed of low importance for a bad model (low
cross-validation score) could be very important for a good model.
Therefore it is always important to evaluate the predictive power of a model
using a held-out set (or better with cross-validation) prior to computing
importances. Permutation importance does not reflect to the intrinsic
predictive value of a feature by itself but how important this feature is
for a particular model.
The permutation_importance function calculates the feature importance
of estimators for a given dataset. The n_repeats parameter sets the
number of times a feature is randomly shuffled and returns a sample of feature
importances.
Let’s consider the following trained regression model:
>>> from sklearn.datasets import load_diabetes
>>> from sklearn.model_selection import train_test_split
>>> from sklearn.linear_model import Ridge
>>> diabetes = load_diabetes()
>>> X_train, X_val, y_train, y_val = train_test_split(
...     diabetes.data, diabetes.target, random_state=0)
...
>>> model = Ridge(alpha=1e-2).fit(X_train, y_train)
>>> model.score(X_val, y_val)
0.356...
Its validation performance, measured via the \(R^2\) score, is
significantly larger than the chance level. This makes it possible to use the
permutation_importance function to probe which features are most
predictive:
>>> from sklearn.inspection import permutation_importance
>>> r = permutation_importance(model, X_val, y_val,
...                            n_repeats=30,
...                            random_state=0)
...
>>> for i in r.importances_mean.argsort()[::-1]:
...     if r.importances_mean[i] - 2 * r.importances_std[i] > 0:
...         print(f"{diabetes.feature_names[i]:<8}"
...               f"{r.importances_mean[i]:.3f}"
...               f" +/- {r.importances_std[i]:.3f}")
...
s5      0.204 +/- 0.050
bmi     0.176 +/- 0.048
bp      0.088 +/- 0.033
sex     0.056 +/- 0.023
Note that the importance values for the top features represent a large
fraction of the reference score of 0.356.
Permutation importances can be computed either on the training set or on a
held-out testing or validation set. Using a held-out set makes it possible to
highlight which features contribute the most to the generalization power of the
inspected model. Features that are important on the training set but not on the
held-out set might cause the model to overfit.
The permutation feature importance depends on the score function that is
specified with the scoring argument. This argument accepts multiple scorers,
which is more computationally efficient than sequentially calling
permutation_importance several times with a different scorer, as it
reuses model predictions.
Example of permutation feature importance using multiple scorers
Click for more details
¶
In the example below we use a list of metrics, but more input formats are
possible, as documented in Using multiple metric evaluation.
>>> scoring = ['r2', 'neg_mean_absolute_percentage_error', 'neg_mean_squared_error']
>>> r_multi = permutation_importance(
...     model, X_val, y_val, n_repeats=30, random_state=0, scoring=scoring)
...
>>> for metric in r_multi:
...     print(f"{metric}")
...     r = r_multi[metric]
...     for i in r.importances_mean.argsort()[::-1]:
...         if r.importances_mean[i] - 2 * r.importances_std[i] > 0:
...             print(f"    {diabetes.feature_names[i]:<8}"
...                   f"{r.importances_mean[i]:.3f}"
...                   f" +/- {r.importances_std[i]:.3f}")
...
r2
s5      0.204 +/- 0.050
bmi     0.176 +/- 0.048
bp      0.088 +/- 0.033
sex     0.056 +/- 0.023
neg_mean_absolute_percentage_error
s5      0.081 +/- 0.020
bmi     0.064 +/- 0.015
bp      0.029 +/- 0.010
neg_mean_squared_error
s5      1013.866 +/- 246.445
bmi     872.726 +/- 240.298
bp      438.663 +/- 163.022
sex     277.376 +/- 115.123
The ranking of the features is approximately the same for different metrics even
if the scales of the importance values are very different. However, this is not
guaranteed and different metrics might lead to significantly different feature
importances, in particular for models trained for imbalanced classification problems,
for which the choice of the classification metric can be critical.
- 4.2.1. Outline of the permutation importance algorithm¶
Inputs: fitted predictive model \(m\), tabular dataset (training or
validation) \(D\).
Compute the reference score \(s\) of the model \(m\) on data
\(D\) (for instance the accuracy for a classifier or the \(R^2\) for
a regressor).
For each feature \(j\) (column of \(D\)):
For each repetition \(k\) in \({1, ..., K}\):
Randomly shuffle column \(j\) of dataset \(D\) to generate a
corrupted version of the data named \(\tilde{D}_{k,j}\).
Compute the score \(s_{k,j}\) of model \(m\) on corrupted data
\(\tilde{D}_{k,j}\).
Compute importance \(i_j\) for feature \(f_j\) defined as:
\[i_j = s - \frac{1}{K} \sum_{k=1}^{K} s_{k,j}\]
- 4.2.2. Relation to impurity-based importance in trees¶
Tree-based models provide an alternative measure of feature importances
based on the mean decrease in impurity
(MDI). Impurity is quantified by the splitting criterion of the decision trees
(Gini, Log Loss or Mean Squared Error). However, this method can give high
importance to features that may not be predictive on unseen data when the model
is overfitting. Permutation-based feature importance, on the other hand, avoids
this issue, since it can be computed on unseen data.
Furthermore, impurity-based feature importance for trees are strongly
biased and favor high cardinality features (typically numerical features)
over low cardinality features such as binary features or categorical variables
with a small number of possible categories.
Permutation-based feature importances do not exhibit such a bias. Additionally,
the permutation feature importance may be computed with any performance metric
on the model predictions and can be used to analyze any model class (not just
tree-based models).
The following example highlights the limitations of impurity-based feature
importance in contrast to permutation-based feature importance:
Permutation Importance vs Random Forest Feature Importance (MDI).
- 4.2.3. Misleading values on strongly correlated features¶
When two features are correlated and one of the features is permuted, the model
still has access to the latter through its correlated feature. This results in a
lower reported importance value for both features, though they might actually
be important.
The figure below shows the permutation feature importance of a
RandomForestClassifier trained using the
Breast cancer wisconsin (diagnostic) dataset, which contains strongly correlated features. A
naive interpretation would suggest that all features are unimportant:
One way to handle the issue is to cluster features that are correlated and only
keep one feature from each cluster.
For more details on such strategy, see the example
Permutation Importance with Multicollinear or Correlated Features.
Examples:
Permutation Importance vs Random Forest Feature Importance (MDI)
Permutation Importance with Multicollinear or Correlated Features
References:
[1]
L. Breiman, “Random Forests”,
Machine Learning, 45(1), 5-32, 2001.
## 5. Visualizations — scikit-learn 1.4.2 documentation

Install
User Guide
API
Examples
Community
Getting Started
Tutorial
What's new
Glossary
Development
FAQ
Support
Related packages
Roadmap
Governance
About us
GitHub
Other Versions and Download
More
Getting Started
Tutorial
What's new
Glossary
Development
FAQ
Support
Related packages
Roadmap
Governance
About us
GitHub
Other Versions and Download
Toggle Menu
PrevUp
Next
scikit-learn 1.4.2
Other versions
Please cite us if you use the software.
User Guide
## 1. Supervised learning

## 2. Unsupervised learning

## 3. Model selection and evaluation

## 4. Inspection

## 5. Visualizations

## 6. Dataset transformations

## 7. Dataset loading utilities

## 8. Computing with scikit-learn

## 9. Model persistence

## 10. Common pitfalls and recommended practices

## 11. Dispatching

## 5. Visualizations¶

Scikit-learn defines a simple API for creating visualizations for machine
learning. The key feature of this API is to allow for quick plotting and
visual adjustments without recalculation. We provide Display classes that
expose two methods for creating plots: from_estimator and
from_predictions. The from_estimator method will take a fitted estimator
and some data (X and y) and create a Display object. Sometimes, we would
like to only compute the predictions once and one should use from_predictions
instead. In the following example, we plot a ROC curve for a fitted support
vector machine:
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.metrics import RocCurveDisplay
from sklearn.datasets import load_wine
X, y = load_wine(return_X_y=True)
y = y == 2  # make binary
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)
svc = SVC(random_state=42)
svc.fit(X_train, y_train)
svc_disp = RocCurveDisplay.from_estimator(svc, X_test, y_test)
The returned svc_disp object allows us to continue using the already computed
ROC curve for SVC in future plots. In this case, the svc_disp is a
RocCurveDisplay that stores the computed values as
attributes called roc_auc, fpr, and tpr. Be aware that we could get
the predictions from the support vector machine and then use from_predictions
instead of from_estimator. Next, we train a random forest classifier and plot
the previously computed roc curve again by using the plot method of the
Display object.
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestClassifier
rfc = RandomForestClassifier(n_estimators=10, random_state=42)
rfc.fit(X_train, y_train)
ax = plt.gca()
rfc_disp = RocCurveDisplay.from_estimator(rfc, X_test, y_test, ax=ax, alpha=0.8)
svc_disp.plot(ax=ax, alpha=0.8)
Notice that we pass alpha=0.8 to the plot functions to adjust the alpha
values of the curves.
Examples:
ROC Curve with Visualization API
Advanced Plotting With Partial Dependence
Visualizations with Display Objects
Comparison of Calibration of Classifiers
### 5.1. Available Plotting Utilities¶

- 5.1.1. Display Objects¶
calibration.CalibrationDisplay(prob_true, ...)
Calibration curve (also known as reliability diagram) visualization.
inspection.PartialDependenceDisplay(...[, ...])
Partial Dependence Plot (PDP).
inspection.DecisionBoundaryDisplay(*, xx0, ...)
Decisions boundary visualization.
metrics.ConfusionMatrixDisplay(...[, ...])
Confusion Matrix visualization.
metrics.DetCurveDisplay(*, fpr, fnr[, ...])
DET curve visualization.
metrics.PrecisionRecallDisplay(precision, ...)
Precision Recall visualization.
metrics.PredictionErrorDisplay(*, y_true, y_pred)
Visualization of the prediction error of a regression model.
metrics.RocCurveDisplay(*, fpr, tpr[, ...])
ROC Curve visualization.
model_selection.LearningCurveDisplay(*, ...)
Learning Curve visualization.
model_selection.ValidationCurveDisplay(*, ...)
Validation Curve visualization.
### 6.1. Pipelines and composite estimators — scikit-learn 1.4.2 documentation

Install
User Guide
API
Examples
Community
Getting Started
Tutorial
What's new
Glossary
Development
FAQ
Support
Related packages
Roadmap
Governance
About us
GitHub
Other Versions and Download
More
Getting Started
Tutorial
What's new
Glossary
Development
FAQ
Support
Related packages
Roadmap
Governance
About us
GitHub
Other Versions and Download
Toggle Menu
PrevUp
Next
scikit-learn 1.4.2
Other versions
Please cite us if you use the software.
### 6.1. Pipelines and composite estimators

- 6.1.1. Pipeline: chaining estimators
6.1.1.1. Usage
6.1.1.1.1. Build a pipeline
6.1.1.1.2. Access pipeline steps
6.1.1.1.3. Tracking feature names in a pipeline
6.1.1.1.4. Access to nested parameters
6.1.1.2. Caching transformers: avoid repeated computation
- 6.1.2. Transforming target in regression
- 6.1.3. FeatureUnion: composite feature spaces
6.1.3.1. Usage
- 6.1.4. ColumnTransformer for heterogeneous data
- 6.1.5. Visualizing Composite Estimators
### 6.1. Pipelines and composite estimators¶

To build a composite estimator, transformers are usually combined with other
transformers or with predictors (such as classifiers or regressors).
The most common tool used for composing estimators is a Pipeline. Pipelines require all steps except the last to be a
transformer. The last step can be anything, a transformer, a
predictor, or a clustering estimator which might have or not have a
.predict(...) method. A pipeline exposes all methods provided by the last
estimator: if the last step provides a transform method, then the pipeline
would have a transform method and behave like a transformer. If the last step
provides a predict method, then the pipeline would expose that method, and
given a data X, use all steps except the last to transform the data,
and then give that transformed data to the predict method of the last step of
the pipeline. The class Pipeline is often used in combination with
ColumnTransformer or
FeatureUnion which concatenate the output of transformers
into a composite feature space.
TransformedTargetRegressor
deals with transforming the target (i.e. log-transform y).
- 6.1.1. Pipeline: chaining estimators¶
Pipeline can be used to chain multiple estimators
into one. This is useful as there is often a fixed sequence
of steps in processing the data, for example feature selection, normalization
and classification. Pipeline serves multiple purposes here:
Convenience and encapsulationYou only have to call fit and predict once on your
data to fit a whole sequence of estimators.
Joint parameter selectionYou can grid search
over parameters of all estimators in the pipeline at once.
SafetyPipelines help avoid leaking statistics from your test data into the
trained model in cross-validation, by ensuring that the same samples are
used to train the transformers and predictors.
All estimators in a pipeline, except the last one, must be transformers
(i.e. must have a transform method).
The last estimator may be any type (transformer, classifier, etc.).
Note
Calling fit on the pipeline is the same as calling fit on
each estimator in turn, transform the input and pass it on to the next step.
The pipeline has all the methods that the last estimator in the pipeline has,
i.e. if the last estimator is a classifier, the Pipeline can be used
as a classifier. If the last estimator is a transformer, again, so is the
pipeline.
6.1.1.1. Usage¶
6.1.1.1.1. Build a pipeline¶
The Pipeline is built using a list of (key, value) pairs, where
the key is a string containing the name you want to give this step and value
is an estimator object:
>>> from sklearn.pipeline import Pipeline
>>> from sklearn.svm import SVC
>>> from sklearn.decomposition import PCA
>>> estimators = [('reduce_dim', PCA()), ('clf', SVC())]
>>> pipe = Pipeline(estimators)
>>> pipe
Pipeline(steps=[('reduce_dim', PCA()), ('clf', SVC())])
Shorthand version using :func:`make_pipeline`
Click for more details
¶
The utility function make_pipeline is a shorthand
for constructing pipelines;
it takes a variable number of estimators and returns a pipeline,
filling in the names automatically:
>>> from sklearn.pipeline import make_pipeline
>>> make_pipeline(PCA(), SVC())
Pipeline(steps=[('pca', PCA()), ('svc', SVC())])
6.1.1.1.2. Access pipeline steps¶
The estimators of a pipeline are stored as a list in the steps attribute.
A sub-pipeline can be extracted using the slicing notation commonly used
for Python Sequences such as lists or strings (although only a step of 1 is
permitted). This is convenient for performing only some of the transformations
(or their inverse):
>>> pipe[:1]
Pipeline(steps=[('reduce_dim', PCA())])
>>> pipe[-1:]
Pipeline(steps=[('clf', SVC())])
Accessing a step by name or position
Click for more details
¶
A specific step can also be accessed by index or name by indexing (with [idx]) the
pipeline:
>>> pipe.steps[0]
('reduce_dim', PCA())
>>> pipe[0]
PCA()
>>> pipe['reduce_dim']
PCA()
Pipeline’s named_steps attribute allows accessing steps by name with tab
completion in interactive environments:
>>> pipe.named_steps.reduce_dim is pipe['reduce_dim']
True
6.1.1.1.3. Tracking feature names in a pipeline¶
To enable model inspection, Pipeline has a
get_feature_names_out() method, just like all transformers. You can use
pipeline slicing to get the feature names going into each step:
>>> from sklearn.datasets import load_iris
>>> from sklearn.linear_model import LogisticRegression
>>> from sklearn.feature_selection import SelectKBest
>>> iris = load_iris()
>>> pipe = Pipeline(steps=[
...    ('select', SelectKBest(k=2)),
...    ('clf', LogisticRegression())])
>>> pipe.fit(iris.data, iris.target)
Pipeline(steps=[('select', SelectKBest(...)), ('clf', LogisticRegression(...))])
>>> pipe[:-1].get_feature_names_out()
array(['x2', 'x3'], ...)
Customize feature names
Click for more details
¶
You can also provide custom feature names for the input data using
get_feature_names_out:
>>> pipe[:-1].get_feature_names_out(iris.feature_names)
array(['petal length (cm)', 'petal width (cm)'], ...)
6.1.1.1.4. Access to nested parameters¶
It is common to adjust the parameters of an estimator within a pipeline. This parameter
is therefore nested because it belongs to a particular sub-step. Parameters of the
estimators in the pipeline are accessible using the <estimator>__<parameter>
syntax:
>>> pipe = Pipeline(steps=[("reduce_dim", PCA()), ("clf", SVC())])
>>> pipe.set_params(clf__C=10)
Pipeline(steps=[('reduce_dim', PCA()), ('clf', SVC(C=10))])
When does it matter?
Click for more details
¶
This is particularly important for doing grid searches:
>>> from sklearn.model_selection import GridSearchCV
>>> param_grid = dict(reduce_dim__n_components=[2, 5, 10],
...                   clf__C=[0.1, 10, 100])
>>> grid_search = GridSearchCV(pipe, param_grid=param_grid)
Individual steps may also be replaced as parameters, and non-final steps may be
ignored by setting them to 'passthrough':
>>> param_grid = dict(reduce_dim=['passthrough', PCA(5), PCA(10)],
...                   clf=[SVC(), LogisticRegression()],
...                   clf__C=[0.1, 10, 100])
>>> grid_search = GridSearchCV(pipe, param_grid=param_grid)
See Also:
Composite estimators and parameter spaces
Examples:
Pipeline ANOVA SVM
Sample pipeline for text feature extraction and evaluation
Pipelining: chaining a PCA and a logistic regression
Explicit feature map approximation for RBF kernels
SVM-Anova: SVM with univariate feature selection
Selecting dimensionality reduction with Pipeline and GridSearchCV
Displaying Pipelines
6.1.1.2. Caching transformers: avoid repeated computation¶
Fitting transformers may be computationally expensive. With its
memory parameter set, Pipeline will cache each transformer
after calling fit.
This feature is used to avoid computing the fit transformers within a pipeline
if the parameters and input data are identical. A typical example is the case of
a grid search in which the transformers can be fitted only once and reused for
each configuration. The last step will never be cached, even if it is a transformer.
The parameter memory is needed in order to cache the transformers.
memory can be either a string containing the directory where to cache the
transformers or a joblib.Memory
object:
>>> from tempfile import mkdtemp
>>> from shutil import rmtree
>>> from sklearn.decomposition import PCA
>>> from sklearn.svm import SVC
>>> from sklearn.pipeline import Pipeline
>>> estimators = [('reduce_dim', PCA()), ('clf', SVC())]
>>> cachedir = mkdtemp()
>>> pipe = Pipeline(estimators, memory=cachedir)
>>> pipe
Pipeline(memory=...,
steps=[('reduce_dim', PCA()), ('clf', SVC())])
>>> # Clear the cache directory when you don't need it anymore
>>> rmtree(cachedir)
Warning: Side effect of caching transformers
Click for more details
¶
Using a Pipeline without cache enabled, it is possible to
inspect the original instance such as:
>>> from sklearn.datasets import load_digits
>>> X_digits, y_digits = load_digits(return_X_y=True)
>>> pca1 = PCA()
>>> svm1 = SVC()
>>> pipe = Pipeline([('reduce_dim', pca1), ('clf', svm1)])
>>> pipe.fit(X_digits, y_digits)
Pipeline(steps=[('reduce_dim', PCA()), ('clf', SVC())])
>>> # The pca instance can be inspected directly
>>> print(pca1.components_)
[[-1.77484909e-19  ... 4.07058917e-18]]
Enabling caching triggers a clone of the transformers before fitting.
Therefore, the transformer instance given to the pipeline cannot be
inspected directly.
In following example, accessing the PCA
instance pca2 will raise an AttributeError since pca2 will be an
unfitted transformer.
Instead, use the attribute named_steps to inspect estimators within
the pipeline:
>>> cachedir = mkdtemp()
>>> pca2 = PCA()
>>> svm2 = SVC()
>>> cached_pipe = Pipeline([('reduce_dim', pca2), ('clf', svm2)],
...                        memory=cachedir)
>>> cached_pipe.fit(X_digits, y_digits)
Pipeline(memory=...,
steps=[('reduce_dim', PCA()), ('clf', SVC())])
>>> print(cached_pipe.named_steps['reduce_dim'].components_)
[[-1.77484909e-19  ... 4.07058917e-18]]
>>> # Remove the cache directory
>>> rmtree(cachedir)
Examples:
Selecting dimensionality reduction with Pipeline and GridSearchCV
- 6.1.2. Transforming target in regression¶
TransformedTargetRegressor transforms the
targets y before fitting a regression model. The predictions are mapped
back to the original space via an inverse transform. It takes as an argument
the regressor that will be used for prediction, and the transformer that will
be applied to the target variable:
>>> import numpy as np
>>> from sklearn.datasets import fetch_california_housing
>>> from sklearn.compose import TransformedTargetRegressor
>>> from sklearn.preprocessing import QuantileTransformer
>>> from sklearn.linear_model import LinearRegression
>>> from sklearn.model_selection import train_test_split
>>> X, y = fetch_california_housing(return_X_y=True)
>>> X, y = X[:2000, :], y[:2000]  # select a subset of data
>>> transformer = QuantileTransformer(output_distribution='normal')
>>> regressor = LinearRegression()
>>> regr = TransformedTargetRegressor(regressor=regressor,
...                                   transformer=transformer)
>>> X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)
>>> regr.fit(X_train, y_train)
TransformedTargetRegressor(...)
>>> print('R2 score: {0:.2f}'.format(regr.score(X_test, y_test)))
R2 score: 0.61
>>> raw_target_regr = LinearRegression().fit(X_train, y_train)
>>> print('R2 score: {0:.2f}'.format(raw_target_regr.score(X_test, y_test)))
R2 score: 0.59
For simple transformations, instead of a Transformer object, a pair of
functions can be passed, defining the transformation and its inverse mapping:
>>> def func(x):
...     return np.log(x)
>>> def inverse_func(x):
...     return np.exp(x)
Subsequently, the object is created as:
>>> regr = TransformedTargetRegressor(regressor=regressor,
...                                   func=func,
...                                   inverse_func=inverse_func)
>>> regr.fit(X_train, y_train)
TransformedTargetRegressor(...)
>>> print('R2 score: {0:.2f}'.format(regr.score(X_test, y_test)))
R2 score: 0.51
By default, the provided functions are checked at each fit to be the inverse of
each other. However, it is possible to bypass this checking by setting
check_inverse to False:
>>> def inverse_func(x):
...     return x
>>> regr = TransformedTargetRegressor(regressor=regressor,
...                                   func=func,
...                                   inverse_func=inverse_func,
...                                   check_inverse=False)
>>> regr.fit(X_train, y_train)
TransformedTargetRegressor(...)
>>> print('R2 score: {0:.2f}'.format(regr.score(X_test, y_test)))
R2 score: -1.57
Note
The transformation can be triggered by setting either transformer or the
pair of functions func and inverse_func. However, setting both
options will raise an error.
Examples:
Effect of transforming the targets in regression model
- 6.1.3. FeatureUnion: composite feature spaces¶
FeatureUnion combines several transformer objects into a new
transformer that combines their output. A FeatureUnion takes
a list of transformer objects. During fitting, each of these
is fit to the data independently. The transformers are applied in parallel,
and the feature matrices they output are concatenated side-by-side into a
larger matrix.
When you want to apply different transformations to each field of the data,
see the related class ColumnTransformer
(see user guide).
FeatureUnion serves the same purposes as Pipeline -
convenience and joint parameter estimation and validation.
FeatureUnion and Pipeline can be combined to
create complex models.
(A FeatureUnion has no way of checking whether two transformers
might produce identical features. It only produces a union when the
feature sets are disjoint, and making sure they are is the caller’s
responsibility.)
6.1.3.1. Usage¶
A FeatureUnion is built using a list of (key, value) pairs,
where the key is the name you want to give to a given transformation
(an arbitrary string; it only serves as an identifier)
and value is an estimator object:
>>> from sklearn.pipeline import FeatureUnion
>>> from sklearn.decomposition import PCA
>>> from sklearn.decomposition import KernelPCA
>>> estimators = [('linear_pca', PCA()), ('kernel_pca', KernelPCA())]
>>> combined = FeatureUnion(estimators)
>>> combined
FeatureUnion(transformer_list=[('linear_pca', PCA()),
('kernel_pca', KernelPCA())])
Like pipelines, feature unions have a shorthand constructor called
make_union that does not require explicit naming of the components.
Like Pipeline, individual steps may be replaced using set_params,
and ignored by setting to 'drop':
>>> combined.set_params(kernel_pca='drop')
FeatureUnion(transformer_list=[('linear_pca', PCA()),
('kernel_pca', 'drop')])
Examples:
Concatenating multiple feature extraction methods
- 6.1.4. ColumnTransformer for heterogeneous data¶
Many datasets contain features of different types, say text, floats, and dates,
where each type of feature requires separate preprocessing or feature
extraction steps.  Often it is easiest to preprocess data before applying
scikit-learn methods, for example using pandas.
Processing your data before passing it to scikit-learn might be problematic for
one of the following reasons:
Incorporating statistics from test data into the preprocessors makes
cross-validation scores unreliable (known as data leakage),
for example in the case of scalers or imputing missing values.
You may want to include the parameters of the preprocessors in a
parameter search.
The ColumnTransformer helps performing different
transformations for different columns of the data, within a
Pipeline that is safe from data leakage and that can
be parametrized. ColumnTransformer works on
arrays, sparse matrices, and
pandas DataFrames.
To each column, a different transformation can be applied, such as
preprocessing or a specific feature extraction method:
>>> import pandas as pd
>>> X = pd.DataFrame(
...     {'city': ['London', 'London', 'Paris', 'Sallisaw'],
...      'title': ["His Last Bow", "How Watson Learned the Trick",
...                "A Moveable Feast", "The Grapes of Wrath"],
...      'expert_rating': [5, 3, 4, 5],
...      'user_rating': [4, 5, 4, 3]})
For this data, we might want to encode the 'city' column as a categorical
variable using OneHotEncoder but apply a
CountVectorizer to the 'title' column.
As we might use multiple feature extraction methods on the same column, we give
each transformer a unique name, say 'city_category' and 'title_bow'.
By default, the remaining rating columns are ignored (remainder='drop'):
>>> from sklearn.compose import ColumnTransformer
>>> from sklearn.feature_extraction.text import CountVectorizer
>>> from sklearn.preprocessing import OneHotEncoder
>>> column_trans = ColumnTransformer(
...     [('categories', OneHotEncoder(dtype='int'), ['city']),
...      ('title_bow', CountVectorizer(), 'title')],
...     remainder='drop', verbose_feature_names_out=False)
>>> column_trans.fit(X)
ColumnTransformer(transformers=[('categories', OneHotEncoder(dtype='int'),
['city']),
('title_bow', CountVectorizer(), 'title')],
verbose_feature_names_out=False)
>>> column_trans.get_feature_names_out()
array(['city_London', 'city_Paris', 'city_Sallisaw', 'bow', 'feast',
'grapes', 'his', 'how', 'last', 'learned', 'moveable', 'of', 'the',
'trick', 'watson', 'wrath'], ...)
>>> column_trans.transform(X).toarray()
array([[1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0],
[1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0],
[0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],
[0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1]]...)
In the above example, the
CountVectorizer expects a 1D array as
input and therefore the columns were specified as a string ('title').
However, OneHotEncoder
as most of other transformers expects 2D data, therefore in that case you need
to specify the column as a list of strings (['city']).
Apart from a scalar or a single item list, the column selection can be specified
as a list of multiple items, an integer array, a slice, a boolean mask, or
with a make_column_selector. The
make_column_selector is used to select columns based
on data type or column name:
>>> from sklearn.preprocessing import StandardScaler
>>> from sklearn.compose import make_column_selector
>>> ct = ColumnTransformer([
...       ('scale', StandardScaler(),
...       make_column_selector(dtype_include=np.number)),
...       ('onehot',
...       OneHotEncoder(),
...       make_column_selector(pattern='city', dtype_include=object))])
>>> ct.fit_transform(X)
array([[ 0.904...,  0.      ,  1. ,  0. ,  0. ],
[-1.507...,  1.414...,  1. ,  0. ,  0. ],
[-0.301...,  0.      ,  0. ,  1. ,  0. ],
[ 0.904..., -1.414...,  0. ,  0. ,  1. ]])
Strings can reference columns if the input is a DataFrame, integers are always
interpreted as the positional columns.
We can keep the remaining rating columns by setting
remainder='passthrough'. The values are appended to the end of the
transformation:
>>> column_trans = ColumnTransformer(
...     [('city_category', OneHotEncoder(dtype='int'),['city']),
...      ('title_bow', CountVectorizer(), 'title')],
...     remainder='passthrough')
>>> column_trans.fit_transform(X)
array([[1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 5, 4],
[1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 3, 5],
[0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 4, 4],
[0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 5, 3]]...)
The remainder parameter can be set to an estimator to transform the
remaining rating columns. The transformed values are appended to the end of
the transformation:
>>> from sklearn.preprocessing import MinMaxScaler
>>> column_trans = ColumnTransformer(
...     [('city_category', OneHotEncoder(), ['city']),
...      ('title_bow', CountVectorizer(), 'title')],
...     remainder=MinMaxScaler())
>>> column_trans.fit_transform(X)[:, -2:]
array([[1. , 0.5],
[0. , 1. ],
[0.5, 0.5],
[1. , 0. ]])
The make_column_transformer function is available
to more easily create a ColumnTransformer object.
Specifically, the names will be given automatically. The equivalent for the
above example would be:
>>> from sklearn.compose import make_column_transformer
>>> column_trans = make_column_transformer(
...     (OneHotEncoder(), ['city']),
...     (CountVectorizer(), 'title'),
...     remainder=MinMaxScaler())
>>> column_trans
ColumnTransformer(remainder=MinMaxScaler(),
transformers=[('onehotencoder', OneHotEncoder(), ['city']),
('countvectorizer', CountVectorizer(),
'title')])
If ColumnTransformer is fitted with a dataframe
and the dataframe only has string column names, then transforming a dataframe
will use the column names to select the columns:
>>> ct = ColumnTransformer(
...          [("scale", StandardScaler(), ["expert_rating"])]).fit(X)
>>> X_new = pd.DataFrame({"expert_rating": [5, 6, 1],
...                       "ignored_new_col": [1.2, 0.3, -0.1]})
>>> ct.transform(X_new)
array([[ 0.9...],
[ 2.1...],
[-3.9...]])
- 6.1.5. Visualizing Composite Estimators¶
Estimators are displayed with an HTML representation when shown in a
jupyter notebook. This is useful to diagnose or visualize a Pipeline with
many estimators. This visualization is activated by default:
>>> column_trans
It can be deactivated by setting the display option in set_config
to ‘text’:
>>> from sklearn import set_config
>>> set_config(display='text')
>>> # displays text representation in a jupyter context
>>> column_trans
An example of the HTML output can be seen in the
HTML representation of Pipeline section of
Column Transformer with Mixed Types.
As an alternative, the HTML can be written to a file using
estimator_html_repr:
>>> from sklearn.utils import estimator_html_repr
>>> with open('my_estimator.html', 'w') as f:
...     f.write(estimator_html_repr(clf))
Examples:
Column Transformer with Heterogeneous Data Sources
Column Transformer with Mixed Types
### 6.2. Feature extraction — scikit-learn 1.4.2 documentation

Install
User Guide
API
Examples
Community
Getting Started
Tutorial
What's new
Glossary
Development
FAQ
Support
Related packages
Roadmap
Governance
About us
GitHub
Other Versions and Download
More
Getting Started
Tutorial
What's new
Glossary
Development
FAQ
Support
Related packages
Roadmap
Governance
About us
GitHub
Other Versions and Download
Toggle Menu
PrevUp
Next
scikit-learn 1.4.2
Other versions
Please cite us if you use the software.
### 6.2. Feature extraction

- 6.2.1. Loading features from dicts
- 6.2.2. Feature hashing
- 6.2.3. Text feature extraction
6.2.3.1. The Bag of Words representation
6.2.3.2. Sparsity
6.2.3.3. Common Vectorizer usage
6.2.3.4. Using stop words
6.2.3.5. Tf–idf term weighting
6.2.3.6. Decoding text files
6.2.3.7. Applications and examples
6.2.3.8. Limitations of the Bag of Words representation
6.2.3.9. Vectorizing a large text corpus with the hashing trick
6.2.3.10. Customizing the vectorizer classes
- 6.2.4. Image feature extraction
6.2.4.1. Patch extraction
6.2.4.2. Connectivity graph of an image
### 6.2. Feature extraction¶

The sklearn.feature_extraction module can be used to extract
features in a format supported by machine learning algorithms from datasets
consisting of formats such as text and image.
Note
Feature extraction is very different from Feature selection:
the former consists in transforming arbitrary data, such as text or
images, into numerical features usable for machine learning. The latter
is a machine learning technique applied on these features.
- 6.2.1. Loading features from dicts¶
The class DictVectorizer can be used to convert feature
arrays represented as lists of standard Python dict objects to the
NumPy/SciPy representation used by scikit-learn estimators.
While not particularly fast to process, Python’s dict has the
advantages of being convenient to use, being sparse (absent features
need not be stored) and storing feature names in addition to values.
DictVectorizer implements what is called one-of-K or “one-hot”
coding for categorical (aka nominal, discrete) features. Categorical
features are “attribute-value” pairs where the value is restricted
to a list of discrete possibilities without ordering (e.g. topic
identifiers, types of objects, tags, names…).
In the following, “city” is a categorical attribute while “temperature”
is a traditional numerical feature:
>>> measurements = [
...     {'city': 'Dubai', 'temperature': 33.},
...     {'city': 'London', 'temperature': 12.},
...     {'city': 'San Francisco', 'temperature': 18.},
... ]
>>> from sklearn.feature_extraction import DictVectorizer
>>> vec = DictVectorizer()
>>> vec.fit_transform(measurements).toarray()
array([[ 1.,  0.,  0., 33.],
[ 0.,  1.,  0., 12.],
[ 0.,  0.,  1., 18.]])
>>> vec.get_feature_names_out()
array(['city=Dubai', 'city=London', 'city=San Francisco', 'temperature'], ...)
DictVectorizer accepts multiple string values for one
feature, like, e.g., multiple categories for a movie.
Assume a database classifies each movie using some categories (not mandatories)
and its year of release.
>>> movie_entry = [{'category': ['thriller', 'drama'], 'year': 2003},
...                {'category': ['animation', 'family'], 'year': 2011},
...                {'year': 1974}]
>>> vec.fit_transform(movie_entry).toarray()
array([[0.000e+00, 1.000e+00, 0.000e+00, 1.000e+00, 2.003e+03],
[1.000e+00, 0.000e+00, 1.000e+00, 0.000e+00, 2.011e+03],
[0.000e+00, 0.000e+00, 0.000e+00, 0.000e+00, 1.974e+03]])
>>> vec.get_feature_names_out()
array(['category=animation', 'category=drama', 'category=family',
'category=thriller', 'year'], ...)
>>> vec.transform({'category': ['thriller'],
...                'unseen_feature': '3'}).toarray()
array([[0., 0., 0., 1., 0.]])
DictVectorizer is also a useful representation transformation
for training sequence classifiers in Natural Language Processing models
that typically work by extracting feature windows around a particular
word of interest.
For example, suppose that we have a first algorithm that extracts Part of
Speech (PoS) tags that we want to use as complementary tags for training
a sequence classifier (e.g. a chunker). The following dict could be
such a window of features extracted around the word ‘sat’ in the sentence
‘The cat sat on the mat.’:
>>> pos_window = [
...     {
...         'word-2': 'the',
...         'pos-2': 'DT',
...         'word-1': 'cat',
...         'pos-1': 'NN',
...         'word+1': 'on',
...         'pos+1': 'PP',
...     },
...     # in a real application one would extract many such dictionaries
... ]
This description can be vectorized into a sparse two-dimensional matrix
suitable for feeding into a classifier (maybe after being piped into a
TfidfTransformer for normalization):
>>> vec = DictVectorizer()
>>> pos_vectorized = vec.fit_transform(pos_window)
>>> pos_vectorized
<1x6 sparse matrix of type '<... 'numpy.float64'>'
with 6 stored elements in Compressed Sparse ... format>
>>> pos_vectorized.toarray()
array([[1., 1., 1., 1., 1., 1.]])
>>> vec.get_feature_names_out()
array(['pos+1=PP', 'pos-1=NN', 'pos-2=DT', 'word+1=on', 'word-1=cat',
'word-2=the'], ...)
As you can imagine, if one extracts such a context around each individual
word of a corpus of documents the resulting matrix will be very wide
(many one-hot-features) with most of them being valued to zero most
of the time. So as to make the resulting data structure able to fit in
memory the DictVectorizer class uses a scipy.sparse matrix by
default instead of a numpy.ndarray.
- 6.2.2. Feature hashing¶
The class FeatureHasher is a high-speed, low-memory vectorizer that
uses a technique known as
feature hashing,
or the “hashing trick”.
Instead of building a hash table of the features encountered in training,
as the vectorizers do, instances of FeatureHasher
apply a hash function to the features
to determine their column index in sample matrices directly.
The result is increased speed and reduced memory usage,
at the expense of inspectability;
the hasher does not remember what the input features looked like
and has no inverse_transform method.
Since the hash function might cause collisions between (unrelated) features,
a signed hash function is used and the sign of the hash value
determines the sign of the value stored in the output matrix for a feature.
This way, collisions are likely to cancel out rather than accumulate error,
and the expected mean of any output feature’s value is zero. This mechanism
is enabled by default with alternate_sign=True and is particularly useful
for small hash table sizes (n_features < 10000). For large hash table
sizes, it can be disabled, to allow the output to be passed to estimators like
MultinomialNB or
chi2
feature selectors that expect non-negative inputs.
FeatureHasher accepts either mappings
(like Python’s dict and its variants in the collections module),
(feature, value) pairs, or strings,
depending on the constructor parameter input_type.
Mapping are treated as lists of (feature, value) pairs,
while single strings have an implicit value of 1,
so ['feat1', 'feat2', 'feat3'] is interpreted as
[('feat1', 1), ('feat2', 1), ('feat3', 1)].
If a single feature occurs multiple times in a sample,
the associated values will be summed
(so ('feat', 2) and ('feat', 3.5) become ('feat', 5.5)).
The output from FeatureHasher is always a scipy.sparse matrix
in the CSR format.
Feature hashing can be employed in document classification,
but unlike CountVectorizer,
FeatureHasher does not do word
splitting or any other preprocessing except Unicode-to-UTF-8 encoding;
see Vectorizing a large text corpus with the hashing trick, below, for a combined tokenizer/hasher.
As an example, consider a word-level natural language processing task
that needs features extracted from (token, part_of_speech) pairs.
One could use a Python generator function to extract features:
def token_features(token, part_of_speech):
if token.isdigit():
yield "numeric"
else:
yield "token={}".format(token.lower())
yield "token,pos={},{}".format(token, part_of_speech)
if token[0].isupper():
yield "uppercase_initial"
if token.isupper():
yield "all_uppercase"
yield "pos={}".format(part_of_speech)
Then, the raw_X to be fed to FeatureHasher.transform
can be constructed using:
raw_X = (token_features(tok, pos_tagger(tok)) for tok in corpus)
and fed to a hasher with:
hasher = FeatureHasher(input_type='string')
X = hasher.transform(raw_X)
to get a scipy.sparse matrix X.
Note the use of a generator comprehension,
which introduces laziness into the feature extraction:
tokens are only processed on demand from the hasher.
Implementation details
Click for more details
¶
FeatureHasher uses the signed 32-bit variant of MurmurHash3.
As a result (and because of limitations in scipy.sparse),
the maximum number of features supported is currently \(2^{31} - 1\).
The original formulation of the hashing trick by Weinberger et al.
used two separate hash functions \(h\) and \(\xi\)
to determine the column index and sign of a feature, respectively.
The present implementation works under the assumption
that the sign bit of MurmurHash3 is independent of its other bits.
Since a simple modulo is used to transform the hash function to a column index,
it is advisable to use a power of two as the n_features parameter;
otherwise the features will not be mapped evenly to the columns.
References:
MurmurHash3.
References:
Kilian Weinberger, Anirban Dasgupta, John Langford, Alex Smola and
Josh Attenberg (2009). Feature hashing for large scale multitask learning. Proc. ICML.
- 6.2.3. Text feature extraction¶
6.2.3.1. The Bag of Words representation¶
Text Analysis is a major application field for machine learning
algorithms. However the raw data, a sequence of symbols cannot be fed
directly to the algorithms themselves as most of them expect numerical
feature vectors with a fixed size rather than the raw text documents
with variable length.
In order to address this, scikit-learn provides utilities for the most
common ways to extract numerical features from text content, namely:
tokenizing strings and giving an integer id for each possible token,
for instance by using white-spaces and punctuation as token separators.
counting the occurrences of tokens in each document.
normalizing and weighting with diminishing importance tokens that
occur in the majority of samples / documents.
In this scheme, features and samples are defined as follows:
each individual token occurrence frequency (normalized or not)
is treated as a feature.
the vector of all the token frequencies for a given document is
considered a multivariate sample.
A corpus of documents can thus be represented by a matrix with one row
per document and one column per token (e.g. word) occurring in the corpus.
We call vectorization the general process of turning a collection
of text documents into numerical feature vectors. This specific strategy
(tokenization, counting and normalization) is called the Bag of Words
or “Bag of n-grams” representation. Documents are described by word
occurrences while completely ignoring the relative position information
of the words in the document.
6.2.3.2. Sparsity¶
As most documents will typically use a very small subset of the words used in
the corpus, the resulting matrix will have many feature values that are
zeros (typically more than 99% of them).
For instance a collection of 10,000 short text documents (such as emails)
will use a vocabulary with a size in the order of 100,000 unique words in
total while each document will use 100 to 1000 unique words individually.
In order to be able to store such a matrix in memory but also to speed
up algebraic operations matrix / vector, implementations will typically
use a sparse representation such as the implementations available in the
scipy.sparse package.
6.2.3.3. Common Vectorizer usage¶
CountVectorizer implements both tokenization and occurrence
counting in a single class:
>>> from sklearn.feature_extraction.text import CountVectorizer
This model has many parameters, however the default values are quite
reasonable (please see  the reference documentation for the details):
>>> vectorizer = CountVectorizer()
>>> vectorizer
CountVectorizer()
Let’s use it to tokenize and count the word occurrences of a minimalistic
corpus of text documents:
>>> corpus = [
...     'This is the first document.',
...     'This is the second second document.',
...     'And the third one.',
...     'Is this the first document?',
... ]
>>> X = vectorizer.fit_transform(corpus)
>>> X
<4x9 sparse matrix of type '<... 'numpy.int64'>'
with 19 stored elements in Compressed Sparse ... format>
The default configuration tokenizes the string by extracting words of
at least 2 letters. The specific function that does this step can be
requested explicitly:
>>> analyze = vectorizer.build_analyzer()
>>> analyze("This is a text document to analyze.") == (
...     ['this', 'is', 'text', 'document', 'to', 'analyze'])
True
Each term found by the analyzer during the fit is assigned a unique
integer index corresponding to a column in the resulting matrix. This
interpretation of the columns can be retrieved as follows:
>>> vectorizer.get_feature_names_out()
array(['and', 'document', 'first', 'is', 'one', 'second', 'the',
'third', 'this'], ...)
>>> X.toarray()
array([[0, 1, 1, 1, 0, 0, 1, 0, 1],
[0, 1, 0, 1, 0, 2, 1, 0, 1],
[1, 0, 0, 0, 1, 0, 1, 1, 0],
[0, 1, 1, 1, 0, 0, 1, 0, 1]]...)
The converse mapping from feature name to column index is stored in the
vocabulary_ attribute of the vectorizer:
>>> vectorizer.vocabulary_.get('document')
1
Hence words that were not seen in the training corpus will be completely
ignored in future calls to the transform method:
>>> vectorizer.transform(['Something completely new.']).toarray()
array([[0, 0, 0, 0, 0, 0, 0, 0, 0]]...)
Note that in the previous corpus, the first and the last documents have
exactly the same words hence are encoded in equal vectors. In particular
we lose the information that the last document is an interrogative form. To
preserve some of the local ordering information we can extract 2-grams
of words in addition to the 1-grams (individual words):
>>> bigram_vectorizer = CountVectorizer(ngram_range=(1, 2),
...                                     token_pattern=r'\b\w+\b', min_df=1)
>>> analyze = bigram_vectorizer.build_analyzer()
>>> analyze('Bi-grams are cool!') == (
...     ['bi', 'grams', 'are', 'cool', 'bi grams', 'grams are', 'are cool'])
True
The vocabulary extracted by this vectorizer is hence much bigger and
can now resolve ambiguities encoded in local positioning patterns:
>>> X_2 = bigram_vectorizer.fit_transform(corpus).toarray()
>>> X_2
array([[0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0],
[0, 0, 1, 0, 0, 1, 1, 0, 0, 2, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0],
[1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0],
[0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1]]...)
In particular the interrogative form “Is this” is only present in the
last document:
>>> feature_index = bigram_vectorizer.vocabulary_.get('is this')
>>> X_2[:, feature_index]
array([0, 0, 0, 1]...)
6.2.3.4. Using stop words¶
Stop words are words like “and”, “the”, “him”, which are presumed to be
uninformative in representing the content of a text, and which may be
removed to avoid them being construed as signal for prediction.  Sometimes,
however, similar words are useful for prediction, such as in classifying
writing style or personality.
There are several known issues in our provided ‘english’ stop word list. It
does not aim to be a general, ‘one-size-fits-all’ solution as some tasks
may require a more custom solution. See [NQY18] for more details.
Please take care in choosing a stop word list.
Popular stop word lists may include words that are highly informative to
some tasks, such as computer.
You should also make sure that the stop word list has had the same
preprocessing and tokenization applied as the one used in the vectorizer.
The word we’ve is split into we and ve by CountVectorizer’s default
tokenizer, so if we’ve is in stop_words, but ve is not, ve will
be retained from we’ve in transformed text.  Our vectorizers will try to
identify and warn about some kinds of inconsistencies.
References
[NQY18]
J. Nothman, H. Qin and R. Yurchak (2018).
“Stop Word Lists in Free Open-source Software Packages”.
In Proc. Workshop for NLP Open Source Software.
6.2.3.5. Tf–idf term weighting¶
In a large text corpus, some words will be very present (e.g. “the”, “a”,
“is” in English) hence carrying very little meaningful information about
the actual contents of the document. If we were to feed the direct count
data directly to a classifier those very frequent terms would shadow
the frequencies of rarer yet more interesting terms.
In order to re-weight the count features into floating point values
suitable for usage by a classifier it is very common to use the tf–idf
transform.
Tf means term-frequency while tf–idf means term-frequency times
inverse document-frequency:
\(\text{tf-idf(t,d)}=\text{tf(t,d)} \times \text{idf(t)}\).
Using the TfidfTransformer’s default settings,
TfidfTransformer(norm='l2', use_idf=True, smooth_idf=True, sublinear_tf=False)
the term frequency, the number of times a term occurs in a given document,
is multiplied with idf component, which is computed as
\(\text{idf}(t) = \log{\frac{1 + n}{1+\text{df}(t)}} + 1\),
where \(n\) is the total number of documents in the document set, and
\(\text{df}(t)\) is the number of documents in the document set that
contain term \(t\). The resulting tf-idf vectors are then normalized by the
Euclidean norm:
\(v_{norm} = \frac{v}{||v||_2} = \frac{v}{\sqrt{v{_1}^2 +
v{_2}^2 + \dots + v{_n}^2}}\).
This was originally a term weighting scheme developed for information retrieval
(as a ranking function for search engines results) that has also found good
use in document classification and clustering.
The following sections contain further explanations and examples that
illustrate how the tf-idfs are computed exactly and how the tf-idfs
computed in scikit-learn’s TfidfTransformer
and TfidfVectorizer differ slightly from the standard textbook
notation that defines the idf as
\(\text{idf}(t) = \log{\frac{n}{1+\text{df}(t)}}.\)
In the TfidfTransformer and TfidfVectorizer
with smooth_idf=False, the
“1” count is added to the idf instead of the idf’s denominator:
\(\text{idf}(t) = \log{\frac{n}{\text{df}(t)}} + 1\)
This normalization is implemented by the TfidfTransformer
class:
>>> from sklearn.feature_extraction.text import TfidfTransformer
>>> transformer = TfidfTransformer(smooth_idf=False)
>>> transformer
TfidfTransformer(smooth_idf=False)
Again please see the reference documentation for the details on all the parameters.
Numeric example of a tf-idf matrix
Click for more details
¶
Let’s take an example with the following counts. The first term is present
100% of the time hence not very interesting. The two other features only
in less than 50% of the time hence probably more representative of the
content of the documents:
>>> counts = [[3, 0, 1],
...           [2, 0, 0],
...           [3, 0, 0],
...           [4, 0, 0],
...           [3, 2, 0],
...           [3, 0, 2]]
...
>>> tfidf = transformer.fit_transform(counts)
>>> tfidf
<6x3 sparse matrix of type '<... 'numpy.float64'>'
with 9 stored elements in Compressed Sparse ... format>
>>> tfidf.toarray()
array([[0.81940995, 0.        , 0.57320793],
[1.        , 0.        , 0.        ],
[1.        , 0.        , 0.        ],
[1.        , 0.        , 0.        ],
[0.47330339, 0.88089948, 0.        ],
[0.58149261, 0.        , 0.81355169]])
Each row is normalized to have unit Euclidean norm:
\(v_{norm} = \frac{v}{||v||_2} = \frac{v}{\sqrt{v{_1}^2 +
v{_2}^2 + \dots + v{_n}^2}}\)
For example, we can compute the tf-idf of the first term in the first
document in the counts array as follows:
\(n = 6\)
\(\text{df}(t)_{\text{term1}} = 6\)
\(\text{idf}(t)_{\text{term1}} =
\log \frac{n}{\text{df}(t)} + 1 = \log(1)+1 = 1\)
\(\text{tf-idf}_{\text{term1}} = \text{tf} \times \text{idf} = 3 \times 1 = 3\)
Now, if we repeat this computation for the remaining 2 terms in the document,
we get
\(\text{tf-idf}_{\text{term2}} = 0 \times (\log(6/1)+1) = 0\)
\(\text{tf-idf}_{\text{term3}} = 1 \times (\log(6/2)+1) \approx 2.0986\)
and the vector of raw tf-idfs:
\(\text{tf-idf}_{\text{raw}} = [3, 0, 2.0986].\)
Then, applying the Euclidean (L2) norm, we obtain the following tf-idfs
for document 1:
\(\frac{[3, 0, 2.0986]}{\sqrt{\big(3^2 + 0^2 + 2.0986^2\big)}}
= [ 0.819,  0,  0.573].\)
Furthermore, the default parameter smooth_idf=True adds “1” to the numerator
and  denominator as if an extra document was seen containing every term in the
collection exactly once, which prevents zero divisions:
\(\text{idf}(t) = \log{\frac{1 + n}{1+\text{df}(t)}} + 1\)
Using this modification, the tf-idf of the third term in document 1 changes to
1.8473:
\(\text{tf-idf}_{\text{term3}} = 1 \times \log(7/3)+1 \approx 1.8473\)
And the L2-normalized tf-idf changes to
\(\frac{[3, 0, 1.8473]}{\sqrt{\big(3^2 + 0^2 + 1.8473^2\big)}}
= [0.8515, 0, 0.5243]\):
>>> transformer = TfidfTransformer()
>>> transformer.fit_transform(counts).toarray()
array([[0.85151335, 0.        , 0.52433293],
[1.        , 0.        , 0.        ],
[1.        , 0.        , 0.        ],
[1.        , 0.        , 0.        ],
[0.55422893, 0.83236428, 0.        ],
[0.63035731, 0.        , 0.77630514]])
The weights of each
feature computed by the fit method call are stored in a model
attribute:
>>> transformer.idf_
array([1. ..., 2.25..., 1.84...])
As tf–idf is very often used for text features, there is also another
class called TfidfVectorizer that combines all the options of
CountVectorizer and TfidfTransformer in a single model:
>>> from sklearn.feature_extraction.text import TfidfVectorizer
>>> vectorizer = TfidfVectorizer()
>>> vectorizer.fit_transform(corpus)
<4x9 sparse matrix of type '<... 'numpy.float64'>'
with 19 stored elements in Compressed Sparse ... format>
While the tf–idf normalization is often very useful, there might
be cases where the binary occurrence markers might offer better
features. This can be achieved by using the binary parameter
of CountVectorizer. In particular, some estimators such as
Bernoulli Naive Bayes explicitly model discrete boolean random
variables. Also, very short texts are likely to have noisy tf–idf values
while the binary occurrence info is more stable.
As usual the best way to adjust the feature extraction parameters
is to use a cross-validated grid search, for instance by pipelining the
feature extractor with a classifier:
Sample pipeline for text feature extraction and evaluation
6.2.3.6. Decoding text files¶
Text is made of characters, but files are made of bytes. These bytes represent
characters according to some encoding. To work with text files in Python,
their bytes must be decoded to a character set called Unicode.
Common encodings are ASCII, Latin-1 (Western Europe), KOI8-R (Russian)
and the universal encodings UTF-8 and UTF-16. Many others exist.
Note
An encoding can also be called a ‘character set’,
but this term is less accurate: several encodings can exist
for a single character set.
The text feature extractors in scikit-learn know how to decode text files,
but only if you tell them what encoding the files are in.
The CountVectorizer takes an encoding parameter for this purpose.
For modern text files, the correct encoding is probably UTF-8,
which is therefore the default (encoding="utf-8").
If the text you are loading is not actually encoded with UTF-8, however,
you will get a UnicodeDecodeError.
The vectorizers can be told to be silent about decoding errors
by setting the decode_error parameter to either "ignore"
or "replace". See the documentation for the Python function
bytes.decode for more details
(type help(bytes.decode) at the Python prompt).
Troubleshooting decoding text
Click for more details
¶
If you are having trouble decoding text, here are some things to try:
Find out what the actual encoding of the text is. The file might come
with a header or README that tells you the encoding, or there might be some
standard encoding you can assume based on where the text comes from.
You may be able to find out what kind of encoding it is in general
using the UNIX command file. The Python chardet module comes with
a script called chardetect.py that will guess the specific encoding,
though you cannot rely on its guess being correct.
You could try UTF-8 and disregard the errors. You can decode byte
strings with bytes.decode(errors='replace') to replace all
decoding errors with a meaningless character, or set
decode_error='replace' in the vectorizer. This may damage the
usefulness of your features.
Real text may come from a variety of sources that may have used different
encodings, or even be sloppily decoded in a different encoding than the
one it was encoded with. This is common in text retrieved from the Web.
The Python package ftfy can automatically sort out some classes of
decoding errors, so you could try decoding the unknown text as latin-1
and then using ftfy to fix errors.
If the text is in a mish-mash of encodings that is simply too hard to sort
out (which is the case for the 20 Newsgroups dataset), you can fall back on
a simple single-byte encoding such as latin-1. Some text may display
incorrectly, but at least the same sequence of bytes will always represent
the same feature.
For example, the following snippet uses chardet
(not shipped with scikit-learn, must be installed separately)
to figure out the encoding of three texts.
It then vectorizes the texts and prints the learned vocabulary.
The output is not shown here.
>>> import chardet
>>> text1 = b"Sei mir gegr\xc3\xbc\xc3\x9ft mein Sauerkraut"
>>> text2 = b"holdselig sind deine Ger\xfcche"
>>> text3 = b"\xff\xfeA\x00u\x00f\x00 \x00F\x00l\x00\xfc\x00g\x00e\x00l\x00n\x00 \x00d\x00e\x00s\x00 \x00G\x00e\x00s\x00a\x00n\x00g\x00e\x00s\x00,\x00 \x00H\x00e\x00r\x00z\x00l\x00i\x00e\x00b\x00c\x00h\x00e\x00n\x00,\x00 \x00t\x00r\x00a\x00g\x00 \x00i\x00c\x00h\x00 \x00d\x00i\x00c\x00h\x00 \x00f\x00o\x00r\x00t\x00"
>>> decoded = [x.decode(chardet.detect(x)['encoding'])
...            for x in (text1, text2, text3)]
>>> v = CountVectorizer().fit(decoded).vocabulary_
>>> for term in v: print(v)
(Depending on the version of chardet, it might get the first one wrong.)
For an introduction to Unicode and character encodings in general,
see Joel Spolsky’s Absolute Minimum Every Software Developer Must Know
About Unicode.
6.2.3.7. Applications and examples¶
The bag of words representation is quite simplistic but surprisingly
useful in practice.
In particular in a supervised setting it can be successfully combined
with fast and scalable linear models to train document classifiers,
for instance:
Classification of text documents using sparse features
In an unsupervised setting it can be used to group similar documents
together by applying clustering algorithms such as K-means:
Clustering text documents using k-means
Finally it is possible to discover the main topics of a corpus by
relaxing the hard assignment constraint of clustering, for instance by
using Non-negative matrix factorization (NMF or NNMF):
Topic extraction with Non-negative Matrix Factorization and Latent Dirichlet Allocation
6.2.3.8. Limitations of the Bag of Words representation¶
A collection of unigrams (what bag of words is) cannot capture phrases
and multi-word expressions, effectively disregarding any word order
dependence. Additionally, the bag of words model doesn’t account for potential
misspellings or word derivations.
N-grams to the rescue! Instead of building a simple collection of
unigrams (n=1), one might prefer a collection of bigrams (n=2), where
occurrences of pairs of consecutive words are counted.
One might alternatively consider a collection of character n-grams, a
representation resilient against misspellings and derivations.
For example, let’s say we’re dealing with a corpus of two documents:
['words', 'wprds']. The second document contains a misspelling
of the word ‘words’.
A simple bag of words representation would consider these two as
very distinct documents, differing in both of the two possible features.
A character 2-gram representation, however, would find the documents
matching in 4 out of 8 features, which may help the preferred classifier
decide better:
>>> ngram_vectorizer = CountVectorizer(analyzer='char_wb', ngram_range=(2, 2))
>>> counts = ngram_vectorizer.fit_transform(['words', 'wprds'])
>>> ngram_vectorizer.get_feature_names_out()
array([' w', 'ds', 'or', 'pr', 'rd', 's ', 'wo', 'wp'], ...)
>>> counts.toarray().astype(int)
array([[1, 1, 1, 0, 1, 1, 1, 0],
[1, 1, 0, 1, 1, 1, 0, 1]])
In the above example, char_wb analyzer is used, which creates n-grams
only from characters inside word boundaries (padded with space on each
side). The char analyzer, alternatively, creates n-grams that
span across words:
>>> ngram_vectorizer = CountVectorizer(analyzer='char_wb', ngram_range=(5, 5))
>>> ngram_vectorizer.fit_transform(['jumpy fox'])
<1x4 sparse matrix of type '<... 'numpy.int64'>'
with 4 stored elements in Compressed Sparse ... format>
>>> ngram_vectorizer.get_feature_names_out()
array([' fox ', ' jump', 'jumpy', 'umpy '], ...)
>>> ngram_vectorizer = CountVectorizer(analyzer='char', ngram_range=(5, 5))
>>> ngram_vectorizer.fit_transform(['jumpy fox'])
<1x5 sparse matrix of type '<... 'numpy.int64'>'
with 5 stored elements in Compressed Sparse ... format>
>>> ngram_vectorizer.get_feature_names_out()
array(['jumpy', 'mpy f', 'py fo', 'umpy ', 'y fox'], ...)
The word boundaries-aware variant char_wb is especially interesting
for languages that use white-spaces for word separation as it generates
significantly less noisy features than the raw char variant in
that case. For such languages it can increase both the predictive
accuracy and convergence speed of classifiers trained using such
features while retaining the robustness with regards to misspellings and
word derivations.
While some local positioning information can be preserved by extracting
n-grams instead of individual words, bag of words and bag of n-grams
destroy most of the inner structure of the document and hence most of
the meaning carried by that internal structure.
In order to address the wider task of Natural Language Understanding,
the local structure of sentences and paragraphs should thus be taken
into account. Many such models will thus be casted as “Structured output”
problems which are currently outside of the scope of scikit-learn.
6.2.3.9. Vectorizing a large text corpus with the hashing trick¶
The above vectorization scheme is simple but the fact that it holds an in-
memory mapping from the string tokens to the integer feature indices (the
vocabulary_ attribute) causes several problems when dealing with large
datasets:
the larger the corpus, the larger the vocabulary will grow and hence the
memory use too,
fitting requires the allocation of intermediate data structures
of size proportional to that of the original dataset.
building the word-mapping requires a full pass over the dataset hence it is
not possible to fit text classifiers in a strictly online manner.
pickling and un-pickling vectorizers with a large vocabulary_ can be very
slow (typically much slower than pickling / un-pickling flat data structures
such as a NumPy array of the same size),
it is not easily possible to split the vectorization work into concurrent sub
tasks as the vocabulary_ attribute would have to be a shared state with a
fine grained synchronization barrier: the mapping from token string to
feature index is dependent on ordering of the first occurrence of each token
hence would have to be shared, potentially harming the concurrent workers’
performance to the point of making them slower than the sequential variant.
It is possible to overcome those limitations by combining the “hashing trick”
(Feature hashing) implemented by the
FeatureHasher class and the text
preprocessing and tokenization features of the CountVectorizer.
This combination is implementing in HashingVectorizer,
a transformer class that is mostly API compatible with CountVectorizer.
HashingVectorizer is stateless,
meaning that you don’t have to call fit on it:
>>> from sklearn.feature_extraction.text import HashingVectorizer
>>> hv = HashingVectorizer(n_features=10)
>>> hv.transform(corpus)
<4x10 sparse matrix of type '<... 'numpy.float64'>'
with 16 stored elements in Compressed Sparse ... format>
You can see that 16 non-zero feature tokens were extracted in the vector
output: this is less than the 19 non-zeros extracted previously by the
CountVectorizer on the same toy corpus. The discrepancy comes from
hash function collisions because of the low value of the n_features parameter.
In a real world setting, the n_features parameter can be left to its
default value of 2 ** 20 (roughly one million possible features). If memory
or downstream models size is an issue selecting a lower value such as 2 **
18 might help without introducing too many additional collisions on typical
text classification tasks.
Note that the dimensionality does not affect the CPU training time of
algorithms which operate on CSR matrices (LinearSVC(dual=True),
Perceptron, SGDClassifier, PassiveAggressive) but it does for
algorithms that work with CSC matrices (LinearSVC(dual=False), Lasso(),
etc.).
Let’s try again with the default setting:
>>> hv = HashingVectorizer()
>>> hv.transform(corpus)
<4x1048576 sparse matrix of type '<... 'numpy.float64'>'
with 19 stored elements in Compressed Sparse ... format>
We no longer get the collisions, but this comes at the expense of a much larger
dimensionality of the output space.
Of course, other terms than the 19 used here
might still collide with each other.
The HashingVectorizer also comes with the following limitations:
it is not possible to invert the model (no inverse_transform method),
nor to access the original string representation of the features,
because of the one-way nature of the hash function that performs the mapping.
it does not provide IDF weighting as that would introduce statefulness in the
model. A TfidfTransformer can be appended to it in a pipeline if
required.
Performing out-of-core scaling with HashingVectorizer
Click for more details
¶
An interesting development of using a HashingVectorizer is the ability
to perform out-of-core scaling. This means that we can learn from data that
does not fit into the computer’s main memory.
A strategy to implement out-of-core scaling is to stream data to the estimator
in mini-batches. Each mini-batch is vectorized using HashingVectorizer
so as to guarantee that the input space of the estimator has always the same
dimensionality. The amount of memory used at any time is thus bounded by the
size of a mini-batch. Although there is no limit to the amount of data that can
be ingested using such an approach, from a practical point of view the learning
time is often limited by the CPU time one wants to spend on the task.
For a full-fledged example of out-of-core scaling in a text classification
task see Out-of-core classification of text documents.
6.2.3.10. Customizing the vectorizer classes¶
It is possible to customize the behavior by passing a callable
to the vectorizer constructor:
>>> def my_tokenizer(s):
...     return s.split()
...
>>> vectorizer = CountVectorizer(tokenizer=my_tokenizer)
>>> vectorizer.build_analyzer()(u"Some... punctuation!") == (
...     ['some...', 'punctuation!'])
True
In particular we name:
preprocessor: a callable that takes an entire document as input (as a
single string), and returns a possibly transformed version of the document,
still as an entire string. This can be used to remove HTML tags, lowercase
the entire document, etc.
tokenizer: a callable that takes the output from the preprocessor
and splits it into tokens, then returns a list of these.
analyzer: a callable that replaces the preprocessor and tokenizer.
The default analyzers all call the preprocessor and tokenizer, but custom
analyzers will skip this. N-gram extraction and stop word filtering take
place at the analyzer level, so a custom analyzer may have to reproduce
these steps.
(Lucene users might recognize these names, but be aware that scikit-learn
concepts may not map one-to-one onto Lucene concepts.)
To make the preprocessor, tokenizer and analyzers aware of the model
parameters it is possible to derive from the class and override the
build_preprocessor, build_tokenizer and build_analyzer
factory methods instead of passing custom functions.
Tips and tricks
Click for more details
¶
Some tips and tricks:
If documents are pre-tokenized by an external package, then store them in
files (or strings) with the tokens separated by whitespace and pass
analyzer=str.split
Fancy token-level analysis such as stemming, lemmatizing, compound
splitting, filtering based on part-of-speech, etc. are not included in the
scikit-learn codebase, but can be added by customizing either the
tokenizer or the analyzer.
Here’s a CountVectorizer with a tokenizer and lemmatizer using
NLTK:
>>> from nltk import word_tokenize
>>> from nltk.stem import WordNetLemmatizer
>>> class LemmaTokenizer:
...     def __init__(self):
...         self.wnl = WordNetLemmatizer()
...     def __call__(self, doc):
...         return [self.wnl.lemmatize(t) for t in word_tokenize(doc)]
...
>>> vect = CountVectorizer(tokenizer=LemmaTokenizer())
(Note that this will not filter out punctuation.)
The following example will, for instance, transform some British spelling
to American spelling:
>>> import re
>>> def to_british(tokens):
...     for t in tokens:
...         t = re.sub(r"(...)our$", r"\1or", t)
...         t = re.sub(r"([bt])re$", r"\1er", t)
...         t = re.sub(r"([iy])s(e$|ing|ation)", r"\1z\2", t)
...         t = re.sub(r"ogue$", "og", t)
...         yield t
...
>>> class CustomVectorizer(CountVectorizer):
...     def build_tokenizer(self):
...         tokenize = super().build_tokenizer()
...         return lambda doc: list(to_british(tokenize(doc)))
...
>>> print(CustomVectorizer().build_analyzer()(u"color colour"))
[...'color', ...'color']
for other styles of preprocessing; examples include stemming, lemmatization,
or normalizing numerical tokens, with the latter illustrated in:
Biclustering documents with the Spectral Co-clustering algorithm
Customizing the vectorizer can also be useful when handling Asian languages
that do not use an explicit word separator such as whitespace.
- 6.2.4. Image feature extraction¶
6.2.4.1. Patch extraction¶
The extract_patches_2d function extracts patches from an image stored
as a two-dimensional array, or three-dimensional with color information along
the third axis. For rebuilding an image from all its patches, use
reconstruct_from_patches_2d. For example let us generate a 4x4 pixel
picture with 3 color channels (e.g. in RGB format):
>>> import numpy as np
>>> from sklearn.feature_extraction import image
>>> one_image = np.arange(4 * 4 * 3).reshape((4, 4, 3))
>>> one_image[:, :, 0]  # R channel of a fake RGB picture
array([[ 0,  3,  6,  9],
[12, 15, 18, 21],
[24, 27, 30, 33],
[36, 39, 42, 45]])
>>> patches = image.extract_patches_2d(one_image, (2, 2), max_patches=2,
...     random_state=0)
>>> patches.shape
(2, 2, 2, 3)
>>> patches[:, :, :, 0]
array([[[ 0,  3],
[12, 15]],
[[15, 18],
[27, 30]]])
>>> patches = image.extract_patches_2d(one_image, (2, 2))
>>> patches.shape
(9, 2, 2, 3)
>>> patches[4, :, :, 0]
array([[15, 18],
[27, 30]])
Let us now try to reconstruct the original image from the patches by averaging
on overlapping areas:
>>> reconstructed = image.reconstruct_from_patches_2d(patches, (4, 4, 3))
>>> np.testing.assert_array_equal(one_image, reconstructed)
The PatchExtractor class works in the same way as
extract_patches_2d, only it supports multiple images as input. It is
implemented as a scikit-learn transformer, so it can be used in pipelines. See:
>>> five_images = np.arange(5 * 4 * 4 * 3).reshape(5, 4, 4, 3)
>>> patches = image.PatchExtractor(patch_size=(2, 2)).transform(five_images)
>>> patches.shape
(45, 2, 2, 3)
6.2.4.2. Connectivity graph of an image¶
Several estimators in the scikit-learn can use connectivity information between
features or samples. For instance Ward clustering
(Hierarchical clustering) can cluster together only neighboring pixels
of an image, thus forming contiguous patches:
For this purpose, the estimators use a ‘connectivity’ matrix, giving
which samples are connected.
The function img_to_graph returns such a matrix from a 2D or 3D
image. Similarly, grid_to_graph build a connectivity matrix for
images given the shape of these image.
These matrices can be used to impose connectivity in estimators that use
connectivity information, such as Ward clustering
(Hierarchical clustering), but also to build precomputed kernels,
or similarity matrices.
Note
Examples
A demo of structured Ward hierarchical clustering on an image of coins
Spectral clustering for image segmentation
Feature agglomeration vs. univariate selection
### 6.3. Preprocessing data — scikit-learn 1.4.2 documentation

Install
User Guide
API
Examples
Community
Getting Started
Tutorial
What's new
Glossary
Development
FAQ
Support
Related packages
Roadmap
Governance
About us
GitHub
Other Versions and Download
More
Getting Started
Tutorial
What's new
Glossary
Development
FAQ
Support
Related packages
Roadmap
Governance
About us
GitHub
Other Versions and Download
Toggle Menu
PrevUp
Next
scikit-learn 1.4.2
Other versions
Please cite us if you use the software.
### 6.3. Preprocessing data

- 6.3.1. Standardization, or mean removal and variance scaling
6.3.1.1. Scaling features to a range
6.3.1.2. Scaling sparse data
6.3.1.3. Scaling data with outliers
6.3.1.4. Centering kernel matrices
- 6.3.2. Non-linear transformation
6.3.2.1. Mapping to a Uniform distribution
6.3.2.2. Mapping to a Gaussian distribution
- 6.3.3. Normalization
- 6.3.4. Encoding categorical features
6.3.4.1. Infrequent categories
6.3.4.2. Target Encoder
- 6.3.5. Discretization
6.3.5.1. K-bins discretization
6.3.5.2. Feature binarization
- 6.3.6. Imputation of missing values
- 6.3.7. Generating polynomial features
6.3.7.1. Polynomial features
6.3.7.2. Spline transformer
- 6.3.8. Custom transformers
### 6.3. Preprocessing data¶

The sklearn.preprocessing package provides several common
utility functions and transformer classes to change raw feature vectors
into a representation that is more suitable for the downstream estimators.
In general, many learning algorithms such as linear models benefit from standardization of the data set
(see Importance of Feature Scaling).
If some outliers are present in the set, robust scalers or other transformers can
be more appropriate. The behaviors of the different scalers, transformers, and
normalizers on a dataset containing marginal outliers is highlighted in
Compare the effect of different scalers on data with outliers.
- 6.3.1. Standardization, or mean removal and variance scaling¶
Standardization of datasets is a common requirement for many
machine learning estimators implemented in scikit-learn; they might behave
badly if the individual features do not more or less look like standard
normally distributed data: Gaussian with zero mean and unit variance.
In practice we often ignore the shape of the distribution and just
transform the data to center it by removing the mean value of each
feature, then scale it by dividing non-constant features by their
standard deviation.
For instance, many elements used in the objective function of
a learning algorithm (such as the RBF kernel of Support Vector
Machines or the l1 and l2 regularizers of linear models) may assume that
all features are centered around zero or have variance in the same
order. If a feature has a variance that is orders of magnitude larger
than others, it might dominate the objective function and make the
estimator unable to learn from other features correctly as expected.
The preprocessing module provides the
StandardScaler utility class, which is a quick and
easy way to perform the following operation on an array-like
dataset:
>>> from sklearn import preprocessing
>>> import numpy as np
>>> X_train = np.array([[ 1., -1.,  2.],
...                     [ 2.,  0.,  0.],
...                     [ 0.,  1., -1.]])
>>> scaler = preprocessing.StandardScaler().fit(X_train)
>>> scaler
StandardScaler()
>>> scaler.mean_
array([1. ..., 0. ..., 0.33...])
>>> scaler.scale_
array([0.81..., 0.81..., 1.24...])
>>> X_scaled = scaler.transform(X_train)
>>> X_scaled
array([[ 0.  ..., -1.22...,  1.33...],
[ 1.22...,  0.  ..., -0.26...],
[-1.22...,  1.22..., -1.06...]])
Scaled data has zero mean and unit variance:
>>> X_scaled.mean(axis=0)
array([0., 0., 0.])
>>> X_scaled.std(axis=0)
array([1., 1., 1.])
This class implements the Transformer API to compute the mean and
standard deviation on a training set so as to be able to later re-apply the
same transformation on the testing set. This class is hence suitable for
use in the early steps of a Pipeline:
>>> from sklearn.datasets import make_classification
>>> from sklearn.linear_model import LogisticRegression
>>> from sklearn.model_selection import train_test_split
>>> from sklearn.pipeline import make_pipeline
>>> from sklearn.preprocessing import StandardScaler
>>> X, y = make_classification(random_state=42)
>>> X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)
>>> pipe = make_pipeline(StandardScaler(), LogisticRegression())
>>> pipe.fit(X_train, y_train)  # apply scaling on training data
Pipeline(steps=[('standardscaler', StandardScaler()),
('logisticregression', LogisticRegression())])
>>> pipe.score(X_test, y_test)  # apply scaling on testing data, without leaking training data.
0.96
It is possible to disable either centering or scaling by either
passing with_mean=False or with_std=False to the constructor
of StandardScaler.
6.3.1.1. Scaling features to a range¶
An alternative standardization is scaling features to
lie between a given minimum and maximum value, often between zero and one,
or so that the maximum absolute value of each feature is scaled to unit size.
This can be achieved using MinMaxScaler or MaxAbsScaler,
respectively.
The motivation to use this scaling include robustness to very small
standard deviations of features and preserving zero entries in sparse data.
Here is an example to scale a toy data matrix to the [0, 1] range:
>>> X_train = np.array([[ 1., -1.,  2.],
...                     [ 2.,  0.,  0.],
...                     [ 0.,  1., -1.]])
...
>>> min_max_scaler = preprocessing.MinMaxScaler()
>>> X_train_minmax = min_max_scaler.fit_transform(X_train)
>>> X_train_minmax
array([[0.5       , 0.        , 1.        ],
[1.        , 0.5       , 0.33333333],
[0.        , 1.        , 0.        ]])
The same instance of the transformer can then be applied to some new test data
unseen during the fit call: the same scaling and shifting operations will be
applied to be consistent with the transformation performed on the train data:
>>> X_test = np.array([[-3., -1.,  4.]])
>>> X_test_minmax = min_max_scaler.transform(X_test)
>>> X_test_minmax
array([[-1.5       ,  0.        ,  1.66666667]])
It is possible to introspect the scaler attributes to find about the exact
nature of the transformation learned on the training data:
>>> min_max_scaler.scale_
array([0.5       , 0.5       , 0.33...])
>>> min_max_scaler.min_
array([0.        , 0.5       , 0.33...])
If MinMaxScaler is given an explicit feature_range=(min, max) the
full formula is:
X_std = (X - X.min(axis=0)) / (X.max(axis=0) - X.min(axis=0))
X_scaled = X_std * (max - min) + min
MaxAbsScaler works in a very similar fashion, but scales in a way
that the training data lies within the range [-1, 1] by dividing through
the largest maximum value in each feature. It is meant for data
that is already centered at zero or sparse data.
Here is how to use the toy data from the previous example with this scaler:
>>> X_train = np.array([[ 1., -1.,  2.],
...                     [ 2.,  0.,  0.],
...                     [ 0.,  1., -1.]])
...
>>> max_abs_scaler = preprocessing.MaxAbsScaler()
>>> X_train_maxabs = max_abs_scaler.fit_transform(X_train)
>>> X_train_maxabs
array([[ 0.5, -1. ,  1. ],
[ 1. ,  0. ,  0. ],
[ 0. ,  1. , -0.5]])
>>> X_test = np.array([[ -3., -1.,  4.]])
>>> X_test_maxabs = max_abs_scaler.transform(X_test)
>>> X_test_maxabs
array([[-1.5, -1. ,  2. ]])
>>> max_abs_scaler.scale_
array([2.,  1.,  2.])
6.3.1.2. Scaling sparse data¶
Centering sparse data would destroy the sparseness structure in the data, and
thus rarely is a sensible thing to do. However, it can make sense to scale
sparse inputs, especially if features are on different scales.
MaxAbsScaler was specifically designed for scaling
sparse data, and is the recommended way to go about this.
However, StandardScaler can accept scipy.sparse
matrices  as input, as long as with_mean=False is explicitly passed
to the constructor. Otherwise a ValueError will be raised as
silently centering would break the sparsity and would often crash the
execution by allocating excessive amounts of memory unintentionally.
RobustScaler cannot be fitted to sparse inputs, but you can use
the transform method on sparse inputs.
Note that the scalers accept both Compressed Sparse Rows and Compressed
Sparse Columns format (see scipy.sparse.csr_matrix and
scipy.sparse.csc_matrix). Any other sparse input will be converted to
the Compressed Sparse Rows representation.  To avoid unnecessary memory
copies, it is recommended to choose the CSR or CSC representation upstream.
Finally, if the centered data is expected to be small enough, explicitly
converting the input to an array using the toarray method of sparse matrices
is another option.
6.3.1.3. Scaling data with outliers¶
If your data contains many outliers, scaling using the mean and variance
of the data is likely to not work very well. In these cases, you can use
RobustScaler as a drop-in replacement instead. It uses
more robust estimates for the center and range of your data.
References:
Further discussion on the importance of centering and scaling data is
available on this FAQ: Should I normalize/standardize/rescale the data?
Scaling vs Whitening
It is sometimes not enough to center and scale the features
independently, since a downstream model can further make some assumption
on the linear independence of the features.
To address this issue you can use PCA with
whiten=True to further remove the linear correlation across features.
6.3.1.4. Centering kernel matrices¶
If you have a kernel matrix of a kernel \(K\) that computes a dot product
in a feature space (possibly implicitly) defined by a function
\(\phi(\cdot)\), a KernelCenterer can transform the kernel matrix
so that it contains inner products in the feature space defined by \(\phi\)
followed by the removal of the mean in that space. In other words,
KernelCenterer computes the centered Gram matrix associated to a
positive semidefinite kernel \(K\).
Mathematical formulation
We can have a look at the mathematical formulation now that we have the
intuition. Let \(K\) be a kernel matrix of shape (n_samples, n_samples)
computed from \(X\), a data matrix of shape (n_samples, n_features),
during the fit step. \(K\) is defined by
\[K(X, X) = \phi(X) . \phi(X)^{T}\]
\(\phi(X)\) is a function mapping of \(X\) to a Hilbert space. A
centered kernel \(\tilde{K}\) is defined as:
\[\tilde{K}(X, X) = \tilde{\phi}(X) . \tilde{\phi}(X)^{T}\]
where \(\tilde{\phi}(X)\) results from centering \(\phi(X)\) in the
Hilbert space.
Thus, one could compute \(\tilde{K}\) by mapping \(X\) using the
function \(\phi(\cdot)\) and center the data in this new space. However,
kernels are often used because they allows some algebra calculations that
avoid computing explicitly this mapping using \(\phi(\cdot)\). Indeed, one
can implicitly center as shown in Appendix B in [Scholkopf1998]:
\[\tilde{K} = K - 1_{\text{n}_{samples}} K - K 1_{\text{n}_{samples}} + 1_{\text{n}_{samples}} K 1_{\text{n}_{samples}}\]
\(1_{\text{n}_{samples}}\) is a matrix of (n_samples, n_samples) where
all entries are equal to \(\frac{1}{\text{n}_{samples}}\). In the
transform step, the kernel becomes \(K_{test}(X, Y)\) defined as:
\[K_{test}(X, Y) = \phi(Y) . \phi(X)^{T}\]
\(Y\) is the test dataset of shape (n_samples_test, n_features) and thus
\(K_{test}\) is of shape (n_samples_test, n_samples). In this case,
centering \(K_{test}\) is done as:
\[\tilde{K}_{test}(X, Y) = K_{test} - 1'_{\text{n}_{samples}} K - K_{test} 1_{\text{n}_{samples}} + 1'_{\text{n}_{samples}} K 1_{\text{n}_{samples}}\]
\(1'_{\text{n}_{samples}}\) is a matrix of shape
(n_samples_test, n_samples) where all entries are equal to
\(\frac{1}{\text{n}_{samples}}\).
References
[Scholkopf1998]
B. Schölkopf, A. Smola, and K.R. Müller,
“Nonlinear component analysis as a kernel eigenvalue problem.”
Neural computation 10.5 (1998): 1299-1319.
- 6.3.2. Non-linear transformation¶
Two types of transformations are available: quantile transforms and power
transforms. Both quantile and power transforms are based on monotonic
transformations of the features and thus preserve the rank of the values
along each feature.
Quantile transforms put all features into the same desired distribution based
on the formula \(G^{-1}(F(X))\) where \(F\) is the cumulative
distribution function of the feature and \(G^{-1}\) the
quantile function of the
desired output distribution \(G\). This formula is using the two following
facts: (i) if \(X\) is a random variable with a continuous cumulative
distribution function \(F\) then \(F(X)\) is uniformly distributed on
\([0,1]\); (ii) if \(U\) is a random variable with uniform distribution
on \([0,1]\) then \(G^{-1}(U)\) has distribution \(G\). By performing
a rank transformation, a quantile transform smooths out unusual distributions
and is less influenced by outliers than scaling methods. It does, however,
distort correlations and distances within and across features.
Power transforms are a family of parametric transformations that aim to map
data from any distribution to as close to a Gaussian distribution.
6.3.2.1. Mapping to a Uniform distribution¶
QuantileTransformer provides a non-parametric
transformation to map the data to a uniform distribution
with values between 0 and 1:
>>> from sklearn.datasets import load_iris
>>> from sklearn.model_selection import train_test_split
>>> X, y = load_iris(return_X_y=True)
>>> X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)
>>> quantile_transformer = preprocessing.QuantileTransformer(random_state=0)
>>> X_train_trans = quantile_transformer.fit_transform(X_train)
>>> X_test_trans = quantile_transformer.transform(X_test)
>>> np.percentile(X_train[:, 0], [0, 25, 50, 75, 100])
array([ 4.3,  5.1,  5.8,  6.5,  7.9])
This feature corresponds to the sepal length in cm. Once the quantile
transformation applied, those landmarks approach closely the percentiles
previously defined:
>>> np.percentile(X_train_trans[:, 0], [0, 25, 50, 75, 100])
...
array([ 0.00... ,  0.24...,  0.49...,  0.73...,  0.99... ])
This can be confirmed on a independent testing set with similar remarks:
>>> np.percentile(X_test[:, 0], [0, 25, 50, 75, 100])
...
array([ 4.4  ,  5.125,  5.75 ,  6.175,  7.3  ])
>>> np.percentile(X_test_trans[:, 0], [0, 25, 50, 75, 100])
...
array([ 0.01...,  0.25...,  0.46...,  0.60... ,  0.94...])
6.3.2.2. Mapping to a Gaussian distribution¶
In many modeling scenarios, normality of the features in a dataset is desirable.
Power transforms are a family of parametric, monotonic transformations that aim
to map data from any distribution to as close to a Gaussian distribution as
possible in order to stabilize variance and minimize skewness.
PowerTransformer currently provides two such power transformations,
the Yeo-Johnson transform and the Box-Cox transform.
The Yeo-Johnson transform is given by:
\[\begin{split}x_i^{(\lambda)} =
\begin{cases}
[(x_i + 1)^\lambda - 1] / \lambda & \text{if } \lambda \neq 0, x_i \geq 0, \\[8pt]
\ln{(x_i + 1)} & \text{if } \lambda = 0, x_i \geq 0 \\[8pt]
-[(-x_i + 1)^{2 - \lambda} - 1] / (2 - \lambda) & \text{if } \lambda \neq 2, x_i < 0, \\[8pt]
- \ln (- x_i + 1) & \text{if } \lambda = 2, x_i < 0
\end{cases}\end{split}\]
while the Box-Cox transform is given by:
\[\begin{split}x_i^{(\lambda)} =
\begin{cases}
\dfrac{x_i^\lambda - 1}{\lambda} & \text{if } \lambda \neq 0, \\[8pt]
\ln{(x_i)} & \text{if } \lambda = 0,
\end{cases}\end{split}\]
Box-Cox can only be applied to strictly positive data. In both methods, the
transformation is parameterized by \(\lambda\), which is determined through
maximum likelihood estimation. Here is an example of using Box-Cox to map
samples drawn from a lognormal distribution to a normal distribution:
>>> pt = preprocessing.PowerTransformer(method='box-cox', standardize=False)
>>> X_lognormal = np.random.RandomState(616).lognormal(size=(3, 3))
>>> X_lognormal
array([[1.28..., 1.18..., 0.84...],
[0.94..., 1.60..., 0.38...],
[1.35..., 0.21..., 1.09...]])
>>> pt.fit_transform(X_lognormal)
array([[ 0.49...,  0.17..., -0.15...],
[-0.05...,  0.58..., -0.57...],
[ 0.69..., -0.84...,  0.10...]])
While the above example sets the standardize option to False,
PowerTransformer will apply zero-mean, unit-variance normalization
to the transformed output by default.
Below are examples of Box-Cox and Yeo-Johnson applied to various probability
distributions.  Note that when applied to certain distributions, the power
transforms achieve very Gaussian-like results, but with others, they are
ineffective. This highlights the importance of visualizing the data before and
after transformation.
It is also possible to map data to a normal distribution using
QuantileTransformer by setting output_distribution='normal'.
Using the earlier example with the iris dataset:
>>> quantile_transformer = preprocessing.QuantileTransformer(
...     output_distribution='normal', random_state=0)
>>> X_trans = quantile_transformer.fit_transform(X)
>>> quantile_transformer.quantiles_
array([[4.3, 2. , 1. , 0.1],
[4.4, 2.2, 1.1, 0.1],
[4.4, 2.2, 1.2, 0.1],
...,
[7.7, 4.1, 6.7, 2.5],
[7.7, 4.2, 6.7, 2.5],
[7.9, 4.4, 6.9, 2.5]])
Thus the median of the input becomes the mean of the output, centered at 0. The
normal output is clipped so that the input’s minimum and maximum —
corresponding to the 1e-7 and 1 - 1e-7 quantiles respectively — do not
become infinite under the transformation.
- 6.3.3. Normalization¶
Normalization is the process of scaling individual samples to have
unit norm. This process can be useful if you plan to use a quadratic form
such as the dot-product or any other kernel to quantify the similarity
of any pair of samples.
This assumption is the base of the Vector Space Model often used in text
classification and clustering contexts.
The function normalize provides a quick and easy way to perform this
operation on a single array-like dataset, either using the l1, l2, or
max norms:
>>> X = [[ 1., -1.,  2.],
...      [ 2.,  0.,  0.],
...      [ 0.,  1., -1.]]
>>> X_normalized = preprocessing.normalize(X, norm='l2')
>>> X_normalized
array([[ 0.40..., -0.40...,  0.81...],
[ 1.  ...,  0.  ...,  0.  ...],
[ 0.  ...,  0.70..., -0.70...]])
The preprocessing module further provides a utility class
Normalizer that implements the same operation using the
Transformer API (even though the fit method is useless in this case:
the class is stateless as this operation treats samples independently).
This class is hence suitable for use in the early steps of a
Pipeline:
>>> normalizer = preprocessing.Normalizer().fit(X)  # fit does nothing
>>> normalizer
Normalizer()
The normalizer instance can then be used on sample vectors as any transformer:
>>> normalizer.transform(X)
array([[ 0.40..., -0.40...,  0.81...],
[ 1.  ...,  0.  ...,  0.  ...],
[ 0.  ...,  0.70..., -0.70...]])
>>> normalizer.transform([[-1.,  1., 0.]])
array([[-0.70...,  0.70...,  0.  ...]])
Note: L2 normalization is also known as spatial sign preprocessing.
Sparse input
normalize and Normalizer accept both dense array-like
and sparse matrices from scipy.sparse as input.
For sparse input the data is converted to the Compressed Sparse Rows
representation (see scipy.sparse.csr_matrix) before being fed to
efficient Cython routines. To avoid unnecessary memory copies, it is
recommended to choose the CSR representation upstream.
- 6.3.4. Encoding categorical features¶
Often features are not given as continuous values but categorical.
For example a person could have features ["male", "female"],
["from Europe", "from US", "from Asia"],
["uses Firefox", "uses Chrome", "uses Safari", "uses Internet Explorer"].
Such features can be efficiently coded as integers, for instance
["male", "from US", "uses Internet Explorer"] could be expressed as
[0, 1, 3] while ["female", "from Asia", "uses Chrome"] would be
[1, 2, 1].
To convert categorical features to such integer codes, we can use the
OrdinalEncoder. This estimator transforms each categorical feature to one
new feature of integers (0 to n_categories - 1):
>>> enc = preprocessing.OrdinalEncoder()
>>> X = [['male', 'from US', 'uses Safari'], ['female', 'from Europe', 'uses Firefox']]
>>> enc.fit(X)
OrdinalEncoder()
>>> enc.transform([['female', 'from US', 'uses Safari']])
array([[0., 1., 1.]])
Such integer representation can, however, not be used directly with all
scikit-learn estimators, as these expect continuous input, and would interpret
the categories as being ordered, which is often not desired (i.e. the set of
browsers was ordered arbitrarily).
By default, OrdinalEncoder will also passthrough missing values that
are indicated by np.nan.
>>> enc = preprocessing.OrdinalEncoder()
>>> X = [['male'], ['female'], [np.nan], ['female']]
>>> enc.fit_transform(X)
array([[ 1.],
[ 0.],
[nan],
[ 0.]])
OrdinalEncoder provides a parameter encoded_missing_value to encode
the missing values without the need to create a pipeline and using
SimpleImputer.
>>> enc = preprocessing.OrdinalEncoder(encoded_missing_value=-1)
>>> X = [['male'], ['female'], [np.nan], ['female']]
>>> enc.fit_transform(X)
array([[ 1.],
[ 0.],
[-1.],
[ 0.]])
The above processing is equivalent to the following pipeline:
>>> from sklearn.pipeline import Pipeline
>>> from sklearn.impute import SimpleImputer
>>> enc = Pipeline(steps=[
...     ("encoder", preprocessing.OrdinalEncoder()),
...     ("imputer", SimpleImputer(strategy="constant", fill_value=-1)),
... ])
>>> enc.fit_transform(X)
array([[ 1.],
[ 0.],
[-1.],
[ 0.]])
Another possibility to convert categorical features to features that can be used
with scikit-learn estimators is to use a one-of-K, also known as one-hot or
dummy encoding.
This type of encoding can be obtained with the OneHotEncoder,
which transforms each categorical feature with
n_categories possible values into n_categories binary features, with
one of them 1, and all others 0.
Continuing the example above:
>>> enc = preprocessing.OneHotEncoder()
>>> X = [['male', 'from US', 'uses Safari'], ['female', 'from Europe', 'uses Firefox']]
>>> enc.fit(X)
OneHotEncoder()
>>> enc.transform([['female', 'from US', 'uses Safari'],
...                ['male', 'from Europe', 'uses Safari']]).toarray()
array([[1., 0., 0., 1., 0., 1.],
[0., 1., 1., 0., 0., 1.]])
By default, the values each feature can take is inferred automatically
from the dataset and can be found in the categories_ attribute:
>>> enc.categories_
[array(['female', 'male'], dtype=object), array(['from Europe', 'from US'], dtype=object), array(['uses Firefox', 'uses Safari'], dtype=object)]
It is possible to specify this explicitly using the parameter categories.
There are two genders, four possible continents and four web browsers in our
dataset:
>>> genders = ['female', 'male']
>>> locations = ['from Africa', 'from Asia', 'from Europe', 'from US']
>>> browsers = ['uses Chrome', 'uses Firefox', 'uses IE', 'uses Safari']
>>> enc = preprocessing.OneHotEncoder(categories=[genders, locations, browsers])
>>> # Note that for there are missing categorical values for the 2nd and 3rd
>>> # feature
>>> X = [['male', 'from US', 'uses Safari'], ['female', 'from Europe', 'uses Firefox']]
>>> enc.fit(X)
OneHotEncoder(categories=[['female', 'male'],
['from Africa', 'from Asia', 'from Europe',
'from US'],
['uses Chrome', 'uses Firefox', 'uses IE',
'uses Safari']])
>>> enc.transform([['female', 'from Asia', 'uses Chrome']]).toarray()
array([[1., 0., 0., 1., 0., 0., 1., 0., 0., 0.]])
If there is a possibility that the training data might have missing categorical
features, it can often be better to specify
handle_unknown='infrequent_if_exist' instead of setting the categories
manually as above. When handle_unknown='infrequent_if_exist' is specified
and unknown categories are encountered during transform, no error will be
raised but the resulting one-hot encoded columns for this feature will be all
zeros or considered as an infrequent category if enabled.
(handle_unknown='infrequent_if_exist' is only supported for one-hot
encoding):
>>> enc = preprocessing.OneHotEncoder(handle_unknown='infrequent_if_exist')
>>> X = [['male', 'from US', 'uses Safari'], ['female', 'from Europe', 'uses Firefox']]
>>> enc.fit(X)
OneHotEncoder(handle_unknown='infrequent_if_exist')
>>> enc.transform([['female', 'from Asia', 'uses Chrome']]).toarray()
array([[1., 0., 0., 0., 0., 0.]])
It is also possible to encode each column into n_categories - 1 columns
instead of n_categories columns by using the drop parameter. This
parameter allows the user to specify a category for each feature to be dropped.
This is useful to avoid co-linearity in the input matrix in some classifiers.
Such functionality is useful, for example, when using non-regularized
regression (LinearRegression),
since co-linearity would cause the covariance matrix to be non-invertible:
>>> X = [['male', 'from US', 'uses Safari'],
...      ['female', 'from Europe', 'uses Firefox']]
>>> drop_enc = preprocessing.OneHotEncoder(drop='first').fit(X)
>>> drop_enc.categories_
[array(['female', 'male'], dtype=object), array(['from Europe', 'from US'], dtype=object),
array(['uses Firefox', 'uses Safari'], dtype=object)]
>>> drop_enc.transform(X).toarray()
array([[1., 1., 1.],
[0., 0., 0.]])
One might want to drop one of the two columns only for features with 2
categories. In this case, you can set the parameter drop='if_binary'.
>>> X = [['male', 'US', 'Safari'],
...      ['female', 'Europe', 'Firefox'],
...      ['female', 'Asia', 'Chrome']]
>>> drop_enc = preprocessing.OneHotEncoder(drop='if_binary').fit(X)
>>> drop_enc.categories_
[array(['female', 'male'], dtype=object), array(['Asia', 'Europe', 'US'], dtype=object),
array(['Chrome', 'Firefox', 'Safari'], dtype=object)]
>>> drop_enc.transform(X).toarray()
array([[1., 0., 0., 1., 0., 0., 1.],
[0., 0., 1., 0., 0., 1., 0.],
[0., 1., 0., 0., 1., 0., 0.]])
In the transformed X, the first column is the encoding of the feature with
categories “male”/”female”, while the remaining 6 columns is the encoding of
the 2 features with respectively 3 categories each.
When handle_unknown='ignore' and drop is not None, unknown categories will
be encoded as all zeros:
>>> drop_enc = preprocessing.OneHotEncoder(drop='first',
...                                        handle_unknown='ignore').fit(X)
>>> X_test = [['unknown', 'America', 'IE']]
>>> drop_enc.transform(X_test).toarray()
array([[0., 0., 0., 0., 0.]])
All the categories in X_test are unknown during transform and will be mapped
to all zeros. This means that unknown categories will have the same mapping as
the dropped category. OneHotEncoder.inverse_transform will map all zeros
to the dropped category if a category is dropped and None if a category is
not dropped:
>>> drop_enc = preprocessing.OneHotEncoder(drop='if_binary', sparse_output=False,
...                                        handle_unknown='ignore').fit(X)
>>> X_test = [['unknown', 'America', 'IE']]
>>> X_trans = drop_enc.transform(X_test)
>>> X_trans
array([[0., 0., 0., 0., 0., 0., 0.]])
>>> drop_enc.inverse_transform(X_trans)
array([['female', None, None]], dtype=object)
OneHotEncoder supports categorical features with missing values by
considering the missing values as an additional category:
>>> X = [['male', 'Safari'],
...      ['female', None],
...      [np.nan, 'Firefox']]
>>> enc = preprocessing.OneHotEncoder(handle_unknown='error').fit(X)
>>> enc.categories_
[array(['female', 'male', nan], dtype=object),
array(['Firefox', 'Safari', None], dtype=object)]
>>> enc.transform(X).toarray()
array([[0., 1., 0., 0., 1., 0.],
[1., 0., 0., 0., 0., 1.],
[0., 0., 1., 1., 0., 0.]])
If a feature contains both np.nan and None, they will be considered
separate categories:
>>> X = [['Safari'], [None], [np.nan], ['Firefox']]
>>> enc = preprocessing.OneHotEncoder(handle_unknown='error').fit(X)
>>> enc.categories_
[array(['Firefox', 'Safari', None, nan], dtype=object)]
>>> enc.transform(X).toarray()
array([[0., 1., 0., 0.],
[0., 0., 1., 0.],
[0., 0., 0., 1.],
[1., 0., 0., 0.]])
See Loading features from dicts for categorical features that are
represented as a dict, not as scalars.
6.3.4.1. Infrequent categories¶
OneHotEncoder and OrdinalEncoder support aggregating
infrequent categories into a single output for each feature. The parameters to
enable the gathering of infrequent categories are min_frequency and
max_categories.
min_frequency is either an  integer greater or equal to 1, or a float in
the interval (0.0, 1.0). If min_frequency is an integer, categories with
a cardinality smaller than min_frequency  will be considered infrequent.
If min_frequency is a float, categories with a cardinality smaller than
this fraction of the total number of samples will be considered infrequent.
The default value is 1, which means every category is encoded separately.
max_categories is either None or any integer greater than 1. This
parameter sets an upper limit to the number of output features for each
input feature. max_categories includes the feature that combines
infrequent categories.
In the following example with OrdinalEncoder, the categories 'dog' and
'snake' are considered infrequent:
>>> X = np.array([['dog'] * 5 + ['cat'] * 20 + ['rabbit'] * 10 +
...               ['snake'] * 3], dtype=object).T
>>> enc = preprocessing.OrdinalEncoder(min_frequency=6).fit(X)
>>> enc.infrequent_categories_
[array(['dog', 'snake'], dtype=object)]
>>> enc.transform(np.array([['dog'], ['cat'], ['rabbit'], ['snake']]))
array([[2.],
[0.],
[1.],
[2.]])
OrdinalEncoder’s max_categories do not take into account missing
or unknown categories. Setting unknown_value or encoded_missing_value to an
integer will increase the number of unique integer codes by one each. This can
result in up to max_categories + 2 integer codes. In the following example,
“a” and “d” are considered infrequent and grouped together into a single
category, “b” and “c” are their own categories, unknown values are encoded as 3
and missing values are encoded as 4.
>>> X_train = np.array(
...     [["a"] * 5 + ["b"] * 20 + ["c"] * 10 + ["d"] * 3 + [np.nan]],
...     dtype=object).T
>>> enc = preprocessing.OrdinalEncoder(
...     handle_unknown="use_encoded_value", unknown_value=3,
...     max_categories=3, encoded_missing_value=4)
>>> _ = enc.fit(X_train)
>>> X_test = np.array([["a"], ["b"], ["c"], ["d"], ["e"], [np.nan]], dtype=object)
>>> enc.transform(X_test)
array([[2.],
[0.],
[1.],
[2.],
[3.],
[4.]])
Similarity, OneHotEncoder can be configured to group together infrequent
categories:
>>> enc = preprocessing.OneHotEncoder(min_frequency=6, sparse_output=False).fit(X)
>>> enc.infrequent_categories_
[array(['dog', 'snake'], dtype=object)]
>>> enc.transform(np.array([['dog'], ['cat'], ['rabbit'], ['snake']]))
array([[0., 0., 1.],
[1., 0., 0.],
[0., 1., 0.],
[0., 0., 1.]])
By setting handle_unknown to 'infrequent_if_exist', unknown categories will
be considered infrequent:
>>> enc = preprocessing.OneHotEncoder(
...    handle_unknown='infrequent_if_exist', sparse_output=False, min_frequency=6)
>>> enc = enc.fit(X)
>>> enc.transform(np.array([['dragon']]))
array([[0., 0., 1.]])
OneHotEncoder.get_feature_names_out uses ‘infrequent’ as the infrequent
feature name:
>>> enc.get_feature_names_out()
array(['x0_cat', 'x0_rabbit', 'x0_infrequent_sklearn'], dtype=object)
When 'handle_unknown' is set to 'infrequent_if_exist' and an unknown
category is encountered in transform:
If infrequent category support was not configured or there was no
infrequent category during training, the resulting one-hot encoded columns
for this feature will be all zeros. In the inverse transform, an unknown
category will be denoted as None.
If there is an infrequent category during training, the unknown category
will be considered infrequent. In the inverse transform, ‘infrequent_sklearn’
will be used to represent the infrequent category.
Infrequent categories can also be configured using max_categories. In the
following example, we set max_categories=2 to limit the number of features in
the output. This will result in all but the 'cat' category to be considered
infrequent, leading to two features, one for 'cat' and one for infrequent
categories - which are all the others:
>>> enc = preprocessing.OneHotEncoder(max_categories=2, sparse_output=False)
>>> enc = enc.fit(X)
>>> enc.transform([['dog'], ['cat'], ['rabbit'], ['snake']])
array([[0., 1.],
[1., 0.],
[0., 1.],
[0., 1.]])
If both max_categories and min_frequency are non-default values, then
categories are selected based on min_frequency first and max_categories
categories are kept. In the following example, min_frequency=4 considers
only snake to be infrequent, but max_categories=3, forces dog to also be
infrequent:
>>> enc = preprocessing.OneHotEncoder(min_frequency=4, max_categories=3, sparse_output=False)
>>> enc = enc.fit(X)
>>> enc.transform([['dog'], ['cat'], ['rabbit'], ['snake']])
array([[0., 0., 1.],
[1., 0., 0.],
[0., 1., 0.],
[0., 0., 1.]])
If there are infrequent categories with the same cardinality at the cutoff of
max_categories, then then the first max_categories are taken based on lexicon
ordering. In the following example, “b”, “c”, and “d”, have the same cardinality
and with max_categories=2, “b” and “c” are infrequent because they have a higher
lexicon order.
>>> X = np.asarray([["a"] * 20 + ["b"] * 10 + ["c"] * 10 + ["d"] * 10], dtype=object).T
>>> enc = preprocessing.OneHotEncoder(max_categories=3).fit(X)
>>> enc.infrequent_categories_
[array(['b', 'c'], dtype=object)]
6.3.4.2. Target Encoder¶
The TargetEncoder uses the target mean conditioned on the categorical
feature for encoding unordered categories, i.e. nominal categories [PAR]
[MIC]. This encoding scheme is useful with categorical features with high
cardinality, where one-hot encoding would inflate the feature space making it
more expensive for a downstream model to process. A classical example of high
cardinality categories are location based such as zip code or region. For the
binary classification target, the target encoding is given by:
\[S_i = \lambda_i\frac{n_{iY}}{n_i} + (1 - \lambda_i)\frac{n_Y}{n}\]
where \(S_i\) is the encoding for category \(i\), \(n_{iY}\) is the
number of observations with \(Y=1\) and category \(i\), \(n_i\) is
the number of observations with category \(i\), \(n_Y\) is the number of
observations with \(Y=1\), \(n\) is the number of observations, and
\(\lambda_i\) is a shrinkage factor for category \(i\). The shrinkage
factor is given by:
\[\lambda_i = \frac{n_i}{m + n_i}\]
where \(m\) is a smoothing factor, which is controlled with the smooth
parameter in TargetEncoder. Large smoothing factors will put more
weight on the global mean. When smooth="auto", the smoothing factor is
computed as an empirical Bayes estimate: \(m=\sigma_i^2/\tau^2\), where
\(\sigma_i^2\) is the variance of y with category \(i\) and
\(\tau^2\) is the global variance of y.
For multiclass classification targets, the formulation is similar to binary
classification:
\[S_{ij} = \lambda_i\frac{n_{iY_j}}{n_i} + (1 - \lambda_i)\frac{n_{Y_j}}{n}\]
where \(S_{ij}\) is the encoding for category \(i\) and class \(j\),
\(n_{iY_j}\) is the number of observations with \(Y=j\) and category
\(i\), \(n_i\) is the number of observations with category \(i\),
\(n_{Y_j}\) is the number of observations with \(Y=j\), \(n\) is the
number of observations, and \(\lambda_i\) is a shrinkage factor for category
\(i\).
For continuous targets, the formulation is similar to binary classification:
\[S_i = \lambda_i\frac{\sum_{k\in L_i}Y_k}{n_i} + (1 - \lambda_i)\frac{\sum_{k=1}^{n}Y_k}{n}\]
where \(L_i\) is the set of observations with category \(i\) and
\(n_i\) is the number of observations with category \(i\).
fit_transform internally relies on a cross fitting
scheme to prevent target information from leaking into the train-time
representation, especially for non-informative high-cardinality categorical
variables, and help prevent the downstream model from overfitting spurious
correlations. Note that as a result, fit(X, y).transform(X) does not equal
fit_transform(X, y). In fit_transform, the training
data is split into k folds (determined by the cv parameter) and each fold is
encoded using the encodings learnt using the other k-1 folds. The following
diagram shows the cross fitting scheme in
fit_transform with the default cv=5:
fit_transform also learns a ‘full data’ encoding using
the whole training set. This is never used in
fit_transform but is saved to the attribute encodings_,
for use when transform is called. Note that the encodings
learned for each fold during the cross fitting scheme are not saved to
an attribute.
The fit method does not use any cross fitting
schemes and learns one encoding on the entire training set, which is used to
encode categories in transform.
This encoding is the same as the ‘full data’
encoding learned in fit_transform.
Note
TargetEncoder considers missing values, such as np.nan or None,
as another category and encodes them like any other category. Categories
that are not seen during fit are encoded with the target mean, i.e.
target_mean_.
Examples:
Comparing Target Encoder with Other Encoders
Target Encoder’s Internal Cross fitting
References
[MIC]
Micci-Barreca, Daniele. “A preprocessing scheme for high-cardinality
categorical attributes in classification and prediction problems”
SIGKDD Explor. Newsl. 3, 1 (July 2001), 27–32.
[PAR]
Pargent, F., Pfisterer, F., Thomas, J. et al. “Regularized target
encoding outperforms traditional methods in supervised machine learning with
high cardinality features” Comput Stat 37, 2671–2692 (2022)
- 6.3.5. Discretization¶
Discretization
(otherwise known as quantization or binning) provides a way to partition continuous
features into discrete values. Certain datasets with continuous features
may benefit from discretization, because discretization can transform the dataset
of continuous attributes to one with only nominal attributes.
One-hot encoded discretized features can make a model more expressive, while
maintaining interpretability. For instance, pre-processing with a discretizer
can introduce nonlinearity to linear models. For more advanced possibilities,
in particular smooth ones, see Generating polynomial features further
below.
6.3.5.1. K-bins discretization¶
KBinsDiscretizer discretizes features into k bins:
>>> X = np.array([[ -3., 5., 15 ],
...               [  0., 6., 14 ],
...               [  6., 3., 11 ]])
>>> est = preprocessing.KBinsDiscretizer(n_bins=[3, 2, 2], encode='ordinal').fit(X)
By default the output is one-hot encoded into a sparse matrix
(See Encoding categorical features)
and this can be configured with the encode parameter.
For each feature, the bin edges are computed during fit and together with
the number of bins, they will define the intervals. Therefore, for the current
example, these intervals are defined as:
feature 1: \({[-\infty, -1), [-1, 2), [2, \infty)}\)
feature 2: \({[-\infty, 5), [5, \infty)}\)
feature 3: \({[-\infty, 14), [14, \infty)}\)
Based on these bin intervals, X is transformed as follows:
>>> est.transform(X)
array([[ 0., 1., 1.],
[ 1., 1., 1.],
[ 2., 0., 0.]])
The resulting dataset contains ordinal attributes which can be further used
in a Pipeline.
Discretization is similar to constructing histograms for continuous data.
However, histograms focus on counting features which fall into particular
bins, whereas discretization focuses on assigning feature values to these bins.
KBinsDiscretizer implements different binning strategies, which can be
selected with the strategy parameter. The ‘uniform’ strategy uses
constant-width bins. The ‘quantile’ strategy uses the quantiles values to have
equally populated bins in each feature. The ‘kmeans’ strategy defines bins based
on a k-means clustering procedure performed on each feature independently.
Be aware that one can specify custom bins by passing a callable defining the
discretization strategy to FunctionTransformer.
For instance, we can use the Pandas function pandas.cut:
>>> import pandas as pd
>>> import numpy as np
>>> from sklearn import preprocessing
>>>
>>> bins = [0, 1, 13, 20, 60, np.inf]
>>> labels = ['infant', 'kid', 'teen', 'adult', 'senior citizen']
>>> transformer = preprocessing.FunctionTransformer(
...     pd.cut, kw_args={'bins': bins, 'labels': labels, 'retbins': False}
... )
>>> X = np.array([0.2, 2, 15, 25, 97])
>>> transformer.fit_transform(X)
['infant', 'kid', 'teen', 'adult', 'senior citizen']
Categories (5, object): ['infant' < 'kid' < 'teen' < 'adult' < 'senior citizen']
Examples:
Using KBinsDiscretizer to discretize continuous features
Feature discretization
Demonstrating the different strategies of KBinsDiscretizer
6.3.5.2. Feature binarization¶
Feature binarization is the process of thresholding numerical
features to get boolean values. This can be useful for downstream
probabilistic estimators that make assumption that the input data
is distributed according to a multi-variate Bernoulli distribution. For instance,
this is the case for the BernoulliRBM.
It is also common among the text processing community to use binary
feature values (probably to simplify the probabilistic reasoning) even
if normalized counts (a.k.a. term frequencies) or TF-IDF valued features
often perform slightly better in practice.
As for the Normalizer, the utility class
Binarizer is meant to be used in the early stages of
Pipeline. The fit method does nothing
as each sample is treated independently of others:
>>> X = [[ 1., -1.,  2.],
...      [ 2.,  0.,  0.],
...      [ 0.,  1., -1.]]
>>> binarizer = preprocessing.Binarizer().fit(X)  # fit does nothing
>>> binarizer
Binarizer()
>>> binarizer.transform(X)
array([[1., 0., 1.],
[1., 0., 0.],
[0., 1., 0.]])
It is possible to adjust the threshold of the binarizer:
>>> binarizer = preprocessing.Binarizer(threshold=1.1)
>>> binarizer.transform(X)
array([[0., 0., 1.],
[1., 0., 0.],
[0., 0., 0.]])
As for the Normalizer class, the preprocessing module
provides a companion function binarize
to be used when the transformer API is not necessary.
Note that the Binarizer is similar to the KBinsDiscretizer
when k = 2, and when the bin edge is at the value threshold.
Sparse input
binarize and Binarizer accept both dense array-like
and sparse matrices from scipy.sparse as input.
For sparse input the data is converted to the Compressed Sparse Rows
representation (see scipy.sparse.csr_matrix).
To avoid unnecessary memory copies, it is recommended to choose the CSR
representation upstream.
- 6.3.6. Imputation of missing values¶
Tools for imputing missing values are discussed at Imputation of missing values.
- 6.3.7. Generating polynomial features¶
Often it’s useful to add complexity to a model by considering nonlinear
features of the input data. We show two possibilities that are both based on
polynomials: The first one uses pure polynomials, the second one uses splines,
i.e. piecewise polynomials.
6.3.7.1. Polynomial features¶
A simple and common method to use is polynomial features, which can get
features’ high-order and interaction terms. It is implemented in
PolynomialFeatures:
>>> import numpy as np
>>> from sklearn.preprocessing import PolynomialFeatures
>>> X = np.arange(6).reshape(3, 2)
>>> X
array([[0, 1],
[2, 3],
[4, 5]])
>>> poly = PolynomialFeatures(2)
>>> poly.fit_transform(X)
array([[ 1.,  0.,  1.,  0.,  0.,  1.],
[ 1.,  2.,  3.,  4.,  6.,  9.],
[ 1.,  4.,  5., 16., 20., 25.]])
The features of X have been transformed from \((X_1, X_2)\) to
\((1, X_1, X_2, X_1^2, X_1X_2, X_2^2)\).
In some cases, only interaction terms among features are required, and it can
be gotten with the setting interaction_only=True:
>>> X = np.arange(9).reshape(3, 3)
>>> X
array([[0, 1, 2],
[3, 4, 5],
[6, 7, 8]])
>>> poly = PolynomialFeatures(degree=3, interaction_only=True)
>>> poly.fit_transform(X)
array([[  1.,   0.,   1.,   2.,   0.,   0.,   2.,   0.],
[  1.,   3.,   4.,   5.,  12.,  15.,  20.,  60.],
[  1.,   6.,   7.,   8.,  42.,  48.,  56., 336.]])
The features of X have been transformed from \((X_1, X_2, X_3)\) to
\((1, X_1, X_2, X_3, X_1X_2, X_1X_3, X_2X_3, X_1X_2X_3)\).
Note that polynomial features are used implicitly in kernel methods (e.g., SVC,
KernelPCA) when using polynomial Kernel functions.
See Polynomial and Spline interpolation
for Ridge regression using created polynomial features.
6.3.7.2. Spline transformer¶
Another way to add nonlinear terms instead of pure polynomials of features is
to generate spline basis functions for each feature with the
SplineTransformer. Splines are piecewise polynomials, parametrized by
their polynomial degree and the positions of the knots. The
SplineTransformer implements a B-spline basis, cf. the references
below.
Note
The SplineTransformer treats each feature separately, i.e. it
won’t give you interaction terms.
Some of the advantages of splines over polynomials are:
B-splines are very flexible and robust if you keep a fixed low degree,
usually 3, and parsimoniously adapt the number of knots. Polynomials
would need a higher degree, which leads to the next point.
B-splines do not have oscillatory behaviour at the boundaries as have
polynomials (the higher the degree, the worse). This is known as Runge’s
phenomenon.
B-splines provide good options for extrapolation beyond the boundaries,
i.e. beyond the range of fitted values. Have a look at the option
extrapolation.
B-splines generate a feature matrix with a banded structure. For a single
feature, every row contains only degree + 1 non-zero elements, which
occur consecutively and are even positive. This results in a matrix with
good numerical properties, e.g. a low condition number, in sharp contrast
to a matrix of polynomials, which goes under the name
Vandermonde matrix.
A low condition number is important for stable algorithms of linear
models.
The following code snippet shows splines in action:
>>> import numpy as np
>>> from sklearn.preprocessing import SplineTransformer
>>> X = np.arange(5).reshape(5, 1)
>>> X
array([[0],
[1],
[2],
[3],
[4]])
>>> spline = SplineTransformer(degree=2, n_knots=3)
>>> spline.fit_transform(X)
array([[0.5  , 0.5  , 0.   , 0.   ],
[0.125, 0.75 , 0.125, 0.   ],
[0.   , 0.5  , 0.5  , 0.   ],
[0.   , 0.125, 0.75 , 0.125],
[0.   , 0.   , 0.5  , 0.5  ]])
As the X is sorted, one can easily see the banded matrix output. Only the
three middle diagonals are non-zero for degree=2. The higher the degree,
the more overlapping of the splines.
Interestingly, a SplineTransformer of degree=0 is the same as
KBinsDiscretizer with
encode='onehot-dense' and n_bins = n_knots - 1 if
knots = strategy.
Examples:
Polynomial and Spline interpolation
Time-related feature engineering
References:
Eilers, P., & Marx, B. (1996). Flexible Smoothing with B-splines and
Penalties. Statist. Sci. 11 (1996), no. 2, 89–121.
Perperoglou, A., Sauerbrei, W., Abrahamowicz, M. et al. A review of
spline function procedures in R.
BMC Med Res Methodol 19, 46 (2019).
- 6.3.8. Custom transformers¶
Often, you will want to convert an existing Python function into a transformer
to assist in data cleaning or processing. You can implement a transformer from
an arbitrary function with FunctionTransformer. For example, to build
a transformer that applies a log transformation in a pipeline, do:
>>> import numpy as np
>>> from sklearn.preprocessing import FunctionTransformer
>>> transformer = FunctionTransformer(np.log1p, validate=True)
>>> X = np.array([[0, 1], [2, 3]])
>>> # Since FunctionTransformer is no-op during fit, we can call transform directly
>>> transformer.transform(X)
array([[0.        , 0.69314718],
[1.09861229, 1.38629436]])
You can ensure that func and inverse_func are the inverse of each other
by setting check_inverse=True and calling fit before
transform. Please note that a warning is raised and can be turned into an
error with a filterwarnings:
>>> import warnings
>>> warnings.filterwarnings("error", message=".*check_inverse*.",
...                         category=UserWarning, append=False)
For a full code example that demonstrates using a FunctionTransformer
to extract features from text data see
Column Transformer with Heterogeneous Data Sources and
Time-related feature engineering.
### 6.4. Imputation of missing values — scikit-learn 1.4.2 documentation

Install
User Guide
API
Examples
Community
Getting Started
Tutorial
What's new
Glossary
Development
FAQ
Support
Related packages
Roadmap
Governance
About us
GitHub
Other Versions and Download
More
Getting Started
Tutorial
What's new
Glossary
Development
FAQ
Support
Related packages
Roadmap
Governance
About us
GitHub
Other Versions and Download
Toggle Menu
PrevUp
Next
scikit-learn 1.4.2
Other versions
Please cite us if you use the software.
### 6.4. Imputation of missing values

- 6.4.1. Univariate vs. Multivariate Imputation
- 6.4.2. Univariate feature imputation
- 6.4.3. Multivariate feature imputation
6.4.3.1. Flexibility of IterativeImputer
6.4.3.2. Multiple vs. Single Imputation
6.4.3.3. References
- 6.4.4. Nearest neighbors imputation
- 6.4.5. Keeping the number of features constant
- 6.4.6. Marking imputed values
- 6.4.7. Estimators that handle NaN values
### 6.4. Imputation of missing values¶

For various reasons, many real world datasets contain missing values, often
encoded as blanks, NaNs or other placeholders. Such datasets however are
incompatible with scikit-learn estimators which assume that all values in an
array are numerical, and that all have and hold meaning. A basic strategy to
use incomplete datasets is to discard entire rows and/or columns containing
missing values. However, this comes at the price of losing data which may be
valuable (even though incomplete). A better strategy is to impute the missing
values, i.e., to infer them from the known part of the data. See the
glossary entry on imputation.
- 6.4.1. Univariate vs. Multivariate Imputation¶
One type of imputation algorithm is univariate, which imputes values in the
i-th feature dimension using only non-missing values in that feature dimension
(e.g. SimpleImputer). By contrast, multivariate imputation
algorithms use the entire set of available feature dimensions to estimate the
missing values (e.g. IterativeImputer).
- 6.4.2. Univariate feature imputation¶
The SimpleImputer class provides basic strategies for imputing missing
values. Missing values can be imputed with a provided constant value, or using
the statistics (mean, median or most frequent) of each column in which the
missing values are located. This class also allows for different missing values
encodings.
The following snippet demonstrates how to replace missing values,
encoded as np.nan, using the mean value of the columns (axis 0)
that contain the missing values:
>>> import numpy as np
>>> from sklearn.impute import SimpleImputer
>>> imp = SimpleImputer(missing_values=np.nan, strategy='mean')
>>> imp.fit([[1, 2], [np.nan, 3], [7, 6]])
SimpleImputer()
>>> X = [[np.nan, 2], [6, np.nan], [7, 6]]
>>> print(imp.transform(X))
[[4.          2.        ]
[6.          3.666...]
[7.          6.        ]]
The SimpleImputer class also supports sparse matrices:
>>> import scipy.sparse as sp
>>> X = sp.csc_matrix([[1, 2], [0, -1], [8, 4]])
>>> imp = SimpleImputer(missing_values=-1, strategy='mean')
>>> imp.fit(X)
SimpleImputer(missing_values=-1)
>>> X_test = sp.csc_matrix([[-1, 2], [6, -1], [7, 6]])
>>> print(imp.transform(X_test).toarray())
[[3. 2.]
[6. 3.]
[7. 6.]]
Note that this format is not meant to be used to implicitly store missing
values in the matrix because it would densify it at transform time. Missing
values encoded by 0 must be used with dense input.
The SimpleImputer class also supports categorical data represented as
string values or pandas categoricals when using the 'most_frequent' or
'constant' strategy:
>>> import pandas as pd
>>> df = pd.DataFrame([["a", "x"],
...                    [np.nan, "y"],
...                    ["a", np.nan],
...                    ["b", "y"]], dtype="category")
...
>>> imp = SimpleImputer(strategy="most_frequent")
>>> print(imp.fit_transform(df))
[['a' 'x']
['a' 'y']
['a' 'y']
['b' 'y']]
For another example on usage, see Imputing missing values before building an estimator.
- 6.4.3. Multivariate feature imputation¶
A more sophisticated approach is to use the IterativeImputer class,
which models each feature with missing values as a function of other features,
and uses that estimate for imputation. It does so in an iterated round-robin
fashion: at each step, a feature column is designated as output y and the
other feature columns are treated as inputs X. A regressor is fit on (X,
y) for known y. Then, the regressor is used to predict the missing values
of y.  This is done for each feature in an iterative fashion, and then is
repeated for max_iter imputation rounds. The results of the final
imputation round are returned.
Note
This estimator is still experimental for now: default parameters or
details of behaviour might change without any deprecation cycle. Resolving
the following issues would help stabilize IterativeImputer:
convergence criteria (#14338), default estimators (#13286),
and use of random state (#15611). To use it, you need to explicitly
import enable_iterative_imputer.
>>> import numpy as np
>>> from sklearn.experimental import enable_iterative_imputer
>>> from sklearn.impute import IterativeImputer
>>> imp = IterativeImputer(max_iter=10, random_state=0)
>>> imp.fit([[1, 2], [3, 6], [4, 8], [np.nan, 3], [7, np.nan]])
IterativeImputer(random_state=0)
>>> X_test = [[np.nan, 2], [6, np.nan], [np.nan, 6]]
>>> # the model learns that the second feature is double the first
>>> print(np.round(imp.transform(X_test)))
[[ 1.  2.]
[ 6. 12.]
[ 3.  6.]]
Both SimpleImputer and IterativeImputer can be used in a
Pipeline as a way to build a composite estimator that supports imputation.
See Imputing missing values before building an estimator.
6.4.3.1. Flexibility of IterativeImputer¶
There are many well-established imputation packages in the R data science
ecosystem: Amelia, mi, mice, missForest, etc. missForest is popular, and turns
out to be a particular instance of different sequential imputation algorithms
that can all be implemented with IterativeImputer by passing in
different regressors to be used for predicting missing feature values. In the
case of missForest, this regressor is a Random Forest.
See Imputing missing values with variants of IterativeImputer.
6.4.3.2. Multiple vs. Single Imputation¶
In the statistics community, it is common practice to perform multiple
imputations, generating, for example, m separate imputations for a single
feature matrix. Each of these m imputations is then put through the
subsequent analysis pipeline (e.g. feature engineering, clustering, regression,
classification). The m final analysis results (e.g. held-out validation
errors) allow the data scientist to obtain understanding of how analytic
results may differ as a consequence of the inherent uncertainty caused by the
missing values. The above practice is called multiple imputation.
Our implementation of IterativeImputer was inspired by the R MICE
package (Multivariate Imputation by Chained Equations) [1], but differs from
it by returning a single imputation instead of multiple imputations.  However,
IterativeImputer can also be used for multiple imputations by applying
it repeatedly to the same dataset with different random seeds when
sample_posterior=True. See [2], chapter 4 for more discussion on multiple
vs. single imputations.
It is still an open problem as to how useful single vs. multiple imputation is
in the context of prediction and classification when the user is not
interested in measuring uncertainty due to missing values.
Note that a call to the transform method of IterativeImputer is
not allowed to change the number of samples. Therefore multiple imputations
cannot be achieved by a single call to transform.
6.4.3.3. References¶
[1]
Stef van Buuren, Karin Groothuis-Oudshoorn (2011). “mice: Multivariate
Imputation by Chained Equations in R”. Journal of Statistical Software 45:
1-67.
[2]
Roderick J A Little and Donald B Rubin (1986). “Statistical Analysis
with Missing Data”. John Wiley & Sons, Inc., New York, NY, USA.
- 6.4.4. Nearest neighbors imputation¶
The KNNImputer class provides imputation for filling in missing values
using the k-Nearest Neighbors approach. By default, a euclidean distance metric
that supports missing values,
nan_euclidean_distances, is used to find the
nearest neighbors. Each missing feature is imputed using values from
n_neighbors nearest neighbors that have a value for the feature. The
feature of the neighbors are averaged uniformly or weighted by distance to each
neighbor. If a sample has more than one feature missing, then the neighbors for
that sample can be different depending on the particular feature being imputed.
When the number of available neighbors is less than n_neighbors and there are
no defined distances to the training set, the training set average for that
feature is used during imputation. If there is at least one neighbor with a
defined distance, the weighted or unweighted average of the remaining neighbors
will be used during imputation. If a feature is always missing in training, it
is removed during transform. For more information on the methodology, see
ref. [OL2001].
The following snippet demonstrates how to replace missing values,
encoded as np.nan, using the mean feature value of the two nearest
neighbors of samples with missing values:
>>> import numpy as np
>>> from sklearn.impute import KNNImputer
>>> nan = np.nan
>>> X = [[1, 2, nan], [3, 4, 3], [nan, 6, 5], [8, 8, 7]]
>>> imputer = KNNImputer(n_neighbors=2, weights="uniform")
>>> imputer.fit_transform(X)
array([[1. , 2. , 4. ],
[3. , 4. , 3. ],
[5.5, 6. , 5. ],
[8. , 8. , 7. ]])
For another example on usage, see Imputing missing values before building an estimator.
References
[OL2001]
Olga Troyanskaya, Michael Cantor, Gavin Sherlock, Pat Brown,
Trevor Hastie, Robert Tibshirani, David Botstein and Russ B. Altman,
Missing value estimation methods for DNA microarrays, BIOINFORMATICS
Vol. 17 no. 6, 2001 Pages 520-525.
- 6.4.5. Keeping the number of features constant¶
By default, the scikit-learn imputers will drop fully empty features, i.e.
columns containing only missing values. For instance:
>>> imputer = SimpleImputer()
>>> X = np.array([[np.nan, 1], [np.nan, 2], [np.nan, 3]])
>>> imputer.fit_transform(X)
array([[1.],
[2.],
[3.]])
The first feature in X containing only np.nan was dropped after the
imputation. While this feature will not help in predictive setting, dropping
the columns will change the shape of X which could be problematic when using
imputers in a more complex machine-learning pipeline. The parameter
keep_empty_features offers the option to keep the empty features by imputing
with a constant values. In most of the cases, this constant value is zero:
>>> imputer.set_params(keep_empty_features=True)
SimpleImputer(keep_empty_features=True)
>>> imputer.fit_transform(X)
array([[0., 1.],
[0., 2.],
[0., 3.]])
- 6.4.6. Marking imputed values¶
The MissingIndicator transformer is useful to transform a dataset into
corresponding binary matrix indicating the presence of missing values in the
dataset. This transformation is useful in conjunction with imputation. When
using imputation, preserving the information about which values had been
missing can be informative. Note that both the SimpleImputer and
IterativeImputer have the boolean parameter add_indicator
(False by default) which when set to True provides a convenient way of
stacking the output of the MissingIndicator transformer with the
output of the imputer.
NaN is usually used as the placeholder for missing values. However, it
enforces the data type to be float. The parameter missing_values allows to
specify other placeholder such as integer. In the following example, we will
use -1 as missing values:
>>> from sklearn.impute import MissingIndicator
>>> X = np.array([[-1, -1, 1, 3],
...               [4, -1, 0, -1],
...               [8, -1, 1, 0]])
>>> indicator = MissingIndicator(missing_values=-1)
>>> mask_missing_values_only = indicator.fit_transform(X)
>>> mask_missing_values_only
array([[ True,  True, False],
[False,  True,  True],
[False,  True, False]])
The features parameter is used to choose the features for which the mask is
constructed. By default, it is 'missing-only' which returns the imputer
mask of the features containing missing values at fit time:
>>> indicator.features_
array([0, 1, 3])
The features parameter can be set to 'all' to return all features
whether or not they contain missing values:
>>> indicator = MissingIndicator(missing_values=-1, features="all")
>>> mask_all = indicator.fit_transform(X)
>>> mask_all
array([[ True,  True, False, False],
[False,  True, False,  True],
[False,  True, False, False]])
>>> indicator.features_
array([0, 1, 2, 3])
When using the MissingIndicator in a
Pipeline, be sure to use the
FeatureUnion or
ColumnTransformer to add the indicator features to
the regular features. First we obtain the iris dataset, and add some missing
values to it.
>>> from sklearn.datasets import load_iris
>>> from sklearn.impute import SimpleImputer, MissingIndicator
>>> from sklearn.model_selection import train_test_split
>>> from sklearn.pipeline import FeatureUnion, make_pipeline
>>> from sklearn.tree import DecisionTreeClassifier
>>> X, y = load_iris(return_X_y=True)
>>> mask = np.random.randint(0, 2, size=X.shape).astype(bool)
>>> X[mask] = np.nan
>>> X_train, X_test, y_train, _ = train_test_split(X, y, test_size=100,
...                                                random_state=0)
Now we create a FeatureUnion. All features will be
imputed using SimpleImputer, in order to enable classifiers to work
with this data. Additionally, it adds the indicator variables from
MissingIndicator.
>>> transformer = FeatureUnion(
...     transformer_list=[
...         ('features', SimpleImputer(strategy='mean')),
...         ('indicators', MissingIndicator())])
>>> transformer = transformer.fit(X_train, y_train)
>>> results = transformer.transform(X_test)
>>> results.shape
(100, 8)
Of course, we cannot use the transformer to make any predictions. We should
wrap this in a Pipeline with a classifier (e.g., a
DecisionTreeClassifier) to be able to make predictions.
>>> clf = make_pipeline(transformer, DecisionTreeClassifier())
>>> clf = clf.fit(X_train, y_train)
>>> results = clf.predict(X_test)
>>> results.shape
(100,)
- 6.4.7. Estimators that handle NaN values¶
Some estimators are designed to handle NaN values without preprocessing.
Below is the list of these estimators, classified by type
(cluster, regressor, classifier, transform):
Estimators that allow NaN values for type cluster:
HDBSCAN
Estimators that allow NaN values for type regressor:
BaggingRegressor
DecisionTreeRegressor
HistGradientBoostingRegressor
RandomForestRegressor
StackingRegressor
VotingRegressor
Estimators that allow NaN values for type classifier:
BaggingClassifier
DecisionTreeClassifier
HistGradientBoostingClassifier
RandomForestClassifier
StackingClassifier
VotingClassifier
Estimators that allow NaN values for type transformer:
IterativeImputer
KNNImputer
MaxAbsScaler
MinMaxScaler
MissingIndicator
OneHotEncoder
OrdinalEncoder
PowerTransformer
QuantileTransformer
RobustScaler
SimpleImputer
StackingClassifier
StackingRegressor
StandardScaler
TargetEncoder
VarianceThreshold
VotingClassifier
VotingRegressor
### 6.5. Unsupervised dimensionality reduction — scikit-learn 1.4.2 documentation

Install
User Guide
API
Examples
Community
Getting Started
Tutorial
What's new
Glossary
Development
FAQ
Support
Related packages
Roadmap
Governance
About us
GitHub
Other Versions and Download
More
Getting Started
Tutorial
What's new
Glossary
Development
FAQ
Support
Related packages
Roadmap
Governance
About us
GitHub
Other Versions and Download
Toggle Menu
PrevUp
Next
scikit-learn 1.4.2
Other versions
Please cite us if you use the software.
### 6.5. Unsupervised dimensionality reduction

- 6.5.1. PCA: principal component analysis
- 6.5.2. Random projections
- 6.5.3. Feature agglomeration
### 6.5. Unsupervised dimensionality reduction¶

If your number of features is high, it may be useful to reduce it with an
unsupervised step prior to supervised steps. Many of the
Unsupervised learning methods implement a transform method that
can be used to reduce the dimensionality. Below we discuss two specific
example of this pattern that are heavily used.
Pipelining
The unsupervised data reduction and the supervised estimator can be
chained in one step. See Pipeline: chaining estimators.
- 6.5.1. PCA: principal component analysis¶
decomposition.PCA looks for a combination of features that
capture well the variance of the original features. See Decomposing signals in components (matrix factorization problems).
Examples
Faces recognition example using eigenfaces and SVMs
- 6.5.2. Random projections¶
The module: random_projection provides several tools for data
reduction by random projections. See the relevant section of the
documentation: Random Projection.
Examples
The Johnson-Lindenstrauss bound for embedding with random projections
- 6.5.3. Feature agglomeration¶
cluster.FeatureAgglomeration applies
Hierarchical clustering to group together features that behave
similarly.
Examples
Feature agglomeration vs. univariate selection
Feature agglomeration
Feature scaling
Note that if features have very different scaling or statistical
properties, cluster.FeatureAgglomeration may not be able to
capture the links between related features. Using a
preprocessing.StandardScaler can be useful in these settings.
### 6.6. Random Projection — scikit-learn 1.4.2 documentation

Install
User Guide
API
Examples
Community
Getting Started
Tutorial
What's new
Glossary
Development
FAQ
Support
Related packages
Roadmap
Governance
About us
GitHub
Other Versions and Download
More
Getting Started
Tutorial
What's new
Glossary
Development
FAQ
Support
Related packages
Roadmap
Governance
About us
GitHub
Other Versions and Download
Toggle Menu
PrevUp
Next
scikit-learn 1.4.2
Other versions
Please cite us if you use the software.
### 6.6. Random Projection

- 6.6.1. The Johnson-Lindenstrauss lemma
- 6.6.2. Gaussian random projection
- 6.6.3. Sparse random projection
- 6.6.4. Inverse Transform
### 6.6. Random Projection¶

The sklearn.random_projection module implements a simple and
computationally efficient way to reduce the dimensionality of the data by
trading a controlled amount of accuracy (as additional variance) for faster
processing times and smaller model sizes. This module implements two types of
unstructured random matrix:
Gaussian random matrix and
sparse random matrix.
The dimensions and distribution of random projections matrices are
controlled so as to preserve the pairwise distances between any two
samples of the dataset. Thus random projection is a suitable approximation
technique for distance based method.
References:
Sanjoy Dasgupta. 2000.
Experiments with random projection.
In Proceedings of the Sixteenth conference on Uncertainty in artificial
intelligence (UAI’00), Craig Boutilier and Moisés Goldszmidt (Eds.). Morgan
Kaufmann Publishers Inc., San Francisco, CA, USA, 143-151.
Ella Bingham and Heikki Mannila. 2001.
Random projection in dimensionality reduction: applications to image and text data.
In Proceedings of the seventh ACM SIGKDD international conference on
Knowledge discovery and data mining (KDD ‘01). ACM, New York, NY, USA,
245-250.
- 6.6.1. The Johnson-Lindenstrauss lemma¶
The main theoretical result behind the efficiency of random projection is the
Johnson-Lindenstrauss lemma (quoting Wikipedia):
In mathematics, the Johnson-Lindenstrauss lemma is a result
concerning low-distortion embeddings of points from high-dimensional
into low-dimensional Euclidean space. The lemma states that a small set
of points in a high-dimensional space can be embedded into a space of
much lower dimension in such a way that distances between the points are
nearly preserved. The map used for the embedding is at least Lipschitz,
and can even be taken to be an orthogonal projection.
Knowing only the number of samples, the
johnson_lindenstrauss_min_dim estimates
conservatively the minimal size of the random subspace to guarantee a
bounded distortion introduced by the random projection:
>>> from sklearn.random_projection import johnson_lindenstrauss_min_dim
>>> johnson_lindenstrauss_min_dim(n_samples=1e6, eps=0.5)
663
>>> johnson_lindenstrauss_min_dim(n_samples=1e6, eps=[0.5, 0.1, 0.01])
array([    663,   11841, 1112658])
>>> johnson_lindenstrauss_min_dim(n_samples=[1e4, 1e5, 1e6], eps=0.1)
array([ 7894,  9868, 11841])
Example:
See The Johnson-Lindenstrauss bound for embedding with random projections
for a theoretical explication on the Johnson-Lindenstrauss lemma and an
empirical validation using sparse random matrices.
References:
Sanjoy Dasgupta and Anupam Gupta, 1999.
An elementary proof of the Johnson-Lindenstrauss Lemma.
- 6.6.2. Gaussian random projection¶
The GaussianRandomProjection reduces the
dimensionality by projecting the original input space on a randomly generated
matrix where components are drawn from the following distribution
\(N(0, \frac{1}{n_{components}})\).
Here a small excerpt which illustrates how to use the Gaussian random
projection transformer:
>>> import numpy as np
>>> from sklearn import random_projection
>>> X = np.random.rand(100, 10000)
>>> transformer = random_projection.GaussianRandomProjection()
>>> X_new = transformer.fit_transform(X)
>>> X_new.shape
(100, 3947)
- 6.6.3. Sparse random projection¶
The SparseRandomProjection reduces the
dimensionality by projecting the original input space using a sparse
random matrix.
Sparse random matrices are an alternative to dense Gaussian random
projection matrix that guarantees similar embedding quality while being much
more memory efficient and allowing faster computation of the projected data.
If we define s = 1 / density, the elements of the random matrix
are drawn from
\[\begin{split}\left\{
\begin{array}{c c l}
-\sqrt{\frac{s}{n_{\text{components}}}} & & 1 / 2s\\
0 &\text{with probability}  & 1 - 1 / s \\
+\sqrt{\frac{s}{n_{\text{components}}}} & & 1 / 2s\\
\end{array}
\right.\end{split}\]
where \(n_{\text{components}}\) is the size of the projected subspace.
By default the density of non zero elements is set to the minimum density as
recommended by Ping Li et al.: \(1 / \sqrt{n_{\text{features}}}\).
Here a small excerpt which illustrates how to use the sparse random
projection transformer:
>>> import numpy as np
>>> from sklearn import random_projection
>>> X = np.random.rand(100, 10000)
>>> transformer = random_projection.SparseRandomProjection()
>>> X_new = transformer.fit_transform(X)
>>> X_new.shape
(100, 3947)
References:
D. Achlioptas. 2003.
Database-friendly random projections: Johnson-Lindenstrauss  with binary
coins.
Journal of Computer and System Sciences 66 (2003) 671–687
Ping Li, Trevor J. Hastie, and Kenneth W. Church. 2006.
Very sparse random projections.
In Proceedings of the 12th ACM SIGKDD international conference on
Knowledge discovery and data mining (KDD ‘06). ACM, New York, NY, USA,
287-296.
- 6.6.4. Inverse Transform¶
The random projection transformers have compute_inverse_components parameter. When
set to True, after creating the random components_ matrix during fitting,
the transformer computes the pseudo-inverse of this matrix and stores it as
inverse_components_. The inverse_components_ matrix has shape
\(n_{features} \times n_{components}\), and it is always a dense matrix,
regardless of whether the components matrix is sparse or dense. So depending on
the number of features and components, it may use a lot of memory.
When the inverse_transform method is called, it computes the product of the
input X and the transpose of the inverse components. If the inverse components have
been computed during fit, they are reused at each call to inverse_transform.
Otherwise they are recomputed each time, which can be costly. The result is always
dense, even if X is sparse.
Here a small code example which illustrates how to use the inverse transform
feature:
>>> import numpy as np
>>> from sklearn.random_projection import SparseRandomProjection
>>> X = np.random.rand(100, 10000)
>>> transformer = SparseRandomProjection(
...   compute_inverse_components=True
... )
...
>>> X_new = transformer.fit_transform(X)
>>> X_new.shape
(100, 3947)
>>> X_new_inversed = transformer.inverse_transform(X_new)
>>> X_new_inversed.shape
(100, 10000)
>>> X_new_again = transformer.transform(X_new_inversed)
>>> np.allclose(X_new, X_new_again)
True
### 6.7. Kernel Approximation — scikit-learn 1.4.2 documentation

Install
User Guide
API
Examples
Community
Getting Started
Tutorial
What's new
Glossary
Development
FAQ
Support
Related packages
Roadmap
Governance
About us
GitHub
Other Versions and Download
More
Getting Started
Tutorial
What's new
Glossary
Development
FAQ
Support
Related packages
Roadmap
Governance
About us
GitHub
Other Versions and Download
Toggle Menu
PrevUp
Next
scikit-learn 1.4.2
Other versions
Please cite us if you use the software.
### 6.7. Kernel Approximation

- 6.7.1. Nystroem Method for Kernel Approximation
- 6.7.2. Radial Basis Function Kernel
- 6.7.3. Additive Chi Squared Kernel
- 6.7.4. Skewed Chi Squared Kernel
- 6.7.5. Polynomial Kernel Approximation via Tensor Sketch
- 6.7.6. Mathematical Details
### 6.7. Kernel Approximation¶

This submodule contains functions that approximate the feature mappings that
correspond to certain kernels, as they are used for example in support vector
machines (see Support Vector Machines).
The following feature functions perform non-linear transformations of the
input, which can serve as a basis for linear classification or other
algorithms.
The advantage of using approximate explicit feature maps compared to the
kernel trick,
which makes use of feature maps implicitly, is that explicit mappings
can be better suited for online learning and can significantly reduce the cost
of learning with very large datasets.
Standard kernelized SVMs do not scale well to large datasets, but using an
approximate kernel map it is possible to use much more efficient linear SVMs.
In particular, the combination of kernel map approximations with
SGDClassifier can make non-linear learning on large datasets possible.
Since there has not been much empirical work using approximate embeddings, it
is advisable to compare results against exact kernel methods when possible.
See also
Polynomial regression: extending linear models with basis functions for an exact polynomial transformation.
- 6.7.1. Nystroem Method for Kernel Approximation¶
The Nystroem method, as implemented in Nystroem is a general method for
reduced rank approximations of kernels. It achieves this by subsampling without
replacement rows/columns of the data on which the kernel is evaluated. While the
computational complexity of the exact method is
\(\mathcal{O}(n^3_{\text{samples}})\), the complexity of the approximation
is \(\mathcal{O}(n^2_{\text{components}} \cdot n_{\text{samples}})\), where
one can set \(n_{\text{components}} \ll n_{\text{samples}}\) without a
significative decrease in performance [WS2001].
We can construct the eigendecomposition of the kernel matrix \(K\), based
on the features of the data, and then split it into sampled and unsampled data
points.
\[\begin{split}K = U \Lambda U^T
= \begin{bmatrix} U_1 \\ U_2\end{bmatrix} \Lambda \begin{bmatrix} U_1 \\ U_2 \end{bmatrix}^T
= \begin{bmatrix} U_1 \Lambda U_1^T & U_1 \Lambda U_2^T \\ U_2 \Lambda U_1^T & U_2 \Lambda U_2^T \end{bmatrix}
\equiv \begin{bmatrix} K_{11} & K_{12} \\ K_{21} & K_{22} \end{bmatrix}\end{split}\]
where:
\(U\) is orthonormal
\(\Lambda\) is diagonal matrix of eigenvalues
\(U_1\) is orthonormal matrix of samples that were chosen
\(U_2\) is orthonormal matrix of samples that were not chosen
Given that \(U_1 \Lambda U_1^T\) can be obtained by orthonormalization of
the matrix \(K_{11}\), and \(U_2 \Lambda U_1^T\) can be evaluated (as
well as its transpose), the only remaining term to elucidate is
\(U_2 \Lambda U_2^T\). To do this we can express it in terms of the already
evaluated matrices:
\[\begin{split}\begin{align} U_2 \Lambda U_2^T &= \left(K_{21} U_1 \Lambda^{-1}\right) \Lambda \left(K_{21} U_1 \Lambda^{-1}\right)^T
\\&= K_{21} U_1 (\Lambda^{-1} \Lambda) \Lambda^{-1} U_1^T K_{21}^T
\\&= K_{21} U_1 \Lambda^{-1} U_1^T K_{21}^T
\\&= K_{21} K_{11}^{-1} K_{21}^T
\\&= \left( K_{21} K_{11}^{-\frac12} \right) \left( K_{21} K_{11}^{-\frac12} \right)^T
.\end{align}\end{split}\]
During fit, the class Nystroem evaluates the basis \(U_1\), and
computes the normalization constant, \(K_{11}^{-\frac12}\). Later, during
transform, the kernel matrix is determined between the basis (given by the
components_ attribute) and the new data points, X. This matrix is then
multiplied by the normalization_ matrix for the final result.
By default Nystroem uses the rbf kernel, but it can use any kernel
function or a precomputed kernel matrix. The number of samples used - which is
also the dimensionality of the features computed - is given by the parameter
n_components.
Examples:
See the example entitled
Time-related feature engineering,
that shows an efficient machine learning pipeline that uses a
Nystroem kernel.
- 6.7.2. Radial Basis Function Kernel¶
The RBFSampler constructs an approximate mapping for the radial basis
function kernel, also known as Random Kitchen Sinks [RR2007]. This
transformation can be used to explicitly model a kernel map, prior to applying
a linear algorithm, for example a linear SVM:
>>> from sklearn.kernel_approximation import RBFSampler
>>> from sklearn.linear_model import SGDClassifier
>>> X = [[0, 0], [1, 1], [1, 0], [0, 1]]
>>> y = [0, 0, 1, 1]
>>> rbf_feature = RBFSampler(gamma=1, random_state=1)
>>> X_features = rbf_feature.fit_transform(X)
>>> clf = SGDClassifier(max_iter=5)
>>> clf.fit(X_features, y)
SGDClassifier(max_iter=5)
>>> clf.score(X_features, y)
1.0
The mapping relies on a Monte Carlo approximation to the
kernel values. The fit function performs the Monte Carlo sampling, whereas
the transform method performs the mapping of the data.  Because of the
inherent randomness of the process, results may vary between different calls to
the fit function.
The fit function takes two arguments:
n_components, which is the target dimensionality of the feature transform,
and gamma, the parameter of the RBF-kernel.  A higher n_components will
result in a better approximation of the kernel and will yield results more
similar to those produced by a kernel SVM. Note that “fitting” the feature
function does not actually depend on the data given to the fit function.
Only the dimensionality of the data is used.
Details on the method can be found in [RR2007].
For a given value of n_components RBFSampler is often less accurate
as Nystroem. RBFSampler is cheaper to compute, though, making
use of larger feature spaces more efficient.
Comparing an exact RBF kernel (left) with the approximation (right)¶
Examples:
Explicit feature map approximation for RBF kernels
- 6.7.3. Additive Chi Squared Kernel¶
The additive chi squared kernel is a kernel on histograms, often used in computer vision.
The additive chi squared kernel as used here is given by
\[k(x, y) = \sum_i \frac{2x_iy_i}{x_i+y_i}\]
This is not exactly the same as sklearn.metrics.pairwise.additive_chi2_kernel.
The authors of [VZ2010] prefer the version above as it is always positive
definite.
Since the kernel is additive, it is possible to treat all components
\(x_i\) separately for embedding. This makes it possible to sample
the Fourier transform in regular intervals, instead of approximating
using Monte Carlo sampling.
The class AdditiveChi2Sampler implements this component wise
deterministic sampling. Each component is sampled \(n\) times, yielding
\(2n+1\) dimensions per input dimension (the multiple of two stems
from the real and complex part of the Fourier transform).
In the literature, \(n\) is usually chosen to be 1 or 2, transforming
the dataset to size n_samples * 5 * n_features (in the case of \(n=2\)).
The approximate feature map provided by AdditiveChi2Sampler can be combined
with the approximate feature map provided by RBFSampler to yield an approximate
feature map for the exponentiated chi squared kernel.
See the [VZ2010] for details and [VVZ2010] for combination with the RBFSampler.
- 6.7.4. Skewed Chi Squared Kernel¶
The skewed chi squared kernel is given by:
\[k(x,y) = \prod_i \frac{2\sqrt{x_i+c}\sqrt{y_i+c}}{x_i + y_i + 2c}\]
It has properties that are similar to the exponentiated chi squared kernel
often used in computer vision, but allows for a simple Monte Carlo
approximation of the feature map.
The usage of the SkewedChi2Sampler is the same as the usage described
above for the RBFSampler. The only difference is in the free
parameter, that is called \(c\).
For a motivation for this mapping and the mathematical details see [LS2010].
- 6.7.5. Polynomial Kernel Approximation via Tensor Sketch¶
The polynomial kernel is a popular type of kernel
function given by:
\[k(x, y) = (\gamma x^\top y +c_0)^d\]
where:
x, y are the input vectors
d is the kernel degree
Intuitively, the feature space of the polynomial kernel of degree d
consists of all possible degree-d products among input features, which enables
learning algorithms using this kernel to account for interactions between features.
The TensorSketch [PP2013] method, as implemented in PolynomialCountSketch, is a
scalable, input data independent method for polynomial kernel approximation.
It is based on the concept of Count sketch [WIKICS] [CCF2002] , a dimensionality
reduction technique similar to feature hashing, which instead uses several
independent hash functions. TensorSketch obtains a Count Sketch of the outer product
of two vectors (or a vector with itself), which can be used as an approximation of the
polynomial kernel feature space. In particular, instead of explicitly computing
the outer product, TensorSketch computes the Count Sketch of the vectors and then
uses polynomial multiplication via the Fast Fourier Transform to compute the
Count Sketch of their outer product.
Conveniently, the training phase of TensorSketch simply consists of initializing
some random variables. It is thus independent of the input data, i.e. it only
depends on the number of input features, but not the data values.
In addition, this method can transform samples in
\(\mathcal{O}(n_{\text{samples}}(n_{\text{features}} + n_{\text{components}} \log(n_{\text{components}})))\)
time, where \(n_{\text{components}}\) is the desired output dimension,
determined by n_components.
Examples:
Scalable learning with polynomial kernel approximation
- 6.7.6. Mathematical Details¶
Kernel methods like support vector machines or kernelized
PCA rely on a property of reproducing kernel Hilbert spaces.
For any positive definite kernel function \(k\) (a so called Mercer kernel),
it is guaranteed that there exists a mapping \(\phi\)
into a Hilbert space \(\mathcal{H}\), such that
\[k(x,y) = \langle \phi(x), \phi(y) \rangle\]
Where \(\langle \cdot, \cdot \rangle\) denotes the inner product in the
Hilbert space.
If an algorithm, such as a linear support vector machine or PCA,
relies only on the scalar product of data points \(x_i\), one may use
the value of \(k(x_i, x_j)\), which corresponds to applying the algorithm
to the mapped data points \(\phi(x_i)\).
The advantage of using \(k\) is that the mapping \(\phi\) never has
to be calculated explicitly, allowing for arbitrary large
features (even infinite).
One drawback of kernel methods is, that it might be necessary
to store many kernel values \(k(x_i, x_j)\) during optimization.
If a kernelized classifier is applied to new data \(y_j\),
\(k(x_i, y_j)\) needs to be computed to make predictions,
possibly for many different \(x_i\) in the training set.
The classes in this submodule allow to approximate the embedding
\(\phi\), thereby working explicitly with the representations
\(\phi(x_i)\), which obviates the need to apply the kernel
or store training examples.
References:
[WS2001]
“Using the Nyström method to speed up kernel machines”
Williams, C.K.I.; Seeger, M. - 2001.
[RR2007]
(1,2)
“Random features for large-scale kernel machines”
Rahimi, A. and Recht, B. - Advances in neural information processing 2007,
[LS2010]
“Random Fourier approximations for skewed multiplicative histogram kernels”
Li, F., Ionescu, C., and Sminchisescu, C.
- Pattern Recognition,  DAGM 2010, Lecture Notes in Computer Science.
[VZ2010]
(1,2)
“Efficient additive kernels via explicit feature maps”
Vedaldi, A. and Zisserman, A. - Computer Vision and Pattern Recognition 2010
[VVZ2010]
“Generalized RBF feature maps for Efficient Detection”
Vempati, S. and Vedaldi, A. and Zisserman, A. and Jawahar, CV - 2010
[PP2013]
“Fast and scalable polynomial kernels via explicit feature maps”
Pham, N., & Pagh, R. - 2013
[CCF2002]
“Finding frequent items in data streams”
Charikar, M., Chen, K., & Farach-Colton - 2002
[WIKICS]
“Wikipedia: Count sketch”
### 6.8. Pairwise metrics, Affinities and Kernels — scikit-learn 1.4.2 documentation

Install
User Guide
API
Examples
Community
Getting Started
Tutorial
What's new
Glossary
Development
FAQ
Support
Related packages
Roadmap
Governance
About us
GitHub
Other Versions and Download
More
Getting Started
Tutorial
What's new
Glossary
Development
FAQ
Support
Related packages
Roadmap
Governance
About us
GitHub
Other Versions and Download
Toggle Menu
PrevUp
Next
scikit-learn 1.4.2
Other versions
Please cite us if you use the software.
### 6.8. Pairwise metrics, Affinities and Kernels

- 6.8.1. Cosine similarity
- 6.8.2. Linear kernel
- 6.8.3. Polynomial kernel
- 6.8.4. Sigmoid kernel
- 6.8.5. RBF kernel
- 6.8.6. Laplacian kernel
- 6.8.7. Chi-squared kernel
### 6.8. Pairwise metrics, Affinities and Kernels¶

The sklearn.metrics.pairwise submodule implements utilities to evaluate
pairwise distances or affinity of sets of samples.
This module contains both distance metrics and kernels. A brief summary is
given on the two here.
Distance metrics are functions d(a, b) such that d(a, b) < d(a, c)
if objects a and b are considered “more similar” than objects a
and c. Two objects exactly alike would have a distance of zero.
One of the most popular examples is Euclidean distance.
To be a ‘true’ metric, it must obey the following four conditions:
## 1. d(a, b) >= 0, for all a and b

## 2. d(a, b) == 0, if and only if a = b, positive definiteness

## 3. d(a, b) == d(b, a), symmetry

## 4. d(a, c) <= d(a, b) + d(b, c), the triangle inequality

Kernels are measures of similarity, i.e. s(a, b) > s(a, c)
if objects a and b are considered “more similar” than objects
a and c. A kernel must also be positive semi-definite.
There are a number of ways to convert between a distance metric and a
similarity measure, such as a kernel. Let D be the distance, and S be
the kernel:
S = np.exp(-D * gamma), where one heuristic for choosinggamma is 1 / num_features
S = 1. / (D / np.max(D))
The distances between the row vectors of X and the row vectors of Y
can be evaluated using pairwise_distances. If Y is omitted the
pairwise distances of the row vectors of X are calculated. Similarly,
pairwise.pairwise_kernels can be used to calculate the kernel between X
and Y using different kernel functions. See the API reference for more
details.
>>> import numpy as np
>>> from sklearn.metrics import pairwise_distances
>>> from sklearn.metrics.pairwise import pairwise_kernels
>>> X = np.array([[2, 3], [3, 5], [5, 8]])
>>> Y = np.array([[1, 0], [2, 1]])
>>> pairwise_distances(X, Y, metric='manhattan')
array([[ 4.,  2.],
[ 7.,  5.],
[12., 10.]])
>>> pairwise_distances(X, metric='manhattan')
array([[0., 3., 8.],
[3., 0., 5.],
[8., 5., 0.]])
>>> pairwise_kernels(X, Y, metric='linear')
array([[ 2.,  7.],
[ 3., 11.],
[ 5., 18.]])
- 6.8.1. Cosine similarity¶
cosine_similarity computes the L2-normalized dot product of vectors.
That is, if \(x\) and \(y\) are row vectors,
their cosine similarity \(k\) is defined as:
\[k(x, y) = \frac{x y^\top}{\|x\| \|y\|}\]
This is called cosine similarity, because Euclidean (L2) normalization
projects the vectors onto the unit sphere,
and their dot product is then the cosine of the angle between the points
denoted by the vectors.
This kernel is a popular choice for computing the similarity of documents
represented as tf-idf vectors.
cosine_similarity accepts scipy.sparse matrices.
(Note that the tf-idf functionality in sklearn.feature_extraction.text
can produce normalized vectors, in which case cosine_similarity
is equivalent to linear_kernel, only slower.)
References:
C.D. Manning, P. Raghavan and H. Schütze (2008). Introduction to
Information Retrieval. Cambridge University Press.
https://nlp.stanford.edu/IR-book/html/htmledition/the-vector-space-model-for-scoring-1.html
- 6.8.2. Linear kernel¶
The function linear_kernel computes the linear kernel, that is, a
special case of polynomial_kernel with degree=1 and coef0=0 (homogeneous).
If x and y are column vectors, their linear kernel is:
\[k(x, y) = x^\top y\]
- 6.8.3. Polynomial kernel¶
The function polynomial_kernel computes the degree-d polynomial kernel
between two vectors. The polynomial kernel represents the similarity between two
vectors. Conceptually, the polynomial kernels considers not only the similarity
between vectors under the same dimension, but also across dimensions. When used
in machine learning algorithms, this allows to account for feature interaction.
The polynomial kernel is defined as:
\[k(x, y) = (\gamma x^\top y +c_0)^d\]
where:
x, y are the input vectors
d is the kernel degree
If \(c_0 = 0\) the kernel is said to be homogeneous.
- 6.8.4. Sigmoid kernel¶
The function sigmoid_kernel computes the sigmoid kernel between two
vectors. The sigmoid kernel is also known as hyperbolic tangent, or Multilayer
Perceptron (because, in the neural network field, it is often used as neuron
activation function). It is defined as:
\[k(x, y) = \tanh( \gamma x^\top y + c_0)\]
where:
x, y are the input vectors
\(\gamma\) is known as slope
\(c_0\) is known as intercept
- 6.8.5. RBF kernel¶
The function rbf_kernel computes the radial basis function (RBF) kernel
between two vectors. This kernel is defined as:
\[k(x, y) = \exp( -\gamma \| x-y \|^2)\]
where x and y are the input vectors. If \(\gamma = \sigma^{-2}\)
the kernel is known as the Gaussian kernel of variance \(\sigma^2\).
- 6.8.6. Laplacian kernel¶
The function laplacian_kernel is a variant on the radial basis
function kernel defined as:
\[k(x, y) = \exp( -\gamma \| x-y \|_1)\]
where x and y are the input vectors and \(\|x-y\|_1\) is the
Manhattan distance between the input vectors.
It has proven useful in ML applied to noiseless data.
See e.g. Machine learning for quantum mechanics in a nutshell.
- 6.8.7. Chi-squared kernel¶
The chi-squared kernel is a very popular choice for training non-linear SVMs in
computer vision applications.
It can be computed using chi2_kernel and then passed to an
SVC with kernel="precomputed":
>>> from sklearn.svm import SVC
>>> from sklearn.metrics.pairwise import chi2_kernel
>>> X = [[0, 1], [1, 0], [.2, .8], [.7, .3]]
>>> y = [0, 1, 0, 1]
>>> K = chi2_kernel(X, gamma=.5)
>>> K
array([[1.        , 0.36787944, 0.89483932, 0.58364548],
[0.36787944, 1.        , 0.51341712, 0.83822343],
[0.89483932, 0.51341712, 1.        , 0.7768366 ],
[0.58364548, 0.83822343, 0.7768366 , 1.        ]])
>>> svm = SVC(kernel='precomputed').fit(K, y)
>>> svm.predict(K)
array([0, 1, 0, 1])
It can also be directly used as the kernel argument:
>>> svm = SVC(kernel=chi2_kernel).fit(X, y)
>>> svm.predict(X)
array([0, 1, 0, 1])
The chi squared kernel is given by
\[k(x, y) = \exp \left (-\gamma \sum_i \frac{(x[i] - y[i]) ^ 2}{x[i] + y[i]} \right )\]
The data is assumed to be non-negative, and is often normalized to have an L1-norm of one.
The normalization is rationalized with the connection to the chi squared distance,
which is a distance between discrete probability distributions.
The chi squared kernel is most commonly used on histograms (bags) of visual words.
References:
Zhang, J. and Marszalek, M. and Lazebnik, S. and Schmid, C.
Local features and kernels for classification of texture and object
categories: A comprehensive study
International Journal of Computer Vision 2007
https://hal.archives-ouvertes.fr/hal-00171412/document
### 6.9. Transforming the prediction target (y) — scikit-learn 1.4.2 documentation

Install
User Guide
API
Examples
Community
Getting Started
Tutorial
What's new
Glossary
Development
FAQ
Support
Related packages
Roadmap
Governance
About us
GitHub
Other Versions and Download
More
Getting Started
Tutorial
What's new
Glossary
Development
FAQ
Support
Related packages
Roadmap
Governance
About us
GitHub
Other Versions and Download
Toggle Menu
PrevUp
Next
scikit-learn 1.4.2
Other versions
Please cite us if you use the software.
### 6.9. Transforming the prediction target (y)

- 6.9.1. Label binarization
6.9.1.1. LabelBinarizer
6.9.1.2. MultiLabelBinarizer
- 6.9.2. Label encoding
### 6.9. Transforming the prediction target (y)¶

These are transformers that are not intended to be used on features, only on
supervised learning targets. See also Transforming target in regression if
you want to transform the prediction target for learning, but evaluate the
model in the original (untransformed) space.
- 6.9.1. Label binarization¶
6.9.1.1. LabelBinarizer¶
LabelBinarizer is a utility class to help create a label
indicator matrix from a list of multiclass labels:
>>> from sklearn import preprocessing
>>> lb = preprocessing.LabelBinarizer()
>>> lb.fit([1, 2, 6, 4, 2])
LabelBinarizer()
>>> lb.classes_
array([1, 2, 4, 6])
>>> lb.transform([1, 6])
array([[1, 0, 0, 0],
[0, 0, 0, 1]])
Using this format can enable multiclass classification in estimators
that support the label indicator matrix format.
Warning
LabelBinarizer is not needed if you are using an estimator that
already supports multiclass data.
For more information about multiclass classification, refer to
Multiclass classification.
6.9.1.2. MultiLabelBinarizer¶
In multilabel learning, the joint set of binary classification tasks is
expressed with a label binary indicator array: each sample is one row of a 2d
array of shape (n_samples, n_classes) with binary values where the one, i.e. the
non zero elements, corresponds to the subset of labels for that sample. An array
such as np.array([[1, 0, 0], [0, 1, 1], [0, 0, 0]]) represents label 0 in the
first sample, labels 1 and 2 in the second sample, and no labels in the third
sample.
Producing multilabel data as a list of sets of labels may be more intuitive.
The MultiLabelBinarizer
transformer can be used to convert between a collection of collections of
labels and the indicator format:
>>> from sklearn.preprocessing import MultiLabelBinarizer
>>> y = [[2, 3, 4], [2], [0, 1, 3], [0, 1, 2, 3, 4], [0, 1, 2]]
>>> MultiLabelBinarizer().fit_transform(y)
array([[0, 0, 1, 1, 1],
[0, 0, 1, 0, 0],
[1, 1, 0, 1, 0],
[1, 1, 1, 1, 1],
[1, 1, 1, 0, 0]])
For more information about multilabel classification, refer to
Multilabel classification.
- 6.9.2. Label encoding¶
LabelEncoder is a utility class to help normalize labels such that
they contain only values between 0 and n_classes-1. This is sometimes useful
for writing efficient Cython routines. LabelEncoder can be used as
follows:
>>> from sklearn import preprocessing
>>> le = preprocessing.LabelEncoder()
>>> le.fit([1, 2, 2, 6])
LabelEncoder()
>>> le.classes_
array([1, 2, 6])
>>> le.transform([1, 1, 2, 6])
array([0, 0, 1, 2])
>>> le.inverse_transform([0, 0, 1, 2])
array([1, 1, 2, 6])
It can also be used to transform non-numerical labels (as long as they are
hashable and comparable) to numerical labels:
>>> le = preprocessing.LabelEncoder()
>>> le.fit(["paris", "paris", "tokyo", "amsterdam"])
LabelEncoder()
>>> list(le.classes_)
['amsterdam', 'paris', 'tokyo']
>>> le.transform(["tokyo", "tokyo", "paris"])
array([2, 2, 1])
>>> list(le.inverse_transform([2, 2, 1]))
['tokyo', 'tokyo', 'paris']
### 7.1. Toy datasets — scikit-learn 1.4.2 documentation

Install
User Guide
API
Examples
Community
Getting Started
Tutorial
What's new
Glossary
Development
FAQ
Support
Related packages
Roadmap
Governance
About us
GitHub
Other Versions and Download
More
Getting Started
Tutorial
What's new
Glossary
Development
FAQ
Support
Related packages
Roadmap
Governance
About us
GitHub
Other Versions and Download
Toggle Menu
PrevUp
Next
scikit-learn 1.4.2
Other versions
Please cite us if you use the software.
User Guide
## 1. Supervised learning

## 2. Unsupervised learning

## 3. Model selection and evaluation

## 4. Inspection

## 5. Visualizations

## 6. Dataset transformations

## 7. Dataset loading utilities

### 7.1. Toy datasets

### 7.2. Real world datasets

### 7.3. Generated datasets

### 7.4. Loading other datasets

## 8. Computing with scikit-learn

## 9. Model persistence

## 10. Common pitfalls and recommended practices

## 11. Dispatching

### 7.1. Toy datasets¶

scikit-learn comes with a few small standard datasets that do not require to
download any file from some external website.
They can be loaded using the following functions:
load_iris(*[, return_X_y, as_frame])
Load and return the iris dataset (classification).
load_diabetes(*[, return_X_y, as_frame, scaled])
Load and return the diabetes dataset (regression).
load_digits(*[, n_class, return_X_y, as_frame])
Load and return the digits dataset (classification).
load_linnerud(*[, return_X_y, as_frame])
Load and return the physical exercise Linnerud dataset.
load_wine(*[, return_X_y, as_frame])
Load and return the wine dataset (classification).
load_breast_cancer(*[, return_X_y, as_frame])
Load and return the breast cancer wisconsin dataset (classification).
These datasets are useful to quickly illustrate the behavior of the
various algorithms implemented in scikit-learn. They are however often too
small to be representative of real world machine learning tasks.
- 7.1.1. Iris plants dataset¶
Data Set Characteristics:
Number of Instances:
150 (50 in each of three classes)
Number of Attributes:
4 numeric, predictive attributes and the class
Attribute Information:
sepal length in cm
sepal width in cm
petal length in cm
petal width in cm
class:
Iris-Setosa
Iris-Versicolour
Iris-Virginica
Summary Statistics:
sepal length:
4.3
7.9
5.84
0.83
0.7826
sepal width:
2.0
4.4
3.05
0.43
-0.4194
petal length:
1.0
6.9
3.76
1.76
0.9490  (high!)
petal width:
0.1
2.5
1.20
0.76
0.9565  (high!)
Missing Attribute Values:
None
Class Distribution:
33.3% for each of 3 classes.
Creator:
R.A. Fisher
Donor:
Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)
Date:
July, 1988
The famous Iris database, first used by Sir R.A. Fisher. The dataset is taken
from Fisher’s paper. Note that it’s the same as in R, but not as in the UCI
Machine Learning Repository, which has two wrong data points.
This is perhaps the best known database to be found in the
pattern recognition literature.  Fisher’s paper is a classic in the field and
is referenced frequently to this day.  (See Duda & Hart, for example.)  The
data set contains 3 classes of 50 instances each, where each class refers to a
type of iris plant.  One class is linearly separable from the other 2; the
latter are NOT linearly separable from each other.
References
Click for more details
¶
Fisher, R.A. “The use of multiple measurements in taxonomic problems”
Annual Eugenics, 7, Part II, 179-188 (1936); also in “Contributions to
Mathematical Statistics” (John Wiley, NY, 1950).
Duda, R.O., & Hart, P.E. (1973) Pattern Classification and Scene Analysis.
(Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.
Dasarathy, B.V. (1980) “Nosing Around the Neighborhood: A New System
Structure and Classification Rule for Recognition in Partially Exposed
Environments”.  IEEE Transactions on Pattern Analysis and Machine
Intelligence, Vol. PAMI-2, No. 1, 67-71.
Gates, G.W. (1972) “The Reduced Nearest Neighbor Rule”.  IEEE Transactions
on Information Theory, May 1972, 431-433.
See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al”s AUTOCLASS II
conceptual clustering system finds 3 classes in the data.
Many, many more …
- 7.1.2. Diabetes dataset¶
Ten baseline variables, age, sex, body mass index, average blood
pressure, and six blood serum measurements were obtained for each of n =
442 diabetes patients, as well as the response of interest, a
quantitative measure of disease progression one year after baseline.
Data Set Characteristics:
Number of Instances:
442
Number of Attributes:
First 10 columns are numeric predictive values
Target:
Column 11 is a quantitative measure of disease progression one year after baseline
Attribute Information:
age     age in years
sex
bmi     body mass index
bp      average blood pressure
s1      tc, total serum cholesterol
s2      ldl, low-density lipoproteins
s3      hdl, high-density lipoproteins
s4      tch, total cholesterol / HDL
s5      ltg, possibly log of serum triglycerides level
s6      glu, blood sugar level
Note: Each of these 10 feature variables have been mean centered and scaled by the standard deviation times the square root of n_samples (i.e. the sum of squares of each column totals 1).
Source URL:
https://www4.stat.ncsu.edu/~boos/var.select/diabetes.html
For more information see:
Bradley Efron, Trevor Hastie, Iain Johnstone and Robert Tibshirani (2004) “Least Angle Regression,” Annals of Statistics (with discussion), 407-499.
(https://web.stanford.edu/~hastie/Papers/LARS/LeastAngle_2002.pdf)
- 7.1.3. Optical recognition of handwritten digits dataset¶
Data Set Characteristics:
Number of Instances:
1797
Number of Attributes:
64
Attribute Information:
8x8 image of integer pixels in the range 0..16.
Missing Attribute Values:
None
Creator:
Alpaydin (alpaydin ‘@’ boun.edu.tr)
Date:
July; 1998
This is a copy of the test set of the UCI ML hand-written digits datasets
https://archive.ics.uci.edu/ml/datasets/Optical+Recognition+of+Handwritten+Digits
The data set contains images of hand-written digits: 10 classes where
each class refers to a digit.
Preprocessing programs made available by NIST were used to extract
normalized bitmaps of handwritten digits from a preprinted form. From a
total of 43 people, 30 contributed to the training set and different 13
to the test set. 32x32 bitmaps are divided into nonoverlapping blocks of
4x4 and the number of on pixels are counted in each block. This generates
an input matrix of 8x8 where each element is an integer in the range
0..16. This reduces dimensionality and gives invariance to small
distortions.
For info on NIST preprocessing routines, see M. D. Garris, J. L. Blue, G.
T. Candela, D. L. Dimmick, J. Geist, P. J. Grother, S. A. Janet, and C.
L. Wilson, NIST Form-Based Handprint Recognition System, NISTIR 5469,
1994.
References
Click for more details
¶
C. Kaynak (1995) Methods of Combining Multiple Classifiers and Their
Applications to Handwritten Digit Recognition, MSc Thesis, Institute of
Graduate Studies in Science and Engineering, Bogazici University.
Alpaydin, C. Kaynak (1998) Cascading Classifiers, Kybernetika.
Ken Tang and Ponnuthurai N. Suganthan and Xi Yao and A. Kai Qin.
Linear dimensionalityreduction using relevance weighted LDA. School of
Electrical and Electronic Engineering Nanyang Technological University.
2005.
Claudio Gentile. A New Approximate Maximal Margin Classification
Algorithm. NIPS. 2000.
- 7.1.4. Linnerrud dataset¶
Data Set Characteristics:
Number of Instances:
20
Number of Attributes:
3
Missing Attribute Values:
None
The Linnerud dataset is a multi-output regression dataset. It consists of three
exercise (data) and three physiological (target) variables collected from
twenty middle-aged men in a fitness club:
physiological - CSV containing 20 observations on 3 physiological variables:Weight, Waist and Pulse.
exercise - CSV containing 20 observations on 3 exercise variables:Chins, Situps and Jumps.
References
Click for more details
¶
Tenenhaus, M. (1998). La regression PLS: theorie et pratique. Paris:
Editions Technic.
- 7.1.5. Wine recognition dataset¶
Data Set Characteristics:
Number of Instances:
178
Number of Attributes:
13 numeric, predictive attributes and the class
Attribute Information:
Alcohol
Malic acid
Ash
Alcalinity of ash
Magnesium
Total phenols
Flavanoids
Nonflavanoid phenols
Proanthocyanins
Color intensity
Hue
OD280/OD315 of diluted wines
Proline
class:
class_0
class_1
class_2
Summary Statistics:
Alcohol:
11.0
14.8
13.0
0.8
Malic Acid:
0.74
5.80
2.34
1.12
Ash:
1.36
3.23
2.36
0.27
Alcalinity of Ash:
10.6
30.0
19.5
3.3
Magnesium:
70.0
162.0
99.7
14.3
Total Phenols:
0.98
3.88
2.29
0.63
Flavanoids:
0.34
5.08
2.03
1.00
Nonflavanoid Phenols:
0.13
0.66
0.36
0.12
Proanthocyanins:
0.41
3.58
1.59
0.57
Colour Intensity:
1.3
13.0
5.1
2.3
Hue:
0.48
1.71
0.96
0.23
OD280/OD315 of diluted wines:
1.27
4.00
2.61
0.71
Proline:
278
1680
746
315
Missing Attribute Values:
None
Class Distribution:
class_0 (59), class_1 (71), class_2 (48)
Creator:
R.A. Fisher
Donor:
Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)
Date:
July, 1988
This is a copy of UCI ML Wine recognition datasets.
https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data
The data is the results of a chemical analysis of wines grown in the same
region in Italy by three different cultivators. There are thirteen different
measurements taken for different constituents found in the three types of
wine.
Original Owners:
Forina, M. et al, PARVUS -
An Extendible Package for Data Exploration, Classification and Correlation.
Institute of Pharmaceutical and Food Analysis and Technologies,
Via Brigata Salerno, 16147 Genoa, Italy.
Citation:
Lichman, M. (2013). UCI Machine Learning Repository
[https://archive.ics.uci.edu/ml]. Irvine, CA: University of California,
School of Information and Computer Science.
References
Click for more details
¶
(1) S. Aeberhard, D. Coomans and O. de Vel,
Comparison of Classifiers in High Dimensional Settings,
Tech. Rep. no. 92-02, (1992), Dept. of Computer Science and Dept. of
Mathematics and Statistics, James Cook University of North Queensland.
(Also submitted to Technometrics).
The data was used with many others for comparing various
classifiers. The classes are separable, though only RDA
has achieved 100% correct classification.
(RDA : 100%, QDA 99.4%, LDA 98.9%, 1NN 96.1% (z-transformed data))
(All results using the leave-one-out technique)
(2) S. Aeberhard, D. Coomans and O. de Vel,
“THE CLASSIFICATION PERFORMANCE OF RDA”
Tech. Rep. no. 92-01, (1992), Dept. of Computer Science and Dept. of
Mathematics and Statistics, James Cook University of North Queensland.
(Also submitted to Journal of Chemometrics).
- 7.1.6. Breast cancer wisconsin (diagnostic) dataset¶
Data Set Characteristics:
Number of Instances:
569
Number of Attributes:
30 numeric, predictive attributes and the class
Attribute Information:
radius (mean of distances from center to points on the perimeter)
texture (standard deviation of gray-scale values)
perimeter
area
smoothness (local variation in radius lengths)
compactness (perimeter^2 / area - 1.0)
concavity (severity of concave portions of the contour)
concave points (number of concave portions of the contour)
symmetry
fractal dimension (“coastline approximation” - 1)
The mean, standard error, and “worst” or largest (mean of the three
worst/largest values) of these features were computed for each image,
resulting in 30 features.  For instance, field 0 is Mean Radius, field
10 is Radius SE, field 20 is Worst Radius.
class:
WDBC-Malignant
WDBC-Benign
Summary Statistics:
radius (mean):
6.981
28.11
texture (mean):
9.71
39.28
perimeter (mean):
43.79
188.5
area (mean):
143.5
2501.0
smoothness (mean):
0.053
0.163
compactness (mean):
0.019
0.345
concavity (mean):
0.0
0.427
concave points (mean):
0.0
0.201
symmetry (mean):
0.106
0.304
fractal dimension (mean):
0.05
0.097
radius (standard error):
0.112
2.873
texture (standard error):
0.36
4.885
perimeter (standard error):
0.757
21.98
area (standard error):
6.802
542.2
smoothness (standard error):
0.002
0.031
compactness (standard error):
0.002
0.135
concavity (standard error):
0.0
0.396
concave points (standard error):
0.0
0.053
symmetry (standard error):
0.008
0.079
fractal dimension (standard error):
0.001
0.03
radius (worst):
7.93
36.04
texture (worst):
12.02
49.54
perimeter (worst):
50.41
251.2
area (worst):
185.2
4254.0
smoothness (worst):
0.071
0.223
compactness (worst):
0.027
1.058
concavity (worst):
0.0
1.252
concave points (worst):
0.0
0.291
symmetry (worst):
0.156
0.664
fractal dimension (worst):
0.055
0.208
Missing Attribute Values:
None
Class Distribution:
212 - Malignant, 357 - Benign
Creator:
Dr. William H. Wolberg, W. Nick Street, Olvi L. Mangasarian
Donor:
Nick Street
Date:
November, 1995
This is a copy of UCI ML Breast Cancer Wisconsin (Diagnostic) datasets.
https://goo.gl/U2Uwz2
Features are computed from a digitized image of a fine needle
aspirate (FNA) of a breast mass.  They describe
characteristics of the cell nuclei present in the image.
Separating plane described above was obtained using
Multisurface Method-Tree (MSM-T) [K. P. Bennett, “Decision Tree
Construction Via Linear Programming.” Proceedings of the 4th
Midwest Artificial Intelligence and Cognitive Science Society,
pp. 97-101, 1992], a classification method which uses linear
programming to construct a decision tree.  Relevant features
were selected using an exhaustive search in the space of 1-4
features and 1-3 separating planes.
The actual linear program used to obtain the separating plane
in the 3-dimensional space is that described in:
[K. P. Bennett and O. L. Mangasarian: “Robust Linear
Programming Discrimination of Two Linearly Inseparable Sets”,
Optimization Methods and Software 1, 1992, 23-34].
This database is also available through the UW CS ftp server:
ftp ftp.cs.wisc.edu
cd math-prog/cpo-dataset/machine-learn/WDBC/
References
Click for more details
¶
W.N. Street, W.H. Wolberg and O.L. Mangasarian. Nuclear feature extraction
for breast tumor diagnosis. IS&T/SPIE 1993 International Symposium on
Electronic Imaging: Science and Technology, volume 1905, pages 861-870,
San Jose, CA, 1993.
O.L. Mangasarian, W.N. Street and W.H. Wolberg. Breast cancer diagnosis and
prognosis via linear programming. Operations Research, 43(4), pages 570-577,
July-August 1995.
W.H. Wolberg, W.N. Street, and O.L. Mangasarian. Machine learning techniques
to diagnose breast cancer from fine-needle aspirates. Cancer Letters 77 (1994)
163-171.
### 7.2. Real world datasets — scikit-learn 1.4.2 documentation

Install
User Guide
API
Examples
Community
Getting Started
Tutorial
What's new
Glossary
Development
FAQ
Support
Related packages
Roadmap
Governance
About us
GitHub
Other Versions and Download
More
Getting Started
Tutorial
What's new
Glossary
Development
FAQ
Support
Related packages
Roadmap
Governance
About us
GitHub
Other Versions and Download
Toggle Menu
PrevUp
Next
scikit-learn 1.4.2
Other versions
Please cite us if you use the software.
User Guide
## 1. Supervised learning

## 2. Unsupervised learning

## 3. Model selection and evaluation

## 4. Inspection

## 5. Visualizations

## 6. Dataset transformations

## 7. Dataset loading utilities

### 7.1. Toy datasets

### 7.2. Real world datasets

### 7.3. Generated datasets

### 7.4. Loading other datasets

## 8. Computing with scikit-learn

## 9. Model persistence

## 10. Common pitfalls and recommended practices

## 11. Dispatching

### 7.2. Real world datasets¶

scikit-learn provides tools to load larger datasets, downloading them if
necessary.
They can be loaded using the following functions:
fetch_olivetti_faces(*[, data_home, ...])
Load the Olivetti faces data-set from AT&T (classification).
fetch_20newsgroups(*[, data_home, subset, ...])
Load the filenames and data from the 20 newsgroups dataset (classification).
fetch_20newsgroups_vectorized(*[, subset, ...])
Load and vectorize the 20 newsgroups dataset (classification).
fetch_lfw_people(*[, data_home, funneled, ...])
Load the Labeled Faces in the Wild (LFW) people dataset (classification).
fetch_lfw_pairs(*[, subset, data_home, ...])
Load the Labeled Faces in the Wild (LFW) pairs dataset (classification).
fetch_covtype(*[, data_home, ...])
Load the covertype dataset (classification).
fetch_rcv1(*[, data_home, subset, ...])
Load the RCV1 multilabel dataset (classification).
fetch_kddcup99(*[, subset, data_home, ...])
Load the kddcup99 dataset (classification).
fetch_california_housing(*[, data_home, ...])
Load the California housing dataset (regression).
fetch_species_distributions(*[, data_home, ...])
Loader for species distribution dataset from Phillips et.
- 7.2.1. The Olivetti faces dataset¶
This dataset contains a set of face images taken between April 1992 and
April 1994 at AT&T Laboratories Cambridge. The
sklearn.datasets.fetch_olivetti_faces function is the data
fetching / caching function that downloads the data
archive from AT&T.
As described on the original website:
There are ten different images of each of 40 distinct subjects. For some
subjects, the images were taken at different times, varying the lighting,
facial expressions (open / closed eyes, smiling / not smiling) and facial
details (glasses / no glasses). All the images were taken against a dark
homogeneous background with the subjects in an upright, frontal position
(with tolerance for some side movement).
Data Set Characteristics:
Classes
40
Samples total
400
Dimensionality
4096
Features
real, between 0 and 1
The image is quantized to 256 grey levels and stored as unsigned 8-bit
integers; the loader will convert these to floating point values on the
interval [0, 1], which are easier to work with for many algorithms.
The “target” for this database is an integer from 0 to 39 indicating the
identity of the person pictured; however, with only 10 examples per class, this
relatively small dataset is more interesting from an unsupervised or
semi-supervised perspective.
The original dataset consisted of 92 x 112, while the version available here
consists of 64x64 images.
When using these images, please give credit to AT&T Laboratories Cambridge.
- 7.2.2. The 20 newsgroups text dataset¶
The 20 newsgroups dataset comprises around 18000 newsgroups posts on
20 topics split in two subsets: one for training (or development)
and the other one for testing (or for performance evaluation). The split
between the train and test set is based upon a messages posted before
and after a specific date.
This module contains two loaders. The first one,
sklearn.datasets.fetch_20newsgroups,
returns a list of the raw texts that can be fed to text feature
extractors such as CountVectorizer
with custom parameters so as to extract feature vectors.
The second one, sklearn.datasets.fetch_20newsgroups_vectorized,
returns ready-to-use features, i.e., it is not necessary to use a feature
extractor.
Data Set Characteristics:
Classes
20
Samples total
18846
Dimensionality
1
Features
text
Usage
Click for more details
¶
The sklearn.datasets.fetch_20newsgroups function is a data
fetching / caching functions that downloads the data archive from
the original 20 newsgroups website, extracts the archive contents
in the ~/scikit_learn_data/20news_home folder and calls the
sklearn.datasets.load_files on either the training or
testing set folder, or both of them:
>>> from sklearn.datasets import fetch_20newsgroups
>>> newsgroups_train = fetch_20newsgroups(subset='train')
>>> from pprint import pprint
>>> pprint(list(newsgroups_train.target_names))
['alt.atheism',
'comp.graphics',
'comp.os.ms-windows.misc',
'comp.sys.ibm.pc.hardware',
'comp.sys.mac.hardware',
'comp.windows.x',
'misc.forsale',
'rec.autos',
'rec.motorcycles',
'rec.sport.baseball',
'rec.sport.hockey',
'sci.crypt',
'sci.electronics',
'sci.med',
'sci.space',
'soc.religion.christian',
'talk.politics.guns',
'talk.politics.mideast',
'talk.politics.misc',
'talk.religion.misc']
The real data lies in the filenames and target attributes. The target
attribute is the integer index of the category:
>>> newsgroups_train.filenames.shape
(11314,)
>>> newsgroups_train.target.shape
(11314,)
>>> newsgroups_train.target[:10]
array([ 7,  4,  4,  1, 14, 16, 13,  3,  2,  4])
It is possible to load only a sub-selection of the categories by passing the
list of the categories to load to the
sklearn.datasets.fetch_20newsgroups function:
>>> cats = ['alt.atheism', 'sci.space']
>>> newsgroups_train = fetch_20newsgroups(subset='train', categories=cats)
>>> list(newsgroups_train.target_names)
['alt.atheism', 'sci.space']
>>> newsgroups_train.filenames.shape
(1073,)
>>> newsgroups_train.target.shape
(1073,)
>>> newsgroups_train.target[:10]
array([0, 1, 1, 1, 0, 1, 1, 0, 0, 0])
Converting text to vectors
Click for more details
¶
In order to feed predictive or clustering models with the text data,
one first need to turn the text into vectors of numerical values suitable
for statistical analysis. This can be achieved with the utilities of the
sklearn.feature_extraction.text as demonstrated in the following
example that extract TF-IDF vectors of unigram tokens
from a subset of 20news:
>>> from sklearn.feature_extraction.text import TfidfVectorizer
>>> categories = ['alt.atheism', 'talk.religion.misc',
...               'comp.graphics', 'sci.space']
>>> newsgroups_train = fetch_20newsgroups(subset='train',
...                                       categories=categories)
>>> vectorizer = TfidfVectorizer()
>>> vectors = vectorizer.fit_transform(newsgroups_train.data)
>>> vectors.shape
(2034, 34118)
The extracted TF-IDF vectors are very sparse, with an average of 159 non-zero
components by sample in a more than 30000-dimensional space
(less than .5% non-zero features):
>>> vectors.nnz / float(vectors.shape[0])
159.01327...
sklearn.datasets.fetch_20newsgroups_vectorized is a function which
returns ready-to-use token counts features instead of file names.
Filtering text for more realistic training
Click for more details
¶
It is easy for a classifier to overfit on particular things that appear in the
20 Newsgroups data, such as newsgroup headers. Many classifiers achieve very
high F-scores, but their results would not generalize to other documents that
aren’t from this window of time.
For example, let’s look at the results of a multinomial Naive Bayes classifier,
which is fast to train and achieves a decent F-score:
>>> from sklearn.naive_bayes import MultinomialNB
>>> from sklearn import metrics
>>> newsgroups_test = fetch_20newsgroups(subset='test',
...                                      categories=categories)
>>> vectors_test = vectorizer.transform(newsgroups_test.data)
>>> clf = MultinomialNB(alpha=.01)
>>> clf.fit(vectors, newsgroups_train.target)
MultinomialNB(alpha=0.01, class_prior=None, fit_prior=True)
>>> pred = clf.predict(vectors_test)
>>> metrics.f1_score(newsgroups_test.target, pred, average='macro')
0.88213...
(The example Classification of text documents using sparse features shuffles
the training and test data, instead of segmenting by time, and in that case
multinomial Naive Bayes gets a much higher F-score of 0.88. Are you suspicious
yet of what’s going on inside this classifier?)
Let’s take a look at what the most informative features are:
>>> import numpy as np
>>> def show_top10(classifier, vectorizer, categories):
...     feature_names = vectorizer.get_feature_names_out()
...     for i, category in enumerate(categories):
...         top10 = np.argsort(classifier.coef_[i])[-10:]
...         print("%s: %s" % (category, " ".join(feature_names[top10])))
...
>>> show_top10(clf, vectorizer, newsgroups_train.target_names)
alt.atheism: edu it and in you that is of to the
comp.graphics: edu in graphics it is for and of to the
sci.space: edu it that is in and space to of the
talk.religion.misc: not it you in is that and to of the
You can now see many things that these features have overfit to:
Almost every group is distinguished by whether headers such as
NNTP-Posting-Host: and Distribution: appear more or less often.
Another significant feature involves whether the sender is affiliated with
a university, as indicated either by their headers or their signature.
The word “article” is a significant feature, based on how often people quote
previous posts like this: “In article [article ID], [name] <[e-mail address]>
wrote:”
Other features match the names and e-mail addresses of particular people who
were posting at the time.
With such an abundance of clues that distinguish newsgroups, the classifiers
barely have to identify topics from text at all, and they all perform at the
same high level.
For this reason, the functions that load 20 Newsgroups data provide a
parameter called remove, telling it what kinds of information to strip out
of each file. remove should be a tuple containing any subset of
('headers', 'footers', 'quotes'), telling it to remove headers, signature
blocks, and quotation blocks respectively.
>>> newsgroups_test = fetch_20newsgroups(subset='test',
...                                      remove=('headers', 'footers', 'quotes'),
...                                      categories=categories)
>>> vectors_test = vectorizer.transform(newsgroups_test.data)
>>> pred = clf.predict(vectors_test)
>>> metrics.f1_score(pred, newsgroups_test.target, average='macro')
0.77310...
This classifier lost over a lot of its F-score, just because we removed
metadata that has little to do with topic classification.
It loses even more if we also strip this metadata from the training data:
>>> newsgroups_train = fetch_20newsgroups(subset='train',
...                                       remove=('headers', 'footers', 'quotes'),
...                                       categories=categories)
>>> vectors = vectorizer.fit_transform(newsgroups_train.data)
>>> clf = MultinomialNB(alpha=.01)
>>> clf.fit(vectors, newsgroups_train.target)
MultinomialNB(alpha=0.01, class_prior=None, fit_prior=True)
>>> vectors_test = vectorizer.transform(newsgroups_test.data)
>>> pred = clf.predict(vectors_test)
>>> metrics.f1_score(newsgroups_test.target, pred, average='macro')
0.76995...
Some other classifiers cope better with this harder version of the task. Try the
Sample pipeline for text feature extraction and evaluation
example with and without the remove option to compare the results.
Data Considerations
The Cleveland Indians is a major league baseball team based in Cleveland,
Ohio, USA. In December 2020, it was reported that “After several months of
discussion sparked by the death of George Floyd and a national reckoning over
race and colonialism, the Cleveland Indians have decided to change their
name.” Team owner Paul Dolan “did make it clear that the team will not make
its informal nickname – the Tribe – its new team name.” “It’s not going to
be a half-step away from the Indians,” Dolan said.”We will not have a Native
American-themed name.”
https://www.mlb.com/news/cleveland-indians-team-name-change
Recommendation
When evaluating text classifiers on the 20 Newsgroups data, you
should strip newsgroup-related metadata. In scikit-learn, you can do this
by setting remove=('headers', 'footers', 'quotes'). The F-score will be
lower because it is more realistic.
This text dataset contains data which may be inappropriate for certain NLP
applications. An example is listed in the “Data Considerations” section
above. The challenge with using current text datasets in NLP for tasks such
as sentence completion, clustering, and other applications is that text
that is culturally biased and inflammatory will propagate biases. This
should be taken into consideration when using the dataset, reviewing the
output, and the bias should be documented.
Examples
Sample pipeline for text feature extraction and evaluation
Classification of text documents using sparse features
FeatureHasher and DictVectorizer Comparison
Clustering text documents using k-means
- 7.2.3. The Labeled Faces in the Wild face recognition dataset¶
This dataset is a collection of JPEG pictures of famous people collected
over the internet, all details are available on the official website:
http://vis-www.cs.umass.edu/lfw/
Each picture is centered on a single face. The typical task is called
Face Verification: given a pair of two pictures, a binary classifier
must predict whether the two images are from the same person.
An alternative task, Face Recognition or Face Identification is:
given the picture of the face of an unknown person, identify the name
of the person by referring to a gallery of previously seen pictures of
identified persons.
Both Face Verification and Face Recognition are tasks that are typically
performed on the output of a model trained to perform Face Detection. The
most popular model for Face Detection is called Viola-Jones and is
implemented in the OpenCV library. The LFW faces were extracted by this
face detector from various online websites.
Data Set Characteristics:
Classes
5749
Samples total
13233
Dimensionality
5828
Features
real, between 0 and 255
Usage
Click for more details
¶
scikit-learn provides two loaders that will automatically download,
cache, parse the metadata files, decode the jpeg and convert the
interesting slices into memmapped numpy arrays. This dataset size is more
than 200 MB. The first load typically takes more than a couple of minutes
to fully decode the relevant part of the JPEG files into numpy arrays. If
the dataset has  been loaded once, the following times the loading times
less than 200ms by using a memmapped version memoized on the disk in the
~/scikit_learn_data/lfw_home/ folder using joblib.
The first loader is used for the Face Identification task: a multi-class
classification task (hence supervised learning):
>>> from sklearn.datasets import fetch_lfw_people
>>> lfw_people = fetch_lfw_people(min_faces_per_person=70, resize=0.4)
>>> for name in lfw_people.target_names:
...     print(name)
...
Ariel Sharon
Colin Powell
Donald Rumsfeld
George W Bush
Gerhard Schroeder
Hugo Chavez
Tony Blair
The default slice is a rectangular shape around the face, removing
most of the background:
>>> lfw_people.data.dtype
dtype('float32')
>>> lfw_people.data.shape
(1288, 1850)
>>> lfw_people.images.shape
(1288, 50, 37)
Each of the 1140 faces is assigned to a single person id in the target
array:
>>> lfw_people.target.shape
(1288,)
>>> list(lfw_people.target[:10])
[5, 6, 3, 1, 0, 1, 3, 4, 3, 0]
The second loader is typically used for the face verification task: each sample
is a pair of two picture belonging or not to the same person:
>>> from sklearn.datasets import fetch_lfw_pairs
>>> lfw_pairs_train = fetch_lfw_pairs(subset='train')
>>> list(lfw_pairs_train.target_names)
['Different persons', 'Same person']
>>> lfw_pairs_train.pairs.shape
(2200, 2, 62, 47)
>>> lfw_pairs_train.data.shape
(2200, 5828)
>>> lfw_pairs_train.target.shape
(2200,)
Both for the sklearn.datasets.fetch_lfw_people and
sklearn.datasets.fetch_lfw_pairs function it is
possible to get an additional dimension with the RGB color channels by
passing color=True, in that case the shape will be
(2200, 2, 62, 47, 3).
The sklearn.datasets.fetch_lfw_pairs datasets is subdivided into
3 subsets: the development train set, the development test set and
an evaluation 10_folds set meant to compute performance metrics using a
10-folds cross validation scheme.
References:
Labeled Faces in the Wild: A Database for Studying Face Recognition
in Unconstrained Environments.
Gary B. Huang, Manu Ramesh, Tamara Berg, and Erik Learned-Miller.
University of Massachusetts, Amherst, Technical Report 07-49, October, 2007.
Examples:
Faces recognition example using eigenfaces and SVMs
- 7.2.4. Forest covertypes¶
The samples in this dataset correspond to 30×30m patches of forest in the US,
collected for the task of predicting each patch’s cover type,
i.e. the dominant species of tree.
There are seven covertypes, making this a multiclass classification problem.
Each sample has 54 features, described on the
dataset’s homepage.
Some of the features are boolean indicators,
while others are discrete or continuous measurements.
Data Set Characteristics:
Classes
7
Samples total
581012
Dimensionality
54
Features
int
sklearn.datasets.fetch_covtype will load the covertype dataset;
it returns a dictionary-like ‘Bunch’ object
with the feature matrix in the data member
and the target values in target. If optional argument ‘as_frame’ is
set to ‘True’, it will return data and target as pandas
data frame, and there will be an additional member frame as well.
The dataset will be downloaded from the web if necessary.
- 7.2.5. RCV1 dataset¶
Reuters Corpus Volume I (RCV1) is an archive of over 800,000 manually
categorized newswire stories made available by Reuters, Ltd. for research
purposes. The dataset is extensively described in [1].
Data Set Characteristics:
Classes
103
Samples total
804414
Dimensionality
47236
Features
real, between 0 and 1
sklearn.datasets.fetch_rcv1 will load the following
version: RCV1-v2, vectors, full sets, topics multilabels:
>>> from sklearn.datasets import fetch_rcv1
>>> rcv1 = fetch_rcv1()
It returns a dictionary-like object, with the following attributes:
data:
The feature matrix is a scipy CSR sparse matrix, with 804414 samples and
47236 features. Non-zero values contains cosine-normalized, log TF-IDF vectors.
A nearly chronological split is proposed in [1]: The first 23149 samples are
the training set. The last 781265 samples are the testing set. This follows
the official LYRL2004 chronological split. The array has 0.16% of non zero
values:
>>> rcv1.data.shape
(804414, 47236)
target:
The target values are stored in a scipy CSR sparse matrix, with 804414 samples
and 103 categories. Each sample has a value of 1 in its categories, and 0 in
others. The array has 3.15% of non zero values:
>>> rcv1.target.shape
(804414, 103)
sample_id:
Each sample can be identified by its ID, ranging (with gaps) from 2286
to 810596:
>>> rcv1.sample_id[:3]
array([2286, 2287, 2288], dtype=uint32)
target_names:
The target values are the topics of each sample. Each sample belongs to at
least one topic, and to up to 17 topics. There are 103 topics, each
represented by a string. Their corpus frequencies span five orders of
magnitude, from 5 occurrences for ‘GMIL’, to 381327 for ‘CCAT’:
>>> rcv1.target_names[:3].tolist()
['E11', 'ECAT', 'M11']
The dataset will be downloaded from the rcv1 homepage if necessary.
The compressed size is about 656 MB.
References
[1]
(1,2)
Lewis, D. D., Yang, Y., Rose, T. G., & Li, F. (2004).
RCV1: A new benchmark collection for text categorization research.
The Journal of Machine Learning Research, 5, 361-397.
- 7.2.6. Kddcup 99 dataset¶
The KDD Cup ‘99 dataset was created by processing the tcpdump portions
of the 1998 DARPA Intrusion Detection System (IDS) Evaluation dataset,
created by MIT Lincoln Lab [2]. The artificial data (described on the dataset’s
homepage) was
generated using a closed network and hand-injected attacks to produce a
large number of different types of attack with normal activity in the
background. As the initial goal was to produce a large training set for
supervised learning algorithms, there is a large proportion (80.1%) of
abnormal data which is unrealistic in real world, and inappropriate for
unsupervised anomaly detection which aims at detecting ‘abnormal’ data, i.e.:
qualitatively different from normal data
in large minority among the observations.
We thus transform the KDD Data set into two different data sets: SA and SF.
SA is obtained by simply selecting all the normal data, and a small
proportion of abnormal data to gives an anomaly proportion of 1%.
SF is obtained as in [3]
by simply picking up the data whose attribute logged_in is positive, thus
focusing on the intrusion attack, which gives a proportion of 0.3% of
attack.
http and smtp are two subsets of SF corresponding with third feature
equal to ‘http’ (resp. to ‘smtp’).
General KDD structure:
Samples total
4898431
Dimensionality
41
Features
discrete (int) or continuous (float)
Targets
str, ‘normal.’ or name of the anomaly type
SA structure:
Samples total
976158
Dimensionality
41
Features
discrete (int) or continuous (float)
Targets
str, ‘normal.’ or name of the anomaly type
SF structure:
Samples total
699691
Dimensionality
4
Features
discrete (int) or continuous (float)
Targets
str, ‘normal.’ or name of the anomaly type
http structure:
Samples total
619052
Dimensionality
3
Features
discrete (int) or continuous (float)
Targets
str, ‘normal.’ or name of the anomaly type
smtp structure:
Samples total
95373
Dimensionality
3
Features
discrete (int) or continuous (float)
Targets
str, ‘normal.’ or name of the anomaly type
sklearn.datasets.fetch_kddcup99 will load the kddcup99 dataset; it
returns a dictionary-like object with the feature matrix in the data member
and the target values in target. The “as_frame” optional argument converts
data into a pandas DataFrame and target into a pandas Series. The
dataset will be downloaded from the web if necessary.
References
[2]
Analysis and Results of the 1999 DARPA Off-Line Intrusion
Detection Evaluation, Richard Lippmann, Joshua W. Haines,
David J. Fried, Jonathan Korba, Kumar Das.
[3]
K. Yamanishi, J.-I. Takeuchi, G. Williams, and P. Milne. Online
unsupervised outlier detection using finite mixtures with
discounting learning algorithms. In Proceedings of the sixth
ACM SIGKDD international conference on Knowledge discovery
and data mining, pages 320-324. ACM Press, 2000.
- 7.2.7. California Housing dataset¶
Data Set Characteristics:
Number of Instances:
20640
Number of Attributes:
8 numeric, predictive attributes and the target
Attribute Information:
MedInc        median income in block group
HouseAge      median house age in block group
AveRooms      average number of rooms per household
AveBedrms     average number of bedrooms per household
Population    block group population
AveOccup      average number of household members
Latitude      block group latitude
Longitude     block group longitude
Missing Attribute Values:
None
This dataset was obtained from the StatLib repository.
https://www.dcc.fc.up.pt/~ltorgo/Regression/cal_housing.html
The target variable is the median house value for California districts,
expressed in hundreds of thousands of dollars ($100,000).
This dataset was derived from the 1990 U.S. census, using one row per census
block group. A block group is the smallest geographical unit for which the U.S.
Census Bureau publishes sample data (a block group typically has a population
of 600 to 3,000 people).
A household is a group of people residing within a home. Since the average
number of rooms and bedrooms in this dataset are provided per household, these
columns may take surprisingly large values for block groups with few households
and many empty houses, such as vacation resorts.
It can be downloaded/loaded using the
sklearn.datasets.fetch_california_housing function.
References
Pace, R. Kelley and Ronald Barry, Sparse Spatial Autoregressions,
Statistics and Probability Letters, 33 (1997) 291-297
- 7.2.8. Species distribution dataset¶
This dataset represents the geographic distribution of two species in Central and
South America. The two species are:
“Bradypus variegatus” ,
the Brown-throated Sloth.
“Microryzomys minutus” ,
also known as the Forest Small Rice Rat, a rodent that lives in Peru,
Colombia, Ecuador, Peru, and Venezuela.
The dataset is not a typical dataset since a Bunch
containing the attributes data and target is not returned. Instead, we have
information allowing to create a “density” map of the different species.
The grid for the map can be built using the attributes x_left_lower_corner,
y_left_lower_corner, Nx, Ny and grid_size, which respectively correspond
to the x and y coordinates of the lower left corner of the grid, the number of
points along the x- and y-axis and the size of the step on the grid.
The density at each location of the grid is contained in the coverage attribute.
Finally, the train and test attributes contain information regarding the location
of a species at a specific location.
The dataset is provided by Phillips et. al. (2006).
References
“Maximum entropy modeling of species geographic distributions” S. J. Phillips,
R. P. Anderson, R. E. Schapire - Ecological Modelling, 190:231-259, 2006.
### 7.3. Generated datasets — scikit-learn 1.4.2 documentation

Install
User Guide
API
Examples
Community
Getting Started
Tutorial
What's new
Glossary
Development
FAQ
Support
Related packages
Roadmap
Governance
About us
GitHub
Other Versions and Download
More
Getting Started
Tutorial
What's new
Glossary
Development
FAQ
Support
Related packages
Roadmap
Governance
About us
GitHub
Other Versions and Download
Toggle Menu
PrevUp
Next
scikit-learn 1.4.2
Other versions
Please cite us if you use the software.
User Guide
## 1. Supervised learning

## 2. Unsupervised learning

## 3. Model selection and evaluation

## 4. Inspection

## 5. Visualizations

## 6. Dataset transformations

## 7. Dataset loading utilities

### 7.1. Toy datasets

### 7.2. Real world datasets

### 7.3. Generated datasets

### 7.4. Loading other datasets

## 8. Computing with scikit-learn

## 9. Model persistence

## 10. Common pitfalls and recommended practices

## 11. Dispatching

### 7.3. Generated datasets¶

In addition, scikit-learn includes various random sample generators that
can be used to build artificial datasets of controlled size and complexity.
- 7.3.1. Generators for classification and clustering¶
These generators produce a matrix of features and corresponding discrete
targets.
7.3.1.1. Single label¶
Both make_blobs and make_classification create multiclass
datasets by allocating each class one or more normally-distributed clusters of
points.  make_blobs provides greater control regarding the centers and
standard deviations of each cluster, and is used to demonstrate clustering.
make_classification specializes in introducing noise by way of:
correlated, redundant and uninformative features; multiple Gaussian clusters
per class; and linear transformations of the feature space.
make_gaussian_quantiles divides a single Gaussian cluster into
near-equal-size classes separated by concentric hyperspheres.
make_hastie_10_2 generates a similar binary, 10-dimensional problem.
make_circles and make_moons generate 2d binary classification
datasets that are challenging to certain algorithms (e.g. centroid-based
clustering or linear classification), including optional Gaussian noise.
They are useful for visualization. make_circles produces Gaussian data
with a spherical decision boundary for binary classification, while
make_moons produces two interleaving half circles.
7.3.1.2. Multilabel¶
make_multilabel_classification generates random samples with multiple
labels, reflecting a bag of words drawn from a mixture of topics. The number of
topics for each document is drawn from a Poisson distribution, and the topics
themselves are drawn from a fixed random distribution. Similarly, the number of
words is drawn from Poisson, with words drawn from a multinomial, where each
topic defines a probability distribution over words. Simplifications with
respect to true bag-of-words mixtures include:
Per-topic word distributions are independently drawn, where in reality all
would be affected by a sparse base distribution, and would be correlated.
For a document generated from multiple topics, all topics are weighted
equally in generating its bag of words.
Documents without labels words at random, rather than from a base
distribution.
7.3.1.3. Biclustering¶
make_biclusters(shape, n_clusters, *[, ...])
Generate a constant block diagonal structure array for biclustering.
make_checkerboard(shape, n_clusters, *[, ...])
Generate an array with block checkerboard structure for biclustering.
- 7.3.2. Generators for regression¶
make_regression produces regression targets as an optionally-sparse
random linear combination of random features, with noise. Its informative
features may be uncorrelated, or low rank (few features account for most of the
variance).
Other regression generators generate functions deterministically from
randomized features.  make_sparse_uncorrelated produces a target as a
linear combination of four features with fixed coefficients.
Others encode explicitly non-linear relations:
make_friedman1 is related by polynomial and sine transforms;
make_friedman2 includes feature multiplication and reciprocation; and
make_friedman3 is similar with an arctan transformation on the target.
- 7.3.3. Generators for manifold learning¶
make_s_curve([n_samples, noise, random_state])
Generate an S curve dataset.
make_swiss_roll([n_samples, noise, ...])
Generate a swiss roll dataset.
- 7.3.4. Generators for decomposition¶
make_low_rank_matrix([n_samples, ...])
Generate a mostly low rank matrix with bell-shaped singular values.
make_sparse_coded_signal(n_samples, *, ...)
Generate a signal as a sparse combination of dictionary elements.
make_spd_matrix(n_dim, *[, random_state])
Generate a random symmetric, positive-definite matrix.
make_sparse_spd_matrix([n_dim, alpha, ...])
Generate a sparse symmetric definite positive matrix.
### 7.4. Loading other datasets — scikit-learn 1.4.2 documentation

Install
User Guide
API
Examples
Community
Getting Started
Tutorial
What's new
Glossary
Development
FAQ
Support
Related packages
Roadmap
Governance
About us
GitHub
Other Versions and Download
More
Getting Started
Tutorial
What's new
Glossary
Development
FAQ
Support
Related packages
Roadmap
Governance
About us
GitHub
Other Versions and Download
Toggle Menu
PrevUp
Next
scikit-learn 1.4.2
Other versions
Please cite us if you use the software.
User Guide
## 1. Supervised learning

## 2. Unsupervised learning

## 3. Model selection and evaluation

## 4. Inspection

## 5. Visualizations

## 6. Dataset transformations

## 7. Dataset loading utilities

### 7.1. Toy datasets

### 7.2. Real world datasets

### 7.3. Generated datasets

### 7.4. Loading other datasets

## 8. Computing with scikit-learn

## 9. Model persistence

## 10. Common pitfalls and recommended practices

## 11. Dispatching

### 7.4. Loading other datasets¶

- 7.4.1. Sample images¶
Scikit-learn also embeds a couple of sample JPEG images published under Creative
Commons license by their authors. Those images can be useful to test algorithms
and pipelines on 2D data.
load_sample_images()
Load sample images for image manipulation.
load_sample_image(image_name)
Load the numpy array of a single sample image.
Warning
The default coding of images is based on the uint8 dtype to
spare memory. Often machine learning algorithms work best if the
input is converted to a floating point representation first. Also,
if you plan to use matplotlib.pyplpt.imshow, don’t forget to scale to the range
0 - 1 as done in the following example.
Examples:
Color Quantization using K-Means
- 7.4.2. Datasets in svmlight / libsvm format¶
scikit-learn includes utility functions for loading
datasets in the svmlight / libsvm format. In this format, each line
takes the form <label> <feature-id>:<feature-value>
<feature-id>:<feature-value> .... This format is especially suitable for sparse datasets.
In this module, scipy sparse CSR matrices are used for X and numpy arrays are used for y.
You may load a dataset like as follows:
>>> from sklearn.datasets import load_svmlight_file
>>> X_train, y_train = load_svmlight_file("/path/to/train_dataset.txt")
...
You may also load two (or more) datasets at once:
>>> X_train, y_train, X_test, y_test = load_svmlight_files(
...     ("/path/to/train_dataset.txt", "/path/to/test_dataset.txt"))
...
In this case, X_train and X_test are guaranteed to have the same number
of features. Another way to achieve the same result is to fix the number of
features:
>>> X_test, y_test = load_svmlight_file(
...     "/path/to/test_dataset.txt", n_features=X_train.shape[1])
...
Related links:
Public datasets in svmlight / libsvm format: https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets
Faster API-compatible implementation: https://github.com/mblondel/svmlight-loader
- 7.4.3. Downloading datasets from the openml.org repository¶
openml.org is a public repository for machine learning
data and experiments, that allows everybody to upload open datasets.
The sklearn.datasets package is able to download datasets
from the repository using the function
sklearn.datasets.fetch_openml.
For example, to download a dataset of gene expressions in mice brains:
>>> from sklearn.datasets import fetch_openml
>>> mice = fetch_openml(name='miceprotein', version=4)
To fully specify a dataset, you need to provide a name and a version, though
the version is optional, see Dataset Versions below.
The dataset contains a total of 1080 examples belonging to 8 different
classes:
>>> mice.data.shape
(1080, 77)
>>> mice.target.shape
(1080,)
>>> np.unique(mice.target)
array(['c-CS-m', 'c-CS-s', 'c-SC-m', 'c-SC-s', 't-CS-m', 't-CS-s', 't-SC-m', 't-SC-s'], dtype=object)
You can get more information on the dataset by looking at the DESCR
and details attributes:
>>> print(mice.DESCR)
**Author**: Clara Higuera, Katheleen J. Gardiner, Krzysztof J. Cios
**Source**: [UCI](https://archive.ics.uci.edu/ml/datasets/Mice+Protein+Expression) - 2015
**Please cite**: Higuera C, Gardiner KJ, Cios KJ (2015) Self-Organizing
Feature Maps Identify Proteins Critical to Learning in a Mouse Model of Down
Syndrome. PLoS ONE 10(6): e0129126...
>>> mice.details
{'id': '40966', 'name': 'MiceProtein', 'version': '4', 'format': 'ARFF',
'upload_date': '2017-11-08T16:00:15', 'licence': 'Public',
'url': 'https://www.openml.org/data/v1/download/17928620/MiceProtein.arff',
'file_id': '17928620', 'default_target_attribute': 'class',
'row_id_attribute': 'MouseID',
'ignore_attribute': ['Genotype', 'Treatment', 'Behavior'],
'tag': ['OpenML-CC18', 'study_135', 'study_98', 'study_99'],
'visibility': 'public', 'status': 'active',
'md5_checksum': '3c479a6885bfa0438971388283a1ce32'}
The DESCR contains a free-text description of the data, while details
contains a dictionary of meta-data stored by openml, like the dataset id.
For more details, see the OpenML documentation The data_id of the mice protein dataset
is 40966, and you can use this (or the name) to get more information on the
dataset on the openml website:
>>> mice.url
'https://www.openml.org/d/40966'
The data_id also uniquely identifies a dataset from OpenML:
>>> mice = fetch_openml(data_id=40966)
>>> mice.details
{'id': '4550', 'name': 'MiceProtein', 'version': '1', 'format': 'ARFF',
'creator': ...,
'upload_date': '2016-02-17T14:32:49', 'licence': 'Public', 'url':
'https://www.openml.org/data/v1/download/1804243/MiceProtein.ARFF', 'file_id':
'1804243', 'default_target_attribute': 'class', 'citation': 'Higuera C,
Gardiner KJ, Cios KJ (2015) Self-Organizing Feature Maps Identify Proteins
Critical to Learning in a Mouse Model of Down Syndrome. PLoS ONE 10(6):
e0129126. [Web Link] journal.pone.0129126', 'tag': ['OpenML100', 'study_14',
'study_34'], 'visibility': 'public', 'status': 'active', 'md5_checksum':
'3c479a6885bfa0438971388283a1ce32'}
7.4.3.1. Dataset Versions¶
A dataset is uniquely specified by its data_id, but not necessarily by its
name. Several different “versions” of a dataset with the same name can exist
which can contain entirely different datasets.
If a particular version of a dataset has been found to contain significant
issues, it might be deactivated. Using a name to specify a dataset will yield
the earliest version of a dataset that is still active. That means that
fetch_openml(name="miceprotein") can yield different results
at different times if earlier versions become inactive.
You can see that the dataset with data_id 40966 that we fetched above is
the first version of the “miceprotein” dataset:
>>> mice.details['version']
'1'
In fact, this dataset only has one version. The iris dataset on the other hand
has multiple versions:
>>> iris = fetch_openml(name="iris")
>>> iris.details['version']
'1'
>>> iris.details['id']
'61'
>>> iris_61 = fetch_openml(data_id=61)
>>> iris_61.details['version']
'1'
>>> iris_61.details['id']
'61'
>>> iris_969 = fetch_openml(data_id=969)
>>> iris_969.details['version']
'3'
>>> iris_969.details['id']
'969'
Specifying the dataset by the name “iris” yields the lowest version, version 1,
with the data_id 61. To make sure you always get this exact dataset, it is
safest to specify it by the dataset data_id. The other dataset, with
data_id 969, is version 3 (version 2 has become inactive), and contains a
binarized version of the data:
>>> np.unique(iris_969.target)
array(['N', 'P'], dtype=object)
You can also specify both the name and the version, which also uniquely
identifies the dataset:
>>> iris_version_3 = fetch_openml(name="iris", version=3)
>>> iris_version_3.details['version']
'3'
>>> iris_version_3.details['id']
'969'
References:
Vanschoren, van Rijn, Bischl and Torgo. “OpenML: networked science in
machine learning” ACM SIGKDD Explorations Newsletter, 15(2), 49-60, 2014.
7.4.3.2. ARFF parser¶
From version 1.2, scikit-learn provides a new keyword argument parser that
provides several options to parse the ARFF files provided by OpenML. The legacy
parser (i.e. parser="liac-arff") is based on the project
LIAC-ARFF. This parser is however
slow and consume more memory than required. A new parser based on pandas
(i.e. parser="pandas") is both faster and more memory efficient.
However, this parser does not support sparse data.
Therefore, we recommend using parser="auto" which will use the best parser
available for the requested dataset.
The "pandas" and "liac-arff" parsers can lead to different data types in
the output. The notable differences are the following:
The "liac-arff" parser always encodes categorical features as str
objects. To the contrary, the "pandas" parser instead infers the type while
reading and numerical categories will be casted into integers whenever
possible.
The "liac-arff" parser uses float64 to encode numerical features tagged as
‘REAL’ and ‘NUMERICAL’ in the metadata. The "pandas" parser instead infers
if these numerical features corresponds to integers and uses panda’s Integer
extension dtype.
In particular, classification datasets with integer categories are typically
loaded as such (0, 1, ...) with the "pandas" parser while "liac-arff"
will force the use of string encoded class labels such as "0", "1" and so
on.
The "pandas" parser will not strip single quotes - i.e. ' - from string
columns. For instance, a string 'my string' will be kept as is while the
"liac-arff" parser will strip the single quotes. For categorical columns,
the single quotes are stripped from the values.
In addition, when as_frame=False is used, the "liac-arff" parser returns
ordinally encoded data where the categories are provided in the attribute
categories of the Bunch instance. Instead, "pandas" returns a NumPy array
were the categories. Then it’s up to the user to design a feature
engineering pipeline with an instance of  OneHotEncoder or
OrdinalEncoder typically wrapped in a ColumnTransformer to
preprocess the categorical columns explicitly. See for instance: Column Transformer with Mixed Types.
- 7.4.4. Loading from external datasets¶
scikit-learn works on any numeric data stored as numpy arrays or scipy sparse
matrices. Other types that are convertible to numeric arrays such as pandas
DataFrame are also acceptable.
Here are some recommended ways to load standard columnar data into a
format usable by scikit-learn:
pandas.io
provides tools to read data from common formats including CSV, Excel, JSON
and SQL. DataFrames may also be constructed from lists of tuples or dicts.
Pandas handles heterogeneous data smoothly and provides tools for
manipulation and conversion into a numeric array suitable for scikit-learn.
scipy.io
specializes in binary formats often used in scientific computing
context such as .mat and .arff
numpy/routines.io
for standard loading of columnar data into numpy arrays
scikit-learn’s load_svmlight_file for the svmlight or libSVM
sparse format
scikit-learn’s load_files for directories of text files where
the name of each directory is the name of each category and each file inside
of each directory corresponds to one sample from that category
For some miscellaneous data such as images, videos, and audio, you may wish to
refer to:
skimage.io or
Imageio
for loading images and videos into numpy arrays
scipy.io.wavfile.read
for reading WAV files into a numpy array
Categorical (or nominal) features stored as strings (common in pandas DataFrames)
will need converting to numerical features using OneHotEncoder
or OrdinalEncoder or similar.
See Preprocessing data.
Note: if you manage your own numerical data it is recommended to use an
optimized file format such as HDF5 to reduce data load times. Various libraries
such as H5Py, PyTables and pandas provides a Python interface for reading and
writing data in that format.
### 8.1. Strategies to scale computationally: bigger data — scikit-learn 1.4.2 documentation

Install
User Guide
API
Examples
Community
Getting Started
Tutorial
What's new
Glossary
Development
FAQ
Support
Related packages
Roadmap
Governance
About us
GitHub
Other Versions and Download
More
Getting Started
Tutorial
What's new
Glossary
Development
FAQ
Support
Related packages
Roadmap
Governance
About us
GitHub
Other Versions and Download
Toggle Menu
PrevUp
Next
scikit-learn 1.4.2
Other versions
Please cite us if you use the software.
User Guide
## 1. Supervised learning

## 2. Unsupervised learning

## 3. Model selection and evaluation

## 4. Inspection

## 5. Visualizations

## 6. Dataset transformations

## 7. Dataset loading utilities

## 8. Computing with scikit-learn

### 8.1. Strategies to scale computationally: bigger data

### 8.2. Computational Performance

### 8.3. Parallelism, resource management, and configuration

## 9. Model persistence

## 10. Common pitfalls and recommended practices

## 11. Dispatching

### 8.1. Strategies to scale computationally: bigger data¶

For some applications the amount of examples, features (or both) and/or the
speed at which they need to be processed are challenging for traditional
approaches. In these cases scikit-learn has a number of options you can
consider to make your system scale.
- 8.1.1. Scaling with instances using out-of-core learning¶
Out-of-core (or “external memory”) learning is a technique used to learn from
data that cannot fit in a computer’s main memory (RAM).
Here is a sketch of a system designed to achieve this goal:
a way to stream instances
a way to extract features from instances
an incremental algorithm
8.1.1.1. Streaming instances¶
Basically, 1. may be a reader that yields instances from files on a
hard drive, a database, from a network stream etc. However,
details on how to achieve this are beyond the scope of this documentation.
8.1.1.2. Extracting features¶
## 2. could be any relevant way to extract features among the

different feature extraction methods supported by
scikit-learn. However, when working with data that needs vectorization and
where the set of features or values is not known in advance one should take
explicit care. A good example is text classification where unknown terms are
likely to be found during training. It is possible to use a stateful
vectorizer if making multiple passes over the data is reasonable from an
application point of view. Otherwise, one can turn up the difficulty by using
a stateless feature extractor. Currently the preferred way to do this is to
use the so-called hashing trick as implemented by
sklearn.feature_extraction.FeatureHasher for datasets with categorical
variables represented as list of Python dicts or
sklearn.feature_extraction.text.HashingVectorizer for text documents.
8.1.1.3. Incremental learning¶
Finally, for 3. we have a number of options inside scikit-learn. Although not
all algorithms can learn incrementally (i.e. without seeing all the instances
at once), all estimators implementing the partial_fit API are candidates.
Actually, the ability to learn incrementally from a mini-batch of instances
(sometimes called “online learning”) is key to out-of-core learning as it
guarantees that at any given time there will be only a small amount of
instances in the main memory. Choosing a good size for the mini-batch that
balances relevancy and memory footprint could involve some tuning [1].
Here is a list of incremental estimators for different tasks:
Classification
sklearn.naive_bayes.MultinomialNB
sklearn.naive_bayes.BernoulliNB
sklearn.linear_model.Perceptron
sklearn.linear_model.SGDClassifier
sklearn.linear_model.PassiveAggressiveClassifier
sklearn.neural_network.MLPClassifier
Regression
sklearn.linear_model.SGDRegressor
sklearn.linear_model.PassiveAggressiveRegressor
sklearn.neural_network.MLPRegressor
Clustering
sklearn.cluster.MiniBatchKMeans
sklearn.cluster.Birch
Decomposition / feature Extraction
sklearn.decomposition.MiniBatchDictionaryLearning
sklearn.decomposition.IncrementalPCA
sklearn.decomposition.LatentDirichletAllocation
sklearn.decomposition.MiniBatchNMF
Preprocessing
sklearn.preprocessing.StandardScaler
sklearn.preprocessing.MinMaxScaler
sklearn.preprocessing.MaxAbsScaler
For classification, a somewhat important thing to note is that although a
stateless feature extraction routine may be able to cope with new/unseen
attributes, the incremental learner itself may be unable to cope with
new/unseen targets classes. In this case you have to pass all the possible
classes to the first partial_fit call using the classes= parameter.
Another aspect to consider when choosing a proper algorithm is that not all of
them put the same importance on each example over time. Namely, the
Perceptron is still sensitive to badly labeled examples even after many
examples whereas the SGD* and PassiveAggressive* families are more
robust to this kind of artifacts. Conversely, the latter also tend to give less
importance to remarkably different, yet properly labeled examples when they
come late in the stream as their learning rate decreases over time.
8.1.1.4. Examples¶
Finally, we have a full-fledged example of
Out-of-core classification of text documents. It is aimed at
providing a starting point for people wanting to build out-of-core learning
systems and demonstrates most of the notions discussed above.
Furthermore, it also shows the evolution of the performance of different
algorithms with the number of processed examples.
Now looking at the computation time of the different parts, we see that the
vectorization is much more expensive than learning itself. From the different
algorithms, MultinomialNB is the most expensive, but its overhead can be
mitigated by increasing the size of the mini-batches (exercise: change
minibatch_size to 100 and 10000 in the program and compare).
8.1.1.5. Notes¶
[1]
Depending on the algorithm the mini-batch size can influence results or
not. SGD*, PassiveAggressive*, and discrete NaiveBayes are truly online
and are not affected by batch size. Conversely, MiniBatchKMeans
convergence rate is affected by the batch size. Also, its memory
footprint can vary dramatically with batch size.
### 8.2. Computational Performance — scikit-learn 1.4.2 documentation

Install
User Guide
API
Examples
Community
Getting Started
Tutorial
What's new
Glossary
Development
FAQ
Support
Related packages
Roadmap
Governance
About us
GitHub
Other Versions and Download
More
Getting Started
Tutorial
What's new
Glossary
Development
FAQ
Support
Related packages
Roadmap
Governance
About us
GitHub
Other Versions and Download
Toggle Menu
PrevUp
Next
scikit-learn 1.4.2
Other versions
Please cite us if you use the software.
User Guide
## 1. Supervised learning

## 2. Unsupervised learning

## 3. Model selection and evaluation

## 4. Inspection

## 5. Visualizations

## 6. Dataset transformations

## 7. Dataset loading utilities

## 8. Computing with scikit-learn

### 8.1. Strategies to scale computationally: bigger data

### 8.2. Computational Performance

### 8.3. Parallelism, resource management, and configuration

## 9. Model persistence

## 10. Common pitfalls and recommended practices

## 11. Dispatching

### 8.2. Computational Performance¶

For some applications the performance (mainly latency and throughput at
prediction time) of estimators is crucial. It may also be of interest to
consider the training throughput but this is often less important in a
production setup (where it often takes place offline).
We will review here the orders of magnitude you can expect from a number of
scikit-learn estimators in different contexts and provide some tips and
tricks for overcoming performance bottlenecks.
Prediction latency is measured as the elapsed time necessary to make a
prediction (e.g. in micro-seconds). Latency is often viewed as a distribution
and operations engineers often focus on the latency at a given percentile of
this distribution (e.g. the 90 percentile).
Prediction throughput is defined as the number of predictions the software can
deliver in a given amount of time (e.g. in predictions per second).
An important aspect of performance optimization is also that it can hurt
prediction accuracy. Indeed, simpler models (e.g. linear instead of
non-linear, or with fewer parameters) often run faster but are not always able
to take into account the same exact properties of the data as more complex ones.
- 8.2.1. Prediction Latency¶
One of the most straight-forward concerns one may have when using/choosing a
machine learning toolkit is the latency at which predictions can be made in a
production environment.
The main factors that influence the prediction latency are
Number of features
Input data representation and sparsity
Model complexity
Feature extraction
A last major parameter is also the possibility to do predictions in bulk or
one-at-a-time mode.
8.2.1.1. Bulk versus Atomic mode¶
In general doing predictions in bulk (many instances at the same time) is
more efficient for a number of reasons (branching predictability, CPU cache,
linear algebra libraries optimizations etc.). Here we see on a setting
with few features that independently of estimator choice the bulk mode is
always faster, and for some of them by 1 to 2 orders of magnitude:
To benchmark different estimators for your case you can simply change the
n_features parameter in this example:
Prediction Latency. This should give
you an estimate of the order of magnitude of the prediction latency.
8.2.1.2. Configuring Scikit-learn for reduced validation overhead¶
Scikit-learn does some validation on data that increases the overhead per
call to predict and similar functions. In particular, checking that
features are finite (not NaN or infinite) involves a full pass over the
data. If you ensure that your data is acceptable, you may suppress
checking for finiteness by setting the environment variable
SKLEARN_ASSUME_FINITE to a non-empty string before importing
scikit-learn, or configure it in Python with set_config.
For more control than these global settings, a config_context
allows you to set this configuration within a specified context:
>>> import sklearn
>>> with sklearn.config_context(assume_finite=True):
...     pass  # do learning/prediction here with reduced validation
Note that this will affect all uses of
assert_all_finite within the context.
8.2.1.3. Influence of the Number of Features¶
Obviously when the number of features increases so does the memory
consumption of each example. Indeed, for a matrix of \(M\) instances
with \(N\) features, the space complexity is in \(O(NM)\).
From a computing perspective it also means that the number of basic operations
(e.g., multiplications for vector-matrix products in linear models) increases
too. Here is a graph of the evolution of the prediction latency with the
number of features:
Overall you can expect the prediction time to increase at least linearly with
the number of features (non-linear cases can happen depending on the global
memory footprint and estimator).
8.2.1.4. Influence of the Input Data Representation¶
Scipy provides sparse matrix data structures which are optimized for storing
sparse data. The main feature of sparse formats is that you don’t store zeros
so if your data is sparse then you use much less memory. A non-zero value in
a sparse (CSR or CSC)
representation will only take on average one 32bit integer position + the 64
bit floating point value + an additional 32bit per row or column in the matrix.
Using sparse input on a dense (or sparse) linear model can speedup prediction
by quite a bit as only the non zero valued features impact the dot product
and thus the model predictions. Hence if you have 100 non zeros in 1e6
dimensional space, you only need 100 multiply and add operation instead of 1e6.
Calculation over a dense representation, however, may leverage highly optimized
vector operations and multithreading in BLAS, and tends to result in fewer CPU
cache misses. So the sparsity should typically be quite high (10% non-zeros
max, to be checked depending on the hardware) for the sparse input
representation to be faster than the dense input representation on a machine
with many CPUs and an optimized BLAS implementation.
Here is sample code to test the sparsity of your input:
def sparsity_ratio(X):
return 1.0 - np.count_nonzero(X) / float(X.shape[0] * X.shape[1])
print("input sparsity ratio:", sparsity_ratio(X))
As a rule of thumb you can consider that if the sparsity ratio is greater
than 90% you can probably benefit from sparse formats. Check Scipy’s sparse
matrix formats documentation
for more information on how to build (or convert your data to) sparse matrix
formats. Most of the time the CSR and CSC formats work best.
8.2.1.5. Influence of the Model Complexity¶
Generally speaking, when model complexity increases, predictive power and
latency are supposed to increase. Increasing predictive power is usually
interesting, but for many applications we would better not increase
prediction latency too much. We will now review this idea for different
families of supervised models.
For sklearn.linear_model (e.g. Lasso, ElasticNet,
SGDClassifier/Regressor, Ridge & RidgeClassifier,
PassiveAggressiveClassifier/Regressor, LinearSVC, LogisticRegression…) the
decision function that is applied at prediction time is the same (a dot product)
, so latency should be equivalent.
Here is an example using
SGDClassifier with the
elasticnet penalty. The regularization strength is globally controlled by
the alpha parameter. With a sufficiently high alpha,
one can then increase the l1_ratio parameter of elasticnet to
enforce various levels of sparsity in the model coefficients. Higher sparsity
here is interpreted as less model complexity as we need fewer coefficients to
describe it fully. Of course sparsity influences in turn the prediction time
as the sparse dot-product takes time roughly proportional to the number of
non-zero coefficients.
For the sklearn.svm family of algorithms with a non-linear kernel,
the latency is tied to the number of support vectors (the fewer the faster).
Latency and throughput should (asymptotically) grow linearly with the number
of support vectors in a SVC or SVR model. The kernel will also influence the
latency as it is used to compute the projection of the input vector once per
support vector. In the following graph the nu parameter of
NuSVR was used to influence the number of
support vectors.
For sklearn.ensemble of trees (e.g. RandomForest, GBT,
ExtraTrees, etc.) the number of trees and their depth play the most
important role. Latency and throughput should scale linearly with the number
of trees. In this case we used directly the n_estimators parameter of
GradientBoostingRegressor.
In any case be warned that decreasing model complexity can hurt accuracy as
mentioned above. For instance a non-linearly separable problem can be handled
with a speedy linear model but prediction power will very likely suffer in
the process.
8.2.1.6. Feature Extraction Latency¶
Most scikit-learn models are usually pretty fast as they are implemented
either with compiled Cython extensions or optimized computing libraries.
On the other hand, in many real world applications the feature extraction
process (i.e. turning raw data like database rows or network packets into
numpy arrays) governs the overall prediction time. For example on the Reuters
text classification task the whole preparation (reading and parsing SGML
files, tokenizing the text and hashing it into a common vector space) is
taking 100 to 500 times more time than the actual prediction code, depending on
the chosen model.
In many cases it is thus recommended to carefully time and profile your
feature extraction code as it may be a good place to start optimizing when
your overall latency is too slow for your application.
- 8.2.2. Prediction Throughput¶
Another important metric to care about when sizing production systems is the
throughput i.e. the number of predictions you can make in a given amount of
time. Here is a benchmark from the
Prediction Latency example that measures
this quantity for a number of estimators on synthetic data:
These throughputs are achieved on a single process. An obvious way to
increase the throughput of your application is to spawn additional instances
(usually processes in Python because of the
GIL) that share the
same model. One might also add machines to spread the load. A detailed
explanation on how to achieve this is beyond the scope of this documentation
though.
- 8.2.3. Tips and Tricks¶
8.2.3.1. Linear algebra libraries¶
As scikit-learn relies heavily on Numpy/Scipy and linear algebra in general it
makes sense to take explicit care of the versions of these libraries.
Basically, you ought to make sure that Numpy is built using an optimized BLAS /
LAPACK library.
Not all models benefit from optimized BLAS and Lapack implementations. For
instance models based on (randomized) decision trees typically do not rely on
BLAS calls in their inner loops, nor do kernel SVMs (SVC, SVR,
NuSVC, NuSVR).  On the other hand a linear model implemented with a
BLAS DGEMM call (via numpy.dot) will typically benefit hugely from a tuned
BLAS implementation and lead to orders of magnitude speedup over a
non-optimized BLAS.
You can display the BLAS / LAPACK implementation used by your NumPy / SciPy /
scikit-learn install with the following command:
python -c "import sklearn; sklearn.show_versions()"
Optimized BLAS / LAPACK implementations include:
Atlas (need hardware specific tuning by rebuilding on the target machine)
OpenBLAS
MKL
Apple Accelerate and vecLib frameworks (OSX only)
More information can be found on the NumPy install page
and in this
blog post
from Daniel Nouri which has some nice step by step install instructions for
Debian / Ubuntu.
8.2.3.2. Limiting Working Memory¶
Some calculations when implemented using standard numpy vectorized operations
involve using a large amount of temporary memory.  This may potentially exhaust
system memory.  Where computations can be performed in fixed-memory chunks, we
attempt to do so, and allow the user to hint at the maximum size of this
working memory (defaulting to 1GB) using set_config or
config_context.  The following suggests to limit temporary working
memory to 128 MiB:
>>> import sklearn
>>> with sklearn.config_context(working_memory=128):
...     pass  # do chunked work here
An example of a chunked operation adhering to this setting is
pairwise_distances_chunked, which facilitates computing
row-wise reductions of a pairwise distance matrix.
8.2.3.3. Model Compression¶
Model compression in scikit-learn only concerns linear models for the moment.
In this context it means that we want to control the model sparsity (i.e. the
number of non-zero coordinates in the model vectors). It is generally a good
idea to combine model sparsity with sparse input data representation.
Here is sample code that illustrates the use of the sparsify() method:
clf = SGDRegressor(penalty='elasticnet', l1_ratio=0.25)
clf.fit(X_train, y_train).sparsify()
clf.predict(X_test)
In this example we prefer the elasticnet penalty as it is often a good
compromise between model compactness and prediction power. One can also
further tune the l1_ratio parameter (in combination with the
regularization strength alpha) to control this tradeoff.
A typical benchmark
on synthetic data yields a >30% decrease in latency when both the model and
input are sparse (with 0.000024 and 0.027400 non-zero coefficients ratio
respectively). Your mileage may vary depending on the sparsity and size of
your data and model.
Furthermore, sparsifying can be very useful to reduce the memory usage of
predictive models deployed on production servers.
8.2.3.4. Model Reshaping¶
Model reshaping consists in selecting only a portion of the available features
to fit a model. In other words, if a model discards features during the
learning phase we can then strip those from the input. This has several
benefits. Firstly it reduces memory (and therefore time) overhead of the
model itself. It also allows to discard explicit
feature selection components in a pipeline once we know which features to
keep from a previous run. Finally, it can help reduce processing time and I/O
usage upstream in the data access and feature extraction layers by not
collecting and building features that are discarded by the model. For instance
if the raw data come from a database, it can make it possible to write simpler
and faster queries or reduce I/O usage by making the queries return lighter
records.
At the moment, reshaping needs to be performed manually in scikit-learn.
In the case of sparse input (particularly in CSR format), it is generally
sufficient to not generate the relevant features, leaving their columns empty.
8.2.3.5. Links¶
scikit-learn developer performance documentation
Scipy sparse matrix formats documentation
### 8.3. Parallelism, resource management, and configuration — scikit-learn 1.4.2 documentation

Install
User Guide
API
Examples
Community
Getting Started
Tutorial
What's new
Glossary
Development
FAQ
Support
Related packages
Roadmap
Governance
About us
GitHub
Other Versions and Download
More
Getting Started
Tutorial
What's new
Glossary
Development
FAQ
Support
Related packages
Roadmap
Governance
About us
GitHub
Other Versions and Download
Toggle Menu
PrevUp
Next
scikit-learn 1.4.2
Other versions
Please cite us if you use the software.
User Guide
## 1. Supervised learning

## 2. Unsupervised learning

## 3. Model selection and evaluation

## 4. Inspection

## 5. Visualizations

## 6. Dataset transformations

## 7. Dataset loading utilities

## 8. Computing with scikit-learn

### 8.1. Strategies to scale computationally: bigger data

### 8.2. Computational Performance

### 8.3. Parallelism, resource management, and configuration

## 9. Model persistence

## 10. Common pitfalls and recommended practices

## 11. Dispatching

### 8.3. Parallelism, resource management, and configuration¶

- 8.3.1. Parallelism¶
Some scikit-learn estimators and utilities parallelize costly operations
using multiple CPU cores.
Depending on the type of estimator and sometimes the values of the
constructor parameters, this is either done:
with higher-level parallelism via joblib.
with lower-level parallelism via OpenMP, used in C or Cython code.
with lower-level parallelism via BLAS, used by NumPy and SciPy for generic operations
on arrays.
The n_jobs parameters of estimators always controls the amount of parallelism
managed by joblib (processes or threads depending on the joblib backend).
The thread-level parallelism managed by OpenMP in scikit-learn’s own Cython code
or by BLAS & LAPACK libraries used by NumPy and SciPy operations used in scikit-learn
is always controlled by environment variables or threadpoolctl as explained below.
Note that some estimators can leverage all three kinds of parallelism at different
points of their training and prediction methods.
We describe these 3 types of parallelism in the following subsections in more details.
8.3.1.1. Higher-level parallelism with joblib¶
When the underlying implementation uses joblib, the number of workers
(threads or processes) that are spawned in parallel can be controlled via the
n_jobs parameter.
Note
Where (and how) parallelization happens in the estimators using joblib by
specifying n_jobs is currently poorly documented.
Please help us by improving our docs and tackle issue 14228!
Joblib is able to support both multi-processing and multi-threading. Whether
joblib chooses to spawn a thread or a process depends on the backend
that it’s using.
scikit-learn generally relies on the loky backend, which is joblib’s
default backend. Loky is a multi-processing backend. When doing
multi-processing, in order to avoid duplicating the memory in each process
(which isn’t reasonable with big datasets), joblib will create a memmap
that all processes can share, when the data is bigger than 1MB.
In some specific cases (when the code that is run in parallel releases the
GIL), scikit-learn will indicate to joblib that a multi-threading
backend is preferable.
As a user, you may control the backend that joblib will use (regardless of
what scikit-learn recommends) by using a context manager:
from joblib import parallel_backend
with parallel_backend('threading', n_jobs=2):
# Your scikit-learn code here
Please refer to the joblib’s docs
for more details.
In practice, whether parallelism is helpful at improving runtime depends on
many factors. It is usually a good idea to experiment rather than assuming
that increasing the number of workers is always a good thing. In some cases
it can be highly detrimental to performance to run multiple copies of some
estimators or functions in parallel (see oversubscription below).
8.3.1.2. Lower-level parallelism with OpenMP¶
OpenMP is used to parallelize code written in Cython or C, relying on
multi-threading exclusively. By default, the implementations using OpenMP
will use as many threads as possible, i.e. as many threads as logical cores.
You can control the exact number of threads that are used either:
via the OMP_NUM_THREADS environment variable, for instance when:
running a python script:
OMP_NUM_THREADS=4 python my_script.py
or via threadpoolctl as explained by this piece of documentation.
8.3.1.3. Parallel NumPy and SciPy routines from numerical libraries¶
scikit-learn relies heavily on NumPy and SciPy, which internally call
multi-threaded linear algebra routines (BLAS & LAPACK) implemented in libraries
such as MKL, OpenBLAS or BLIS.
You can control the exact number of threads used by BLAS for each library
using environment variables, namely:
MKL_NUM_THREADS sets the number of thread MKL uses,
OPENBLAS_NUM_THREADS sets the number of threads OpenBLAS uses
BLIS_NUM_THREADS sets the number of threads BLIS uses
Note that BLAS & LAPACK implementations can also be impacted by
OMP_NUM_THREADS. To check whether this is the case in your environment,
you can inspect how the number of threads effectively used by those libraries
is affected when running the following command in a bash or zsh terminal
for different values of OMP_NUM_THREADS:
OMP_NUM_THREADS=2 python -m threadpoolctl -i numpy scipy
Note
At the time of writing (2022), NumPy and SciPy packages which are
distributed on pypi.org (i.e. the ones installed via pip install)
and on the conda-forge channel (i.e. the ones installed via
conda install --channel conda-forge) are linked with OpenBLAS, while
NumPy and SciPy packages packages shipped on the defaults conda
channel from Anaconda.org (i.e. the ones installed via conda install)
are linked by default with MKL.
8.3.1.4. Oversubscription: spawning too many threads¶
It is generally recommended to avoid using significantly more processes or
threads than the number of CPUs on a machine. Over-subscription happens when
a program is running too many threads at the same time.
Suppose you have a machine with 8 CPUs. Consider a case where you’re running
a GridSearchCV (parallelized with joblib)
with n_jobs=8 over a
HistGradientBoostingClassifier (parallelized with
OpenMP). Each instance of
HistGradientBoostingClassifier will spawn 8 threads
(since you have 8 CPUs). That’s a total of 8 * 8 = 64 threads, which
leads to oversubscription of threads for physical CPU resources and thus
to scheduling overhead.
Oversubscription can arise in the exact same fashion with parallelized
routines from MKL, OpenBLAS or BLIS that are nested in joblib calls.
Starting from joblib >= 0.14, when the loky backend is used (which
is the default), joblib will tell its child processes to limit the
number of threads they can use, so as to avoid oversubscription. In practice
the heuristic that joblib uses is to tell the processes to use max_threads
= n_cpus // n_jobs, via their corresponding environment variable. Back to
our example from above, since the joblib backend of
GridSearchCV is loky, each process will
only be able to use 1 thread instead of 8, thus mitigating the
oversubscription issue.
Note that:
Manually setting one of the environment variables (OMP_NUM_THREADS,
MKL_NUM_THREADS, OPENBLAS_NUM_THREADS, or BLIS_NUM_THREADS)
will take precedence over what joblib tries to do. The total number of
threads will be n_jobs * <LIB>_NUM_THREADS. Note that setting this
limit will also impact your computations in the main process, which will
only use <LIB>_NUM_THREADS. Joblib exposes a context manager for
finer control over the number of threads in its workers (see joblib docs
linked below).
When joblib is configured to use the threading backend, there is no
mechanism to avoid oversubscriptions when calling into parallel native
libraries in the joblib-managed threads.
All scikit-learn estimators that explicitly rely on OpenMP in their Cython code
always use threadpoolctl internally to automatically adapt the numbers of
threads used by OpenMP and potentially nested BLAS calls so as to avoid
oversubscription.
You will find additional details about joblib mitigation of oversubscription
in joblib documentation.
You will find additional details about parallelism in numerical python libraries
in this document from Thomas J. Fan.
- 8.3.2. Configuration switches¶
8.3.2.1. Python API¶
sklearn.set_config and sklearn.config_context can be used to change
parameters of the configuration which control aspect of parallelism.
8.3.2.2. Environment variables¶
These environment variables should be set before importing scikit-learn.
8.3.2.2.1. SKLEARN_ASSUME_FINITE¶
Sets the default value for the assume_finite argument of
sklearn.set_config.
8.3.2.2.2. SKLEARN_WORKING_MEMORY¶
Sets the default value for the working_memory argument of
sklearn.set_config.
8.3.2.2.3. SKLEARN_SEED¶
Sets the seed of the global random generator when running the tests, for
reproducibility.
Note that scikit-learn tests are expected to run deterministically with
explicit seeding of their own independent RNG instances instead of relying on
the numpy or Python standard library RNG singletons to make sure that test
results are independent of the test execution order. However some tests might
forget to use explicit seeding and this variable is a way to control the initial
state of the aforementioned singletons.
8.3.2.2.4. SKLEARN_TESTS_GLOBAL_RANDOM_SEED¶
Controls the seeding of the random number generator used in tests that rely on
the global_random_seed` fixture.
All tests that use this fixture accept the contract that they should
deterministically pass for any seed value from 0 to 99 included.
If the SKLEARN_TESTS_GLOBAL_RANDOM_SEED environment variable is set to
"any" (which should be the case on nightly builds on the CI), the fixture
will choose an arbitrary seed in the above range (based on the BUILD_NUMBER or
the current day) and all fixtured tests will run for that specific seed. The
goal is to ensure that, over time, our CI will run all tests with different
seeds while keeping the test duration of a single run of the full test suite
limited. This will check that the assertions of tests written to use this
fixture are not dependent on a specific seed value.
The range of admissible seed values is limited to [0, 99] because it is often
not possible to write a test that can work for any possible seed and we want to
avoid having tests that randomly fail on the CI.
Valid values for SKLEARN_TESTS_GLOBAL_RANDOM_SEED:
SKLEARN_TESTS_GLOBAL_RANDOM_SEED="42": run tests with a fixed seed of 42
SKLEARN_TESTS_GLOBAL_RANDOM_SEED="40-42": run the tests with all seeds
between 40 and 42 included
SKLEARN_TESTS_GLOBAL_RANDOM_SEED="any": run the tests with an arbitrary
seed selected between 0 and 99 included
SKLEARN_TESTS_GLOBAL_RANDOM_SEED="all": run the tests with all seeds
between 0 and 99 included. This can take a long time: only use for individual
tests, not the full test suite!
If the variable is not set, then 42 is used as the global seed in a
deterministic manner. This ensures that, by default, the scikit-learn test
suite is as deterministic as possible to avoid disrupting our friendly
third-party package maintainers. Similarly, this variable should not be set in
the CI config of pull-requests to make sure that our friendly contributors are
not the first people to encounter a seed-sensitivity regression in a test
unrelated to the changes of their own PR. Only the scikit-learn maintainers who
watch the results of the nightly builds are expected to be annoyed by this.
When writing a new test function that uses this fixture, please use the
following command to make sure that it passes deterministically for all
admissible seeds on your local machine:
SKLEARN_TESTS_GLOBAL_RANDOM_SEED="all" pytest -v -k test_your_test_name
8.3.2.2.5. SKLEARN_SKIP_NETWORK_TESTS¶
When this environment variable is set to a non zero value, the tests that need
network access are skipped. When this environment variable is not set then
network tests are skipped.
8.3.2.2.6. SKLEARN_RUN_FLOAT32_TESTS¶
When this environment variable is set to ‘1’, the tests using the
global_dtype fixture are also run on float32 data.
When this environment variable is not set, the tests are only run on
float64 data.
8.3.2.2.7. SKLEARN_ENABLE_DEBUG_CYTHON_DIRECTIVES¶
When this environment variable is set to a non zero value, the Cython
derivative, boundscheck is set to True. This is useful for finding
segfaults.
8.3.2.2.8. SKLEARN_BUILD_ENABLE_DEBUG_SYMBOLS¶
When this environment variable is set to a non zero value, the debug symbols
will be included in the compiled C extensions. Only debug symbols for POSIX
systems is configured.
8.3.2.2.9. SKLEARN_PAIRWISE_DIST_CHUNK_SIZE¶
This sets the size of chunk to be used by the underlying PairwiseDistancesReductions
implementations. The default value is 256 which has been showed to be adequate on
most machines.
Users looking for the best performance might want to tune this variable using
powers of 2 so as to get the best parallelism behavior for their hardware,
especially with respect to their caches’ sizes.
8.3.2.2.10. SKLEARN_WARNINGS_AS_ERRORS¶
This environment variable is used to turn warnings into errors in tests and
documentation build.
Some CI (Continuous Integration) builds set SKLEARN_WARNINGS_AS_ERRORS=1, for
example to make sure that we catch deprecation warnings from our dependencies
and that we adapt our code.
To locally run with the same “warnings as errors” setting as in these CI builds
you can set SKLEARN_WARNINGS_AS_ERRORS=1.
By default, warnings are not turned into errors. This is the case if
SKLEARN_WARNINGS_AS_ERRORS is unset, or SKLEARN_WARNINGS_AS_ERRORS=0.
This environment variable use specific warning filters to ignore some warnings,
since sometimes warnings originate from third-party libraries and there is not
much we can do about it. You can see the warning filters in the
_get_warnings_filters_info_list function in sklearn/utils/_testing.py.
Note that for documentation build, SKLEARN_WARNING_AS_ERRORS=1 is checking
that the documentation build, in particular running examples, does not produce
any warnings. This is different from the -W sphinx-build argument that
catches syntax warnings in the rst files.
## 9. Model persistence — scikit-learn 1.4.2 documentation

Install
User Guide
API
Examples
Community
Getting Started
Tutorial
What's new
Glossary
Development
FAQ
Support
Related packages
Roadmap
Governance
About us
GitHub
Other Versions and Download
More
Getting Started
Tutorial
What's new
Glossary
Development
FAQ
Support
Related packages
Roadmap
Governance
About us
GitHub
Other Versions and Download
Toggle Menu
PrevUp
Next
scikit-learn 1.4.2
Other versions
Please cite us if you use the software.
User Guide
## 1. Supervised learning

## 2. Unsupervised learning

## 3. Model selection and evaluation

## 4. Inspection

## 5. Visualizations

## 6. Dataset transformations

## 7. Dataset loading utilities

## 8. Computing with scikit-learn

## 9. Model persistence

## 10. Common pitfalls and recommended practices

## 11. Dispatching

## 9. Model persistence¶

After training a scikit-learn model, it is desirable to have a way to persist
the model for future use without having to retrain. The following sections give
you some hints on how to persist a scikit-learn model.
### 9.1. Python specific serialization¶

It is possible to save a model in scikit-learn by using Python’s built-in
persistence model, namely pickle:
>>> from sklearn import svm
>>> from sklearn import datasets
>>> clf = svm.SVC()
>>> X, y= datasets.load_iris(return_X_y=True)
>>> clf.fit(X, y)
SVC()
>>> import pickle
>>> s = pickle.dumps(clf)
>>> clf2 = pickle.loads(s)
>>> clf2.predict(X[0:1])
array([0])
>>> y[0]
0
In the specific case of scikit-learn, it may be better to use joblib’s
replacement of pickle (dump & load), which is more efficient on
objects that carry large numpy arrays internally as is often the case for
fitted scikit-learn estimators, but can only pickle to the disk and not to a
string:
>>> from joblib import dump, load
>>> dump(clf, 'filename.joblib')
Later you can load back the pickled model (possibly in another Python process)
with:
>>> clf = load('filename.joblib')
Note
dump and load functions also accept file-like object
instead of filenames. More information on data persistence with Joblib is
available here.
InconsistentVersionWarning
Click for more details
¶
When an estimator is unpickled with a scikit-learn version that is inconsistent
with the version the estimator was pickled with, a
InconsistentVersionWarning is raised. This warning
can be caught to obtain the original version the estimator was pickled with:
from sklearn.exceptions import InconsistentVersionWarning
warnings.simplefilter("error", InconsistentVersionWarning)
try:
est = pickle.loads("model_from_prevision_version.pickle")
except InconsistentVersionWarning as w:
print(w.original_sklearn_version)
- 9.1.1. Security & maintainability limitations¶
pickle (and joblib by extension), has some issues regarding maintainability
and security. Because of this,
Never unpickle untrusted data as it could lead to malicious code being
executed upon loading.
While models saved using one version of scikit-learn might load in
other versions, this is entirely unsupported and inadvisable. It should
also be kept in mind that operations performed on such data could give
different and unexpected results.
In order to rebuild a similar model with future versions of scikit-learn,
additional metadata should be saved along the pickled model:
The training data, e.g. a reference to an immutable snapshot
The python source code used to generate the model
The versions of scikit-learn and its dependencies
The cross validation score obtained on the training data
This should make it possible to check that the cross-validation score is in the
same range as before.
Aside for a few exceptions, pickled models should be portable across
architectures assuming the same versions of dependencies and Python are used.
If you encounter an estimator that is not portable please open an issue on
GitHub. Pickled models are often deployed in production using containers, like
Docker, in order to freeze the environment and dependencies.
If you want to know more about these issues and explore other possible
serialization methods, please refer to this
talk by Alex Gaynor.
- 9.1.2. A more secure format: skops¶
skops provides a more secure
format via the skops.io module. It avoids using pickle and only
loads files which have types and references to functions which are trusted
either by default or by the user.
Using skops
Click for more details
¶
The API is very similar to pickle, and
you can persist your models as explain in the docs using
skops.io.dump and skops.io.dumps:
import skops.io as sio
obj = sio.dumps(clf)
And you can load them back using skops.io.load and
skops.io.loads. However, you need to specify the types which are
trusted by you. You can get existing unknown types in a dumped object / file
using skops.io.get_untrusted_types, and after checking its contents,
pass it to the load function:
unknown_types = sio.get_untrusted_types(data=obj)
clf = sio.loads(obj, trusted=unknown_types)
If you trust the source of the file / object, you can pass trusted=True:
clf = sio.loads(obj, trusted=True)
Please report issues and feature requests related to this format on the skops
issue tracker.
### 9.2. Interoperable formats¶

For reproducibility and quality control needs, when different architectures
and environments should be taken into account, exporting the model in
Open Neural Network
Exchange format or Predictive Model Markup Language
(PMML) format
might be a better approach than using pickle alone.
These are helpful where you may want to use your model for prediction in a
different environment from where the model was trained.
ONNX is a binary serialization of the model. It has been developed to improve
the usability of the interoperable representation of data models.
It aims to facilitate the conversion of the data
models between different machine learning frameworks, and to improve their
portability on different computing architectures. More details are available
from the ONNX tutorial.
To convert scikit-learn model to ONNX a specific tool sklearn-onnx has been developed.
PMML is an implementation of the XML document standard
defined to represent data models together with the data used to generate them.
Being human and machine readable,
PMML is a good option for model validation on different platforms and
long term archiving. On the other hand, as XML in general, its verbosity does
not help in production when performance is critical.
To convert scikit-learn model to PMML you can use for example sklearn2pmml distributed under the Affero GPLv3
license.
## 9. Model persistence — scikit-learn 1.4.2 documentation

Install
User Guide
API
Examples
Community
Getting Started
Tutorial
What's new
Glossary
Development
FAQ
Support
Related packages
Roadmap
Governance
About us
GitHub
Other Versions and Download
More
Getting Started
Tutorial
What's new
Glossary
Development
FAQ
Support
Related packages
Roadmap
Governance
About us
GitHub
Other Versions and Download
Toggle Menu
PrevUp
Next
scikit-learn 1.4.2
Other versions
Please cite us if you use the software.
User Guide
## 1. Supervised learning

## 2. Unsupervised learning

## 3. Model selection and evaluation

## 4. Inspection

## 5. Visualizations

## 6. Dataset transformations

## 7. Dataset loading utilities

## 8. Computing with scikit-learn

## 9. Model persistence

## 10. Common pitfalls and recommended practices

## 11. Dispatching

## 9. Model persistence¶

After training a scikit-learn model, it is desirable to have a way to persist
the model for future use without having to retrain. The following sections give
you some hints on how to persist a scikit-learn model.
### 9.1. Python specific serialization¶

It is possible to save a model in scikit-learn by using Python’s built-in
persistence model, namely pickle:
>>> from sklearn import svm
>>> from sklearn import datasets
>>> clf = svm.SVC()
>>> X, y= datasets.load_iris(return_X_y=True)
>>> clf.fit(X, y)
SVC()
>>> import pickle
>>> s = pickle.dumps(clf)
>>> clf2 = pickle.loads(s)
>>> clf2.predict(X[0:1])
array([0])
>>> y[0]
0
In the specific case of scikit-learn, it may be better to use joblib’s
replacement of pickle (dump & load), which is more efficient on
objects that carry large numpy arrays internally as is often the case for
fitted scikit-learn estimators, but can only pickle to the disk and not to a
string:
>>> from joblib import dump, load
>>> dump(clf, 'filename.joblib')
Later you can load back the pickled model (possibly in another Python process)
with:
>>> clf = load('filename.joblib')
Note
dump and load functions also accept file-like object
instead of filenames. More information on data persistence with Joblib is
available here.
InconsistentVersionWarning
Click for more details
¶
When an estimator is unpickled with a scikit-learn version that is inconsistent
with the version the estimator was pickled with, a
InconsistentVersionWarning is raised. This warning
can be caught to obtain the original version the estimator was pickled with:
from sklearn.exceptions import InconsistentVersionWarning
warnings.simplefilter("error", InconsistentVersionWarning)
try:
est = pickle.loads("model_from_prevision_version.pickle")
except InconsistentVersionWarning as w:
print(w.original_sklearn_version)
- 9.1.1. Security & maintainability limitations¶
pickle (and joblib by extension), has some issues regarding maintainability
and security. Because of this,
Never unpickle untrusted data as it could lead to malicious code being
executed upon loading.
While models saved using one version of scikit-learn might load in
other versions, this is entirely unsupported and inadvisable. It should
also be kept in mind that operations performed on such data could give
different and unexpected results.
In order to rebuild a similar model with future versions of scikit-learn,
additional metadata should be saved along the pickled model:
The training data, e.g. a reference to an immutable snapshot
The python source code used to generate the model
The versions of scikit-learn and its dependencies
The cross validation score obtained on the training data
This should make it possible to check that the cross-validation score is in the
same range as before.
Aside for a few exceptions, pickled models should be portable across
architectures assuming the same versions of dependencies and Python are used.
If you encounter an estimator that is not portable please open an issue on
GitHub. Pickled models are often deployed in production using containers, like
Docker, in order to freeze the environment and dependencies.
If you want to know more about these issues and explore other possible
serialization methods, please refer to this
talk by Alex Gaynor.
- 9.1.2. A more secure format: skops¶
skops provides a more secure
format via the skops.io module. It avoids using pickle and only
loads files which have types and references to functions which are trusted
either by default or by the user.
Using skops
Click for more details
¶
The API is very similar to pickle, and
you can persist your models as explain in the docs using
skops.io.dump and skops.io.dumps:
import skops.io as sio
obj = sio.dumps(clf)
And you can load them back using skops.io.load and
skops.io.loads. However, you need to specify the types which are
trusted by you. You can get existing unknown types in a dumped object / file
using skops.io.get_untrusted_types, and after checking its contents,
pass it to the load function:
unknown_types = sio.get_untrusted_types(data=obj)
clf = sio.loads(obj, trusted=unknown_types)
If you trust the source of the file / object, you can pass trusted=True:
clf = sio.loads(obj, trusted=True)
Please report issues and feature requests related to this format on the skops
issue tracker.
### 9.2. Interoperable formats¶

For reproducibility and quality control needs, when different architectures
and environments should be taken into account, exporting the model in
Open Neural Network
Exchange format or Predictive Model Markup Language
(PMML) format
might be a better approach than using pickle alone.
These are helpful where you may want to use your model for prediction in a
different environment from where the model was trained.
ONNX is a binary serialization of the model. It has been developed to improve
the usability of the interoperable representation of data models.
It aims to facilitate the conversion of the data
models between different machine learning frameworks, and to improve their
portability on different computing architectures. More details are available
from the ONNX tutorial.
To convert scikit-learn model to ONNX a specific tool sklearn-onnx has been developed.
PMML is an implementation of the XML document standard
defined to represent data models together with the data used to generate them.
Being human and machine readable,
PMML is a good option for model validation on different platforms and
long term archiving. On the other hand, as XML in general, its verbosity does
not help in production when performance is critical.
To convert scikit-learn model to PMML you can use for example sklearn2pmml distributed under the Affero GPLv3
license.
## 1. Metadata Routing — scikit-learn 1.4.2 documentation

Install
User Guide
API
Examples
Community
Getting Started
Tutorial
What's new
Glossary
Development
FAQ
Support
Related packages
Roadmap
Governance
About us
GitHub
Other Versions and Download
More
Getting Started
Tutorial
What's new
Glossary
Development
FAQ
Support
Related packages
Roadmap
Governance
About us
GitHub
Other Versions and Download
Toggle Menu
PrevUp
Next
scikit-learn 1.4.2
Other versions
Please cite us if you use the software.
## 1. Metadata Routing

### 1.1. Usage Examples

- 1.1.1. Weighted scoring and fitting
- 1.1.2. Weighted scoring and unweighted fitting
- 1.1.3. Unweighted feature selection
- 1.1.4. Advanced: Different scoring and fitting weights
### 1.2. API Interface

### 1.3. Metadata Routing Support Status

## 1. Metadata Routing¶

Note
The Metadata Routing API is experimental, and is not implemented yet for many
estimators. Please refer to the list of supported and unsupported
models for more information. It may change without
the usual deprecation cycle. By default this feature is not enabled. You can
enable this feature  by setting the enable_metadata_routing flag to
True:
>>> import sklearn
>>> sklearn.set_config(enable_metadata_routing=True)
This guide demonstrates how metadata such as sample_weight can be routed
and passed along to estimators, scorers, and CV splitters through
meta-estimators such as Pipeline and
GridSearchCV. In order to pass metadata to a method
such as fit or score, the object consuming the metadata, must request
it. For estimators and splitters, this is done via set_*_request methods,
e.g. set_fit_request(...), and for scorers this is done via the
set_score_request method. For grouped splitters such as
GroupKFold, a groups parameter is requested by
default. This is best demonstrated by the following examples.
If you are developing a scikit-learn compatible estimator or meta-estimator,
you can check our related developer guide:
Metadata Routing.
Note
Note that the methods and requirements introduced in this document are only
relevant if you want to pass metadata (e.g. sample_weight) to a method.
If you’re only passing X and y and no other parameter / metadata to
methods such as fit, transform, etc, then you don’t need to set
anything.
### 1.1. Usage Examples¶

Here we present a few examples to show different common use-cases. The examples
in this section require the following imports and data:
>>> import numpy as np
>>> from sklearn.metrics import make_scorer, accuracy_score
>>> from sklearn.linear_model import LogisticRegressionCV, LogisticRegression
>>> from sklearn.model_selection import cross_validate, GridSearchCV, GroupKFold
>>> from sklearn.feature_selection import SelectKBest
>>> from sklearn.pipeline import make_pipeline
>>> n_samples, n_features = 100, 4
>>> rng = np.random.RandomState(42)
>>> X = rng.rand(n_samples, n_features)
>>> y = rng.randint(0, 2, size=n_samples)
>>> my_groups = rng.randint(0, 10, size=n_samples)
>>> my_weights = rng.rand(n_samples)
>>> my_other_weights = rng.rand(n_samples)
- 1.1.1. Weighted scoring and fitting¶
Here GroupKFold requests groups by default. However, we
need to explicitly request weights for our scorer and the internal cross validation of
LogisticRegressionCV. Both of these consumers know how to use
metadata called sample_weight:
>>> weighted_acc = make_scorer(accuracy_score).set_score_request(
...     sample_weight=True
... )
>>> lr = LogisticRegressionCV(
...     cv=GroupKFold(), scoring=weighted_acc,
... ).set_fit_request(sample_weight=True)
>>> cv_results = cross_validate(
...     lr,
...     X,
...     y,
...     params={"sample_weight": my_weights, "groups": my_groups},
...     cv=GroupKFold(),
...     scoring=weighted_acc,
... )
Note that in this example, my_weights is passed to both the scorer and
LogisticRegressionCV.
Error handling: if params={"sample_weigh": my_weights, ...} were passed
(note the typo), cross_validate would raise an error,
since sample_weigh was not requested by any of its underlying objects.
- 1.1.2. Weighted scoring and unweighted fitting¶
When passing metadata such as sample_weight around, all sample_weight
consumers require weights to be either explicitly requested
or not requested (i.e. True or False) when used in another
router such as a Pipeline or a *GridSearchCV. To
perform an unweighted fit, we need to configure
LogisticRegressionCV to not request sample weights, so
that cross_validate does not pass the weights along:
>>> weighted_acc = make_scorer(accuracy_score).set_score_request(
...     sample_weight=True
... )
>>> lr = LogisticRegressionCV(
...     cv=GroupKFold(), scoring=weighted_acc,
... ).set_fit_request(sample_weight=False)
>>> cv_results = cross_validate(
...     lr,
...     X,
...     y,
...     cv=GroupKFold(),
...     params={"sample_weight": my_weights, "groups": my_groups},
...     scoring=weighted_acc,
... )
If linear_model.LogisticRegressionCV.set_fit_request has not
been called, cross_validate will raise an
error because sample_weight is passed in but
LogisticRegressionCV would not be explicitly configured
to recognize the weights.
- 1.1.3. Unweighted feature selection¶
Setting request values for metadata are only required if the object, e.g. estimator,
scorer, etc., is a consumer of that metadata Unlike
LogisticRegressionCV, SelectKBest
doesn’t consume weights and therefore no request value for sample_weight on its
instance is set and sample_weight is not routed to it:
>>> weighted_acc = make_scorer(accuracy_score).set_score_request(
...     sample_weight=True
... )
>>> lr = LogisticRegressionCV(
...     cv=GroupKFold(), scoring=weighted_acc,
... ).set_fit_request(sample_weight=True)
>>> sel = SelectKBest(k=2)
>>> pipe = make_pipeline(sel, lr)
>>> cv_results = cross_validate(
...     pipe,
...     X,
...     y,
...     cv=GroupKFold(),
...     params={"sample_weight": my_weights, "groups": my_groups},
...     scoring=weighted_acc,
... )
- 1.1.4. Advanced: Different scoring and fitting weights¶
Despite make_scorer and
LogisticRegressionCV both expecting the key
sample_weight, we can use aliases to pass different weights to different
consumers. In this example, we pass scoring_weight to the scorer, and
fitting_weight to LogisticRegressionCV:
>>> weighted_acc = make_scorer(accuracy_score).set_score_request(
...    sample_weight="scoring_weight"
... )
>>> lr = LogisticRegressionCV(
...     cv=GroupKFold(), scoring=weighted_acc,
... ).set_fit_request(sample_weight="fitting_weight")
>>> cv_results = cross_validate(
...     lr,
...     X,
...     y,
...     cv=GroupKFold(),
...     params={
...         "scoring_weight": my_weights,
...         "fitting_weight": my_other_weights,
...         "groups": my_groups,
...     },
...     scoring=weighted_acc,
... )
### 1.2. API Interface¶

A consumer is an object (estimator, meta-estimator, scorer, splitter)
which accepts and uses some metadata in at least one of its methods
(fit, predict, inverse_transform, transform, score,
split). Meta-estimators which only forward the metadata to other objects
(the child estimator, scorers, or splitters) and don’t use the metadata
themselves are not consumers. (Meta-)Estimators which route metadata to other
objects are routers. A(n) (meta-)estimator can be a
consumer and a router at the same time. (Meta-)Estimators and
splitters expose a set_*_request method for each method which accepts at
least one metadata. For instance, if an estimator supports sample_weight in
fit and score, it exposes
estimator.set_fit_request(sample_weight=value) and
estimator.set_score_request(sample_weight=value). Here value can be:
True: method requests a sample_weight. This means if the metadata is
provided, it will be used, otherwise no error is raised.
False: method does not request a sample_weight.
None: router will raise an error if sample_weight is passed. This is
in almost all cases the default value when an object is instantiated and
ensures the user sets the metadata requests explicitly when a metadata is
passed. The only exception are Group*Fold splitters.
"param_name": if this estimator is used in a meta-estimator, the
meta-estimator should forward "param_name" as sample_weight to this
estimator. This means the mapping between the metadata required by the
object, e.g. sample_weight and what is provided by the user, e.g.
my_weights is done at the router level, and not by the object, e.g.
estimator, itself.
Metadata are requested in the same way for scorers using set_score_request.
If a metadata, e.g. sample_weight, is passed by the user, the metadata
request for all objects which potentially can consume sample_weight should
be set by the user, otherwise an error is raised by the router object. For
example, the following code raises an error, since it hasn’t been explicitly
specified whether sample_weight should be passed to the estimator’s scorer
or not:
>>> param_grid = {"C": [0.1, 1]}
>>> lr = LogisticRegression().set_fit_request(sample_weight=True)
>>> try:
...     GridSearchCV(
...         estimator=lr, param_grid=param_grid
...     ).fit(X, y, sample_weight=my_weights)
... except ValueError as e:
...     print(e)
[sample_weight] are passed but are not explicitly set as requested or not for
LogisticRegression.score
The issue can be fixed by explicitly setting the request value:
>>> lr = LogisticRegression().set_fit_request(
...     sample_weight=True
... ).set_score_request(sample_weight=False)
At the end we disable the configuration flag for metadata routing:
>>> sklearn.set_config(enable_metadata_routing=False)
### 1.3. Metadata Routing Support Status¶

All consumers (i.e. simple estimators which only consume metadata and don’t
route them) support metadata routing, meaning they can be used inside
meta-estimators which support metadata routing. However, development of support
for metadata routing for meta-estimators is in progress, and here is a list of
meta-estimators and tools which support and don’t yet support metadata routing.
Meta-estimators and functions supporting metadata routing:
sklearn.calibration.CalibratedClassifierCV
sklearn.compose.ColumnTransformer
sklearn.feature_selection.SelectFromModel
sklearn.linear_model.ElasticNetCV
sklearn.linear_model.LarsCV
sklearn.linear_model.LassoCV
sklearn.linear_model.LassoLarsCV
sklearn.linear_model.LogisticRegressionCV
sklearn.linear_model.MultiTaskElasticNetCV
sklearn.linear_model.MultiTaskLassoCV
sklearn.model_selection.GridSearchCV
sklearn.model_selection.HalvingGridSearchCV
sklearn.model_selection.HalvingRandomSearchCV
sklearn.model_selection.RandomizedSearchCV
sklearn.model_selection.cross_validate
sklearn.model_selection.cross_val_score
sklearn.model_selection.cross_val_predict
sklearn.multiclass.OneVsOneClassifier
sklearn.multiclass.OneVsRestClassifier
sklearn.multiclass.OutputCodeClassifier
sklearn.multioutput.ClassifierChain
sklearn.multioutput.MultiOutputClassifier
sklearn.multioutput.MultiOutputRegressor
sklearn.linear_model.OrthogonalMatchingPursuitCV
sklearn.multioutput.RegressorChain
sklearn.pipeline.Pipeline
Meta-estimators and tools not supporting metadata routing yet:
sklearn.compose.TransformedTargetRegressor
sklearn.covariance.GraphicalLassoCV
sklearn.ensemble.AdaBoostClassifier
sklearn.ensemble.AdaBoostRegressor
sklearn.ensemble.BaggingClassifier
sklearn.ensemble.BaggingRegressor
sklearn.ensemble.StackingClassifier
sklearn.ensemble.StackingRegressor
sklearn.ensemble.VotingClassifier
sklearn.ensemble.VotingRegressor
sklearn.feature_selection.RFE
sklearn.feature_selection.RFECV
sklearn.feature_selection.SequentialFeatureSelector
sklearn.impute.IterativeImputer
sklearn.linear_model.RANSACRegressor
sklearn.linear_model.RidgeClassifierCV
sklearn.linear_model.RidgeCV
sklearn.model_selection.learning_curve
sklearn.model_selection.permutation_test_score
sklearn.model_selection.validation_curve
sklearn.pipeline.FeatureUnion
sklearn.semi_supervised.SelfTrainingClassifier
© 2007 - 2024, scikit-learn developers (BSD License).
Show this page source
